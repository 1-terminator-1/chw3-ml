{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b46fe41",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Introduction to Machine Learning - Course Code: 25737</h1>\n",
    "<h4 align=\"center\">Instructor: Dr. Amiri</h4>\n",
    "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
    "<h4 align=\"center\">Computer Assignment 3</h4>\n",
    "<h4 align=\"center\">\n",
    "\n",
    "Question 1\n",
    "\n",
    "</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a0fc13",
   "metadata": {
    "id": "a6c2fd6d"
   },
   "source": [
    "# Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44babb65",
   "metadata": {
    "id": "4e90a030"
   },
   "outputs": [],
   "source": [
    "# Set your student number\n",
    "student_number = 400101872\n",
    "Name = 'Matin'\n",
    "Last_Name = 'Mohamadghasemi'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a337a",
   "metadata": {
    "id": "339da203"
   },
   "source": [
    "# Rules\n",
    "- You are not allowed to add or remove cells. You **must use the provided space to write your code**. If you don't follow this rule, **your Practical Assignment won't be graded**.  \n",
    "\n",
    "- Collaboration and using the internet is allowed, but your code **must be written by yourself**. **Copying code** from each other or from available resources will result in a **zero score for the assignment**.\n",
    "\n",
    "- You are not allowed to use `torch.nn`, `torch.optim` and any activation function and loss function implemented in torch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b76789",
   "metadata": {
    "id": "881f2e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/armin/anaconda3/lib/python3.11/site-packages (1.24.3)\n",
      "Requirement already satisfied: matplotlib in /Users/armin/anaconda3/lib/python3.11/site-packages (3.7.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/armin/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/armin/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/armin/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/armin/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/armin/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/armin/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/armin/anaconda3/lib/python3.11/site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/armin/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/armin/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/armin/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: torchvision in /Users/armin/anaconda3/lib/python3.11/site-packages (0.16.0)\n",
      "Requirement already satisfied: numpy in /Users/armin/anaconda3/lib/python3.11/site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in /Users/armin/anaconda3/lib/python3.11/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch in /Users/armin/anaconda3/lib/python3.11/site-packages (from torchvision) (2.1.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/armin/anaconda3/lib/python3.11/site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/armin/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/armin/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/armin/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/armin/anaconda3/lib/python3.11/site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: filelock in /Users/armin/anaconda3/lib/python3.11/site-packages (from torch->torchvision) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/armin/anaconda3/lib/python3.11/site-packages (from torch->torchvision) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/armin/anaconda3/lib/python3.11/site-packages (from torch->torchvision) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/armin/anaconda3/lib/python3.11/site-packages (from torch->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/armin/anaconda3/lib/python3.11/site-packages (from torch->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/armin/anaconda3/lib/python3.11/site-packages (from torch->torchvision) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/armin/anaconda3/lib/python3.11/site-packages (from jinja2->torch->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/armin/anaconda3/lib/python3.11/site-packages (from sympy->torch->torchvision) (1.3.0)\n",
      "Requirement already satisfied: torch in /Users/armin/anaconda3/lib/python3.11/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /Users/armin/anaconda3/lib/python3.11/site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/armin/anaconda3/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/armin/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/armin/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/armin/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/armin/anaconda3/lib/python3.11/site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/armin/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/armin/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install torchvision\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886188c7",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a0adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18510868",
   "metadata": {},
   "source": [
    "## Datasets and Dataloaders\n",
    "\n",
    "Here, we download and load the train and test `FashionMNIST` dataset with the desired transforms. Then, we define the dataloaders for `train` and `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc8759e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = FashionMNIST(root='.', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_set = FashionMNIST(root='.', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df47fcb",
   "metadata": {},
   "source": [
    "\n",
    "Here you have to calculate the number of classes amd input dimention of the first layer (how many pixels does each image have?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f6763e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10\n",
      "Input dimension of the first layer: 784 pixels\n"
     ]
    }
   ],
   "source": [
    "## FILL HERE\n",
    "# Assuming train_set is already defined and is a dataset object\n",
    "num_classes = len(train_set.classes)\n",
    "input_dim = train_set[0][0].numel()  # Number of elements (pixels) in the first image of the dataset\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Input dimension of the first layer: {input_dim} pixels\")\n",
    "\n",
    "\n",
    "\n",
    "##Here’s a more complete example with a PyTorch dataset:\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Example transform (if needed)\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the dataset (replace with your dataset and path)\n",
    "train_set = datasets.FakeData(transform=transform)\n",
    "\n",
    "# Calculate the number of classes and input dimension\n",
    "num_classes = len(train_set.classes)\n",
    "input_dim = train_set[0][0].numel()  # Assuming the dataset returns (image, label) tuples\n",
    "\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Input dimension of the first layer: {input_dim} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c695ff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, 64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dac6c2",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Visualize 1 random image from each class by using `plt.subplots`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3d6b0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def visualize_random_images_from_each_class(dataset, num_classes):\n",
    "    class_indices = {i: [] for i in range(num_classes)}\n",
    "    \n",
    "    # Collect indices for each class\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        if len(class_indices[label]) < 1:\n",
    "            class_indices[label].append(idx)\n",
    "        if all(len(class_indices[c]) > 0 for c in range(num_classes)):\n",
    "            break\n",
    "    \n",
    "    fig, axs = plt.subplots(1, num_classes, figsize=(15, 15))\n",
    "    for class_label, indices in class_indices.items():\n",
    "        idx = indices[0]\n",
    "        image, label = dataset[idx]\n",
    "        if len(image.shape) == 3:  # Check if image is colored (C, H, W)\n",
    "            image = image.permute(1, 2, 0)  # Change to (H, W, C) format\n",
    "        axs[class_label].imshow(image.squeeze(), cmap='gray' if image.shape[-1] == 1 else None)\n",
    "        axs[class_label].set_title(f'Class: {class_label}')\n",
    "        axs[class_label].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage (assuming train_set is already defined and num_classes is known)\n",
    "visualize_random_images_from_each_class(train_set, num_classes)\n",
    "\n",
    "##Here’s a more complete example with a PyTorch dataset:\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def visualize_random_images_from_each_class(dataset, num_classes, num_samples=1):\n",
    "    # Shuffle the dataset indices\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    class_indices = {i: [] for i in range(num_classes)}\n",
    "    \n",
    "    # Collect indices for each class\n",
    "    for idx in indices:\n",
    "        _, label = dataset[idx]\n",
    "        if len(class_indices[label]) < num_samples:\n",
    "            class_indices[label].append(idx)\n",
    "        if all(len(class_indices[c]) >= num_samples for c in range(num_classes)):\n",
    "            break\n",
    "\n",
    "    fig, axs = plt.subplots(num_samples, num_classes, figsize=(15, 15))\n",
    "    \n",
    "    # Ensure axs is a 2D array even if num_samples is 1\n",
    "    if num_samples == 1:\n",
    "        axs = [axs]\n",
    "    \n",
    "    for class_label, indices in class_indices.items():\n",
    "        for sample_idx, idx in enumerate(indices):\n",
    "            image, label = dataset[idx]\n",
    "            axs[sample_idx][class_label].imshow(image.permute(1, 2, 0).squeeze(), cmap='gray')\n",
    "            axs[sample_idx][class_label].set_title(f'Class: {class_label}')\n",
    "            axs[sample_idx][class_label].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage (assuming train_set is already defined and num_classes is known)\n",
    "visualize_random_images_from_each_class(train_set, num_classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94c5aba",
   "metadata": {},
   "source": [
    "## Initializing model's parameters\n",
    "\n",
    "In this part, we create the model and initialize its parameters and store the values of these parameters in the variable `parameters` which is a dictionary including the weigths and biases of each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6d40952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_linear_layer(parameters: dict, shape, device, i=None):\n",
    "    \"\"\"\n",
    "    This function adds parameters of a linear unit of shape `shape` to the `parameters` dictionary.\n",
    "    \"\"\"\n",
    "    n_in, n_out = shape\n",
    "    with torch.no_grad():\n",
    "        w = torch.zeros(*shape, device=device)\n",
    "        # kaiming initialization for ReLU activations:\n",
    "        bound = 1 / np.sqrt(n_in).item()\n",
    "        w.uniform_(-bound, bound)\n",
    "        b = torch.zeros(n_out, device=device)  # no need to (1, n_out). it will broadcast itself.\n",
    "    w.requires_grad = True\n",
    "    b.requires_grad = True\n",
    "    # `i` is used to give numbers to parameter names\n",
    "    parameters.update({f'w{i}': w, f'b{i}': b})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce914706",
   "metadata": {},
   "source": [
    "Now we define our neural network with the given layers and add the weights and biases to the dictionary `parameters`. **You are allowed to modify the values of the layers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f3867d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['w0', 'b0', 'w1', 'b1', 'w2', 'b2', 'w3', 'b3', 'w4', 'b4'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_dim : input dimention of the first layer, which you have calculated before.\n",
    "layers = [\n",
    "    (input_dim, 512),\n",
    "    (512, 256),\n",
    "    (256, 128),\n",
    "    (128, 64),\n",
    "    (64, num_classes)\n",
    "]\n",
    "num_layers = len(layers)\n",
    "parameters = {}\n",
    "\n",
    "# setting the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# adding the parameters to the dictionary\n",
    "for i, shape in enumerate(layers):\n",
    "    add_linear_layer(parameters, shape, device, i)\n",
    "\n",
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd2c8e",
   "metadata": {},
   "source": [
    "## Defining the required functions\n",
    "\n",
    "In this section, we should define the required functions. For each of these functions, the inputs and the desired outputs are given and you should write all or part of the function. **You are not allowed to use the activation functions and the loss functions implemented in torch**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b413d8",
   "metadata": {},
   "source": [
    "Computing affine and relu outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bebeeb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Affine forward function\n",
    "def affine_forward(x, w, b):\n",
    "    return x @ w + b    \n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return torch.maximum(x, torch.tensor(0.0, device=x.device))\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(5, 10)  # Input tensor with shape (batch_size, input_dim)\n",
    "w = torch.randn(10, 3)  # Weight matrix with shape (input_dim, output_dim)\n",
    "b = torch.randn(3)      # Bias vector with shape (output_dim)\n",
    "\n",
    "# Perform affine forward operation\n",
    "affine_output = affine_forward(x, w, b)\n",
    "print(\"Affine forward output:\")\n",
    "print(affine_output)\n",
    "\n",
    "# Apply ReLU activation\n",
    "relu_output = relu(affine_output)\n",
    "print(\"ReLU activation output:\")\n",
    "print(relu_output)\n",
    "\n",
    "# Visualization function for random images from each class\n",
    "def visualize_random_images_from_each_class(dataset, num_classes):\n",
    "    class_indices = {i: [] for i in range(num_classes)}\n",
    "    \n",
    "    # Collect indices for each class\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        if len(class_indices[label]) < 1:\n",
    "            class_indices[label].append(idx)\n",
    "        if all(len(class_indices[c]) > 0 for c in range(num_classes)):\n",
    "            break\n",
    "    \n",
    "    fig, axs = plt.subplots(1, num_classes, figsize=(15, 15))\n",
    "    for class_label, indices in class_indices.items():\n",
    "        idx = indices[0]\n",
    "        image, label = dataset[idx]\n",
    "        if len(image.shape) == 3:  # Check if image is colored (C, H, W)\n",
    "            image = image.permute(1, 2, 0)  # Change to (H, W, C) format\n",
    "        axs[class_label].imshow(image.squeeze(), cmap='gray' if image.shape[-1] == 1 else None)\n",
    "        axs[class_label].set_title(f'Class: {class_label}')\n",
    "        axs[class_label].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Example usage (assuming train_set is already defined and num_classes is known)\n",
    "# visualize_random_images_from_each_class(train_set, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9baa5e",
   "metadata": {},
   "source": [
    "Function `model` returns output of the whole model for the input `x` using the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2562962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Affine forward function\n",
    "def affine_forward(x, w, b):\n",
    "    return x @ w + b    \n",
    "\n",
    "# ReLU activation function\n",
    "def relu(x):\n",
    "    return torch.maximum(x, torch.tensor(0.0, device=x.device))\n",
    "\n",
    "# Model function\n",
    "def model(x: torch.Tensor, parameters, num_layers):\n",
    "    # number of batches\n",
    "    B = x.shape[0]\n",
    "    x = x.view(B, -1)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        w = parameters[f'w{i}']\n",
    "        b = parameters[f'b{i}']\n",
    "        x = affine_forward(x, w, b)\n",
    "        if i < num_layers - 1:\n",
    "            x = relu(x)\n",
    "    \n",
    "    output = x\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "# Define parameters for a 3-layer model\n",
    "input_dim = 20\n",
    "hidden_dim = 10\n",
    "output_dim = 5\n",
    "num_layers = 3\n",
    "\n",
    "parameters = {\n",
    "    'w0': torch.randn(input_dim, hidden_dim),\n",
    "    'b0': torch.randn(hidden_dim),\n",
    "    'w1': torch.randn(hidden_dim, hidden_dim),\n",
    "    'b1': torch.randn(hidden_dim),\n",
    "    'w2': torch.randn(hidden_dim, output_dim),\n",
    "    'b2': torch.randn(output_dim),\n",
    "}\n",
    "\n",
    "x = torch.randn(8, input_dim)  # Input tensor with shape (batch_size, input_dim)\n",
    "\n",
    "# Get the model output\n",
    "output = model(x, parameters, num_layers)\n",
    "print(\"Model output:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a9b4c",
   "metadata": {},
   "source": [
    "Implementing cross entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6959621c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy_loss(scores, y):\n",
    "    n = len(y)\n",
    "    \n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    log_probs = np.log(probs)\n",
    "    \n",
    "    correct_log_probs = log_probs[np.arange(n), y]\n",
    "    \n",
    "    loss = -np.mean(correct_log_probs)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / exps.sum(axis=1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a589af",
   "metadata": {},
   "source": [
    "Implementing a function for optimizing paramters and a function to zeroing out their gradients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3121c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def sgd_optimizer(parameters: Dict[str, torch.Tensor], learning_rate=0.001):\n",
    "    '''This function gets the parameters and a learning rate. Then updates the parameters using their\n",
    "    gradient. Finally, you should zero the gradients of the parameters after updating\n",
    "    the parameter value.'''\n",
    "    for param_name, param_tensor in parameters.items():\n",
    "        if param_tensor.grad is not None:\n",
    "            # Update the parameter using SGD\n",
    "            updated_param_tensor = param_tensor - learning_rate * param_tensor.grad\n",
    "            \n",
    "            # Update the parameter in the dictionary\n",
    "            parameters[param_name] = updated_param_tensor\n",
    "            \n",
    "            # Zero out the gradient\n",
    "            param_tensor.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17b4cf8",
   "metadata": {},
   "source": [
    "Training functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76c0f03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    import numpy as np\n",
    "\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    '''This function calculates the accuracy given the predicted labels and true labels.'''\n",
    "    correct_predictions = np.sum(y_pred == y_true)\n",
    "    acc = correct_predictions / len(y_true)\n",
    "    return acc\n",
    "\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def train(train_loader, learning_rate=0.001, epoch=None):\n",
    "    '''This function implements the training loop for a single epoch. For each batch you should do the following:\n",
    "        1- Calculate the output of the model to the given input batch\n",
    "        2- Calculate the loss based on the model output\n",
    "        3- Update the gradients using backward method\n",
    "        4- Optimize the model parameters using the sgd_optimizer function defined previously\n",
    "        5- Print the train loss (Show the epoch and batch as well)\n",
    "        '''\n",
    "    train_loss = 0\n",
    "    N_train = len(train_loader.dataset)\n",
    "    \n",
    "    # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n",
    "    # for calculateing the accuracy later\n",
    "    Y = []\n",
    "    Y_pred = []\n",
    "    \n",
    "    \n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        p = model(x, parameters)\n",
    "\n",
    "        loss = cross_entropy_loss(p, y)\n",
    "for param_name, param_tensor in parameters.items():\n",
    "    if param_tensor.grad is not None:\n",
    "        param_tensor.grad.zero_()\n",
    "        \n",
    "grads = {}  # Dictionary to store gradients\n",
    "loss.backward()\n",
    "for name, param in parameters.items():\n",
    "    grads[name] = param.grad.numpy()\n",
    "\n",
    "sgd_optimizer(parameters, grads, learning_rate)\n",
    "\n",
    "train_loss += loss\n",
    "\n",
    "print(f'Epoch {epoch}, Batch {i + 1}, Loss: {loss}')\n",
    "\n",
    "        y_pred = p.argmax(dim=-1)\n",
    "        Y.append(y.cpu().numpy())\n",
    "        Y_pred.append(y_pred.cpu().numpy())\n",
    "\n",
    "    Y = np.concatenate(Y)\n",
    "    Y_pred = np.concatenate(Y_pred)\n",
    "    acc = accuracy(Y_pred, Y)\n",
    "    print(f'Accuracy of train set: {acc}')\n",
    "    return train_loss, acc\n",
    "\n",
    "\n",
    "def validate(loader, epoch=None, set_name=None):\n",
    "    '''This function validates the model on the test dataloader. The function goes through each batch and does\n",
    "    the following on each batch:\n",
    "        1- Calculate the model output\n",
    "        2- Calculate the loss using the model output\n",
    "        3- Print the loss for each batch and epoch\n",
    "    \n",
    "    Finally the function calculates the model accuracy.'''\n",
    "    total_loss = 0\n",
    "    N = len(loader.dataset)\n",
    "    \n",
    "    # Creating empty lists Y and Y_pred to store the labels and predictions of each batch\n",
    "    # for calculateing the accuracy later\n",
    "    Y = []\n",
    "    Y_pred = []\n",
    "    for i, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        p = model(x, parameters)\n",
    "\n",
    "       # Assume p and y are your predictions and labels, and x is your input\n",
    "total_loss = 0\n",
    "for i, (x, y) in enumerate(train_loader):  # replace train_loader with your data loader\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    p = model(x)  # replace model(x) with your model's forward pass\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = custom_cross_entropy_loss(p, y)\n",
    "    total_loss += loss.item() * x.size(0)\n",
    "    print(f'Epoch {epoch}, Batch {i + 1}, Training Loss: {loss.item()}')\n",
    "            \n",
    "        \n",
    "        y_pred = p.argmax(dim=-1)\n",
    "        Y.append(y.cpu().numpy())\n",
    "        Y_pred.append(y_pred.cpu().numpy())\n",
    "    Y = np.concatenate(Y)\n",
    "    Y_pred = np.concatenate(Y_pred)\n",
    "    total_loss /= N\n",
    "    acc = accuracy(Y_pred, Y)\n",
    "    print(f'Accuracy of {set_name} set: {acc}')\n",
    "\n",
    "    return total_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87ebb4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28d4eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataloaders, num_epochs, learning_rate=0.001, model_name='pytorch_model'):\n",
    "    '''This function trains the model for the number of epochs given and stores, calculates and prints the train\n",
    "    and test losses and accuracies. Finally, it plots the accuracy and loss history for training and test sets'''\n",
    "    train_loader, test_loader = dataloaders\n",
    "\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    train_loss, train_acc = train(train_loader, learning_rate, epoch)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    # Validation Phase\n",
    "    test_loss, test_acc = validate(test_loader, epoch, set_name=\"Test\")\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "# Plot loss and accuracy history\n",
    "plot_loss_and_accuracy(train_losses, train_accuracies, test_losses, test_accuracies, num_epochs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   def train(train_loader, learning_rate, epoch):\n",
    "    # Training Phase\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # Forward pass (replace with your own training function)\n",
    "        output = model(data)\n",
    "        loss = compute_loss(output, target)\n",
    "        \n",
    "        # Backward pass (replace with your own optimizer)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        \n",
    "        _, predicted = output.max(1)\n",
    "        total_train += target.size(0)\n",
    "        correct_train += predicted.eq(target).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = correct_train / total_train\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def validate(test_loader, epoch, set_name=\"Test\"):\n",
    "    # Validation Phase\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Forward pass (replace with your own model)\n",
    "            output = model(data)\n",
    "            loss = compute_loss(output, target)\n",
    "            \n",
    "            test_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            _, predicted = output.max(1)\n",
    "            total_test += target.size(0)\n",
    "            correct_test += predicted.eq(target).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_acc = correct_test / total_test\n",
    "    print(f'Epoch {epoch}, {set_name} Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}')\n",
    "    return test_loss, test_acc\n",
    "\n",
    "def plot_loss_and_accuracy(train_losses, train_accuracies, test_losses, test_accuracies, num_epochs):\n",
    "    # Plot the loss history of training and test sets\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss History')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot the accuracy history of training and test sets\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(range(1, num_epochs + 1), test_accuracies, label='Test Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy History')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_losses = []\n",
    "test_accuracies = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ec4bdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 1, Loss: 2.25996470451355\n",
      "Epoch 0, Batch 2, Loss: 2.2585628032684326\n",
      "Epoch 0, Batch 3, Loss: 2.2628915309906006\n",
      "Epoch 0, Batch 4, Loss: 2.264178991317749\n",
      "Epoch 0, Batch 5, Loss: 2.268101692199707\n",
      "Epoch 0, Batch 6, Loss: 2.2619411945343018\n",
      "Epoch 0, Batch 7, Loss: 2.2634546756744385\n",
      "Epoch 0, Batch 8, Loss: 2.264596700668335\n",
      "Epoch 0, Batch 9, Loss: 2.2584939002990723\n",
      "Epoch 0, Batch 10, Loss: 2.267008066177368\n",
      "Epoch 0, Batch 11, Loss: 2.2628018856048584\n",
      "Epoch 0, Batch 12, Loss: 2.2640380859375\n",
      "Epoch 0, Batch 13, Loss: 2.263481855392456\n",
      "Epoch 0, Batch 14, Loss: 2.2557735443115234\n",
      "Epoch 0, Batch 15, Loss: 2.2688722610473633\n",
      "Epoch 0, Batch 16, Loss: 2.2684621810913086\n",
      "Epoch 0, Batch 17, Loss: 2.2600605487823486\n",
      "Epoch 0, Batch 18, Loss: 2.2639973163604736\n",
      "Epoch 0, Batch 19, Loss: 2.259251594543457\n",
      "Epoch 0, Batch 20, Loss: 2.2618441581726074\n",
      "Epoch 0, Batch 21, Loss: 2.2625575065612793\n",
      "Epoch 0, Batch 22, Loss: 2.2621259689331055\n",
      "Epoch 0, Batch 23, Loss: 2.256052017211914\n",
      "Epoch 0, Batch 24, Loss: 2.2650527954101562\n",
      "Epoch 0, Batch 25, Loss: 2.2630162239074707\n",
      "Epoch 0, Batch 26, Loss: 2.2642157077789307\n",
      "Epoch 0, Batch 27, Loss: 2.2580442428588867\n",
      "Epoch 0, Batch 28, Loss: 2.258901834487915\n",
      "Epoch 0, Batch 29, Loss: 2.25779390335083\n",
      "Epoch 0, Batch 30, Loss: 2.264765501022339\n",
      "Epoch 0, Batch 31, Loss: 2.2588555812835693\n",
      "Epoch 0, Batch 32, Loss: 2.259570837020874\n",
      "Epoch 0, Batch 33, Loss: 2.252528429031372\n",
      "Epoch 0, Batch 34, Loss: 2.264699697494507\n",
      "Epoch 0, Batch 35, Loss: 2.256953477859497\n",
      "Epoch 0, Batch 36, Loss: 2.2621147632598877\n",
      "Epoch 0, Batch 37, Loss: 2.2597992420196533\n",
      "Epoch 0, Batch 38, Loss: 2.2630910873413086\n",
      "Epoch 0, Batch 39, Loss: 2.2621359825134277\n",
      "Epoch 0, Batch 40, Loss: 2.260305881500244\n",
      "Epoch 0, Batch 41, Loss: 2.2615573406219482\n",
      "Epoch 0, Batch 42, Loss: 2.260049819946289\n",
      "Epoch 0, Batch 43, Loss: 2.2619431018829346\n",
      "Epoch 0, Batch 44, Loss: 2.2654519081115723\n",
      "Epoch 0, Batch 45, Loss: 2.261777639389038\n",
      "Epoch 0, Batch 46, Loss: 2.259578227996826\n",
      "Epoch 0, Batch 47, Loss: 2.2585582733154297\n",
      "Epoch 0, Batch 48, Loss: 2.2607762813568115\n",
      "Epoch 0, Batch 49, Loss: 2.2604446411132812\n",
      "Epoch 0, Batch 50, Loss: 2.263359785079956\n",
      "Epoch 0, Batch 51, Loss: 2.257129430770874\n",
      "Epoch 0, Batch 52, Loss: 2.2572250366210938\n",
      "Epoch 0, Batch 53, Loss: 2.2510666847229004\n",
      "Epoch 0, Batch 54, Loss: 2.2507152557373047\n",
      "Epoch 0, Batch 55, Loss: 2.2563977241516113\n",
      "Epoch 0, Batch 56, Loss: 2.254995107650757\n",
      "Epoch 0, Batch 57, Loss: 2.255751132965088\n",
      "Epoch 0, Batch 58, Loss: 2.2565927505493164\n",
      "Epoch 0, Batch 59, Loss: 2.252856492996216\n",
      "Epoch 0, Batch 60, Loss: 2.257490634918213\n",
      "Epoch 0, Batch 61, Loss: 2.2626333236694336\n",
      "Epoch 0, Batch 62, Loss: 2.262073040008545\n",
      "Epoch 0, Batch 63, Loss: 2.2617242336273193\n",
      "Epoch 0, Batch 64, Loss: 2.2484896183013916\n",
      "Epoch 0, Batch 65, Loss: 2.24920654296875\n",
      "Epoch 0, Batch 66, Loss: 2.257966995239258\n",
      "Epoch 0, Batch 67, Loss: 2.2557625770568848\n",
      "Epoch 0, Batch 68, Loss: 2.2553436756134033\n",
      "Epoch 0, Batch 69, Loss: 2.2464725971221924\n",
      "Epoch 0, Batch 70, Loss: 2.251157760620117\n",
      "Epoch 0, Batch 71, Loss: 2.259799003601074\n",
      "Epoch 0, Batch 72, Loss: 2.2571845054626465\n",
      "Epoch 0, Batch 73, Loss: 2.2653400897979736\n",
      "Epoch 0, Batch 74, Loss: 2.256268262863159\n",
      "Epoch 0, Batch 75, Loss: 2.2548646926879883\n",
      "Epoch 0, Batch 76, Loss: 2.2566497325897217\n",
      "Epoch 0, Batch 77, Loss: 2.2603278160095215\n",
      "Epoch 0, Batch 78, Loss: 2.2541210651397705\n",
      "Epoch 0, Batch 79, Loss: 2.253981113433838\n",
      "Epoch 0, Batch 80, Loss: 2.249127149581909\n",
      "Epoch 0, Batch 81, Loss: 2.2569286823272705\n",
      "Epoch 0, Batch 82, Loss: 2.2535860538482666\n",
      "Epoch 0, Batch 83, Loss: 2.2536613941192627\n",
      "Epoch 0, Batch 84, Loss: 2.2574923038482666\n",
      "Epoch 0, Batch 85, Loss: 2.257699966430664\n",
      "Epoch 0, Batch 86, Loss: 2.2584261894226074\n",
      "Epoch 0, Batch 87, Loss: 2.251444101333618\n",
      "Epoch 0, Batch 88, Loss: 2.241203546524048\n",
      "Epoch 0, Batch 89, Loss: 2.2534778118133545\n",
      "Epoch 0, Batch 90, Loss: 2.2575297355651855\n",
      "Epoch 0, Batch 91, Loss: 2.250645160675049\n",
      "Epoch 0, Batch 92, Loss: 2.250717878341675\n",
      "Epoch 0, Batch 93, Loss: 2.252556324005127\n",
      "Epoch 0, Batch 94, Loss: 2.2589519023895264\n",
      "Epoch 0, Batch 95, Loss: 2.2522189617156982\n",
      "Epoch 0, Batch 96, Loss: 2.2530200481414795\n",
      "Epoch 0, Batch 97, Loss: 2.2512590885162354\n",
      "Epoch 0, Batch 98, Loss: 2.2445709705352783\n",
      "Epoch 0, Batch 99, Loss: 2.2495083808898926\n",
      "Epoch 0, Batch 100, Loss: 2.2527167797088623\n",
      "Epoch 0, Batch 101, Loss: 2.25470232963562\n",
      "Epoch 0, Batch 102, Loss: 2.250678062438965\n",
      "Epoch 0, Batch 103, Loss: 2.253518581390381\n",
      "Epoch 0, Batch 104, Loss: 2.2544500827789307\n",
      "Epoch 0, Batch 105, Loss: 2.2535910606384277\n",
      "Epoch 0, Batch 106, Loss: 2.2523279190063477\n",
      "Epoch 0, Batch 107, Loss: 2.2444562911987305\n",
      "Epoch 0, Batch 108, Loss: 2.2600483894348145\n",
      "Epoch 0, Batch 109, Loss: 2.2499866485595703\n",
      "Epoch 0, Batch 110, Loss: 2.2548394203186035\n",
      "Epoch 0, Batch 111, Loss: 2.2563233375549316\n",
      "Epoch 0, Batch 112, Loss: 2.2517545223236084\n",
      "Epoch 0, Batch 113, Loss: 2.25295352935791\n",
      "Epoch 0, Batch 114, Loss: 2.250048875808716\n",
      "Epoch 0, Batch 115, Loss: 2.2508018016815186\n",
      "Epoch 0, Batch 116, Loss: 2.2471225261688232\n",
      "Epoch 0, Batch 117, Loss: 2.256385087966919\n",
      "Epoch 0, Batch 118, Loss: 2.250523328781128\n",
      "Epoch 0, Batch 119, Loss: 2.24855637550354\n",
      "Epoch 0, Batch 120, Loss: 2.250173807144165\n",
      "Epoch 0, Batch 121, Loss: 2.2480056285858154\n",
      "Epoch 0, Batch 122, Loss: 2.2530441284179688\n",
      "Epoch 0, Batch 123, Loss: 2.2479777336120605\n",
      "Epoch 0, Batch 124, Loss: 2.2484686374664307\n",
      "Epoch 0, Batch 125, Loss: 2.243435859680176\n",
      "Epoch 0, Batch 126, Loss: 2.2585248947143555\n",
      "Epoch 0, Batch 127, Loss: 2.252103328704834\n",
      "Epoch 0, Batch 128, Loss: 2.2444562911987305\n",
      "Epoch 0, Batch 129, Loss: 2.2482094764709473\n",
      "Epoch 0, Batch 130, Loss: 2.247173309326172\n",
      "Epoch 0, Batch 131, Loss: 2.250718116760254\n",
      "Epoch 0, Batch 132, Loss: 2.24648118019104\n",
      "Epoch 0, Batch 133, Loss: 2.2583682537078857\n",
      "Epoch 0, Batch 134, Loss: 2.247145414352417\n",
      "Epoch 0, Batch 135, Loss: 2.2451252937316895\n",
      "Epoch 0, Batch 136, Loss: 2.2425942420959473\n",
      "Epoch 0, Batch 137, Loss: 2.240011215209961\n",
      "Epoch 0, Batch 138, Loss: 2.24166202545166\n",
      "Epoch 0, Batch 139, Loss: 2.2454068660736084\n",
      "Epoch 0, Batch 140, Loss: 2.2495932579040527\n",
      "Epoch 0, Batch 141, Loss: 2.244067907333374\n",
      "Epoch 0, Batch 142, Loss: 2.250990629196167\n",
      "Epoch 0, Batch 143, Loss: 2.253838539123535\n",
      "Epoch 0, Batch 144, Loss: 2.2404894828796387\n",
      "Epoch 0, Batch 145, Loss: 2.246689796447754\n",
      "Epoch 0, Batch 146, Loss: 2.245295763015747\n",
      "Epoch 0, Batch 147, Loss: 2.2494218349456787\n",
      "Epoch 0, Batch 148, Loss: 2.2413783073425293\n",
      "Epoch 0, Batch 149, Loss: 2.2357144355773926\n",
      "Epoch 0, Batch 150, Loss: 2.2461745738983154\n",
      "Epoch 0, Batch 151, Loss: 2.244204044342041\n",
      "Epoch 0, Batch 152, Loss: 2.2479240894317627\n",
      "Epoch 0, Batch 153, Loss: 2.2522754669189453\n",
      "Epoch 0, Batch 154, Loss: 2.2418599128723145\n",
      "Epoch 0, Batch 155, Loss: 2.246546983718872\n",
      "Epoch 0, Batch 156, Loss: 2.24627947807312\n",
      "Epoch 0, Batch 157, Loss: 2.2375471591949463\n",
      "Epoch 0, Batch 158, Loss: 2.2390894889831543\n",
      "Epoch 0, Batch 159, Loss: 2.256725311279297\n",
      "Epoch 0, Batch 160, Loss: 2.2480227947235107\n",
      "Epoch 0, Batch 161, Loss: 2.2479939460754395\n",
      "Epoch 0, Batch 162, Loss: 2.235837697982788\n",
      "Epoch 0, Batch 163, Loss: 2.244417667388916\n",
      "Epoch 0, Batch 164, Loss: 2.2528979778289795\n",
      "Epoch 0, Batch 165, Loss: 2.233802080154419\n",
      "Epoch 0, Batch 166, Loss: 2.24627947807312\n",
      "Epoch 0, Batch 167, Loss: 2.2388429641723633\n",
      "Epoch 0, Batch 168, Loss: 2.2405736446380615\n",
      "Epoch 0, Batch 169, Loss: 2.2392148971557617\n",
      "Epoch 0, Batch 170, Loss: 2.2380104064941406\n",
      "Epoch 0, Batch 171, Loss: 2.241981029510498\n",
      "Epoch 0, Batch 172, Loss: 2.2465450763702393\n",
      "Epoch 0, Batch 173, Loss: 2.237332344055176\n",
      "Epoch 0, Batch 174, Loss: 2.243252992630005\n",
      "Epoch 0, Batch 175, Loss: 2.238386631011963\n",
      "Epoch 0, Batch 176, Loss: 2.2463600635528564\n",
      "Epoch 0, Batch 177, Loss: 2.2416961193084717\n",
      "Epoch 0, Batch 178, Loss: 2.228536605834961\n",
      "Epoch 0, Batch 179, Loss: 2.239567279815674\n",
      "Epoch 0, Batch 180, Loss: 2.2354016304016113\n",
      "Epoch 0, Batch 181, Loss: 2.2535881996154785\n",
      "Epoch 0, Batch 182, Loss: 2.2467503547668457\n",
      "Epoch 0, Batch 183, Loss: 2.2316694259643555\n",
      "Epoch 0, Batch 184, Loss: 2.240795612335205\n",
      "Epoch 0, Batch 185, Loss: 2.2442073822021484\n",
      "Epoch 0, Batch 186, Loss: 2.2339236736297607\n",
      "Epoch 0, Batch 187, Loss: 2.24061918258667\n",
      "Epoch 0, Batch 188, Loss: 2.2419815063476562\n",
      "Epoch 0, Batch 189, Loss: 2.2340586185455322\n",
      "Epoch 0, Batch 190, Loss: 2.2329797744750977\n",
      "Epoch 0, Batch 191, Loss: 2.228975534439087\n",
      "Epoch 0, Batch 192, Loss: 2.2508838176727295\n",
      "Epoch 0, Batch 193, Loss: 2.2359671592712402\n",
      "Epoch 0, Batch 194, Loss: 2.2317256927490234\n",
      "Epoch 0, Batch 195, Loss: 2.2315008640289307\n",
      "Epoch 0, Batch 196, Loss: 2.232966184616089\n",
      "Epoch 0, Batch 197, Loss: 2.2365307807922363\n",
      "Epoch 0, Batch 198, Loss: 2.234438896179199\n",
      "Epoch 0, Batch 199, Loss: 2.238191604614258\n",
      "Epoch 0, Batch 200, Loss: 2.237783908843994\n",
      "Epoch 0, Batch 201, Loss: 2.2289512157440186\n",
      "Epoch 0, Batch 202, Loss: 2.2392237186431885\n",
      "Epoch 0, Batch 203, Loss: 2.230349063873291\n",
      "Epoch 0, Batch 204, Loss: 2.235156297683716\n",
      "Epoch 0, Batch 205, Loss: 2.2299866676330566\n",
      "Epoch 0, Batch 206, Loss: 2.2374372482299805\n",
      "Epoch 0, Batch 207, Loss: 2.225533962249756\n",
      "Epoch 0, Batch 208, Loss: 2.2279510498046875\n",
      "Epoch 0, Batch 209, Loss: 2.2385268211364746\n",
      "Epoch 0, Batch 210, Loss: 2.2270660400390625\n",
      "Epoch 0, Batch 211, Loss: 2.235145092010498\n",
      "Epoch 0, Batch 212, Loss: 2.237076759338379\n",
      "Epoch 0, Batch 213, Loss: 2.2402970790863037\n",
      "Epoch 0, Batch 214, Loss: 2.227351427078247\n",
      "Epoch 0, Batch 215, Loss: 2.2280240058898926\n",
      "Epoch 0, Batch 216, Loss: 2.227980852127075\n",
      "Epoch 0, Batch 217, Loss: 2.240199089050293\n",
      "Epoch 0, Batch 218, Loss: 2.223076581954956\n",
      "Epoch 0, Batch 219, Loss: 2.2393391132354736\n",
      "Epoch 0, Batch 220, Loss: 2.223931074142456\n",
      "Epoch 0, Batch 221, Loss: 2.2196121215820312\n",
      "Epoch 0, Batch 222, Loss: 2.2333741188049316\n",
      "Epoch 0, Batch 223, Loss: 2.2269773483276367\n",
      "Epoch 0, Batch 224, Loss: 2.224222183227539\n",
      "Epoch 0, Batch 225, Loss: 2.229391574859619\n",
      "Epoch 0, Batch 226, Loss: 2.234818935394287\n",
      "Epoch 0, Batch 227, Loss: 2.24019193649292\n",
      "Epoch 0, Batch 228, Loss: 2.2184159755706787\n",
      "Epoch 0, Batch 229, Loss: 2.235396385192871\n",
      "Epoch 0, Batch 230, Loss: 2.234257698059082\n",
      "Epoch 0, Batch 231, Loss: 2.2192025184631348\n",
      "Epoch 0, Batch 232, Loss: 2.237492322921753\n",
      "Epoch 0, Batch 233, Loss: 2.239278554916382\n",
      "Epoch 0, Batch 234, Loss: 2.2251176834106445\n",
      "Epoch 0, Batch 235, Loss: 2.2323689460754395\n",
      "Epoch 0, Batch 236, Loss: 2.2257509231567383\n",
      "Epoch 0, Batch 237, Loss: 2.242741346359253\n",
      "Epoch 0, Batch 238, Loss: 2.2299678325653076\n",
      "Epoch 0, Batch 239, Loss: 2.2309648990631104\n",
      "Epoch 0, Batch 240, Loss: 2.2298786640167236\n",
      "Epoch 0, Batch 241, Loss: 2.2067973613739014\n",
      "Epoch 0, Batch 242, Loss: 2.223372220993042\n",
      "Epoch 0, Batch 243, Loss: 2.225104808807373\n",
      "Epoch 0, Batch 244, Loss: 2.226292133331299\n",
      "Epoch 0, Batch 245, Loss: 2.227822780609131\n",
      "Epoch 0, Batch 246, Loss: 2.2245242595672607\n",
      "Epoch 0, Batch 247, Loss: 2.223249912261963\n",
      "Epoch 0, Batch 248, Loss: 2.22719669342041\n",
      "Epoch 0, Batch 249, Loss: 2.2373719215393066\n",
      "Epoch 0, Batch 250, Loss: 2.2228305339813232\n",
      "Epoch 0, Batch 251, Loss: 2.215737819671631\n",
      "Epoch 0, Batch 252, Loss: 2.226484775543213\n",
      "Epoch 0, Batch 253, Loss: 2.2229197025299072\n",
      "Epoch 0, Batch 254, Loss: 2.217222213745117\n",
      "Epoch 0, Batch 255, Loss: 2.21770977973938\n",
      "Epoch 0, Batch 256, Loss: 2.221259355545044\n",
      "Epoch 0, Batch 257, Loss: 2.2119293212890625\n",
      "Epoch 0, Batch 258, Loss: 2.214158773422241\n",
      "Epoch 0, Batch 259, Loss: 2.227593421936035\n",
      "Epoch 0, Batch 260, Loss: 2.212719202041626\n",
      "Epoch 0, Batch 261, Loss: 2.2151317596435547\n",
      "Epoch 0, Batch 262, Loss: 2.2219860553741455\n",
      "Epoch 0, Batch 263, Loss: 2.216463088989258\n",
      "Epoch 0, Batch 264, Loss: 2.219514846801758\n",
      "Epoch 0, Batch 265, Loss: 2.227635145187378\n",
      "Epoch 0, Batch 266, Loss: 2.2192633152008057\n",
      "Epoch 0, Batch 267, Loss: 2.2218105792999268\n",
      "Epoch 0, Batch 268, Loss: 2.223816394805908\n",
      "Epoch 0, Batch 269, Loss: 2.2285819053649902\n",
      "Epoch 0, Batch 270, Loss: 2.2208144664764404\n",
      "Epoch 0, Batch 271, Loss: 2.210583209991455\n",
      "Epoch 0, Batch 272, Loss: 2.21419095993042\n",
      "Epoch 0, Batch 273, Loss: 2.2289011478424072\n",
      "Epoch 0, Batch 274, Loss: 2.2280232906341553\n",
      "Epoch 0, Batch 275, Loss: 2.221776247024536\n",
      "Epoch 0, Batch 276, Loss: 2.2200429439544678\n",
      "Epoch 0, Batch 277, Loss: 2.2132198810577393\n",
      "Epoch 0, Batch 278, Loss: 2.213554620742798\n",
      "Epoch 0, Batch 279, Loss: 2.218775749206543\n",
      "Epoch 0, Batch 280, Loss: 2.214466094970703\n",
      "Epoch 0, Batch 281, Loss: 2.201061248779297\n",
      "Epoch 0, Batch 282, Loss: 2.2197232246398926\n",
      "Epoch 0, Batch 283, Loss: 2.21114182472229\n",
      "Epoch 0, Batch 284, Loss: 2.1989572048187256\n",
      "Epoch 0, Batch 285, Loss: 2.2239768505096436\n",
      "Epoch 0, Batch 286, Loss: 2.208366632461548\n",
      "Epoch 0, Batch 287, Loss: 2.2111780643463135\n",
      "Epoch 0, Batch 288, Loss: 2.2206881046295166\n",
      "Epoch 0, Batch 289, Loss: 2.2147297859191895\n",
      "Epoch 0, Batch 290, Loss: 2.2175841331481934\n",
      "Epoch 0, Batch 291, Loss: 2.225400924682617\n",
      "Epoch 0, Batch 292, Loss: 2.208794593811035\n",
      "Epoch 0, Batch 293, Loss: 2.2257614135742188\n",
      "Epoch 0, Batch 294, Loss: 2.2137632369995117\n",
      "Epoch 0, Batch 295, Loss: 2.21711802482605\n",
      "Epoch 0, Batch 296, Loss: 2.2060182094573975\n",
      "Epoch 0, Batch 297, Loss: 2.200984477996826\n",
      "Epoch 0, Batch 298, Loss: 2.2170028686523438\n",
      "Epoch 0, Batch 299, Loss: 2.2138147354125977\n",
      "Epoch 0, Batch 300, Loss: 2.2004358768463135\n",
      "Epoch 0, Batch 301, Loss: 2.1992673873901367\n",
      "Epoch 0, Batch 302, Loss: 2.2093424797058105\n",
      "Epoch 0, Batch 303, Loss: 2.2180237770080566\n",
      "Epoch 0, Batch 304, Loss: 2.2061307430267334\n",
      "Epoch 0, Batch 305, Loss: 2.205533027648926\n",
      "Epoch 0, Batch 306, Loss: 2.209749221801758\n",
      "Epoch 0, Batch 307, Loss: 2.216308116912842\n",
      "Epoch 0, Batch 308, Loss: 2.208695888519287\n",
      "Epoch 0, Batch 309, Loss: 2.220311403274536\n",
      "Epoch 0, Batch 310, Loss: 2.206977605819702\n",
      "Epoch 0, Batch 311, Loss: 2.2039222717285156\n",
      "Epoch 0, Batch 312, Loss: 2.2023818492889404\n",
      "Epoch 0, Batch 313, Loss: 2.1990456581115723\n",
      "Epoch 0, Batch 314, Loss: 2.198786973953247\n",
      "Epoch 0, Batch 315, Loss: 2.2057299613952637\n",
      "Epoch 0, Batch 316, Loss: 2.198180913925171\n",
      "Epoch 0, Batch 317, Loss: 2.200848340988159\n",
      "Epoch 0, Batch 318, Loss: 2.199403762817383\n",
      "Epoch 0, Batch 319, Loss: 2.1996231079101562\n",
      "Epoch 0, Batch 320, Loss: 2.212878465652466\n",
      "Epoch 0, Batch 321, Loss: 2.209105968475342\n",
      "Epoch 0, Batch 322, Loss: 2.2100791931152344\n",
      "Epoch 0, Batch 323, Loss: 2.1942625045776367\n",
      "Epoch 0, Batch 324, Loss: 2.189984083175659\n",
      "Epoch 0, Batch 325, Loss: 2.212212085723877\n",
      "Epoch 0, Batch 326, Loss: 2.20806622505188\n",
      "Epoch 0, Batch 327, Loss: 2.204780101776123\n",
      "Epoch 0, Batch 328, Loss: 2.2183592319488525\n",
      "Epoch 0, Batch 329, Loss: 2.2110893726348877\n",
      "Epoch 0, Batch 330, Loss: 2.1988234519958496\n",
      "Epoch 0, Batch 331, Loss: 2.188631296157837\n",
      "Epoch 0, Batch 332, Loss: 2.1964163780212402\n",
      "Epoch 0, Batch 333, Loss: 2.1920530796051025\n",
      "Epoch 0, Batch 334, Loss: 2.20782732963562\n",
      "Epoch 0, Batch 335, Loss: 2.1928791999816895\n",
      "Epoch 0, Batch 336, Loss: 2.1773762702941895\n",
      "Epoch 0, Batch 337, Loss: 2.1916911602020264\n",
      "Epoch 0, Batch 338, Loss: 2.188915252685547\n",
      "Epoch 0, Batch 339, Loss: 2.196726083755493\n",
      "Epoch 0, Batch 340, Loss: 2.205798864364624\n",
      "Epoch 0, Batch 341, Loss: 2.2113118171691895\n",
      "Epoch 0, Batch 342, Loss: 2.189699649810791\n",
      "Epoch 0, Batch 343, Loss: 2.1836609840393066\n",
      "Epoch 0, Batch 344, Loss: 2.2020864486694336\n",
      "Epoch 0, Batch 345, Loss: 2.2086925506591797\n",
      "Epoch 0, Batch 346, Loss: 2.1964595317840576\n",
      "Epoch 0, Batch 347, Loss: 2.186843156814575\n",
      "Epoch 0, Batch 348, Loss: 2.206615686416626\n",
      "Epoch 0, Batch 349, Loss: 2.2003047466278076\n",
      "Epoch 0, Batch 350, Loss: 2.1805498600006104\n",
      "Epoch 0, Batch 351, Loss: 2.1890923976898193\n",
      "Epoch 0, Batch 352, Loss: 2.1963772773742676\n",
      "Epoch 0, Batch 353, Loss: 2.187737226486206\n",
      "Epoch 0, Batch 354, Loss: 2.1910507678985596\n",
      "Epoch 0, Batch 355, Loss: 2.196873903274536\n",
      "Epoch 0, Batch 356, Loss: 2.1720950603485107\n",
      "Epoch 0, Batch 357, Loss: 2.1877307891845703\n",
      "Epoch 0, Batch 358, Loss: 2.2079718112945557\n",
      "Epoch 0, Batch 359, Loss: 2.1919913291931152\n",
      "Epoch 0, Batch 360, Loss: 2.2048187255859375\n",
      "Epoch 0, Batch 361, Loss: 2.1893162727355957\n",
      "Epoch 0, Batch 362, Loss: 2.176267623901367\n",
      "Epoch 0, Batch 363, Loss: 2.188161849975586\n",
      "Epoch 0, Batch 364, Loss: 2.185664176940918\n",
      "Epoch 0, Batch 365, Loss: 2.1907734870910645\n",
      "Epoch 0, Batch 366, Loss: 2.1831986904144287\n",
      "Epoch 0, Batch 367, Loss: 2.1819169521331787\n",
      "Epoch 0, Batch 368, Loss: 2.1618757247924805\n",
      "Epoch 0, Batch 369, Loss: 2.1835641860961914\n",
      "Epoch 0, Batch 370, Loss: 2.1973469257354736\n",
      "Epoch 0, Batch 371, Loss: 2.1885297298431396\n",
      "Epoch 0, Batch 372, Loss: 2.2028324604034424\n",
      "Epoch 0, Batch 373, Loss: 2.170895576477051\n",
      "Epoch 0, Batch 374, Loss: 2.187520980834961\n",
      "Epoch 0, Batch 375, Loss: 2.170713424682617\n",
      "Epoch 0, Batch 376, Loss: 2.2066614627838135\n",
      "Epoch 0, Batch 377, Loss: 2.179388999938965\n",
      "Epoch 0, Batch 378, Loss: 2.192451000213623\n",
      "Epoch 0, Batch 379, Loss: 2.1741435527801514\n",
      "Epoch 0, Batch 380, Loss: 2.1919190883636475\n",
      "Epoch 0, Batch 381, Loss: 2.1731204986572266\n",
      "Epoch 0, Batch 382, Loss: 2.1782584190368652\n",
      "Epoch 0, Batch 383, Loss: 2.172436237335205\n",
      "Epoch 0, Batch 384, Loss: 2.1645395755767822\n",
      "Epoch 0, Batch 385, Loss: 2.1723711490631104\n",
      "Epoch 0, Batch 386, Loss: 2.1882705688476562\n",
      "Epoch 0, Batch 387, Loss: 2.1819210052490234\n",
      "Epoch 0, Batch 388, Loss: 2.196333408355713\n",
      "Epoch 0, Batch 389, Loss: 2.179478168487549\n",
      "Epoch 0, Batch 390, Loss: 2.167234420776367\n",
      "Epoch 0, Batch 391, Loss: 2.169466018676758\n",
      "Epoch 0, Batch 392, Loss: 2.1669161319732666\n",
      "Epoch 0, Batch 393, Loss: 2.15175199508667\n",
      "Epoch 0, Batch 394, Loss: 2.1729562282562256\n",
      "Epoch 0, Batch 395, Loss: 2.1728737354278564\n",
      "Epoch 0, Batch 396, Loss: 2.1740729808807373\n",
      "Epoch 0, Batch 397, Loss: 2.1643898487091064\n",
      "Epoch 0, Batch 398, Loss: 2.1952857971191406\n",
      "Epoch 0, Batch 399, Loss: 2.1966683864593506\n",
      "Epoch 0, Batch 400, Loss: 2.183586597442627\n",
      "Epoch 0, Batch 401, Loss: 2.1981360912323\n",
      "Epoch 0, Batch 402, Loss: 2.167313575744629\n",
      "Epoch 0, Batch 403, Loss: 2.1833999156951904\n",
      "Epoch 0, Batch 404, Loss: 2.151099681854248\n",
      "Epoch 0, Batch 405, Loss: 2.174531936645508\n",
      "Epoch 0, Batch 406, Loss: 2.169804096221924\n",
      "Epoch 0, Batch 407, Loss: 2.165478467941284\n",
      "Epoch 0, Batch 408, Loss: 2.17437744140625\n",
      "Epoch 0, Batch 409, Loss: 2.177546262741089\n",
      "Epoch 0, Batch 410, Loss: 2.1659419536590576\n",
      "Epoch 0, Batch 411, Loss: 2.1536662578582764\n",
      "Epoch 0, Batch 412, Loss: 2.1627771854400635\n",
      "Epoch 0, Batch 413, Loss: 2.1821248531341553\n",
      "Epoch 0, Batch 414, Loss: 2.1693623065948486\n",
      "Epoch 0, Batch 415, Loss: 2.1638400554656982\n",
      "Epoch 0, Batch 416, Loss: 2.152801990509033\n",
      "Epoch 0, Batch 417, Loss: 2.151747703552246\n",
      "Epoch 0, Batch 418, Loss: 2.1689584255218506\n",
      "Epoch 0, Batch 419, Loss: 2.1611340045928955\n",
      "Epoch 0, Batch 420, Loss: 2.1756246089935303\n",
      "Epoch 0, Batch 421, Loss: 2.158522605895996\n",
      "Epoch 0, Batch 422, Loss: 2.1548328399658203\n",
      "Epoch 0, Batch 423, Loss: 2.1415443420410156\n",
      "Epoch 0, Batch 424, Loss: 2.1741015911102295\n",
      "Epoch 0, Batch 425, Loss: 2.152555465698242\n",
      "Epoch 0, Batch 426, Loss: 2.1359384059906006\n",
      "Epoch 0, Batch 427, Loss: 2.1590428352355957\n",
      "Epoch 0, Batch 428, Loss: 2.146284341812134\n",
      "Epoch 0, Batch 429, Loss: 2.153285503387451\n",
      "Epoch 0, Batch 430, Loss: 2.1653904914855957\n",
      "Epoch 0, Batch 431, Loss: 2.1416964530944824\n",
      "Epoch 0, Batch 432, Loss: 2.139434576034546\n",
      "Epoch 0, Batch 433, Loss: 2.150766134262085\n",
      "Epoch 0, Batch 434, Loss: 2.1636486053466797\n",
      "Epoch 0, Batch 435, Loss: 2.1546754837036133\n",
      "Epoch 0, Batch 436, Loss: 2.1509311199188232\n",
      "Epoch 0, Batch 437, Loss: 2.162029504776001\n",
      "Epoch 0, Batch 438, Loss: 2.1374244689941406\n",
      "Epoch 0, Batch 439, Loss: 2.1579790115356445\n",
      "Epoch 0, Batch 440, Loss: 2.1385271549224854\n",
      "Epoch 0, Batch 441, Loss: 2.166473627090454\n",
      "Epoch 0, Batch 442, Loss: 2.1398074626922607\n",
      "Epoch 0, Batch 443, Loss: 2.1442527770996094\n",
      "Epoch 0, Batch 444, Loss: 2.1351213455200195\n",
      "Epoch 0, Batch 445, Loss: 2.1485116481781006\n",
      "Epoch 0, Batch 446, Loss: 2.144909143447876\n",
      "Epoch 0, Batch 447, Loss: 2.1606156826019287\n",
      "Epoch 0, Batch 448, Loss: 2.122284412384033\n",
      "Epoch 0, Batch 449, Loss: 2.1680383682250977\n",
      "Epoch 0, Batch 450, Loss: 2.1538548469543457\n",
      "Epoch 0, Batch 451, Loss: 2.1565744876861572\n",
      "Epoch 0, Batch 452, Loss: 2.1289429664611816\n",
      "Epoch 0, Batch 453, Loss: 2.1429147720336914\n",
      "Epoch 0, Batch 454, Loss: 2.149627923965454\n",
      "Epoch 0, Batch 455, Loss: 2.1386876106262207\n",
      "Epoch 0, Batch 456, Loss: 2.1374247074127197\n",
      "Epoch 0, Batch 457, Loss: 2.152188301086426\n",
      "Epoch 0, Batch 458, Loss: 2.1489169597625732\n",
      "Epoch 0, Batch 459, Loss: 2.1299707889556885\n",
      "Epoch 0, Batch 460, Loss: 2.1154446601867676\n",
      "Epoch 0, Batch 461, Loss: 2.1638097763061523\n",
      "Epoch 0, Batch 462, Loss: 2.1506712436676025\n",
      "Epoch 0, Batch 463, Loss: 2.143326997756958\n",
      "Epoch 0, Batch 464, Loss: 2.143825054168701\n",
      "Epoch 0, Batch 465, Loss: 2.1337225437164307\n",
      "Epoch 0, Batch 466, Loss: 2.1523172855377197\n",
      "Epoch 0, Batch 467, Loss: 2.140348434448242\n",
      "Epoch 0, Batch 468, Loss: 2.120678424835205\n",
      "Epoch 0, Batch 469, Loss: 2.1291885375976562\n",
      "Epoch 0, Batch 470, Loss: 2.1207196712493896\n",
      "Epoch 0, Batch 471, Loss: 2.128911256790161\n",
      "Epoch 0, Batch 472, Loss: 2.161486864089966\n",
      "Epoch 0, Batch 473, Loss: 2.1433968544006348\n",
      "Epoch 0, Batch 474, Loss: 2.139845132827759\n",
      "Epoch 0, Batch 475, Loss: 2.135127544403076\n",
      "Epoch 0, Batch 476, Loss: 2.123002290725708\n",
      "Epoch 0, Batch 477, Loss: 2.1295156478881836\n",
      "Epoch 0, Batch 478, Loss: 2.1239335536956787\n",
      "Epoch 0, Batch 479, Loss: 2.0971264839172363\n",
      "Epoch 0, Batch 480, Loss: 2.127333641052246\n",
      "Epoch 0, Batch 481, Loss: 2.1444575786590576\n",
      "Epoch 0, Batch 482, Loss: 2.138460636138916\n",
      "Epoch 0, Batch 483, Loss: 2.117861270904541\n",
      "Epoch 0, Batch 484, Loss: 2.1173202991485596\n",
      "Epoch 0, Batch 485, Loss: 2.0894689559936523\n",
      "Epoch 0, Batch 486, Loss: 2.1284053325653076\n",
      "Epoch 0, Batch 487, Loss: 2.1243762969970703\n",
      "Epoch 0, Batch 488, Loss: 2.1214728355407715\n",
      "Epoch 0, Batch 489, Loss: 2.1186234951019287\n",
      "Epoch 0, Batch 490, Loss: 2.1142737865448\n",
      "Epoch 0, Batch 491, Loss: 2.108905076980591\n",
      "Epoch 0, Batch 492, Loss: 2.092742443084717\n",
      "Epoch 0, Batch 493, Loss: 2.1057345867156982\n",
      "Epoch 0, Batch 494, Loss: 2.121323823928833\n",
      "Epoch 0, Batch 495, Loss: 2.1297338008880615\n",
      "Epoch 0, Batch 496, Loss: 2.1010708808898926\n",
      "Epoch 0, Batch 497, Loss: 2.113814353942871\n",
      "Epoch 0, Batch 498, Loss: 2.1025869846343994\n",
      "Epoch 0, Batch 499, Loss: 2.1049129962921143\n",
      "Epoch 0, Batch 500, Loss: 2.1317615509033203\n",
      "Epoch 0, Batch 501, Loss: 2.1479039192199707\n",
      "Epoch 0, Batch 502, Loss: 2.13627290725708\n",
      "Epoch 0, Batch 503, Loss: 2.0923209190368652\n",
      "Epoch 0, Batch 504, Loss: 2.1151506900787354\n",
      "Epoch 0, Batch 505, Loss: 2.116123914718628\n",
      "Epoch 0, Batch 506, Loss: 2.1115357875823975\n",
      "Epoch 0, Batch 507, Loss: 2.1325297355651855\n",
      "Epoch 0, Batch 508, Loss: 2.0862207412719727\n",
      "Epoch 0, Batch 509, Loss: 2.1151504516601562\n",
      "Epoch 0, Batch 510, Loss: 2.1085238456726074\n",
      "Epoch 0, Batch 511, Loss: 2.1197140216827393\n",
      "Epoch 0, Batch 512, Loss: 2.1057937145233154\n",
      "Epoch 0, Batch 513, Loss: 2.116253137588501\n",
      "Epoch 0, Batch 514, Loss: 2.1129376888275146\n",
      "Epoch 0, Batch 515, Loss: 2.085437774658203\n",
      "Epoch 0, Batch 516, Loss: 2.0992391109466553\n",
      "Epoch 0, Batch 517, Loss: 2.1088554859161377\n",
      "Epoch 0, Batch 518, Loss: 2.09487247467041\n",
      "Epoch 0, Batch 519, Loss: 2.11181378364563\n",
      "Epoch 0, Batch 520, Loss: 2.1099085807800293\n",
      "Epoch 0, Batch 521, Loss: 2.069871664047241\n",
      "Epoch 0, Batch 522, Loss: 2.1028478145599365\n",
      "Epoch 0, Batch 523, Loss: 2.1309399604797363\n",
      "Epoch 0, Batch 524, Loss: 2.0821290016174316\n",
      "Epoch 0, Batch 525, Loss: 2.095026731491089\n",
      "Epoch 0, Batch 526, Loss: 2.085373878479004\n",
      "Epoch 0, Batch 527, Loss: 2.1186277866363525\n",
      "Epoch 0, Batch 528, Loss: 2.1230967044830322\n",
      "Epoch 0, Batch 529, Loss: 2.0916662216186523\n",
      "Epoch 0, Batch 530, Loss: 2.0912206172943115\n",
      "Epoch 0, Batch 531, Loss: 2.094773530960083\n",
      "Epoch 0, Batch 532, Loss: 2.0766167640686035\n",
      "Epoch 0, Batch 533, Loss: 2.0516464710235596\n",
      "Epoch 0, Batch 534, Loss: 2.0926473140716553\n",
      "Epoch 0, Batch 535, Loss: 2.1269357204437256\n",
      "Epoch 0, Batch 536, Loss: 2.098712682723999\n",
      "Epoch 0, Batch 537, Loss: 2.0438501834869385\n",
      "Epoch 0, Batch 538, Loss: 2.0856070518493652\n",
      "Epoch 0, Batch 539, Loss: 2.089768648147583\n",
      "Epoch 0, Batch 540, Loss: 2.1157569885253906\n",
      "Epoch 0, Batch 541, Loss: 2.0672738552093506\n",
      "Epoch 0, Batch 542, Loss: 2.079024314880371\n",
      "Epoch 0, Batch 543, Loss: 2.0886547565460205\n",
      "Epoch 0, Batch 544, Loss: 2.1062426567077637\n",
      "Epoch 0, Batch 545, Loss: 2.088181972503662\n",
      "Epoch 0, Batch 546, Loss: 2.064326763153076\n",
      "Epoch 0, Batch 547, Loss: 2.0550146102905273\n",
      "Epoch 0, Batch 548, Loss: 2.07209849357605\n",
      "Epoch 0, Batch 549, Loss: 2.0909714698791504\n",
      "Epoch 0, Batch 550, Loss: 2.082690715789795\n",
      "Epoch 0, Batch 551, Loss: 2.1057705879211426\n",
      "Epoch 0, Batch 552, Loss: 2.091735601425171\n",
      "Epoch 0, Batch 553, Loss: 2.097630739212036\n",
      "Epoch 0, Batch 554, Loss: 2.0894649028778076\n",
      "Epoch 0, Batch 555, Loss: 2.07849383354187\n",
      "Epoch 0, Batch 556, Loss: 2.0982065200805664\n",
      "Epoch 0, Batch 557, Loss: 2.0711498260498047\n",
      "Epoch 0, Batch 558, Loss: 2.080749273300171\n",
      "Epoch 0, Batch 559, Loss: 2.057063102722168\n",
      "Epoch 0, Batch 560, Loss: 2.0815868377685547\n",
      "Epoch 0, Batch 561, Loss: 2.0565850734710693\n",
      "Epoch 0, Batch 562, Loss: 2.072099208831787\n",
      "Epoch 0, Batch 563, Loss: 2.0616555213928223\n",
      "Epoch 0, Batch 564, Loss: 2.085170030593872\n",
      "Epoch 0, Batch 565, Loss: 2.0660688877105713\n",
      "Epoch 0, Batch 566, Loss: 2.0216305255889893\n",
      "Epoch 0, Batch 567, Loss: 2.0599730014801025\n",
      "Epoch 0, Batch 568, Loss: 2.0646634101867676\n",
      "Epoch 0, Batch 569, Loss: 2.0865018367767334\n",
      "Epoch 0, Batch 570, Loss: 2.0883705615997314\n",
      "Epoch 0, Batch 571, Loss: 2.0757346153259277\n",
      "Epoch 0, Batch 572, Loss: 2.0681169033050537\n",
      "Epoch 0, Batch 573, Loss: 2.0892832279205322\n",
      "Epoch 0, Batch 574, Loss: 2.061469316482544\n",
      "Epoch 0, Batch 575, Loss: 2.041924476623535\n",
      "Epoch 0, Batch 576, Loss: 2.03153657913208\n",
      "Epoch 0, Batch 577, Loss: 2.046638011932373\n",
      "Epoch 0, Batch 578, Loss: 2.067668914794922\n",
      "Epoch 0, Batch 579, Loss: 2.0658986568450928\n",
      "Epoch 0, Batch 580, Loss: 2.0327415466308594\n",
      "Epoch 0, Batch 581, Loss: 2.0594570636749268\n",
      "Epoch 0, Batch 582, Loss: 2.0637574195861816\n",
      "Epoch 0, Batch 583, Loss: 2.062605619430542\n",
      "Epoch 0, Batch 584, Loss: 2.0815608501434326\n",
      "Epoch 0, Batch 585, Loss: 2.045732259750366\n",
      "Epoch 0, Batch 586, Loss: 2.0431337356567383\n",
      "Epoch 0, Batch 587, Loss: 2.070859432220459\n",
      "Epoch 0, Batch 588, Loss: 2.050480842590332\n",
      "Epoch 0, Batch 589, Loss: 2.091507911682129\n",
      "Epoch 0, Batch 590, Loss: 2.0425474643707275\n",
      "Epoch 0, Batch 591, Loss: 2.0667672157287598\n",
      "Epoch 0, Batch 592, Loss: 2.0269970893859863\n",
      "Epoch 0, Batch 593, Loss: 2.069150924682617\n",
      "Epoch 0, Batch 594, Loss: 2.087594509124756\n",
      "Epoch 0, Batch 595, Loss: 2.081686496734619\n",
      "Epoch 0, Batch 596, Loss: 2.073369026184082\n",
      "Epoch 0, Batch 597, Loss: 2.008014678955078\n",
      "Epoch 0, Batch 598, Loss: 2.053341865539551\n",
      "Epoch 0, Batch 599, Loss: 2.0268025398254395\n",
      "Epoch 0, Batch 600, Loss: 2.054718494415283\n",
      "Epoch 0, Batch 601, Loss: 2.0036938190460205\n",
      "Epoch 0, Batch 602, Loss: 2.06329345703125\n",
      "Epoch 0, Batch 603, Loss: 2.014700174331665\n",
      "Epoch 0, Batch 604, Loss: 2.043400287628174\n",
      "Epoch 0, Batch 605, Loss: 2.0300559997558594\n",
      "Epoch 0, Batch 606, Loss: 2.0037107467651367\n",
      "Epoch 0, Batch 607, Loss: 2.027613401412964\n",
      "Epoch 0, Batch 608, Loss: 2.019700527191162\n",
      "Epoch 0, Batch 609, Loss: 2.0132224559783936\n",
      "Epoch 0, Batch 610, Loss: 1.9984638690948486\n",
      "Epoch 0, Batch 611, Loss: 2.017137050628662\n",
      "Epoch 0, Batch 612, Loss: 2.0314812660217285\n",
      "Epoch 0, Batch 613, Loss: 2.045198440551758\n",
      "Epoch 0, Batch 614, Loss: 2.069640874862671\n",
      "Epoch 0, Batch 615, Loss: 2.0346381664276123\n",
      "Epoch 0, Batch 616, Loss: 2.0466558933258057\n",
      "Epoch 0, Batch 617, Loss: 2.0167620182037354\n",
      "Epoch 0, Batch 618, Loss: 2.0104362964630127\n",
      "Epoch 0, Batch 619, Loss: 2.0241470336914062\n",
      "Epoch 0, Batch 620, Loss: 2.018397808074951\n",
      "Epoch 0, Batch 621, Loss: 2.0316274166107178\n",
      "Epoch 0, Batch 622, Loss: 2.0093374252319336\n",
      "Epoch 0, Batch 623, Loss: 2.0225374698638916\n",
      "Epoch 0, Batch 624, Loss: 1.995253324508667\n",
      "Epoch 0, Batch 625, Loss: 1.9888553619384766\n",
      "Epoch 0, Batch 626, Loss: 2.0051088333129883\n",
      "Epoch 0, Batch 627, Loss: 2.01267409324646\n",
      "Epoch 0, Batch 628, Loss: 2.063523530960083\n",
      "Epoch 0, Batch 629, Loss: 1.999410629272461\n",
      "Epoch 0, Batch 630, Loss: 2.0221362113952637\n",
      "Epoch 0, Batch 631, Loss: 2.000880718231201\n",
      "Epoch 0, Batch 632, Loss: 1.9802744388580322\n",
      "Epoch 0, Batch 633, Loss: 1.9991494417190552\n",
      "Epoch 0, Batch 634, Loss: 2.0005078315734863\n",
      "Epoch 0, Batch 635, Loss: 2.0023353099823\n",
      "Epoch 0, Batch 636, Loss: 1.9704676866531372\n",
      "Epoch 0, Batch 637, Loss: 1.9705140590667725\n",
      "Epoch 0, Batch 638, Loss: 2.0110840797424316\n",
      "Epoch 0, Batch 639, Loss: 1.9582995176315308\n",
      "Epoch 0, Batch 640, Loss: 1.9664971828460693\n",
      "Epoch 0, Batch 641, Loss: 1.9839166402816772\n",
      "Epoch 0, Batch 642, Loss: 2.002932548522949\n",
      "Epoch 0, Batch 643, Loss: 1.99711012840271\n",
      "Epoch 0, Batch 644, Loss: 2.017796277999878\n",
      "Epoch 0, Batch 645, Loss: 1.9576232433319092\n",
      "Epoch 0, Batch 646, Loss: 1.9679811000823975\n",
      "Epoch 0, Batch 647, Loss: 1.9945119619369507\n",
      "Epoch 0, Batch 648, Loss: 2.0017778873443604\n",
      "Epoch 0, Batch 649, Loss: 1.9935438632965088\n",
      "Epoch 0, Batch 650, Loss: 1.954481840133667\n",
      "Epoch 0, Batch 651, Loss: 1.9891178607940674\n",
      "Epoch 0, Batch 652, Loss: 2.0084822177886963\n",
      "Epoch 0, Batch 653, Loss: 2.00189208984375\n",
      "Epoch 0, Batch 654, Loss: 2.023923635482788\n",
      "Epoch 0, Batch 655, Loss: 1.9848415851593018\n",
      "Epoch 0, Batch 656, Loss: 1.9828975200653076\n",
      "Epoch 0, Batch 657, Loss: 2.019179582595825\n",
      "Epoch 0, Batch 658, Loss: 1.962836503982544\n",
      "Epoch 0, Batch 659, Loss: 1.9653044939041138\n",
      "Epoch 0, Batch 660, Loss: 1.9760535955429077\n",
      "Epoch 0, Batch 661, Loss: 1.9907037019729614\n",
      "Epoch 0, Batch 662, Loss: 2.02011775970459\n",
      "Epoch 0, Batch 663, Loss: 1.9762053489685059\n",
      "Epoch 0, Batch 664, Loss: 1.9248108863830566\n",
      "Epoch 0, Batch 665, Loss: 1.9323577880859375\n",
      "Epoch 0, Batch 666, Loss: 2.0093908309936523\n",
      "Epoch 0, Batch 667, Loss: 2.0037665367126465\n",
      "Epoch 0, Batch 668, Loss: 1.9642161130905151\n",
      "Epoch 0, Batch 669, Loss: 2.0214133262634277\n",
      "Epoch 0, Batch 670, Loss: 1.9878926277160645\n",
      "Epoch 0, Batch 671, Loss: 1.974714756011963\n",
      "Epoch 0, Batch 672, Loss: 2.0266060829162598\n",
      "Epoch 0, Batch 673, Loss: 1.9442888498306274\n",
      "Epoch 0, Batch 674, Loss: 1.9747194051742554\n",
      "Epoch 0, Batch 675, Loss: 1.9695216417312622\n",
      "Epoch 0, Batch 676, Loss: 1.926632046699524\n",
      "Epoch 0, Batch 677, Loss: 1.9891952276229858\n",
      "Epoch 0, Batch 678, Loss: 1.931355595588684\n",
      "Epoch 0, Batch 679, Loss: 1.9696855545043945\n",
      "Epoch 0, Batch 680, Loss: 1.9566856622695923\n",
      "Epoch 0, Batch 681, Loss: 1.9473639726638794\n",
      "Epoch 0, Batch 682, Loss: 1.9415361881256104\n",
      "Epoch 0, Batch 683, Loss: 1.9324913024902344\n",
      "Epoch 0, Batch 684, Loss: 1.97184419631958\n",
      "Epoch 0, Batch 685, Loss: 1.963773250579834\n",
      "Epoch 0, Batch 686, Loss: 1.9388777017593384\n",
      "Epoch 0, Batch 687, Loss: 1.9468493461608887\n",
      "Epoch 0, Batch 688, Loss: 1.913210153579712\n",
      "Epoch 0, Batch 689, Loss: 1.9868232011795044\n",
      "Epoch 0, Batch 690, Loss: 1.9205811023712158\n",
      "Epoch 0, Batch 691, Loss: 1.9851620197296143\n",
      "Epoch 0, Batch 692, Loss: 1.9317901134490967\n",
      "Epoch 0, Batch 693, Loss: 1.960605502128601\n",
      "Epoch 0, Batch 694, Loss: 1.915405035018921\n",
      "Epoch 0, Batch 695, Loss: 1.9748704433441162\n",
      "Epoch 0, Batch 696, Loss: 1.9067574739456177\n",
      "Epoch 0, Batch 697, Loss: 1.9484658241271973\n",
      "Epoch 0, Batch 698, Loss: 1.9023290872573853\n",
      "Epoch 0, Batch 699, Loss: 1.9675366878509521\n",
      "Epoch 0, Batch 700, Loss: 1.9273486137390137\n",
      "Epoch 0, Batch 701, Loss: 1.891829013824463\n",
      "Epoch 0, Batch 702, Loss: 1.9083229303359985\n",
      "Epoch 0, Batch 703, Loss: 1.9348901510238647\n",
      "Epoch 0, Batch 704, Loss: 1.943288803100586\n",
      "Epoch 0, Batch 705, Loss: 1.9465190172195435\n",
      "Epoch 0, Batch 706, Loss: 1.9110071659088135\n",
      "Epoch 0, Batch 707, Loss: 1.9404656887054443\n",
      "Epoch 0, Batch 708, Loss: 1.886698842048645\n",
      "Epoch 0, Batch 709, Loss: 1.8953943252563477\n",
      "Epoch 0, Batch 710, Loss: 1.9359768629074097\n",
      "Epoch 0, Batch 711, Loss: 1.9050509929656982\n",
      "Epoch 0, Batch 712, Loss: 1.9535428285598755\n",
      "Epoch 0, Batch 713, Loss: 1.904744267463684\n",
      "Epoch 0, Batch 714, Loss: 1.9178744554519653\n",
      "Epoch 0, Batch 715, Loss: 1.9323631525039673\n",
      "Epoch 0, Batch 716, Loss: 1.9589838981628418\n",
      "Epoch 0, Batch 717, Loss: 1.9237093925476074\n",
      "Epoch 0, Batch 718, Loss: 1.8632475137710571\n",
      "Epoch 0, Batch 719, Loss: 1.9072697162628174\n",
      "Epoch 0, Batch 720, Loss: 1.9329901933670044\n",
      "Epoch 0, Batch 721, Loss: 1.8846170902252197\n",
      "Epoch 0, Batch 722, Loss: 1.8819584846496582\n",
      "Epoch 0, Batch 723, Loss: 1.891822338104248\n",
      "Epoch 0, Batch 724, Loss: 1.9094455242156982\n",
      "Epoch 0, Batch 725, Loss: 1.9150985479354858\n",
      "Epoch 0, Batch 726, Loss: 1.924989104270935\n",
      "Epoch 0, Batch 727, Loss: 1.9392542839050293\n",
      "Epoch 0, Batch 728, Loss: 1.8498116731643677\n",
      "Epoch 0, Batch 729, Loss: 1.8689002990722656\n",
      "Epoch 0, Batch 730, Loss: 1.9160592555999756\n",
      "Epoch 0, Batch 731, Loss: 1.9159150123596191\n",
      "Epoch 0, Batch 732, Loss: 1.8534340858459473\n",
      "Epoch 0, Batch 733, Loss: 1.8694759607315063\n",
      "Epoch 0, Batch 734, Loss: 1.940618634223938\n",
      "Epoch 0, Batch 735, Loss: 1.86460542678833\n",
      "Epoch 0, Batch 736, Loss: 1.8927159309387207\n",
      "Epoch 0, Batch 737, Loss: 1.9010910987854004\n",
      "Epoch 0, Batch 738, Loss: 1.8578802347183228\n",
      "Epoch 0, Batch 739, Loss: 1.8895565271377563\n",
      "Epoch 0, Batch 740, Loss: 1.8596396446228027\n",
      "Epoch 0, Batch 741, Loss: 1.8961098194122314\n",
      "Epoch 0, Batch 742, Loss: 1.8651015758514404\n",
      "Epoch 0, Batch 743, Loss: 1.8971298933029175\n",
      "Epoch 0, Batch 744, Loss: 1.9374898672103882\n",
      "Epoch 0, Batch 745, Loss: 1.8553303480148315\n",
      "Epoch 0, Batch 746, Loss: 1.8522553443908691\n",
      "Epoch 0, Batch 747, Loss: 1.8441085815429688\n",
      "Epoch 0, Batch 748, Loss: 1.8978655338287354\n",
      "Epoch 0, Batch 749, Loss: 1.8985613584518433\n",
      "Epoch 0, Batch 750, Loss: 1.8438892364501953\n",
      "Epoch 0, Batch 751, Loss: 1.8679702281951904\n",
      "Epoch 0, Batch 752, Loss: 1.8003907203674316\n",
      "Epoch 0, Batch 753, Loss: 1.843536615371704\n",
      "Epoch 0, Batch 754, Loss: 1.905030369758606\n",
      "Epoch 0, Batch 755, Loss: 1.865391731262207\n",
      "Epoch 0, Batch 756, Loss: 1.9015352725982666\n",
      "Epoch 0, Batch 757, Loss: 1.8854416608810425\n",
      "Epoch 0, Batch 758, Loss: 1.8888585567474365\n",
      "Epoch 0, Batch 759, Loss: 1.9065014123916626\n",
      "Epoch 0, Batch 760, Loss: 1.81913423538208\n",
      "Epoch 0, Batch 761, Loss: 1.8511598110198975\n",
      "Epoch 0, Batch 762, Loss: 1.8788644075393677\n",
      "Epoch 0, Batch 763, Loss: 1.8158454895019531\n",
      "Epoch 0, Batch 764, Loss: 1.8224140405654907\n",
      "Epoch 0, Batch 765, Loss: 1.8627657890319824\n",
      "Epoch 0, Batch 766, Loss: 1.8147032260894775\n",
      "Epoch 0, Batch 767, Loss: 1.8531639575958252\n",
      "Epoch 0, Batch 768, Loss: 1.8640530109405518\n",
      "Epoch 0, Batch 769, Loss: 1.7973265647888184\n",
      "Epoch 0, Batch 770, Loss: 1.8120882511138916\n",
      "Epoch 0, Batch 771, Loss: 1.8995893001556396\n",
      "Epoch 0, Batch 772, Loss: 1.7958760261535645\n",
      "Epoch 0, Batch 773, Loss: 1.822570562362671\n",
      "Epoch 0, Batch 774, Loss: 1.7954051494598389\n",
      "Epoch 0, Batch 775, Loss: 1.8617379665374756\n",
      "Epoch 0, Batch 776, Loss: 1.801936388015747\n",
      "Epoch 0, Batch 777, Loss: 1.787514090538025\n",
      "Epoch 0, Batch 778, Loss: 1.7978146076202393\n",
      "Epoch 0, Batch 779, Loss: 1.76560640335083\n",
      "Epoch 0, Batch 780, Loss: 1.8355541229248047\n",
      "Epoch 0, Batch 781, Loss: 1.818314552307129\n",
      "Epoch 0, Batch 782, Loss: 1.8812299966812134\n",
      "Epoch 0, Batch 783, Loss: 1.7852404117584229\n",
      "Epoch 0, Batch 784, Loss: 1.7995071411132812\n",
      "Epoch 0, Batch 785, Loss: 1.836840271949768\n",
      "Epoch 0, Batch 786, Loss: 1.8168225288391113\n",
      "Epoch 0, Batch 787, Loss: 1.8415353298187256\n",
      "Epoch 0, Batch 788, Loss: 1.7888915538787842\n",
      "Epoch 0, Batch 789, Loss: 1.825780987739563\n",
      "Epoch 0, Batch 790, Loss: 1.8350672721862793\n",
      "Epoch 0, Batch 791, Loss: 1.7910813093185425\n",
      "Epoch 0, Batch 792, Loss: 1.7698190212249756\n",
      "Epoch 0, Batch 793, Loss: 1.7445086240768433\n",
      "Epoch 0, Batch 794, Loss: 1.8006969690322876\n",
      "Epoch 0, Batch 795, Loss: 1.7711412906646729\n",
      "Epoch 0, Batch 796, Loss: 1.8662184476852417\n",
      "Epoch 0, Batch 797, Loss: 1.7699291706085205\n",
      "Epoch 0, Batch 798, Loss: 1.7605985403060913\n",
      "Epoch 0, Batch 799, Loss: 1.7594740390777588\n",
      "Epoch 0, Batch 800, Loss: 1.7935476303100586\n",
      "Epoch 0, Batch 801, Loss: 1.7584779262542725\n",
      "Epoch 0, Batch 802, Loss: 1.7744587659835815\n",
      "Epoch 0, Batch 803, Loss: 1.836439609527588\n",
      "Epoch 0, Batch 804, Loss: 1.8167929649353027\n",
      "Epoch 0, Batch 805, Loss: 1.7971349954605103\n",
      "Epoch 0, Batch 806, Loss: 1.7364896535873413\n",
      "Epoch 0, Batch 807, Loss: 1.8018206357955933\n",
      "Epoch 0, Batch 808, Loss: 1.787498950958252\n",
      "Epoch 0, Batch 809, Loss: 1.7751721143722534\n",
      "Epoch 0, Batch 810, Loss: 1.793447494506836\n",
      "Epoch 0, Batch 811, Loss: 1.7533990144729614\n",
      "Epoch 0, Batch 812, Loss: 1.7940306663513184\n",
      "Epoch 0, Batch 813, Loss: 1.7809377908706665\n",
      "Epoch 0, Batch 814, Loss: 1.7704026699066162\n",
      "Epoch 0, Batch 815, Loss: 1.7138423919677734\n",
      "Epoch 0, Batch 816, Loss: 1.762752890586853\n",
      "Epoch 0, Batch 817, Loss: 1.797729730606079\n",
      "Epoch 0, Batch 818, Loss: 1.7862062454223633\n",
      "Epoch 0, Batch 819, Loss: 1.7405132055282593\n",
      "Epoch 0, Batch 820, Loss: 1.8000566959381104\n",
      "Epoch 0, Batch 821, Loss: 1.745126724243164\n",
      "Epoch 0, Batch 822, Loss: 1.7225960493087769\n",
      "Epoch 0, Batch 823, Loss: 1.8004817962646484\n",
      "Epoch 0, Batch 824, Loss: 1.7512898445129395\n",
      "Epoch 0, Batch 825, Loss: 1.747244954109192\n",
      "Epoch 0, Batch 826, Loss: 1.7445807456970215\n",
      "Epoch 0, Batch 827, Loss: 1.7235174179077148\n",
      "Epoch 0, Batch 828, Loss: 1.7848625183105469\n",
      "Epoch 0, Batch 829, Loss: 1.8024744987487793\n",
      "Epoch 0, Batch 830, Loss: 1.6818329095840454\n",
      "Epoch 0, Batch 831, Loss: 1.803132176399231\n",
      "Epoch 0, Batch 832, Loss: 1.7763450145721436\n",
      "Epoch 0, Batch 833, Loss: 1.6875501871109009\n",
      "Epoch 0, Batch 834, Loss: 1.7606755495071411\n",
      "Epoch 0, Batch 835, Loss: 1.7041575908660889\n",
      "Epoch 0, Batch 836, Loss: 1.7753798961639404\n",
      "Epoch 0, Batch 837, Loss: 1.7330114841461182\n",
      "Epoch 0, Batch 838, Loss: 1.6639811992645264\n",
      "Epoch 0, Batch 839, Loss: 1.6513780355453491\n",
      "Epoch 0, Batch 840, Loss: 1.8092535734176636\n",
      "Epoch 0, Batch 841, Loss: 1.7104461193084717\n",
      "Epoch 0, Batch 842, Loss: 1.7071993350982666\n",
      "Epoch 0, Batch 843, Loss: 1.6910314559936523\n",
      "Epoch 0, Batch 844, Loss: 1.7021890878677368\n",
      "Epoch 0, Batch 845, Loss: 1.6783316135406494\n",
      "Epoch 0, Batch 846, Loss: 1.717810034751892\n",
      "Epoch 0, Batch 847, Loss: 1.7000141143798828\n",
      "Epoch 0, Batch 848, Loss: 1.7538893222808838\n",
      "Epoch 0, Batch 849, Loss: 1.7408820390701294\n",
      "Epoch 0, Batch 850, Loss: 1.7188533544540405\n",
      "Epoch 0, Batch 851, Loss: 1.7196842432022095\n",
      "Epoch 0, Batch 852, Loss: 1.6923516988754272\n",
      "Epoch 0, Batch 853, Loss: 1.63930082321167\n",
      "Epoch 0, Batch 854, Loss: 1.7178270816802979\n",
      "Epoch 0, Batch 855, Loss: 1.699254035949707\n",
      "Epoch 0, Batch 856, Loss: 1.71123206615448\n",
      "Epoch 0, Batch 857, Loss: 1.6850557327270508\n",
      "Epoch 0, Batch 858, Loss: 1.6851040124893188\n",
      "Epoch 0, Batch 859, Loss: 1.7583833932876587\n",
      "Epoch 0, Batch 860, Loss: 1.7145522832870483\n",
      "Epoch 0, Batch 861, Loss: 1.610581874847412\n",
      "Epoch 0, Batch 862, Loss: 1.6649870872497559\n",
      "Epoch 0, Batch 863, Loss: 1.6858406066894531\n",
      "Epoch 0, Batch 864, Loss: 1.6826764345169067\n",
      "Epoch 0, Batch 865, Loss: 1.6682521104812622\n",
      "Epoch 0, Batch 866, Loss: 1.7336305379867554\n",
      "Epoch 0, Batch 867, Loss: 1.6228671073913574\n",
      "Epoch 0, Batch 868, Loss: 1.7334043979644775\n",
      "Epoch 0, Batch 869, Loss: 1.725236177444458\n",
      "Epoch 0, Batch 870, Loss: 1.6853080987930298\n",
      "Epoch 0, Batch 871, Loss: 1.6523666381835938\n",
      "Epoch 0, Batch 872, Loss: 1.6342159509658813\n",
      "Epoch 0, Batch 873, Loss: 1.6080883741378784\n",
      "Epoch 0, Batch 874, Loss: 1.6433676481246948\n",
      "Epoch 0, Batch 875, Loss: 1.6168324947357178\n",
      "Epoch 0, Batch 876, Loss: 1.7752113342285156\n",
      "Epoch 0, Batch 877, Loss: 1.6910852193832397\n",
      "Epoch 0, Batch 878, Loss: 1.671176791191101\n",
      "Epoch 0, Batch 879, Loss: 1.6140755414962769\n",
      "Epoch 0, Batch 880, Loss: 1.7019999027252197\n",
      "Epoch 0, Batch 881, Loss: 1.6439855098724365\n",
      "Epoch 0, Batch 882, Loss: 1.6372312307357788\n",
      "Epoch 0, Batch 883, Loss: 1.6764941215515137\n",
      "Epoch 0, Batch 884, Loss: 1.671055555343628\n",
      "Epoch 0, Batch 885, Loss: 1.6766893863677979\n",
      "Epoch 0, Batch 886, Loss: 1.659858226776123\n",
      "Epoch 0, Batch 887, Loss: 1.6009438037872314\n",
      "Epoch 0, Batch 888, Loss: 1.6434539556503296\n",
      "Epoch 0, Batch 889, Loss: 1.6541779041290283\n",
      "Epoch 0, Batch 890, Loss: 1.6054058074951172\n",
      "Epoch 0, Batch 891, Loss: 1.765834093093872\n",
      "Epoch 0, Batch 892, Loss: 1.737439513206482\n",
      "Epoch 0, Batch 893, Loss: 1.6464200019836426\n",
      "Epoch 0, Batch 894, Loss: 1.6556189060211182\n",
      "Epoch 0, Batch 895, Loss: 1.6521762609481812\n",
      "Epoch 0, Batch 896, Loss: 1.5880968570709229\n",
      "Epoch 0, Batch 897, Loss: 1.672396183013916\n",
      "Epoch 0, Batch 898, Loss: 1.6426479816436768\n",
      "Epoch 0, Batch 899, Loss: 1.6527374982833862\n",
      "Epoch 0, Batch 900, Loss: 1.557100772857666\n",
      "Epoch 0, Batch 901, Loss: 1.597500205039978\n",
      "Epoch 0, Batch 902, Loss: 1.559557318687439\n",
      "Epoch 0, Batch 903, Loss: 1.5598763227462769\n",
      "Epoch 0, Batch 904, Loss: 1.6311724185943604\n",
      "Epoch 0, Batch 905, Loss: 1.60512113571167\n",
      "Epoch 0, Batch 906, Loss: 1.6170921325683594\n",
      "Epoch 0, Batch 907, Loss: 1.6460868120193481\n",
      "Epoch 0, Batch 908, Loss: 1.721733808517456\n",
      "Epoch 0, Batch 909, Loss: 1.5647361278533936\n",
      "Epoch 0, Batch 910, Loss: 1.5517659187316895\n",
      "Epoch 0, Batch 911, Loss: 1.773526668548584\n",
      "Epoch 0, Batch 912, Loss: 1.6274772882461548\n",
      "Epoch 0, Batch 913, Loss: 1.5891622304916382\n",
      "Epoch 0, Batch 914, Loss: 1.5904426574707031\n",
      "Epoch 0, Batch 915, Loss: 1.6023588180541992\n",
      "Epoch 0, Batch 916, Loss: 1.6369562149047852\n",
      "Epoch 0, Batch 917, Loss: 1.5977104902267456\n",
      "Epoch 0, Batch 918, Loss: 1.5776598453521729\n",
      "Epoch 0, Batch 919, Loss: 1.593731164932251\n",
      "Epoch 0, Batch 920, Loss: 1.7619855403900146\n",
      "Epoch 0, Batch 921, Loss: 1.6023181676864624\n",
      "Epoch 0, Batch 922, Loss: 1.5914607048034668\n",
      "Epoch 0, Batch 923, Loss: 1.6725369691848755\n",
      "Epoch 0, Batch 924, Loss: 1.6124287843704224\n",
      "Epoch 0, Batch 925, Loss: 1.6206188201904297\n",
      "Epoch 0, Batch 926, Loss: 1.5785411596298218\n",
      "Epoch 0, Batch 927, Loss: 1.6710957288742065\n",
      "Epoch 0, Batch 928, Loss: 1.593010663986206\n",
      "Epoch 0, Batch 929, Loss: 1.6315410137176514\n",
      "Epoch 0, Batch 930, Loss: 1.6450917720794678\n",
      "Epoch 0, Batch 931, Loss: 1.5732660293579102\n",
      "Epoch 0, Batch 932, Loss: 1.6517053842544556\n",
      "Epoch 0, Batch 933, Loss: 1.5999343395233154\n",
      "Epoch 0, Batch 934, Loss: 1.6048016548156738\n",
      "Epoch 0, Batch 935, Loss: 1.5947493314743042\n",
      "Epoch 0, Batch 936, Loss: 1.5750467777252197\n",
      "Epoch 0, Batch 937, Loss: 1.6230192184448242\n",
      "Epoch 0, Batch 938, Loss: 1.6693958044052124\n",
      "Accuracy of train set: 0.2696\n",
      "Epoch 0, Batch 1, Test Loss: 1.6045268774032593\n",
      "Epoch 0, Batch 2, Test Loss: 1.570285677909851\n",
      "Epoch 0, Batch 3, Test Loss: 1.514683485031128\n",
      "Epoch 0, Batch 4, Test Loss: 1.5858196020126343\n",
      "Epoch 0, Batch 5, Test Loss: 1.6262619495391846\n",
      "Epoch 0, Batch 6, Test Loss: 1.5325508117675781\n",
      "Epoch 0, Batch 7, Test Loss: 1.6941198110580444\n",
      "Epoch 0, Batch 8, Test Loss: 1.5499504804611206\n",
      "Epoch 0, Batch 9, Test Loss: 1.516048550605774\n",
      "Epoch 0, Batch 10, Test Loss: 1.596940517425537\n",
      "Epoch 0, Batch 11, Test Loss: 1.5441919565200806\n",
      "Epoch 0, Batch 12, Test Loss: 1.5774093866348267\n",
      "Epoch 0, Batch 13, Test Loss: 1.5368043184280396\n",
      "Epoch 0, Batch 14, Test Loss: 1.5600496530532837\n",
      "Epoch 0, Batch 15, Test Loss: 1.6002053022384644\n",
      "Epoch 0, Batch 16, Test Loss: 1.5315324068069458\n",
      "Epoch 0, Batch 17, Test Loss: 1.6150169372558594\n",
      "Epoch 0, Batch 18, Test Loss: 1.708787202835083\n",
      "Epoch 0, Batch 19, Test Loss: 1.5865757465362549\n",
      "Epoch 0, Batch 20, Test Loss: 1.5594736337661743\n",
      "Epoch 0, Batch 21, Test Loss: 1.591505765914917\n",
      "Epoch 0, Batch 22, Test Loss: 1.6098417043685913\n",
      "Epoch 0, Batch 23, Test Loss: 1.6677138805389404\n",
      "Epoch 0, Batch 24, Test Loss: 1.5585076808929443\n",
      "Epoch 0, Batch 25, Test Loss: 1.629696011543274\n",
      "Epoch 0, Batch 26, Test Loss: 1.6431183815002441\n",
      "Epoch 0, Batch 27, Test Loss: 1.602034091949463\n",
      "Epoch 0, Batch 28, Test Loss: 1.5332106351852417\n",
      "Epoch 0, Batch 29, Test Loss: 1.5250126123428345\n",
      "Epoch 0, Batch 30, Test Loss: 1.5603853464126587\n",
      "Epoch 0, Batch 31, Test Loss: 1.622355341911316\n",
      "Epoch 0, Batch 32, Test Loss: 1.6068921089172363\n",
      "Epoch 0, Batch 33, Test Loss: 1.671688437461853\n",
      "Epoch 0, Batch 34, Test Loss: 1.541452407836914\n",
      "Epoch 0, Batch 35, Test Loss: 1.6204627752304077\n",
      "Epoch 0, Batch 36, Test Loss: 1.5819295644760132\n",
      "Epoch 0, Batch 37, Test Loss: 1.5003026723861694\n",
      "Epoch 0, Batch 38, Test Loss: 1.586897611618042\n",
      "Epoch 0, Batch 39, Test Loss: 1.6331787109375\n",
      "Epoch 0, Batch 40, Test Loss: 1.5702812671661377\n",
      "Epoch 0, Batch 41, Test Loss: 1.5990458726882935\n",
      "Epoch 0, Batch 42, Test Loss: 1.558375358581543\n",
      "Epoch 0, Batch 43, Test Loss: 1.6305729150772095\n",
      "Epoch 0, Batch 44, Test Loss: 1.6525641679763794\n",
      "Epoch 0, Batch 45, Test Loss: 1.6185905933380127\n",
      "Epoch 0, Batch 46, Test Loss: 1.6458462476730347\n",
      "Epoch 0, Batch 47, Test Loss: 1.5996620655059814\n",
      "Epoch 0, Batch 48, Test Loss: 1.5785691738128662\n",
      "Epoch 0, Batch 49, Test Loss: 1.7126648426055908\n",
      "Epoch 0, Batch 50, Test Loss: 1.6107330322265625\n",
      "Epoch 0, Batch 51, Test Loss: 1.5656274557113647\n",
      "Epoch 0, Batch 52, Test Loss: 1.580372929573059\n",
      "Epoch 0, Batch 53, Test Loss: 1.4818629026412964\n",
      "Epoch 0, Batch 54, Test Loss: 1.513045310974121\n",
      "Epoch 0, Batch 55, Test Loss: 1.680641770362854\n",
      "Epoch 0, Batch 56, Test Loss: 1.6181131601333618\n",
      "Epoch 0, Batch 57, Test Loss: 1.498947262763977\n",
      "Epoch 0, Batch 58, Test Loss: 1.5289360284805298\n",
      "Epoch 0, Batch 59, Test Loss: 1.5972005128860474\n",
      "Epoch 0, Batch 60, Test Loss: 1.5543651580810547\n",
      "Epoch 0, Batch 61, Test Loss: 1.6225683689117432\n",
      "Epoch 0, Batch 62, Test Loss: 1.5984097719192505\n",
      "Epoch 0, Batch 63, Test Loss: 1.6407541036605835\n",
      "Epoch 0, Batch 64, Test Loss: 1.5773180723190308\n",
      "Epoch 0, Batch 65, Test Loss: 1.4991655349731445\n",
      "Epoch 0, Batch 66, Test Loss: 1.6062313318252563\n",
      "Epoch 0, Batch 67, Test Loss: 1.5696488618850708\n",
      "Epoch 0, Batch 68, Test Loss: 1.6217647790908813\n",
      "Epoch 0, Batch 69, Test Loss: 1.5714406967163086\n",
      "Epoch 0, Batch 70, Test Loss: 1.5386565923690796\n",
      "Epoch 0, Batch 71, Test Loss: 1.5885324478149414\n",
      "Epoch 0, Batch 72, Test Loss: 1.628924012184143\n",
      "Epoch 0, Batch 73, Test Loss: 1.5401228666305542\n",
      "Epoch 0, Batch 74, Test Loss: 1.5996992588043213\n",
      "Epoch 0, Batch 75, Test Loss: 1.6352872848510742\n",
      "Epoch 0, Batch 76, Test Loss: 1.5696907043457031\n",
      "Epoch 0, Batch 77, Test Loss: 1.5255460739135742\n",
      "Epoch 0, Batch 78, Test Loss: 1.6074681282043457\n",
      "Epoch 0, Batch 79, Test Loss: 1.6822288036346436\n",
      "Epoch 0, Batch 80, Test Loss: 1.6191015243530273\n",
      "Epoch 0, Batch 81, Test Loss: 1.499800443649292\n",
      "Epoch 0, Batch 82, Test Loss: 1.5564656257629395\n",
      "Epoch 0, Batch 83, Test Loss: 1.4968341588974\n",
      "Epoch 0, Batch 84, Test Loss: 1.512284517288208\n",
      "Epoch 0, Batch 85, Test Loss: 1.6494094133377075\n",
      "Epoch 0, Batch 86, Test Loss: 1.5534306764602661\n",
      "Epoch 0, Batch 87, Test Loss: 1.6278197765350342\n",
      "Epoch 0, Batch 88, Test Loss: 1.6804667711257935\n",
      "Epoch 0, Batch 89, Test Loss: 1.6294214725494385\n",
      "Epoch 0, Batch 90, Test Loss: 1.5669758319854736\n",
      "Epoch 0, Batch 91, Test Loss: 1.5924992561340332\n",
      "Epoch 0, Batch 92, Test Loss: 1.6220678091049194\n",
      "Epoch 0, Batch 93, Test Loss: 1.6383036375045776\n",
      "Epoch 0, Batch 94, Test Loss: 1.6125895977020264\n",
      "Epoch 0, Batch 95, Test Loss: 1.54167902469635\n",
      "Epoch 0, Batch 96, Test Loss: 1.5694551467895508\n",
      "Epoch 0, Batch 97, Test Loss: 1.5761688947677612\n",
      "Epoch 0, Batch 98, Test Loss: 1.5478315353393555\n",
      "Epoch 0, Batch 99, Test Loss: 1.6072957515716553\n",
      "Epoch 0, Batch 100, Test Loss: 1.5568122863769531\n",
      "Epoch 0, Batch 101, Test Loss: 1.556420087814331\n",
      "Epoch 0, Batch 102, Test Loss: 1.557544469833374\n",
      "Epoch 0, Batch 103, Test Loss: 1.5583240985870361\n",
      "Epoch 0, Batch 104, Test Loss: 1.6005417108535767\n",
      "Epoch 0, Batch 105, Test Loss: 1.6043435335159302\n",
      "Epoch 0, Batch 106, Test Loss: 1.5784989595413208\n",
      "Epoch 0, Batch 107, Test Loss: 1.592408537864685\n",
      "Epoch 0, Batch 108, Test Loss: 1.556970238685608\n",
      "Epoch 0, Batch 109, Test Loss: 1.5839812755584717\n",
      "Epoch 0, Batch 110, Test Loss: 1.5614346265792847\n",
      "Epoch 0, Batch 111, Test Loss: 1.5607961416244507\n",
      "Epoch 0, Batch 112, Test Loss: 1.5602377653121948\n",
      "Epoch 0, Batch 113, Test Loss: 1.5723921060562134\n",
      "Epoch 0, Batch 114, Test Loss: 1.6544228792190552\n",
      "Epoch 0, Batch 115, Test Loss: 1.553548812866211\n",
      "Epoch 0, Batch 116, Test Loss: 1.56488037109375\n",
      "Epoch 0, Batch 117, Test Loss: 1.543485403060913\n",
      "Epoch 0, Batch 118, Test Loss: 1.6352168321609497\n",
      "Epoch 0, Batch 119, Test Loss: 1.5512094497680664\n",
      "Epoch 0, Batch 120, Test Loss: 1.6492897272109985\n",
      "Epoch 0, Batch 121, Test Loss: 1.555307388305664\n",
      "Epoch 0, Batch 122, Test Loss: 1.5641415119171143\n",
      "Epoch 0, Batch 123, Test Loss: 1.5967222452163696\n",
      "Epoch 0, Batch 124, Test Loss: 1.5617462396621704\n",
      "Epoch 0, Batch 125, Test Loss: 1.6563231945037842\n",
      "Epoch 0, Batch 126, Test Loss: 1.6164528131484985\n",
      "Epoch 0, Batch 127, Test Loss: 1.575038194656372\n",
      "Epoch 0, Batch 128, Test Loss: 1.57087242603302\n",
      "Epoch 0, Batch 129, Test Loss: 1.5790287256240845\n",
      "Epoch 0, Batch 130, Test Loss: 1.5789343118667603\n",
      "Epoch 0, Batch 131, Test Loss: 1.5950833559036255\n",
      "Epoch 0, Batch 132, Test Loss: 1.5699352025985718\n",
      "Epoch 0, Batch 133, Test Loss: 1.5497914552688599\n",
      "Epoch 0, Batch 134, Test Loss: 1.6025192737579346\n",
      "Epoch 0, Batch 135, Test Loss: 1.6723153591156006\n",
      "Epoch 0, Batch 136, Test Loss: 1.5866552591323853\n",
      "Epoch 0, Batch 137, Test Loss: 1.5562211275100708\n",
      "Epoch 0, Batch 138, Test Loss: 1.6303194761276245\n",
      "Epoch 0, Batch 139, Test Loss: 1.5738980770111084\n",
      "Epoch 0, Batch 140, Test Loss: 1.5909749269485474\n",
      "Epoch 0, Batch 141, Test Loss: 1.6428368091583252\n",
      "Epoch 0, Batch 142, Test Loss: 1.502414584159851\n",
      "Epoch 0, Batch 143, Test Loss: 1.6548460721969604\n",
      "Epoch 0, Batch 144, Test Loss: 1.5537996292114258\n",
      "Epoch 0, Batch 145, Test Loss: 1.6097466945648193\n",
      "Epoch 0, Batch 146, Test Loss: 1.577121376991272\n",
      "Epoch 0, Batch 147, Test Loss: 1.5894675254821777\n",
      "Epoch 0, Batch 148, Test Loss: 1.6030539274215698\n",
      "Epoch 0, Batch 149, Test Loss: 1.6199979782104492\n",
      "Epoch 0, Batch 150, Test Loss: 1.6195454597473145\n",
      "Epoch 0, Batch 151, Test Loss: 1.5044996738433838\n",
      "Epoch 0, Batch 152, Test Loss: 1.525564193725586\n",
      "Epoch 0, Batch 153, Test Loss: 1.5830343961715698\n",
      "Epoch 0, Batch 154, Test Loss: 1.5469982624053955\n",
      "Epoch 0, Batch 155, Test Loss: 1.561041235923767\n",
      "Epoch 0, Batch 156, Test Loss: 1.5916154384613037\n",
      "Epoch 0, Batch 157, Test Loss: 1.5663931369781494\n",
      "Epoch 0, Batch 158, Test Loss: 1.5315147638320923\n",
      "Epoch 0, Batch 159, Test Loss: 1.5356322526931763\n",
      "Epoch 0, Batch 160, Test Loss: 1.5892822742462158\n",
      "Epoch 0, Batch 161, Test Loss: 1.582904577255249\n",
      "Epoch 0, Batch 162, Test Loss: 1.631954312324524\n",
      "Epoch 0, Batch 163, Test Loss: 1.5524946451187134\n",
      "Epoch 0, Batch 164, Test Loss: 1.6114751100540161\n",
      "Epoch 0, Batch 165, Test Loss: 1.5597044229507446\n",
      "Epoch 0, Batch 166, Test Loss: 1.5821443796157837\n",
      "Epoch 0, Batch 167, Test Loss: 1.6391897201538086\n",
      "Epoch 0, Batch 168, Test Loss: 1.573848009109497\n",
      "Epoch 0, Batch 169, Test Loss: 1.566054105758667\n",
      "Epoch 0, Batch 170, Test Loss: 1.620416283607483\n",
      "Epoch 0, Batch 171, Test Loss: 1.6251720190048218\n",
      "Epoch 0, Batch 172, Test Loss: 1.6327555179595947\n",
      "Epoch 0, Batch 173, Test Loss: 1.5421419143676758\n",
      "Epoch 0, Batch 174, Test Loss: 1.5818760395050049\n",
      "Epoch 0, Batch 175, Test Loss: 1.5604459047317505\n",
      "Epoch 0, Batch 176, Test Loss: 1.5692031383514404\n",
      "Epoch 0, Batch 177, Test Loss: 1.5619701147079468\n",
      "Epoch 0, Batch 178, Test Loss: 1.6269181966781616\n",
      "Epoch 0, Batch 179, Test Loss: 1.629554271697998\n",
      "Epoch 0, Batch 180, Test Loss: 1.5246915817260742\n",
      "Epoch 0, Batch 181, Test Loss: 1.566131591796875\n",
      "Epoch 0, Batch 182, Test Loss: 1.5975334644317627\n",
      "Epoch 0, Batch 183, Test Loss: 1.5214331150054932\n",
      "Epoch 0, Batch 184, Test Loss: 1.5454597473144531\n",
      "Epoch 0, Batch 185, Test Loss: 1.5382922887802124\n",
      "Epoch 0, Batch 186, Test Loss: 1.5430854558944702\n",
      "Epoch 0, Batch 187, Test Loss: 1.6251140832901\n",
      "Epoch 0, Batch 188, Test Loss: 1.563259482383728\n",
      "Epoch 0, Batch 189, Test Loss: 1.60039222240448\n",
      "Epoch 0, Batch 190, Test Loss: 1.5861713886260986\n",
      "Epoch 0, Batch 191, Test Loss: 1.5883393287658691\n",
      "Epoch 0, Batch 192, Test Loss: 1.5542352199554443\n",
      "Epoch 0, Batch 193, Test Loss: 1.57004714012146\n",
      "Epoch 0, Batch 194, Test Loss: 1.5954372882843018\n",
      "Epoch 0, Batch 195, Test Loss: 1.514782190322876\n",
      "Epoch 0, Batch 196, Test Loss: 1.580208659172058\n",
      "Epoch 0, Batch 197, Test Loss: 1.600825309753418\n",
      "Epoch 0, Batch 198, Test Loss: 1.5819824934005737\n",
      "Epoch 0, Batch 199, Test Loss: 1.5861819982528687\n",
      "Epoch 0, Batch 200, Test Loss: 1.5294601917266846\n",
      "Epoch 0, Batch 201, Test Loss: 1.6145350933074951\n",
      "Epoch 0, Batch 202, Test Loss: 1.6361570358276367\n",
      "Epoch 0, Batch 203, Test Loss: 1.5912058353424072\n",
      "Epoch 0, Batch 204, Test Loss: 1.5900299549102783\n",
      "Epoch 0, Batch 205, Test Loss: 1.6630747318267822\n",
      "Epoch 0, Batch 206, Test Loss: 1.5684802532196045\n",
      "Epoch 0, Batch 207, Test Loss: 1.5147663354873657\n",
      "Epoch 0, Batch 208, Test Loss: 1.6141438484191895\n",
      "Epoch 0, Batch 209, Test Loss: 1.6058834791183472\n",
      "Epoch 0, Batch 210, Test Loss: 1.5343793630599976\n",
      "Epoch 0, Batch 211, Test Loss: 1.644657015800476\n",
      "Epoch 0, Batch 212, Test Loss: 1.6051671504974365\n",
      "Epoch 0, Batch 213, Test Loss: 1.6444809436798096\n",
      "Epoch 0, Batch 214, Test Loss: 1.5225180387496948\n",
      "Epoch 0, Batch 215, Test Loss: 1.5932731628417969\n",
      "Epoch 0, Batch 216, Test Loss: 1.5405919551849365\n",
      "Epoch 0, Batch 217, Test Loss: 1.5817148685455322\n",
      "Epoch 0, Batch 218, Test Loss: 1.5418922901153564\n",
      "Epoch 0, Batch 219, Test Loss: 1.5731180906295776\n",
      "Epoch 0, Batch 220, Test Loss: 1.613390564918518\n",
      "Epoch 0, Batch 221, Test Loss: 1.5656839609146118\n",
      "Epoch 0, Batch 222, Test Loss: 1.5692058801651\n",
      "Epoch 0, Batch 223, Test Loss: 1.5361080169677734\n",
      "Epoch 0, Batch 224, Test Loss: 1.5533435344696045\n",
      "Epoch 0, Batch 225, Test Loss: 1.6392343044281006\n",
      "Epoch 0, Batch 226, Test Loss: 1.5736064910888672\n",
      "Epoch 0, Batch 227, Test Loss: 1.6815621852874756\n",
      "Epoch 0, Batch 228, Test Loss: 1.6715986728668213\n",
      "Epoch 0, Batch 229, Test Loss: 1.5491888523101807\n",
      "Epoch 0, Batch 230, Test Loss: 1.5642417669296265\n",
      "Epoch 0, Batch 231, Test Loss: 1.5490444898605347\n",
      "Epoch 0, Batch 232, Test Loss: 1.4672183990478516\n",
      "Epoch 0, Batch 233, Test Loss: 1.5578768253326416\n",
      "Epoch 0, Batch 234, Test Loss: 1.5772024393081665\n",
      "Epoch 0, Batch 235, Test Loss: 1.5474796295166016\n",
      "Epoch 0, Batch 236, Test Loss: 1.5515084266662598\n",
      "Epoch 0, Batch 237, Test Loss: 1.5243940353393555\n",
      "Epoch 0, Batch 238, Test Loss: 1.499282956123352\n",
      "Epoch 0, Batch 239, Test Loss: 1.563615322113037\n",
      "Epoch 0, Batch 240, Test Loss: 1.5519847869873047\n",
      "Epoch 0, Batch 241, Test Loss: 1.5951037406921387\n",
      "Epoch 0, Batch 242, Test Loss: 1.606126308441162\n",
      "Epoch 0, Batch 243, Test Loss: 1.59346342086792\n",
      "Epoch 0, Batch 244, Test Loss: 1.6246612071990967\n",
      "Epoch 0, Batch 245, Test Loss: 1.6051743030548096\n",
      "Epoch 0, Batch 246, Test Loss: 1.5681551694869995\n",
      "Epoch 0, Batch 247, Test Loss: 1.5413925647735596\n",
      "Epoch 0, Batch 248, Test Loss: 1.552276611328125\n",
      "Epoch 0, Batch 249, Test Loss: 1.5397582054138184\n",
      "Epoch 0, Batch 250, Test Loss: 1.620395541191101\n",
      "Epoch 0, Batch 251, Test Loss: 1.6475732326507568\n",
      "Epoch 0, Batch 252, Test Loss: 1.5990604162216187\n",
      "Epoch 0, Batch 253, Test Loss: 1.5391018390655518\n",
      "Epoch 0, Batch 254, Test Loss: 1.5657306909561157\n",
      "Epoch 0, Batch 255, Test Loss: 1.5455632209777832\n",
      "Epoch 0, Batch 256, Test Loss: 1.5630007982254028\n",
      "Epoch 0, Batch 257, Test Loss: 1.5365386009216309\n",
      "Epoch 0, Batch 258, Test Loss: 1.5480045080184937\n",
      "Epoch 0, Batch 259, Test Loss: 1.5268837213516235\n",
      "Epoch 0, Batch 260, Test Loss: 1.657045841217041\n",
      "Epoch 0, Batch 261, Test Loss: 1.6375198364257812\n",
      "Epoch 0, Batch 262, Test Loss: 1.5602144002914429\n",
      "Epoch 0, Batch 263, Test Loss: 1.592998743057251\n",
      "Epoch 0, Batch 264, Test Loss: 1.5804524421691895\n",
      "Epoch 0, Batch 265, Test Loss: 1.573982834815979\n",
      "Epoch 0, Batch 266, Test Loss: 1.5849967002868652\n",
      "Epoch 0, Batch 267, Test Loss: 1.507265329360962\n",
      "Epoch 0, Batch 268, Test Loss: 1.5390077829360962\n",
      "Epoch 0, Batch 269, Test Loss: 1.6264904737472534\n",
      "Epoch 0, Batch 270, Test Loss: 1.526374101638794\n",
      "Epoch 0, Batch 271, Test Loss: 1.6518374681472778\n",
      "Epoch 0, Batch 272, Test Loss: 1.6283200979232788\n",
      "Epoch 0, Batch 273, Test Loss: 1.5790327787399292\n",
      "Epoch 0, Batch 274, Test Loss: 1.6591356992721558\n",
      "Epoch 0, Batch 275, Test Loss: 1.7021539211273193\n",
      "Epoch 0, Batch 276, Test Loss: 1.6803381443023682\n",
      "Epoch 0, Batch 277, Test Loss: 1.5062766075134277\n",
      "Epoch 0, Batch 278, Test Loss: 1.67314612865448\n",
      "Epoch 0, Batch 279, Test Loss: 1.6484977006912231\n",
      "Epoch 0, Batch 280, Test Loss: 1.549964427947998\n",
      "Epoch 0, Batch 281, Test Loss: 1.5877001285552979\n",
      "Epoch 0, Batch 282, Test Loss: 1.6150343418121338\n",
      "Epoch 0, Batch 283, Test Loss: 1.601426362991333\n",
      "Epoch 0, Batch 284, Test Loss: 1.4794423580169678\n",
      "Epoch 0, Batch 285, Test Loss: 1.638312578201294\n",
      "Epoch 0, Batch 286, Test Loss: 1.6317914724349976\n",
      "Epoch 0, Batch 287, Test Loss: 1.5624032020568848\n",
      "Epoch 0, Batch 288, Test Loss: 1.6370209455490112\n",
      "Epoch 0, Batch 289, Test Loss: 1.6192071437835693\n",
      "Epoch 0, Batch 290, Test Loss: 1.649017095565796\n",
      "Epoch 0, Batch 291, Test Loss: 1.628859281539917\n",
      "Epoch 0, Batch 292, Test Loss: 1.493104338645935\n",
      "Epoch 0, Batch 293, Test Loss: 1.5339617729187012\n",
      "Epoch 0, Batch 294, Test Loss: 1.523197889328003\n",
      "Epoch 0, Batch 295, Test Loss: 1.538945198059082\n",
      "Epoch 0, Batch 296, Test Loss: 1.6326357126235962\n",
      "Epoch 0, Batch 297, Test Loss: 1.5811690092086792\n",
      "Epoch 0, Batch 298, Test Loss: 1.6540461778640747\n",
      "Epoch 0, Batch 299, Test Loss: 1.5728849172592163\n",
      "Epoch 0, Batch 300, Test Loss: 1.562362790107727\n",
      "Epoch 0, Batch 301, Test Loss: 1.631616473197937\n",
      "Epoch 0, Batch 302, Test Loss: 1.5589115619659424\n",
      "Epoch 0, Batch 303, Test Loss: 1.560494303703308\n",
      "Epoch 0, Batch 304, Test Loss: 1.5828906297683716\n",
      "Epoch 0, Batch 305, Test Loss: 1.5687845945358276\n",
      "Epoch 0, Batch 306, Test Loss: 1.620664119720459\n",
      "Epoch 0, Batch 307, Test Loss: 1.5825438499450684\n",
      "Epoch 0, Batch 308, Test Loss: 1.5646450519561768\n",
      "Epoch 0, Batch 309, Test Loss: 1.6468989849090576\n",
      "Epoch 0, Batch 310, Test Loss: 1.6021181344985962\n",
      "Epoch 0, Batch 311, Test Loss: 1.6069319248199463\n",
      "Epoch 0, Batch 312, Test Loss: 1.5343635082244873\n",
      "Epoch 0, Batch 313, Test Loss: 1.6276947259902954\n",
      "Epoch 0, Batch 314, Test Loss: 1.5894455909729004\n",
      "Epoch 0, Batch 315, Test Loss: 1.5650460720062256\n",
      "Epoch 0, Batch 316, Test Loss: 1.5962069034576416\n",
      "Epoch 0, Batch 317, Test Loss: 1.5948987007141113\n",
      "Epoch 0, Batch 318, Test Loss: 1.6486296653747559\n",
      "Epoch 0, Batch 319, Test Loss: 1.6026712656021118\n",
      "Epoch 0, Batch 320, Test Loss: 1.5726146697998047\n",
      "Epoch 0, Batch 321, Test Loss: 1.685685634613037\n",
      "Epoch 0, Batch 322, Test Loss: 1.5825001001358032\n",
      "Epoch 0, Batch 323, Test Loss: 1.5751090049743652\n",
      "Epoch 0, Batch 324, Test Loss: 1.5778648853302002\n",
      "Epoch 0, Batch 325, Test Loss: 1.6501518487930298\n",
      "Epoch 0, Batch 326, Test Loss: 1.478777289390564\n",
      "Epoch 0, Batch 327, Test Loss: 1.4804131984710693\n",
      "Epoch 0, Batch 328, Test Loss: 1.5771243572235107\n",
      "Epoch 0, Batch 329, Test Loss: 1.6293745040893555\n",
      "Epoch 0, Batch 330, Test Loss: 1.634373426437378\n",
      "Epoch 0, Batch 331, Test Loss: 1.5722620487213135\n",
      "Epoch 0, Batch 332, Test Loss: 1.5573813915252686\n",
      "Epoch 0, Batch 333, Test Loss: 1.5893712043762207\n",
      "Epoch 0, Batch 334, Test Loss: 1.608947515487671\n",
      "Epoch 0, Batch 335, Test Loss: 1.6903164386749268\n",
      "Epoch 0, Batch 336, Test Loss: 1.6623961925506592\n",
      "Epoch 0, Batch 337, Test Loss: 1.5766818523406982\n",
      "Epoch 0, Batch 338, Test Loss: 1.5745508670806885\n",
      "Epoch 0, Batch 339, Test Loss: 1.5820435285568237\n",
      "Epoch 0, Batch 340, Test Loss: 1.5760000944137573\n",
      "Epoch 0, Batch 341, Test Loss: 1.5117357969284058\n",
      "Epoch 0, Batch 342, Test Loss: 1.6475826501846313\n",
      "Epoch 0, Batch 343, Test Loss: 1.5623279809951782\n",
      "Epoch 0, Batch 344, Test Loss: 1.6060714721679688\n",
      "Epoch 0, Batch 345, Test Loss: 1.6075365543365479\n",
      "Epoch 0, Batch 346, Test Loss: 1.5528433322906494\n",
      "Epoch 0, Batch 347, Test Loss: 1.5253839492797852\n",
      "Epoch 0, Batch 348, Test Loss: 1.5255783796310425\n",
      "Epoch 0, Batch 349, Test Loss: 1.615141749382019\n",
      "Epoch 0, Batch 350, Test Loss: 1.6305400133132935\n",
      "Epoch 0, Batch 351, Test Loss: 1.4894139766693115\n",
      "Epoch 0, Batch 352, Test Loss: 1.7091809511184692\n",
      "Epoch 0, Batch 353, Test Loss: 1.5882009267807007\n",
      "Epoch 0, Batch 354, Test Loss: 1.5790351629257202\n",
      "Epoch 0, Batch 355, Test Loss: 1.5362985134124756\n",
      "Epoch 0, Batch 356, Test Loss: 1.600174069404602\n",
      "Epoch 0, Batch 357, Test Loss: 1.6060516834259033\n",
      "Epoch 0, Batch 358, Test Loss: 1.560355305671692\n",
      "Epoch 0, Batch 359, Test Loss: 1.7117069959640503\n",
      "Epoch 0, Batch 360, Test Loss: 1.6276707649230957\n",
      "Epoch 0, Batch 361, Test Loss: 1.6080909967422485\n",
      "Epoch 0, Batch 362, Test Loss: 1.5488462448120117\n",
      "Epoch 0, Batch 363, Test Loss: 1.513551950454712\n",
      "Epoch 0, Batch 364, Test Loss: 1.5494844913482666\n",
      "Epoch 0, Batch 365, Test Loss: 1.5950387716293335\n",
      "Epoch 0, Batch 366, Test Loss: 1.5963155031204224\n",
      "Epoch 0, Batch 367, Test Loss: 1.5638291835784912\n",
      "Epoch 0, Batch 368, Test Loss: 1.521645188331604\n",
      "Epoch 0, Batch 369, Test Loss: 1.5107624530792236\n",
      "Epoch 0, Batch 370, Test Loss: 1.5569552183151245\n",
      "Epoch 0, Batch 371, Test Loss: 1.601467251777649\n",
      "Epoch 0, Batch 372, Test Loss: 1.5582339763641357\n",
      "Epoch 0, Batch 373, Test Loss: 1.5811296701431274\n",
      "Epoch 0, Batch 374, Test Loss: 1.6236512660980225\n",
      "Epoch 0, Batch 375, Test Loss: 1.5616954565048218\n",
      "Epoch 0, Batch 376, Test Loss: 1.6285736560821533\n",
      "Epoch 0, Batch 377, Test Loss: 1.604188323020935\n",
      "Epoch 0, Batch 378, Test Loss: 1.586352825164795\n",
      "Epoch 0, Batch 379, Test Loss: 1.5615293979644775\n",
      "Epoch 0, Batch 380, Test Loss: 1.5228116512298584\n",
      "Epoch 0, Batch 381, Test Loss: 1.587525486946106\n",
      "Epoch 0, Batch 382, Test Loss: 1.5415611267089844\n",
      "Epoch 0, Batch 383, Test Loss: 1.5526657104492188\n",
      "Epoch 0, Batch 384, Test Loss: 1.535427212715149\n",
      "Epoch 0, Batch 385, Test Loss: 1.559688925743103\n",
      "Epoch 0, Batch 386, Test Loss: 1.5816630125045776\n",
      "Epoch 0, Batch 387, Test Loss: 1.5581729412078857\n",
      "Epoch 0, Batch 388, Test Loss: 1.581449270248413\n",
      "Epoch 0, Batch 389, Test Loss: 1.5010218620300293\n",
      "Epoch 0, Batch 390, Test Loss: 1.5758910179138184\n",
      "Epoch 0, Batch 391, Test Loss: 1.5930434465408325\n",
      "Epoch 0, Batch 392, Test Loss: 1.5685184001922607\n",
      "Epoch 0, Batch 393, Test Loss: 1.5931832790374756\n",
      "Epoch 0, Batch 394, Test Loss: 1.5861897468566895\n",
      "Epoch 0, Batch 395, Test Loss: 1.6338952779769897\n",
      "Epoch 0, Batch 396, Test Loss: 1.538597583770752\n",
      "Epoch 0, Batch 397, Test Loss: 1.6623895168304443\n",
      "Epoch 0, Batch 398, Test Loss: 1.6177830696105957\n",
      "Epoch 0, Batch 399, Test Loss: 1.5788929462432861\n",
      "Epoch 0, Batch 400, Test Loss: 1.6105077266693115\n",
      "Epoch 0, Batch 401, Test Loss: 1.5145189762115479\n",
      "Epoch 0, Batch 402, Test Loss: 1.557651400566101\n",
      "Epoch 0, Batch 403, Test Loss: 1.5799752473831177\n",
      "Epoch 0, Batch 404, Test Loss: 1.5355687141418457\n",
      "Epoch 0, Batch 405, Test Loss: 1.6169893741607666\n",
      "Epoch 0, Batch 406, Test Loss: 1.5963126420974731\n",
      "Epoch 0, Batch 407, Test Loss: 1.5926872491836548\n",
      "Epoch 0, Batch 408, Test Loss: 1.5759936571121216\n",
      "Epoch 0, Batch 409, Test Loss: 1.5511376857757568\n",
      "Epoch 0, Batch 410, Test Loss: 1.5712400674819946\n",
      "Epoch 0, Batch 411, Test Loss: 1.5917437076568604\n",
      "Epoch 0, Batch 412, Test Loss: 1.5590126514434814\n",
      "Epoch 0, Batch 413, Test Loss: 1.6143906116485596\n",
      "Epoch 0, Batch 414, Test Loss: 1.6352933645248413\n",
      "Epoch 0, Batch 415, Test Loss: 1.6084972620010376\n",
      "Epoch 0, Batch 416, Test Loss: 1.5770162343978882\n",
      "Epoch 0, Batch 417, Test Loss: 1.534867525100708\n",
      "Epoch 0, Batch 418, Test Loss: 1.680875539779663\n",
      "Epoch 0, Batch 419, Test Loss: 1.5719208717346191\n",
      "Epoch 0, Batch 420, Test Loss: 1.5980606079101562\n",
      "Epoch 0, Batch 421, Test Loss: 1.5836511850357056\n",
      "Epoch 0, Batch 422, Test Loss: 1.590369701385498\n",
      "Epoch 0, Batch 423, Test Loss: 1.5889670848846436\n",
      "Epoch 0, Batch 424, Test Loss: 1.580460786819458\n",
      "Epoch 0, Batch 425, Test Loss: 1.5866076946258545\n",
      "Epoch 0, Batch 426, Test Loss: 1.6683971881866455\n",
      "Epoch 0, Batch 427, Test Loss: 1.5587702989578247\n",
      "Epoch 0, Batch 428, Test Loss: 1.6429028511047363\n",
      "Epoch 0, Batch 429, Test Loss: 1.602972149848938\n",
      "Epoch 0, Batch 430, Test Loss: 1.5968891382217407\n",
      "Epoch 0, Batch 431, Test Loss: 1.542840600013733\n",
      "Epoch 0, Batch 432, Test Loss: 1.6021864414215088\n",
      "Epoch 0, Batch 433, Test Loss: 1.5403162240982056\n",
      "Epoch 0, Batch 434, Test Loss: 1.5762104988098145\n",
      "Epoch 0, Batch 435, Test Loss: 1.6546564102172852\n",
      "Epoch 0, Batch 436, Test Loss: 1.6360304355621338\n",
      "Epoch 0, Batch 437, Test Loss: 1.6129674911499023\n",
      "Epoch 0, Batch 438, Test Loss: 1.592860460281372\n",
      "Epoch 0, Batch 439, Test Loss: 1.5557405948638916\n",
      "Epoch 0, Batch 440, Test Loss: 1.5756645202636719\n",
      "Epoch 0, Batch 441, Test Loss: 1.5409457683563232\n",
      "Epoch 0, Batch 442, Test Loss: 1.5578043460845947\n",
      "Epoch 0, Batch 443, Test Loss: 1.5975217819213867\n",
      "Epoch 0, Batch 444, Test Loss: 1.5474517345428467\n",
      "Epoch 0, Batch 445, Test Loss: 1.615985631942749\n",
      "Epoch 0, Batch 446, Test Loss: 1.719765543937683\n",
      "Epoch 0, Batch 447, Test Loss: 1.5965015888214111\n",
      "Epoch 0, Batch 448, Test Loss: 1.6553454399108887\n",
      "Epoch 0, Batch 449, Test Loss: 1.5617326498031616\n",
      "Epoch 0, Batch 450, Test Loss: 1.4410700798034668\n",
      "Epoch 0, Batch 451, Test Loss: 1.6307756900787354\n",
      "Epoch 0, Batch 452, Test Loss: 1.437998652458191\n",
      "Epoch 0, Batch 453, Test Loss: 1.6532090902328491\n",
      "Epoch 0, Batch 454, Test Loss: 1.550605297088623\n",
      "Epoch 0, Batch 455, Test Loss: 1.6060395240783691\n",
      "Epoch 0, Batch 456, Test Loss: 1.6103307008743286\n",
      "Epoch 0, Batch 457, Test Loss: 1.550955057144165\n",
      "Epoch 0, Batch 458, Test Loss: 1.6009061336517334\n",
      "Epoch 0, Batch 459, Test Loss: 1.6037317514419556\n",
      "Epoch 0, Batch 460, Test Loss: 1.4556628465652466\n",
      "Epoch 0, Batch 461, Test Loss: 1.5897458791732788\n",
      "Epoch 0, Batch 462, Test Loss: 1.5614416599273682\n",
      "Epoch 0, Batch 463, Test Loss: 1.6501331329345703\n",
      "Epoch 0, Batch 464, Test Loss: 1.5817657709121704\n",
      "Epoch 0, Batch 465, Test Loss: 1.5359641313552856\n",
      "Epoch 0, Batch 466, Test Loss: 1.5534257888793945\n",
      "Epoch 0, Batch 467, Test Loss: 1.54965341091156\n",
      "Epoch 0, Batch 468, Test Loss: 1.6230566501617432\n",
      "Epoch 0, Batch 469, Test Loss: 1.448168158531189\n",
      "Epoch 0, Batch 470, Test Loss: 1.5738334655761719\n",
      "Epoch 0, Batch 471, Test Loss: 1.5580610036849976\n",
      "Epoch 0, Batch 472, Test Loss: 1.6166585683822632\n",
      "Epoch 0, Batch 473, Test Loss: 1.5092604160308838\n",
      "Epoch 0, Batch 474, Test Loss: 1.5064501762390137\n",
      "Epoch 0, Batch 475, Test Loss: 1.5856879949569702\n",
      "Epoch 0, Batch 476, Test Loss: 1.6173522472381592\n",
      "Epoch 0, Batch 477, Test Loss: 1.5710711479187012\n",
      "Epoch 0, Batch 478, Test Loss: 1.5902307033538818\n",
      "Epoch 0, Batch 479, Test Loss: 1.5588829517364502\n",
      "Epoch 0, Batch 480, Test Loss: 1.6114610433578491\n",
      "Epoch 0, Batch 481, Test Loss: 1.5695699453353882\n",
      "Epoch 0, Batch 482, Test Loss: 1.5815242528915405\n",
      "Epoch 0, Batch 483, Test Loss: 1.5519471168518066\n",
      "Epoch 0, Batch 484, Test Loss: 1.582252025604248\n",
      "Epoch 0, Batch 485, Test Loss: 1.5927784442901611\n",
      "Epoch 0, Batch 486, Test Loss: 1.574790596961975\n",
      "Epoch 0, Batch 487, Test Loss: 1.4991909265518188\n",
      "Epoch 0, Batch 488, Test Loss: 1.5170296430587769\n",
      "Epoch 0, Batch 489, Test Loss: 1.5582311153411865\n",
      "Epoch 0, Batch 490, Test Loss: 1.5057355165481567\n",
      "Epoch 0, Batch 491, Test Loss: 1.570425033569336\n",
      "Epoch 0, Batch 492, Test Loss: 1.5959677696228027\n",
      "Epoch 0, Batch 493, Test Loss: 1.5337392091751099\n",
      "Epoch 0, Batch 494, Test Loss: 1.5178334712982178\n",
      "Epoch 0, Batch 495, Test Loss: 1.571712613105774\n",
      "Epoch 0, Batch 496, Test Loss: 1.5044090747833252\n",
      "Epoch 0, Batch 497, Test Loss: 1.6207904815673828\n",
      "Epoch 0, Batch 498, Test Loss: 1.599658727645874\n",
      "Epoch 0, Batch 499, Test Loss: 1.6053526401519775\n",
      "Epoch 0, Batch 500, Test Loss: 1.6198455095291138\n",
      "Epoch 0, Batch 501, Test Loss: 1.5547881126403809\n",
      "Epoch 0, Batch 502, Test Loss: 1.5923067331314087\n",
      "Epoch 0, Batch 503, Test Loss: 1.5324406623840332\n",
      "Epoch 0, Batch 504, Test Loss: 1.564535140991211\n",
      "Epoch 0, Batch 505, Test Loss: 1.594809889793396\n",
      "Epoch 0, Batch 506, Test Loss: 1.6116960048675537\n",
      "Epoch 0, Batch 507, Test Loss: 1.5798237323760986\n",
      "Epoch 0, Batch 508, Test Loss: 1.6024956703186035\n",
      "Epoch 0, Batch 509, Test Loss: 1.6178998947143555\n",
      "Epoch 0, Batch 510, Test Loss: 1.5258805751800537\n",
      "Epoch 0, Batch 511, Test Loss: 1.623118281364441\n",
      "Epoch 0, Batch 512, Test Loss: 1.580701470375061\n",
      "Epoch 0, Batch 513, Test Loss: 1.6215492486953735\n",
      "Epoch 0, Batch 514, Test Loss: 1.625618815422058\n",
      "Epoch 0, Batch 515, Test Loss: 1.51017427444458\n",
      "Epoch 0, Batch 516, Test Loss: 1.5049352645874023\n",
      "Epoch 0, Batch 517, Test Loss: 1.6017194986343384\n",
      "Epoch 0, Batch 518, Test Loss: 1.540040373802185\n",
      "Epoch 0, Batch 519, Test Loss: 1.5682247877120972\n",
      "Epoch 0, Batch 520, Test Loss: 1.5493900775909424\n",
      "Epoch 0, Batch 521, Test Loss: 1.6468021869659424\n",
      "Epoch 0, Batch 522, Test Loss: 1.5460634231567383\n",
      "Epoch 0, Batch 523, Test Loss: 1.533545970916748\n",
      "Epoch 0, Batch 524, Test Loss: 1.6877326965332031\n",
      "Epoch 0, Batch 525, Test Loss: 1.787890911102295\n",
      "Epoch 0, Batch 526, Test Loss: 1.6079107522964478\n",
      "Epoch 0, Batch 527, Test Loss: 1.5871853828430176\n",
      "Epoch 0, Batch 528, Test Loss: 1.5749207735061646\n",
      "Epoch 0, Batch 529, Test Loss: 1.5613089799880981\n",
      "Epoch 0, Batch 530, Test Loss: 1.623581051826477\n",
      "Epoch 0, Batch 531, Test Loss: 1.568245768547058\n",
      "Epoch 0, Batch 532, Test Loss: 1.5634130239486694\n",
      "Epoch 0, Batch 533, Test Loss: 1.5446903705596924\n",
      "Epoch 0, Batch 534, Test Loss: 1.6335984468460083\n",
      "Epoch 0, Batch 535, Test Loss: 1.554922342300415\n",
      "Epoch 0, Batch 536, Test Loss: 1.5820521116256714\n",
      "Epoch 0, Batch 537, Test Loss: 1.5876773595809937\n",
      "Epoch 0, Batch 538, Test Loss: 1.508948802947998\n",
      "Epoch 0, Batch 539, Test Loss: 1.5905721187591553\n",
      "Epoch 0, Batch 540, Test Loss: 1.5897345542907715\n",
      "Epoch 0, Batch 541, Test Loss: 1.5998872518539429\n",
      "Epoch 0, Batch 542, Test Loss: 1.5868996381759644\n",
      "Epoch 0, Batch 543, Test Loss: 1.6867014169692993\n",
      "Epoch 0, Batch 544, Test Loss: 1.609613060951233\n",
      "Epoch 0, Batch 545, Test Loss: 1.6045448780059814\n",
      "Epoch 0, Batch 546, Test Loss: 1.555742859840393\n",
      "Epoch 0, Batch 547, Test Loss: 1.5397374629974365\n",
      "Epoch 0, Batch 548, Test Loss: 1.5756934881210327\n",
      "Epoch 0, Batch 549, Test Loss: 1.6114929914474487\n",
      "Epoch 0, Batch 550, Test Loss: 1.676578402519226\n",
      "Epoch 0, Batch 551, Test Loss: 1.6035531759262085\n",
      "Epoch 0, Batch 552, Test Loss: 1.6238428354263306\n",
      "Epoch 0, Batch 553, Test Loss: 1.5597198009490967\n",
      "Epoch 0, Batch 554, Test Loss: 1.5868936777114868\n",
      "Epoch 0, Batch 555, Test Loss: 1.5457569360733032\n",
      "Epoch 0, Batch 556, Test Loss: 1.6074268817901611\n",
      "Epoch 0, Batch 557, Test Loss: 1.5903704166412354\n",
      "Epoch 0, Batch 558, Test Loss: 1.6228753328323364\n",
      "Epoch 0, Batch 559, Test Loss: 1.5978385210037231\n",
      "Epoch 0, Batch 560, Test Loss: 1.5180381536483765\n",
      "Epoch 0, Batch 561, Test Loss: 1.6573352813720703\n",
      "Epoch 0, Batch 562, Test Loss: 1.531553864479065\n",
      "Epoch 0, Batch 563, Test Loss: 1.53299081325531\n",
      "Epoch 0, Batch 564, Test Loss: 1.690559983253479\n",
      "Epoch 0, Batch 565, Test Loss: 1.5209403038024902\n",
      "Epoch 0, Batch 566, Test Loss: 1.5734748840332031\n",
      "Epoch 0, Batch 567, Test Loss: 1.5220187902450562\n",
      "Epoch 0, Batch 568, Test Loss: 1.6104363203048706\n",
      "Epoch 0, Batch 569, Test Loss: 1.6017056703567505\n",
      "Epoch 0, Batch 570, Test Loss: 1.6340373754501343\n",
      "Epoch 0, Batch 571, Test Loss: 1.5790451765060425\n",
      "Epoch 0, Batch 572, Test Loss: 1.580952763557434\n",
      "Epoch 0, Batch 573, Test Loss: 1.506758689880371\n",
      "Epoch 0, Batch 574, Test Loss: 1.5714285373687744\n",
      "Epoch 0, Batch 575, Test Loss: 1.5506261587142944\n",
      "Epoch 0, Batch 576, Test Loss: 1.6061654090881348\n",
      "Epoch 0, Batch 577, Test Loss: 1.5130811929702759\n",
      "Epoch 0, Batch 578, Test Loss: 1.6305443048477173\n",
      "Epoch 0, Batch 579, Test Loss: 1.633544683456421\n",
      "Epoch 0, Batch 580, Test Loss: 1.6242420673370361\n",
      "Epoch 0, Batch 581, Test Loss: 1.61575186252594\n",
      "Epoch 0, Batch 582, Test Loss: 1.5971840620040894\n",
      "Epoch 0, Batch 583, Test Loss: 1.6386594772338867\n",
      "Epoch 0, Batch 584, Test Loss: 1.5966870784759521\n",
      "Epoch 0, Batch 585, Test Loss: 1.5953160524368286\n",
      "Epoch 0, Batch 586, Test Loss: 1.6059749126434326\n",
      "Epoch 0, Batch 587, Test Loss: 1.6176165342330933\n",
      "Epoch 0, Batch 588, Test Loss: 1.6608630418777466\n",
      "Epoch 0, Batch 589, Test Loss: 1.6724416017532349\n",
      "Epoch 0, Batch 590, Test Loss: 1.608152985572815\n",
      "Epoch 0, Batch 591, Test Loss: 1.5708115100860596\n",
      "Epoch 0, Batch 592, Test Loss: 1.5564643144607544\n",
      "Epoch 0, Batch 593, Test Loss: 1.5858415365219116\n",
      "Epoch 0, Batch 594, Test Loss: 1.661466360092163\n",
      "Epoch 0, Batch 595, Test Loss: 1.546223759651184\n",
      "Epoch 0, Batch 596, Test Loss: 1.5646579265594482\n",
      "Epoch 0, Batch 597, Test Loss: 1.633892297744751\n",
      "Epoch 0, Batch 598, Test Loss: 1.6303608417510986\n",
      "Epoch 0, Batch 599, Test Loss: 1.6860535144805908\n",
      "Epoch 0, Batch 600, Test Loss: 1.569012999534607\n",
      "Epoch 0, Batch 601, Test Loss: 1.5278818607330322\n",
      "Epoch 0, Batch 602, Test Loss: 1.635596513748169\n",
      "Epoch 0, Batch 603, Test Loss: 1.677641749382019\n",
      "Epoch 0, Batch 604, Test Loss: 1.6293227672576904\n",
      "Epoch 0, Batch 605, Test Loss: 1.6033120155334473\n",
      "Epoch 0, Batch 606, Test Loss: 1.6384767293930054\n",
      "Epoch 0, Batch 607, Test Loss: 1.6057536602020264\n",
      "Epoch 0, Batch 608, Test Loss: 1.6273353099822998\n",
      "Epoch 0, Batch 609, Test Loss: 1.6357089281082153\n",
      "Epoch 0, Batch 610, Test Loss: 1.5897645950317383\n",
      "Epoch 0, Batch 611, Test Loss: 1.5639328956604004\n",
      "Epoch 0, Batch 612, Test Loss: 1.5921998023986816\n",
      "Epoch 0, Batch 613, Test Loss: 1.5133419036865234\n",
      "Epoch 0, Batch 614, Test Loss: 1.6492763757705688\n",
      "Epoch 0, Batch 615, Test Loss: 1.57040274143219\n",
      "Epoch 0, Batch 616, Test Loss: 1.5542043447494507\n",
      "Epoch 0, Batch 617, Test Loss: 1.6184251308441162\n",
      "Epoch 0, Batch 618, Test Loss: 1.576304316520691\n",
      "Epoch 0, Batch 619, Test Loss: 1.6042722463607788\n",
      "Epoch 0, Batch 620, Test Loss: 1.565034031867981\n",
      "Epoch 0, Batch 621, Test Loss: 1.5668020248413086\n",
      "Epoch 0, Batch 622, Test Loss: 1.4999191761016846\n",
      "Epoch 0, Batch 623, Test Loss: 1.6157604455947876\n",
      "Epoch 0, Batch 624, Test Loss: 1.577942132949829\n",
      "Epoch 0, Batch 625, Test Loss: 1.5196759700775146\n",
      "Epoch 0, Batch 626, Test Loss: 1.545300006866455\n",
      "Epoch 0, Batch 627, Test Loss: 1.7064235210418701\n",
      "Epoch 0, Batch 628, Test Loss: 1.6084716320037842\n",
      "Epoch 0, Batch 629, Test Loss: 1.6241458654403687\n",
      "Epoch 0, Batch 630, Test Loss: 1.678114652633667\n",
      "Epoch 0, Batch 631, Test Loss: 1.6570961475372314\n",
      "Epoch 0, Batch 632, Test Loss: 1.5485761165618896\n",
      "Epoch 0, Batch 633, Test Loss: 1.6325796842575073\n",
      "Epoch 0, Batch 634, Test Loss: 1.5747472047805786\n",
      "Epoch 0, Batch 635, Test Loss: 1.6157423257827759\n",
      "Epoch 0, Batch 636, Test Loss: 1.5353314876556396\n",
      "Epoch 0, Batch 637, Test Loss: 1.580796718597412\n",
      "Epoch 0, Batch 638, Test Loss: 1.629148006439209\n",
      "Epoch 0, Batch 639, Test Loss: 1.572230577468872\n",
      "Epoch 0, Batch 640, Test Loss: 1.6986470222473145\n",
      "Epoch 0, Batch 641, Test Loss: 1.5023711919784546\n",
      "Epoch 0, Batch 642, Test Loss: 1.6515774726867676\n",
      "Epoch 0, Batch 643, Test Loss: 1.5793540477752686\n",
      "Epoch 0, Batch 644, Test Loss: 1.5333775281906128\n",
      "Epoch 0, Batch 645, Test Loss: 1.5558602809906006\n",
      "Epoch 0, Batch 646, Test Loss: 1.5304845571517944\n",
      "Epoch 0, Batch 647, Test Loss: 1.5937814712524414\n",
      "Epoch 0, Batch 648, Test Loss: 1.5749760866165161\n",
      "Epoch 0, Batch 649, Test Loss: 1.650754451751709\n",
      "Epoch 0, Batch 650, Test Loss: 1.5919585227966309\n",
      "Epoch 0, Batch 651, Test Loss: 1.6174342632293701\n",
      "Epoch 0, Batch 652, Test Loss: 1.5999600887298584\n",
      "Epoch 0, Batch 653, Test Loss: 1.5599206686019897\n",
      "Epoch 0, Batch 654, Test Loss: 1.5619661808013916\n",
      "Epoch 0, Batch 655, Test Loss: 1.659189224243164\n",
      "Epoch 0, Batch 656, Test Loss: 1.5943336486816406\n",
      "Epoch 0, Batch 657, Test Loss: 1.6797332763671875\n",
      "Epoch 0, Batch 658, Test Loss: 1.6894841194152832\n",
      "Epoch 0, Batch 659, Test Loss: 1.6172466278076172\n",
      "Epoch 0, Batch 660, Test Loss: 1.4858472347259521\n",
      "Epoch 0, Batch 661, Test Loss: 1.5016989707946777\n",
      "Epoch 0, Batch 662, Test Loss: 1.53508722782135\n",
      "Epoch 0, Batch 663, Test Loss: 1.5051347017288208\n",
      "Epoch 0, Batch 664, Test Loss: 1.5551526546478271\n",
      "Epoch 0, Batch 665, Test Loss: 1.5873730182647705\n",
      "Epoch 0, Batch 666, Test Loss: 1.5978856086730957\n",
      "Epoch 0, Batch 667, Test Loss: 1.584119439125061\n",
      "Epoch 0, Batch 668, Test Loss: 1.5782322883605957\n",
      "Epoch 0, Batch 669, Test Loss: 1.637417197227478\n",
      "Epoch 0, Batch 670, Test Loss: 1.5712392330169678\n",
      "Epoch 0, Batch 671, Test Loss: 1.5749390125274658\n",
      "Epoch 0, Batch 672, Test Loss: 1.6206644773483276\n",
      "Epoch 0, Batch 673, Test Loss: 1.6251188516616821\n",
      "Epoch 0, Batch 674, Test Loss: 1.6088141202926636\n",
      "Epoch 0, Batch 675, Test Loss: 1.5585827827453613\n",
      "Epoch 0, Batch 676, Test Loss: 1.613484263420105\n",
      "Epoch 0, Batch 677, Test Loss: 1.560521125793457\n",
      "Epoch 0, Batch 678, Test Loss: 1.6235458850860596\n",
      "Epoch 0, Batch 679, Test Loss: 1.610909104347229\n",
      "Epoch 0, Batch 680, Test Loss: 1.61297607421875\n",
      "Epoch 0, Batch 681, Test Loss: 1.608161211013794\n",
      "Epoch 0, Batch 682, Test Loss: 1.5017207860946655\n",
      "Epoch 0, Batch 683, Test Loss: 1.5802220106124878\n",
      "Epoch 0, Batch 684, Test Loss: 1.6198190450668335\n",
      "Epoch 0, Batch 685, Test Loss: 1.5038063526153564\n",
      "Epoch 0, Batch 686, Test Loss: 1.5685851573944092\n",
      "Epoch 0, Batch 687, Test Loss: 1.5416560173034668\n",
      "Epoch 0, Batch 688, Test Loss: 1.5791339874267578\n",
      "Epoch 0, Batch 689, Test Loss: 1.5608834028244019\n",
      "Epoch 0, Batch 690, Test Loss: 1.5910760164260864\n",
      "Epoch 0, Batch 691, Test Loss: 1.5627391338348389\n",
      "Epoch 0, Batch 692, Test Loss: 1.543091893196106\n",
      "Epoch 0, Batch 693, Test Loss: 1.5712040662765503\n",
      "Epoch 0, Batch 694, Test Loss: 1.6033134460449219\n",
      "Epoch 0, Batch 695, Test Loss: 1.5526220798492432\n",
      "Epoch 0, Batch 696, Test Loss: 1.5888535976409912\n",
      "Epoch 0, Batch 697, Test Loss: 1.5294785499572754\n",
      "Epoch 0, Batch 698, Test Loss: 1.576014757156372\n",
      "Epoch 0, Batch 699, Test Loss: 1.534452199935913\n",
      "Epoch 0, Batch 700, Test Loss: 1.5652598142623901\n",
      "Epoch 0, Batch 701, Test Loss: 1.7026864290237427\n",
      "Epoch 0, Batch 702, Test Loss: 1.6197909116744995\n",
      "Epoch 0, Batch 703, Test Loss: 1.62592613697052\n",
      "Epoch 0, Batch 704, Test Loss: 1.547526240348816\n",
      "Epoch 0, Batch 705, Test Loss: 1.5066840648651123\n",
      "Epoch 0, Batch 706, Test Loss: 1.5992869138717651\n",
      "Epoch 0, Batch 707, Test Loss: 1.6000709533691406\n",
      "Epoch 0, Batch 708, Test Loss: 1.5528590679168701\n",
      "Epoch 0, Batch 709, Test Loss: 1.4825921058654785\n",
      "Epoch 0, Batch 710, Test Loss: 1.5507683753967285\n",
      "Epoch 0, Batch 711, Test Loss: 1.5833418369293213\n",
      "Epoch 0, Batch 712, Test Loss: 1.6313329935073853\n",
      "Epoch 0, Batch 713, Test Loss: 1.5833953619003296\n",
      "Epoch 0, Batch 714, Test Loss: 1.6122925281524658\n",
      "Epoch 0, Batch 715, Test Loss: 1.6098895072937012\n",
      "Epoch 0, Batch 716, Test Loss: 1.5662169456481934\n",
      "Epoch 0, Batch 717, Test Loss: 1.6213374137878418\n",
      "Epoch 0, Batch 718, Test Loss: 1.5883184671401978\n",
      "Epoch 0, Batch 719, Test Loss: 1.5462735891342163\n",
      "Epoch 0, Batch 720, Test Loss: 1.5384573936462402\n",
      "Epoch 0, Batch 721, Test Loss: 1.5336273908615112\n",
      "Epoch 0, Batch 722, Test Loss: 1.5698814392089844\n",
      "Epoch 0, Batch 723, Test Loss: 1.5690494775772095\n",
      "Epoch 0, Batch 724, Test Loss: 1.5201036930084229\n",
      "Epoch 0, Batch 725, Test Loss: 1.639319658279419\n",
      "Epoch 0, Batch 726, Test Loss: 1.5486634969711304\n",
      "Epoch 0, Batch 727, Test Loss: 1.5404828786849976\n",
      "Epoch 0, Batch 728, Test Loss: 1.5538870096206665\n",
      "Epoch 0, Batch 729, Test Loss: 1.5131003856658936\n",
      "Epoch 0, Batch 730, Test Loss: 1.5451786518096924\n",
      "Epoch 0, Batch 731, Test Loss: 1.640845775604248\n",
      "Epoch 0, Batch 732, Test Loss: 1.5921579599380493\n",
      "Epoch 0, Batch 733, Test Loss: 1.5750219821929932\n",
      "Epoch 0, Batch 734, Test Loss: 1.6561630964279175\n",
      "Epoch 0, Batch 735, Test Loss: 1.6373709440231323\n",
      "Epoch 0, Batch 736, Test Loss: 1.6726926565170288\n",
      "Epoch 0, Batch 737, Test Loss: 1.5603127479553223\n",
      "Epoch 0, Batch 738, Test Loss: 1.539183497428894\n",
      "Epoch 0, Batch 739, Test Loss: 1.586524248123169\n",
      "Epoch 0, Batch 740, Test Loss: 1.573972463607788\n",
      "Epoch 0, Batch 741, Test Loss: 1.524371862411499\n",
      "Epoch 0, Batch 742, Test Loss: 1.5346966981887817\n",
      "Epoch 0, Batch 743, Test Loss: 1.5193068981170654\n",
      "Epoch 0, Batch 744, Test Loss: 1.5474342107772827\n",
      "Epoch 0, Batch 745, Test Loss: 1.587789535522461\n",
      "Epoch 0, Batch 746, Test Loss: 1.546637773513794\n",
      "Epoch 0, Batch 747, Test Loss: 1.5661568641662598\n",
      "Epoch 0, Batch 748, Test Loss: 1.6420159339904785\n",
      "Epoch 0, Batch 749, Test Loss: 1.5484164953231812\n",
      "Epoch 0, Batch 750, Test Loss: 1.5156071186065674\n",
      "Epoch 0, Batch 751, Test Loss: 1.60017728805542\n",
      "Epoch 0, Batch 752, Test Loss: 1.6025798320770264\n",
      "Epoch 0, Batch 753, Test Loss: 1.6109145879745483\n",
      "Epoch 0, Batch 754, Test Loss: 1.6360769271850586\n",
      "Epoch 0, Batch 755, Test Loss: 1.5472042560577393\n",
      "Epoch 0, Batch 756, Test Loss: 1.6153184175491333\n",
      "Epoch 0, Batch 757, Test Loss: 1.5759178400039673\n",
      "Epoch 0, Batch 758, Test Loss: 1.5778807401657104\n",
      "Epoch 0, Batch 759, Test Loss: 1.5979417562484741\n",
      "Epoch 0, Batch 760, Test Loss: 1.5878325700759888\n",
      "Epoch 0, Batch 761, Test Loss: 1.553133249282837\n",
      "Epoch 0, Batch 762, Test Loss: 1.6720166206359863\n",
      "Epoch 0, Batch 763, Test Loss: 1.6394754648208618\n",
      "Epoch 0, Batch 764, Test Loss: 1.5840729475021362\n",
      "Epoch 0, Batch 765, Test Loss: 1.635674238204956\n",
      "Epoch 0, Batch 766, Test Loss: 1.5096949338912964\n",
      "Epoch 0, Batch 767, Test Loss: 1.5425504446029663\n",
      "Epoch 0, Batch 768, Test Loss: 1.5990939140319824\n",
      "Epoch 0, Batch 769, Test Loss: 1.5691001415252686\n",
      "Epoch 0, Batch 770, Test Loss: 1.5515415668487549\n",
      "Epoch 0, Batch 771, Test Loss: 1.610503911972046\n",
      "Epoch 0, Batch 772, Test Loss: 1.660138726234436\n",
      "Epoch 0, Batch 773, Test Loss: 1.5845686197280884\n",
      "Epoch 0, Batch 774, Test Loss: 1.62269926071167\n",
      "Epoch 0, Batch 775, Test Loss: 1.5673470497131348\n",
      "Epoch 0, Batch 776, Test Loss: 1.6158150434494019\n",
      "Epoch 0, Batch 777, Test Loss: 1.591116189956665\n",
      "Epoch 0, Batch 778, Test Loss: 1.6356602907180786\n",
      "Epoch 0, Batch 779, Test Loss: 1.583397388458252\n",
      "Epoch 0, Batch 780, Test Loss: 1.5956993103027344\n",
      "Epoch 0, Batch 781, Test Loss: 1.6354504823684692\n",
      "Epoch 0, Batch 782, Test Loss: 1.5227329730987549\n",
      "Epoch 0, Batch 783, Test Loss: 1.5900824069976807\n",
      "Epoch 0, Batch 784, Test Loss: 1.6816487312316895\n",
      "Epoch 0, Batch 785, Test Loss: 1.5801005363464355\n",
      "Epoch 0, Batch 786, Test Loss: 1.4960428476333618\n",
      "Epoch 0, Batch 787, Test Loss: 1.6316837072372437\n",
      "Epoch 0, Batch 788, Test Loss: 1.5349302291870117\n",
      "Epoch 0, Batch 789, Test Loss: 1.6137778759002686\n",
      "Epoch 0, Batch 790, Test Loss: 1.539986252784729\n",
      "Epoch 0, Batch 791, Test Loss: 1.642776608467102\n",
      "Epoch 0, Batch 792, Test Loss: 1.5127315521240234\n",
      "Epoch 0, Batch 793, Test Loss: 1.6209532022476196\n",
      "Epoch 0, Batch 794, Test Loss: 1.6398804187774658\n",
      "Epoch 0, Batch 795, Test Loss: 1.5883723497390747\n",
      "Epoch 0, Batch 796, Test Loss: 1.5360748767852783\n",
      "Epoch 0, Batch 797, Test Loss: 1.6151031255722046\n",
      "Epoch 0, Batch 798, Test Loss: 1.58921480178833\n",
      "Epoch 0, Batch 799, Test Loss: 1.588315486907959\n",
      "Epoch 0, Batch 800, Test Loss: 1.6211719512939453\n",
      "Epoch 0, Batch 801, Test Loss: 1.4888994693756104\n",
      "Epoch 0, Batch 802, Test Loss: 1.5213648080825806\n",
      "Epoch 0, Batch 803, Test Loss: 1.5659594535827637\n",
      "Epoch 0, Batch 804, Test Loss: 1.574626088142395\n",
      "Epoch 0, Batch 805, Test Loss: 1.5390963554382324\n",
      "Epoch 0, Batch 806, Test Loss: 1.6200242042541504\n",
      "Epoch 0, Batch 807, Test Loss: 1.7284512519836426\n",
      "Epoch 0, Batch 808, Test Loss: 1.607334017753601\n",
      "Epoch 0, Batch 809, Test Loss: 1.5359272956848145\n",
      "Epoch 0, Batch 810, Test Loss: 1.6113096475601196\n",
      "Epoch 0, Batch 811, Test Loss: 1.6257814168930054\n",
      "Epoch 0, Batch 812, Test Loss: 1.5890634059906006\n",
      "Epoch 0, Batch 813, Test Loss: 1.6344821453094482\n",
      "Epoch 0, Batch 814, Test Loss: 1.5233981609344482\n",
      "Epoch 0, Batch 815, Test Loss: 1.64639151096344\n",
      "Epoch 0, Batch 816, Test Loss: 1.5724544525146484\n",
      "Epoch 0, Batch 817, Test Loss: 1.6347849369049072\n",
      "Epoch 0, Batch 818, Test Loss: 1.6088048219680786\n",
      "Epoch 0, Batch 819, Test Loss: 1.6777631044387817\n",
      "Epoch 0, Batch 820, Test Loss: 1.54667329788208\n",
      "Epoch 0, Batch 821, Test Loss: 1.6171056032180786\n",
      "Epoch 0, Batch 822, Test Loss: 1.6124733686447144\n",
      "Epoch 0, Batch 823, Test Loss: 1.5411938428878784\n",
      "Epoch 0, Batch 824, Test Loss: 1.5947074890136719\n",
      "Epoch 0, Batch 825, Test Loss: 1.5153846740722656\n",
      "Epoch 0, Batch 826, Test Loss: 1.589910626411438\n",
      "Epoch 0, Batch 827, Test Loss: 1.543792963027954\n",
      "Epoch 0, Batch 828, Test Loss: 1.5838649272918701\n",
      "Epoch 0, Batch 829, Test Loss: 1.565313696861267\n",
      "Epoch 0, Batch 830, Test Loss: 1.573466420173645\n",
      "Epoch 0, Batch 831, Test Loss: 1.5964608192443848\n",
      "Epoch 0, Batch 832, Test Loss: 1.5009748935699463\n",
      "Epoch 0, Batch 833, Test Loss: 1.6588398218154907\n",
      "Epoch 0, Batch 834, Test Loss: 1.6718816757202148\n",
      "Epoch 0, Batch 835, Test Loss: 1.5463552474975586\n",
      "Epoch 0, Batch 836, Test Loss: 1.5559678077697754\n",
      "Epoch 0, Batch 837, Test Loss: 1.538099765777588\n",
      "Epoch 0, Batch 838, Test Loss: 1.5980453491210938\n",
      "Epoch 0, Batch 839, Test Loss: 1.5639503002166748\n",
      "Epoch 0, Batch 840, Test Loss: 1.595414161682129\n",
      "Epoch 0, Batch 841, Test Loss: 1.5941144227981567\n",
      "Epoch 0, Batch 842, Test Loss: 1.518363118171692\n",
      "Epoch 0, Batch 843, Test Loss: 1.5421258211135864\n",
      "Epoch 0, Batch 844, Test Loss: 1.5402469635009766\n",
      "Epoch 0, Batch 845, Test Loss: 1.6538522243499756\n",
      "Epoch 0, Batch 846, Test Loss: 1.536017894744873\n",
      "Epoch 0, Batch 847, Test Loss: 1.5994818210601807\n",
      "Epoch 0, Batch 848, Test Loss: 1.5769391059875488\n",
      "Epoch 0, Batch 849, Test Loss: 1.6938698291778564\n",
      "Epoch 0, Batch 850, Test Loss: 1.5993014574050903\n",
      "Epoch 0, Batch 851, Test Loss: 1.5270190238952637\n",
      "Epoch 0, Batch 852, Test Loss: 1.5146890878677368\n",
      "Epoch 0, Batch 853, Test Loss: 1.523883581161499\n",
      "Epoch 0, Batch 854, Test Loss: 1.5798325538635254\n",
      "Epoch 0, Batch 855, Test Loss: 1.5573055744171143\n",
      "Epoch 0, Batch 856, Test Loss: 1.5445184707641602\n",
      "Epoch 0, Batch 857, Test Loss: 1.5289628505706787\n",
      "Epoch 0, Batch 858, Test Loss: 1.5779423713684082\n",
      "Epoch 0, Batch 859, Test Loss: 1.5964802503585815\n",
      "Epoch 0, Batch 860, Test Loss: 1.702123761177063\n",
      "Epoch 0, Batch 861, Test Loss: 1.554669976234436\n",
      "Epoch 0, Batch 862, Test Loss: 1.6352393627166748\n",
      "Epoch 0, Batch 863, Test Loss: 1.6073883771896362\n",
      "Epoch 0, Batch 864, Test Loss: 1.5651646852493286\n",
      "Epoch 0, Batch 865, Test Loss: 1.5369393825531006\n",
      "Epoch 0, Batch 866, Test Loss: 1.580313801765442\n",
      "Epoch 0, Batch 867, Test Loss: 1.6091641187667847\n",
      "Epoch 0, Batch 868, Test Loss: 1.5834779739379883\n",
      "Epoch 0, Batch 869, Test Loss: 1.5487102270126343\n",
      "Epoch 0, Batch 870, Test Loss: 1.6137580871582031\n",
      "Epoch 0, Batch 871, Test Loss: 1.5875624418258667\n",
      "Epoch 0, Batch 872, Test Loss: 1.572249412536621\n",
      "Epoch 0, Batch 873, Test Loss: 1.6106691360473633\n",
      "Epoch 0, Batch 874, Test Loss: 1.608256220817566\n",
      "Epoch 0, Batch 875, Test Loss: 1.5448981523513794\n",
      "Epoch 0, Batch 876, Test Loss: 1.6307402849197388\n",
      "Epoch 0, Batch 877, Test Loss: 1.533874273300171\n",
      "Epoch 0, Batch 878, Test Loss: 1.541208028793335\n",
      "Epoch 0, Batch 879, Test Loss: 1.5559748411178589\n",
      "Epoch 0, Batch 880, Test Loss: 1.559631586074829\n",
      "Epoch 0, Batch 881, Test Loss: 1.508615493774414\n",
      "Epoch 0, Batch 882, Test Loss: 1.6533129215240479\n",
      "Epoch 0, Batch 883, Test Loss: 1.676702857017517\n",
      "Epoch 0, Batch 884, Test Loss: 1.5629806518554688\n",
      "Epoch 0, Batch 885, Test Loss: 1.577405571937561\n",
      "Epoch 0, Batch 886, Test Loss: 1.6105471849441528\n",
      "Epoch 0, Batch 887, Test Loss: 1.5679049491882324\n",
      "Epoch 0, Batch 888, Test Loss: 1.5644233226776123\n",
      "Epoch 0, Batch 889, Test Loss: 1.5810755491256714\n",
      "Epoch 0, Batch 890, Test Loss: 1.650188684463501\n",
      "Epoch 0, Batch 891, Test Loss: 1.5963951349258423\n",
      "Epoch 0, Batch 892, Test Loss: 1.5717501640319824\n",
      "Epoch 0, Batch 893, Test Loss: 1.6277234554290771\n",
      "Epoch 0, Batch 894, Test Loss: 1.5421030521392822\n",
      "Epoch 0, Batch 895, Test Loss: 1.629116415977478\n",
      "Epoch 0, Batch 896, Test Loss: 1.6192996501922607\n",
      "Epoch 0, Batch 897, Test Loss: 1.5136643648147583\n",
      "Epoch 0, Batch 898, Test Loss: 1.5426433086395264\n",
      "Epoch 0, Batch 899, Test Loss: 1.583470106124878\n",
      "Epoch 0, Batch 900, Test Loss: 1.5525192022323608\n",
      "Epoch 0, Batch 901, Test Loss: 1.5918174982070923\n",
      "Epoch 0, Batch 902, Test Loss: 1.6173566579818726\n",
      "Epoch 0, Batch 903, Test Loss: 1.573818325996399\n",
      "Epoch 0, Batch 904, Test Loss: 1.5735739469528198\n",
      "Epoch 0, Batch 905, Test Loss: 1.6558794975280762\n",
      "Epoch 0, Batch 906, Test Loss: 1.6251753568649292\n",
      "Epoch 0, Batch 907, Test Loss: 1.6069982051849365\n",
      "Epoch 0, Batch 908, Test Loss: 1.5628174543380737\n",
      "Epoch 0, Batch 909, Test Loss: 1.6564801931381226\n",
      "Epoch 0, Batch 910, Test Loss: 1.6348663568496704\n",
      "Epoch 0, Batch 911, Test Loss: 1.5741828680038452\n",
      "Epoch 0, Batch 912, Test Loss: 1.5324413776397705\n",
      "Epoch 0, Batch 913, Test Loss: 1.5263636112213135\n",
      "Epoch 0, Batch 914, Test Loss: 1.5918958187103271\n",
      "Epoch 0, Batch 915, Test Loss: 1.5702307224273682\n",
      "Epoch 0, Batch 916, Test Loss: 1.5349915027618408\n",
      "Epoch 0, Batch 917, Test Loss: 1.532193660736084\n",
      "Epoch 0, Batch 918, Test Loss: 1.5667630434036255\n",
      "Epoch 0, Batch 919, Test Loss: 1.67897629737854\n",
      "Epoch 0, Batch 920, Test Loss: 1.5457830429077148\n",
      "Epoch 0, Batch 921, Test Loss: 1.5518178939819336\n",
      "Epoch 0, Batch 922, Test Loss: 1.5211023092269897\n",
      "Epoch 0, Batch 923, Test Loss: 1.5947723388671875\n",
      "Epoch 0, Batch 924, Test Loss: 1.6345162391662598\n",
      "Epoch 0, Batch 925, Test Loss: 1.5798871517181396\n",
      "Epoch 0, Batch 926, Test Loss: 1.6219171285629272\n",
      "Epoch 0, Batch 927, Test Loss: 1.5342769622802734\n",
      "Epoch 0, Batch 928, Test Loss: 1.505136251449585\n",
      "Epoch 0, Batch 929, Test Loss: 1.6454408168792725\n",
      "Epoch 0, Batch 930, Test Loss: 1.5607948303222656\n",
      "Epoch 0, Batch 931, Test Loss: 1.696018099784851\n",
      "Epoch 0, Batch 932, Test Loss: 1.5573145151138306\n",
      "Epoch 0, Batch 933, Test Loss: 1.5912928581237793\n",
      "Epoch 0, Batch 934, Test Loss: 1.5570670366287231\n",
      "Epoch 0, Batch 935, Test Loss: 1.6505835056304932\n",
      "Epoch 0, Batch 936, Test Loss: 1.555993914604187\n",
      "Epoch 0, Batch 937, Test Loss: 1.499714732170105\n",
      "Epoch 0, Batch 938, Test Loss: 1.497544288635254\n",
      "Accuracy of Test set: 0.46375\n",
      "Epoch 1, Batch 1, Loss: 1.6460275650024414\n",
      "Epoch 1, Batch 2, Loss: 1.6477112770080566\n",
      "Epoch 1, Batch 3, Loss: 1.6017348766326904\n",
      "Epoch 1, Batch 4, Loss: 1.5620834827423096\n",
      "Epoch 1, Batch 5, Loss: 1.639474630355835\n",
      "Epoch 1, Batch 6, Loss: 1.559820294380188\n",
      "Epoch 1, Batch 7, Loss: 1.5649547576904297\n",
      "Epoch 1, Batch 8, Loss: 1.5450323820114136\n",
      "Epoch 1, Batch 9, Loss: 1.569197654724121\n",
      "Epoch 1, Batch 10, Loss: 1.595113754272461\n",
      "Epoch 1, Batch 11, Loss: 1.5453228950500488\n",
      "Epoch 1, Batch 12, Loss: 1.6066157817840576\n",
      "Epoch 1, Batch 13, Loss: 1.643911361694336\n",
      "Epoch 1, Batch 14, Loss: 1.5655423402786255\n",
      "Epoch 1, Batch 15, Loss: 1.603240728378296\n",
      "Epoch 1, Batch 16, Loss: 1.5845377445220947\n",
      "Epoch 1, Batch 17, Loss: 1.530185580253601\n",
      "Epoch 1, Batch 18, Loss: 1.5878067016601562\n",
      "Epoch 1, Batch 19, Loss: 1.578223705291748\n",
      "Epoch 1, Batch 20, Loss: 1.6184121370315552\n",
      "Epoch 1, Batch 21, Loss: 1.6169532537460327\n",
      "Epoch 1, Batch 22, Loss: 1.655761957168579\n",
      "Epoch 1, Batch 23, Loss: 1.5499348640441895\n",
      "Epoch 1, Batch 24, Loss: 1.5446704626083374\n",
      "Epoch 1, Batch 25, Loss: 1.5755620002746582\n",
      "Epoch 1, Batch 26, Loss: 1.568304419517517\n",
      "Epoch 1, Batch 27, Loss: 1.4997955560684204\n",
      "Epoch 1, Batch 28, Loss: 1.501770257949829\n",
      "Epoch 1, Batch 29, Loss: 1.6371570825576782\n",
      "Epoch 1, Batch 30, Loss: 1.4615470170974731\n",
      "Epoch 1, Batch 31, Loss: 1.462467908859253\n",
      "Epoch 1, Batch 32, Loss: 1.5687885284423828\n",
      "Epoch 1, Batch 33, Loss: 1.5154705047607422\n",
      "Epoch 1, Batch 34, Loss: 1.5412647724151611\n",
      "Epoch 1, Batch 35, Loss: 1.5496314764022827\n",
      "Epoch 1, Batch 36, Loss: 1.5732691287994385\n",
      "Epoch 1, Batch 37, Loss: 1.5067527294158936\n",
      "Epoch 1, Batch 38, Loss: 1.5552836656570435\n",
      "Epoch 1, Batch 39, Loss: 1.4638346433639526\n",
      "Epoch 1, Batch 40, Loss: 1.4423041343688965\n",
      "Epoch 1, Batch 41, Loss: 1.4633158445358276\n",
      "Epoch 1, Batch 42, Loss: 1.596262812614441\n",
      "Epoch 1, Batch 43, Loss: 1.4649094343185425\n",
      "Epoch 1, Batch 44, Loss: 1.5514318943023682\n",
      "Epoch 1, Batch 45, Loss: 1.450149655342102\n",
      "Epoch 1, Batch 46, Loss: 1.5511202812194824\n",
      "Epoch 1, Batch 47, Loss: 1.5728809833526611\n",
      "Epoch 1, Batch 48, Loss: 1.4623810052871704\n",
      "Epoch 1, Batch 49, Loss: 1.4835364818572998\n",
      "Epoch 1, Batch 50, Loss: 1.5952304601669312\n",
      "Epoch 1, Batch 51, Loss: 1.5978959798812866\n",
      "Epoch 1, Batch 52, Loss: 1.4451420307159424\n",
      "Epoch 1, Batch 53, Loss: 1.6417802572250366\n",
      "Epoch 1, Batch 54, Loss: 1.5399242639541626\n",
      "Epoch 1, Batch 55, Loss: 1.4824213981628418\n",
      "Epoch 1, Batch 56, Loss: 1.5127308368682861\n",
      "Epoch 1, Batch 57, Loss: 1.5081384181976318\n",
      "Epoch 1, Batch 58, Loss: 1.490440845489502\n",
      "Epoch 1, Batch 59, Loss: 1.502281904220581\n",
      "Epoch 1, Batch 60, Loss: 1.47731614112854\n",
      "Epoch 1, Batch 61, Loss: 1.5367517471313477\n",
      "Epoch 1, Batch 62, Loss: 1.4438031911849976\n",
      "Epoch 1, Batch 63, Loss: 1.587876319885254\n",
      "Epoch 1, Batch 64, Loss: 1.4964284896850586\n",
      "Epoch 1, Batch 65, Loss: 1.504480004310608\n",
      "Epoch 1, Batch 66, Loss: 1.5445547103881836\n",
      "Epoch 1, Batch 67, Loss: 1.479863166809082\n",
      "Epoch 1, Batch 68, Loss: 1.5476605892181396\n",
      "Epoch 1, Batch 69, Loss: 1.5290000438690186\n",
      "Epoch 1, Batch 70, Loss: 1.5023465156555176\n",
      "Epoch 1, Batch 71, Loss: 1.4658960103988647\n",
      "Epoch 1, Batch 72, Loss: 1.4836845397949219\n",
      "Epoch 1, Batch 73, Loss: 1.4976123571395874\n",
      "Epoch 1, Batch 74, Loss: 1.537753939628601\n",
      "Epoch 1, Batch 75, Loss: 1.5202586650848389\n",
      "Epoch 1, Batch 76, Loss: 1.4502577781677246\n",
      "Epoch 1, Batch 77, Loss: 1.4321775436401367\n",
      "Epoch 1, Batch 78, Loss: 1.5864890813827515\n",
      "Epoch 1, Batch 79, Loss: 1.4812167882919312\n",
      "Epoch 1, Batch 80, Loss: 1.575013279914856\n",
      "Epoch 1, Batch 81, Loss: 1.417425274848938\n",
      "Epoch 1, Batch 82, Loss: 1.5116515159606934\n",
      "Epoch 1, Batch 83, Loss: 1.4860827922821045\n",
      "Epoch 1, Batch 84, Loss: 1.4635974168777466\n",
      "Epoch 1, Batch 85, Loss: 1.490164041519165\n",
      "Epoch 1, Batch 86, Loss: 1.473031759262085\n",
      "Epoch 1, Batch 87, Loss: 1.4457865953445435\n",
      "Epoch 1, Batch 88, Loss: 1.504223108291626\n",
      "Epoch 1, Batch 89, Loss: 1.446911334991455\n",
      "Epoch 1, Batch 90, Loss: 1.417656421661377\n",
      "Epoch 1, Batch 91, Loss: 1.481264591217041\n",
      "Epoch 1, Batch 92, Loss: 1.4933618307113647\n",
      "Epoch 1, Batch 93, Loss: 1.5596097707748413\n",
      "Epoch 1, Batch 94, Loss: 1.498103141784668\n",
      "Epoch 1, Batch 95, Loss: 1.5239626169204712\n",
      "Epoch 1, Batch 96, Loss: 1.4645289182662964\n",
      "Epoch 1, Batch 97, Loss: 1.5216357707977295\n",
      "Epoch 1, Batch 98, Loss: 1.4818207025527954\n",
      "Epoch 1, Batch 99, Loss: 1.513920783996582\n",
      "Epoch 1, Batch 100, Loss: 1.4264072179794312\n",
      "Epoch 1, Batch 101, Loss: 1.4573732614517212\n",
      "Epoch 1, Batch 102, Loss: 1.4197379350662231\n",
      "Epoch 1, Batch 103, Loss: 1.428746223449707\n",
      "Epoch 1, Batch 104, Loss: 1.523898959159851\n",
      "Epoch 1, Batch 105, Loss: 1.517856240272522\n",
      "Epoch 1, Batch 106, Loss: 1.3942174911499023\n",
      "Epoch 1, Batch 107, Loss: 1.4457868337631226\n",
      "Epoch 1, Batch 108, Loss: 1.531327247619629\n",
      "Epoch 1, Batch 109, Loss: 1.438882827758789\n",
      "Epoch 1, Batch 110, Loss: 1.524484395980835\n",
      "Epoch 1, Batch 111, Loss: 1.428454041481018\n",
      "Epoch 1, Batch 112, Loss: 1.4628843069076538\n",
      "Epoch 1, Batch 113, Loss: 1.4236212968826294\n",
      "Epoch 1, Batch 114, Loss: 1.4033125638961792\n",
      "Epoch 1, Batch 115, Loss: 1.4981340169906616\n",
      "Epoch 1, Batch 116, Loss: 1.4448739290237427\n",
      "Epoch 1, Batch 117, Loss: 1.4379371404647827\n",
      "Epoch 1, Batch 118, Loss: 1.4779536724090576\n",
      "Epoch 1, Batch 119, Loss: 1.3816787004470825\n",
      "Epoch 1, Batch 120, Loss: 1.3595556020736694\n",
      "Epoch 1, Batch 121, Loss: 1.5379879474639893\n",
      "Epoch 1, Batch 122, Loss: 1.4642952680587769\n",
      "Epoch 1, Batch 123, Loss: 1.5018267631530762\n",
      "Epoch 1, Batch 124, Loss: 1.431326150894165\n",
      "Epoch 1, Batch 125, Loss: 1.4715512990951538\n",
      "Epoch 1, Batch 126, Loss: 1.3885934352874756\n",
      "Epoch 1, Batch 127, Loss: 1.3736579418182373\n",
      "Epoch 1, Batch 128, Loss: 1.4251960515975952\n",
      "Epoch 1, Batch 129, Loss: 1.4830513000488281\n",
      "Epoch 1, Batch 130, Loss: 1.4820257425308228\n",
      "Epoch 1, Batch 131, Loss: 1.4991480112075806\n",
      "Epoch 1, Batch 132, Loss: 1.376262903213501\n",
      "Epoch 1, Batch 133, Loss: 1.4661561250686646\n",
      "Epoch 1, Batch 134, Loss: 1.4526987075805664\n",
      "Epoch 1, Batch 135, Loss: 1.3442366123199463\n",
      "Epoch 1, Batch 136, Loss: 1.3880531787872314\n",
      "Epoch 1, Batch 137, Loss: 1.3582499027252197\n",
      "Epoch 1, Batch 138, Loss: 1.3848426342010498\n",
      "Epoch 1, Batch 139, Loss: 1.4787033796310425\n",
      "Epoch 1, Batch 140, Loss: 1.395897388458252\n",
      "Epoch 1, Batch 141, Loss: 1.3843350410461426\n",
      "Epoch 1, Batch 142, Loss: 1.4863896369934082\n",
      "Epoch 1, Batch 143, Loss: 1.4464260339736938\n",
      "Epoch 1, Batch 144, Loss: 1.3687028884887695\n",
      "Epoch 1, Batch 145, Loss: 1.4846594333648682\n",
      "Epoch 1, Batch 146, Loss: 1.4160467386245728\n",
      "Epoch 1, Batch 147, Loss: 1.369512915611267\n",
      "Epoch 1, Batch 148, Loss: 1.3942124843597412\n",
      "Epoch 1, Batch 149, Loss: 1.5261505842208862\n",
      "Epoch 1, Batch 150, Loss: 1.4102351665496826\n",
      "Epoch 1, Batch 151, Loss: 1.3777062892913818\n",
      "Epoch 1, Batch 152, Loss: 1.4441874027252197\n",
      "Epoch 1, Batch 153, Loss: 1.396191120147705\n",
      "Epoch 1, Batch 154, Loss: 1.3723502159118652\n",
      "Epoch 1, Batch 155, Loss: 1.3714838027954102\n",
      "Epoch 1, Batch 156, Loss: 1.371406078338623\n",
      "Epoch 1, Batch 157, Loss: 1.3429113626480103\n",
      "Epoch 1, Batch 158, Loss: 1.4251482486724854\n",
      "Epoch 1, Batch 159, Loss: 1.4036437273025513\n",
      "Epoch 1, Batch 160, Loss: 1.4024587869644165\n",
      "Epoch 1, Batch 161, Loss: 1.4854025840759277\n",
      "Epoch 1, Batch 162, Loss: 1.3982757329940796\n",
      "Epoch 1, Batch 163, Loss: 1.3763058185577393\n",
      "Epoch 1, Batch 164, Loss: 1.390084981918335\n",
      "Epoch 1, Batch 165, Loss: 1.369666576385498\n",
      "Epoch 1, Batch 166, Loss: 1.370914101600647\n",
      "Epoch 1, Batch 167, Loss: 1.2801272869110107\n",
      "Epoch 1, Batch 168, Loss: 1.3346768617630005\n",
      "Epoch 1, Batch 169, Loss: 1.5173636674880981\n",
      "Epoch 1, Batch 170, Loss: 1.4596633911132812\n",
      "Epoch 1, Batch 171, Loss: 1.3728030920028687\n",
      "Epoch 1, Batch 172, Loss: 1.4176360368728638\n",
      "Epoch 1, Batch 173, Loss: 1.3908991813659668\n",
      "Epoch 1, Batch 174, Loss: 1.4339780807495117\n",
      "Epoch 1, Batch 175, Loss: 1.3260135650634766\n",
      "Epoch 1, Batch 176, Loss: 1.4722702503204346\n",
      "Epoch 1, Batch 177, Loss: 1.5143640041351318\n",
      "Epoch 1, Batch 178, Loss: 1.4358563423156738\n",
      "Epoch 1, Batch 179, Loss: 1.3372853994369507\n",
      "Epoch 1, Batch 180, Loss: 1.3918287754058838\n",
      "Epoch 1, Batch 181, Loss: 1.4833685159683228\n",
      "Epoch 1, Batch 182, Loss: 1.3839550018310547\n",
      "Epoch 1, Batch 183, Loss: 1.349768877029419\n",
      "Epoch 1, Batch 184, Loss: 1.3615798950195312\n",
      "Epoch 1, Batch 185, Loss: 1.3183900117874146\n",
      "Epoch 1, Batch 186, Loss: 1.3927843570709229\n",
      "Epoch 1, Batch 187, Loss: 1.4321415424346924\n",
      "Epoch 1, Batch 188, Loss: 1.3179616928100586\n",
      "Epoch 1, Batch 189, Loss: 1.4312604665756226\n",
      "Epoch 1, Batch 190, Loss: 1.3511213064193726\n",
      "Epoch 1, Batch 191, Loss: 1.3952628374099731\n",
      "Epoch 1, Batch 192, Loss: 1.4976857900619507\n",
      "Epoch 1, Batch 193, Loss: 1.3941943645477295\n",
      "Epoch 1, Batch 194, Loss: 1.3574187755584717\n",
      "Epoch 1, Batch 195, Loss: 1.4693745374679565\n",
      "Epoch 1, Batch 196, Loss: 1.3399240970611572\n",
      "Epoch 1, Batch 197, Loss: 1.3225171566009521\n",
      "Epoch 1, Batch 198, Loss: 1.3814796209335327\n",
      "Epoch 1, Batch 199, Loss: 1.4830269813537598\n",
      "Epoch 1, Batch 200, Loss: 1.4364738464355469\n",
      "Epoch 1, Batch 201, Loss: 1.3415582180023193\n",
      "Epoch 1, Batch 202, Loss: 1.4055485725402832\n",
      "Epoch 1, Batch 203, Loss: 1.4545893669128418\n",
      "Epoch 1, Batch 204, Loss: 1.4209206104278564\n",
      "Epoch 1, Batch 205, Loss: 1.3933144807815552\n",
      "Epoch 1, Batch 206, Loss: 1.4370073080062866\n",
      "Epoch 1, Batch 207, Loss: 1.4174736738204956\n",
      "Epoch 1, Batch 208, Loss: 1.3116240501403809\n",
      "Epoch 1, Batch 209, Loss: 1.3547394275665283\n",
      "Epoch 1, Batch 210, Loss: 1.2842864990234375\n",
      "Epoch 1, Batch 211, Loss: 1.2984446287155151\n",
      "Epoch 1, Batch 212, Loss: 1.3275198936462402\n",
      "Epoch 1, Batch 213, Loss: 1.2682353258132935\n",
      "Epoch 1, Batch 214, Loss: 1.2841455936431885\n",
      "Epoch 1, Batch 215, Loss: 1.3992035388946533\n",
      "Epoch 1, Batch 216, Loss: 1.371113896369934\n",
      "Epoch 1, Batch 217, Loss: 1.3734138011932373\n",
      "Epoch 1, Batch 218, Loss: 1.5001556873321533\n",
      "Epoch 1, Batch 219, Loss: 1.3993644714355469\n",
      "Epoch 1, Batch 220, Loss: 1.476870059967041\n",
      "Epoch 1, Batch 221, Loss: 1.2893956899642944\n",
      "Epoch 1, Batch 222, Loss: 1.4431524276733398\n",
      "Epoch 1, Batch 223, Loss: 1.4114491939544678\n",
      "Epoch 1, Batch 224, Loss: 1.3291503190994263\n",
      "Epoch 1, Batch 225, Loss: 1.3645267486572266\n",
      "Epoch 1, Batch 226, Loss: 1.2617133855819702\n",
      "Epoch 1, Batch 227, Loss: 1.29802405834198\n",
      "Epoch 1, Batch 228, Loss: 1.426722526550293\n",
      "Epoch 1, Batch 229, Loss: 1.3454878330230713\n",
      "Epoch 1, Batch 230, Loss: 1.3585654497146606\n",
      "Epoch 1, Batch 231, Loss: 1.3315927982330322\n",
      "Epoch 1, Batch 232, Loss: 1.3137611150741577\n",
      "Epoch 1, Batch 233, Loss: 1.435657262802124\n",
      "Epoch 1, Batch 234, Loss: 1.4438830614089966\n",
      "Epoch 1, Batch 235, Loss: 1.4034454822540283\n",
      "Epoch 1, Batch 236, Loss: 1.3588751554489136\n",
      "Epoch 1, Batch 237, Loss: 1.3801937103271484\n",
      "Epoch 1, Batch 238, Loss: 1.2923583984375\n",
      "Epoch 1, Batch 239, Loss: 1.3223316669464111\n",
      "Epoch 1, Batch 240, Loss: 1.3399170637130737\n",
      "Epoch 1, Batch 241, Loss: 1.2309991121292114\n",
      "Epoch 1, Batch 242, Loss: 1.311673641204834\n",
      "Epoch 1, Batch 243, Loss: 1.3883368968963623\n",
      "Epoch 1, Batch 244, Loss: 1.2887507677078247\n",
      "Epoch 1, Batch 245, Loss: 1.2884440422058105\n",
      "Epoch 1, Batch 246, Loss: 1.2893344163894653\n",
      "Epoch 1, Batch 247, Loss: 1.3121275901794434\n",
      "Epoch 1, Batch 248, Loss: 1.4186367988586426\n",
      "Epoch 1, Batch 249, Loss: 1.34080171585083\n",
      "Epoch 1, Batch 250, Loss: 1.3735883235931396\n",
      "Epoch 1, Batch 251, Loss: 1.3394134044647217\n",
      "Epoch 1, Batch 252, Loss: 1.361305594444275\n",
      "Epoch 1, Batch 253, Loss: 1.2993801832199097\n",
      "Epoch 1, Batch 254, Loss: 1.3283573389053345\n",
      "Epoch 1, Batch 255, Loss: 1.3546252250671387\n",
      "Epoch 1, Batch 256, Loss: 1.3140853643417358\n",
      "Epoch 1, Batch 257, Loss: 1.2752324342727661\n",
      "Epoch 1, Batch 258, Loss: 1.3030894994735718\n",
      "Epoch 1, Batch 259, Loss: 1.2554214000701904\n",
      "Epoch 1, Batch 260, Loss: 1.3752328157424927\n",
      "Epoch 1, Batch 261, Loss: 1.2888765335083008\n",
      "Epoch 1, Batch 262, Loss: 1.2770463228225708\n",
      "Epoch 1, Batch 263, Loss: 1.3170313835144043\n",
      "Epoch 1, Batch 264, Loss: 1.434187650680542\n",
      "Epoch 1, Batch 265, Loss: 1.2714282274246216\n",
      "Epoch 1, Batch 266, Loss: 1.374358057975769\n",
      "Epoch 1, Batch 267, Loss: 1.3181709051132202\n",
      "Epoch 1, Batch 268, Loss: 1.30082106590271\n",
      "Epoch 1, Batch 269, Loss: 1.3508394956588745\n",
      "Epoch 1, Batch 270, Loss: 1.3951597213745117\n",
      "Epoch 1, Batch 271, Loss: 1.2587167024612427\n",
      "Epoch 1, Batch 272, Loss: 1.2561304569244385\n",
      "Epoch 1, Batch 273, Loss: 1.2625036239624023\n",
      "Epoch 1, Batch 274, Loss: 1.2351149320602417\n",
      "Epoch 1, Batch 275, Loss: 1.3160772323608398\n",
      "Epoch 1, Batch 276, Loss: 1.2813791036605835\n",
      "Epoch 1, Batch 277, Loss: 1.3375747203826904\n",
      "Epoch 1, Batch 278, Loss: 1.3010097742080688\n",
      "Epoch 1, Batch 279, Loss: 1.4430456161499023\n",
      "Epoch 1, Batch 280, Loss: 1.3237591981887817\n",
      "Epoch 1, Batch 281, Loss: 1.3367856740951538\n",
      "Epoch 1, Batch 282, Loss: 1.2827467918395996\n",
      "Epoch 1, Batch 283, Loss: 1.283764362335205\n",
      "Epoch 1, Batch 284, Loss: 1.3502798080444336\n",
      "Epoch 1, Batch 285, Loss: 1.3391519784927368\n",
      "Epoch 1, Batch 286, Loss: 1.2955865859985352\n",
      "Epoch 1, Batch 287, Loss: 1.2528389692306519\n",
      "Epoch 1, Batch 288, Loss: 1.2775053977966309\n",
      "Epoch 1, Batch 289, Loss: 1.372909665107727\n",
      "Epoch 1, Batch 290, Loss: 1.2804824113845825\n",
      "Epoch 1, Batch 291, Loss: 1.3137367963790894\n",
      "Epoch 1, Batch 292, Loss: 1.3825905323028564\n",
      "Epoch 1, Batch 293, Loss: 1.340795636177063\n",
      "Epoch 1, Batch 294, Loss: 1.320501685142517\n",
      "Epoch 1, Batch 295, Loss: 1.2990282773971558\n",
      "Epoch 1, Batch 296, Loss: 1.3829030990600586\n",
      "Epoch 1, Batch 297, Loss: 1.286052942276001\n",
      "Epoch 1, Batch 298, Loss: 1.446336030960083\n",
      "Epoch 1, Batch 299, Loss: 1.2359020709991455\n",
      "Epoch 1, Batch 300, Loss: 1.2943994998931885\n",
      "Epoch 1, Batch 301, Loss: 1.2503834962844849\n",
      "Epoch 1, Batch 302, Loss: 1.3500635623931885\n",
      "Epoch 1, Batch 303, Loss: 1.2160309553146362\n",
      "Epoch 1, Batch 304, Loss: 1.2855790853500366\n",
      "Epoch 1, Batch 305, Loss: 1.3982793092727661\n",
      "Epoch 1, Batch 306, Loss: 1.2055155038833618\n",
      "Epoch 1, Batch 307, Loss: 1.290576457977295\n",
      "Epoch 1, Batch 308, Loss: 1.158163070678711\n",
      "Epoch 1, Batch 309, Loss: 1.3144489526748657\n",
      "Epoch 1, Batch 310, Loss: 1.249575138092041\n",
      "Epoch 1, Batch 311, Loss: 1.454209327697754\n",
      "Epoch 1, Batch 312, Loss: 1.2747888565063477\n",
      "Epoch 1, Batch 313, Loss: 1.2302606105804443\n",
      "Epoch 1, Batch 314, Loss: 1.2495471239089966\n",
      "Epoch 1, Batch 315, Loss: 1.2555177211761475\n",
      "Epoch 1, Batch 316, Loss: 1.3133645057678223\n",
      "Epoch 1, Batch 317, Loss: 1.3033427000045776\n",
      "Epoch 1, Batch 318, Loss: 1.3991657495498657\n",
      "Epoch 1, Batch 319, Loss: 1.1978425979614258\n",
      "Epoch 1, Batch 320, Loss: 1.2643128633499146\n",
      "Epoch 1, Batch 321, Loss: 1.3292722702026367\n",
      "Epoch 1, Batch 322, Loss: 1.2245696783065796\n",
      "Epoch 1, Batch 323, Loss: 1.4000282287597656\n",
      "Epoch 1, Batch 324, Loss: 1.2974257469177246\n",
      "Epoch 1, Batch 325, Loss: 1.2467035055160522\n",
      "Epoch 1, Batch 326, Loss: 1.3253456354141235\n",
      "Epoch 1, Batch 327, Loss: 1.3202043771743774\n",
      "Epoch 1, Batch 328, Loss: 1.181006669998169\n",
      "Epoch 1, Batch 329, Loss: 1.2451133728027344\n",
      "Epoch 1, Batch 330, Loss: 1.2903953790664673\n",
      "Epoch 1, Batch 331, Loss: 1.3763432502746582\n",
      "Epoch 1, Batch 332, Loss: 1.2119760513305664\n",
      "Epoch 1, Batch 333, Loss: 1.3626644611358643\n",
      "Epoch 1, Batch 334, Loss: 1.3732056617736816\n",
      "Epoch 1, Batch 335, Loss: 1.3106200695037842\n",
      "Epoch 1, Batch 336, Loss: 1.3542982339859009\n",
      "Epoch 1, Batch 337, Loss: 1.171342134475708\n",
      "Epoch 1, Batch 338, Loss: 1.3701642751693726\n",
      "Epoch 1, Batch 339, Loss: 1.2743061780929565\n",
      "Epoch 1, Batch 340, Loss: 1.3252637386322021\n",
      "Epoch 1, Batch 341, Loss: 1.2492685317993164\n",
      "Epoch 1, Batch 342, Loss: 1.4024580717086792\n",
      "Epoch 1, Batch 343, Loss: 1.231823444366455\n",
      "Epoch 1, Batch 344, Loss: 1.3319158554077148\n",
      "Epoch 1, Batch 345, Loss: 1.2801306247711182\n",
      "Epoch 1, Batch 346, Loss: 1.3155956268310547\n",
      "Epoch 1, Batch 347, Loss: 1.3213838338851929\n",
      "Epoch 1, Batch 348, Loss: 1.325447916984558\n",
      "Epoch 1, Batch 349, Loss: 1.253635287284851\n",
      "Epoch 1, Batch 350, Loss: 1.4129531383514404\n",
      "Epoch 1, Batch 351, Loss: 1.2466627359390259\n",
      "Epoch 1, Batch 352, Loss: 1.3480784893035889\n",
      "Epoch 1, Batch 353, Loss: 1.3880046606063843\n",
      "Epoch 1, Batch 354, Loss: 1.3008601665496826\n",
      "Epoch 1, Batch 355, Loss: 1.3891319036483765\n",
      "Epoch 1, Batch 356, Loss: 1.2292094230651855\n",
      "Epoch 1, Batch 357, Loss: 1.363574743270874\n",
      "Epoch 1, Batch 358, Loss: 1.254515528678894\n",
      "Epoch 1, Batch 359, Loss: 1.2388184070587158\n",
      "Epoch 1, Batch 360, Loss: 1.2394938468933105\n",
      "Epoch 1, Batch 361, Loss: 1.2541720867156982\n",
      "Epoch 1, Batch 362, Loss: 1.3113458156585693\n",
      "Epoch 1, Batch 363, Loss: 1.213533878326416\n",
      "Epoch 1, Batch 364, Loss: 1.3148913383483887\n",
      "Epoch 1, Batch 365, Loss: 1.197763442993164\n",
      "Epoch 1, Batch 366, Loss: 1.2538957595825195\n",
      "Epoch 1, Batch 367, Loss: 1.3127321004867554\n",
      "Epoch 1, Batch 368, Loss: 1.2572296857833862\n",
      "Epoch 1, Batch 369, Loss: 1.286607027053833\n",
      "Epoch 1, Batch 370, Loss: 1.297892689704895\n",
      "Epoch 1, Batch 371, Loss: 1.3502838611602783\n",
      "Epoch 1, Batch 372, Loss: 1.1967151165008545\n",
      "Epoch 1, Batch 373, Loss: 1.220588207244873\n",
      "Epoch 1, Batch 374, Loss: 1.2434840202331543\n",
      "Epoch 1, Batch 375, Loss: 1.1815745830535889\n",
      "Epoch 1, Batch 376, Loss: 1.4045145511627197\n",
      "Epoch 1, Batch 377, Loss: 1.2063665390014648\n",
      "Epoch 1, Batch 378, Loss: 1.2687040567398071\n",
      "Epoch 1, Batch 379, Loss: 1.1028351783752441\n",
      "Epoch 1, Batch 380, Loss: 1.2389514446258545\n",
      "Epoch 1, Batch 381, Loss: 1.2105759382247925\n",
      "Epoch 1, Batch 382, Loss: 1.240508794784546\n",
      "Epoch 1, Batch 383, Loss: 1.1948809623718262\n",
      "Epoch 1, Batch 384, Loss: 1.2027761936187744\n",
      "Epoch 1, Batch 385, Loss: 1.2632205486297607\n",
      "Epoch 1, Batch 386, Loss: 1.444199800491333\n",
      "Epoch 1, Batch 387, Loss: 1.3182203769683838\n",
      "Epoch 1, Batch 388, Loss: 1.1932575702667236\n",
      "Epoch 1, Batch 389, Loss: 1.2680494785308838\n",
      "Epoch 1, Batch 390, Loss: 1.1323637962341309\n",
      "Epoch 1, Batch 391, Loss: 1.2427570819854736\n",
      "Epoch 1, Batch 392, Loss: 1.3306020498275757\n",
      "Epoch 1, Batch 393, Loss: 1.3716844320297241\n",
      "Epoch 1, Batch 394, Loss: 1.304319143295288\n",
      "Epoch 1, Batch 395, Loss: 1.2367002964019775\n",
      "Epoch 1, Batch 396, Loss: 1.2635334730148315\n",
      "Epoch 1, Batch 397, Loss: 1.321942687034607\n",
      "Epoch 1, Batch 398, Loss: 1.1532682180404663\n",
      "Epoch 1, Batch 399, Loss: 1.2309390306472778\n",
      "Epoch 1, Batch 400, Loss: 1.2868645191192627\n",
      "Epoch 1, Batch 401, Loss: 1.211336374282837\n",
      "Epoch 1, Batch 402, Loss: 1.2876636981964111\n",
      "Epoch 1, Batch 403, Loss: 1.2290050983428955\n",
      "Epoch 1, Batch 404, Loss: 1.2633943557739258\n",
      "Epoch 1, Batch 405, Loss: 1.2530852556228638\n",
      "Epoch 1, Batch 406, Loss: 1.1632797718048096\n",
      "Epoch 1, Batch 407, Loss: 1.1885480880737305\n",
      "Epoch 1, Batch 408, Loss: 1.2926998138427734\n",
      "Epoch 1, Batch 409, Loss: 1.265910267829895\n",
      "Epoch 1, Batch 410, Loss: 1.2781575918197632\n",
      "Epoch 1, Batch 411, Loss: 1.3385586738586426\n",
      "Epoch 1, Batch 412, Loss: 1.0673236846923828\n",
      "Epoch 1, Batch 413, Loss: 1.176544427871704\n",
      "Epoch 1, Batch 414, Loss: 1.1624436378479004\n",
      "Epoch 1, Batch 415, Loss: 1.1862573623657227\n",
      "Epoch 1, Batch 416, Loss: 1.2261669635772705\n",
      "Epoch 1, Batch 417, Loss: 1.2828927040100098\n",
      "Epoch 1, Batch 418, Loss: 1.3236401081085205\n",
      "Epoch 1, Batch 419, Loss: 1.2185392379760742\n",
      "Epoch 1, Batch 420, Loss: 1.1297906637191772\n",
      "Epoch 1, Batch 421, Loss: 1.2712805271148682\n",
      "Epoch 1, Batch 422, Loss: 1.2708795070648193\n",
      "Epoch 1, Batch 423, Loss: 1.2452512979507446\n",
      "Epoch 1, Batch 424, Loss: 1.3300970792770386\n",
      "Epoch 1, Batch 425, Loss: 1.3740777969360352\n",
      "Epoch 1, Batch 426, Loss: 1.2135274410247803\n",
      "Epoch 1, Batch 427, Loss: 1.3492436408996582\n",
      "Epoch 1, Batch 428, Loss: 1.2831590175628662\n",
      "Epoch 1, Batch 429, Loss: 1.344150424003601\n",
      "Epoch 1, Batch 430, Loss: 1.1727696657180786\n",
      "Epoch 1, Batch 431, Loss: 1.2348895072937012\n",
      "Epoch 1, Batch 432, Loss: 1.2337735891342163\n",
      "Epoch 1, Batch 433, Loss: 1.2199535369873047\n",
      "Epoch 1, Batch 434, Loss: 1.3827167749404907\n",
      "Epoch 1, Batch 435, Loss: 1.28160560131073\n",
      "Epoch 1, Batch 436, Loss: 1.2443478107452393\n",
      "Epoch 1, Batch 437, Loss: 1.1425946950912476\n",
      "Epoch 1, Batch 438, Loss: 1.1801685094833374\n",
      "Epoch 1, Batch 439, Loss: 1.0896732807159424\n",
      "Epoch 1, Batch 440, Loss: 1.236131191253662\n",
      "Epoch 1, Batch 441, Loss: 1.162388563156128\n",
      "Epoch 1, Batch 442, Loss: 1.130930781364441\n",
      "Epoch 1, Batch 443, Loss: 1.2273621559143066\n",
      "Epoch 1, Batch 444, Loss: 1.2199395895004272\n",
      "Epoch 1, Batch 445, Loss: 1.2371680736541748\n",
      "Epoch 1, Batch 446, Loss: 1.1943482160568237\n",
      "Epoch 1, Batch 447, Loss: 1.265358328819275\n",
      "Epoch 1, Batch 448, Loss: 1.139050841331482\n",
      "Epoch 1, Batch 449, Loss: 1.2946784496307373\n",
      "Epoch 1, Batch 450, Loss: 1.1744877099990845\n",
      "Epoch 1, Batch 451, Loss: 1.2294578552246094\n",
      "Epoch 1, Batch 452, Loss: 1.2498984336853027\n",
      "Epoch 1, Batch 453, Loss: 1.1556658744812012\n",
      "Epoch 1, Batch 454, Loss: 1.2346714735031128\n",
      "Epoch 1, Batch 455, Loss: 1.2303465604782104\n",
      "Epoch 1, Batch 456, Loss: 1.1342966556549072\n",
      "Epoch 1, Batch 457, Loss: 1.191325306892395\n",
      "Epoch 1, Batch 458, Loss: 1.313321828842163\n",
      "Epoch 1, Batch 459, Loss: 1.1802797317504883\n",
      "Epoch 1, Batch 460, Loss: 1.213666558265686\n",
      "Epoch 1, Batch 461, Loss: 1.1868305206298828\n",
      "Epoch 1, Batch 462, Loss: 1.2290645837783813\n",
      "Epoch 1, Batch 463, Loss: 1.1999884843826294\n",
      "Epoch 1, Batch 464, Loss: 1.1433137655258179\n",
      "Epoch 1, Batch 465, Loss: 1.151441216468811\n",
      "Epoch 1, Batch 466, Loss: 1.30917227268219\n",
      "Epoch 1, Batch 467, Loss: 1.0971951484680176\n",
      "Epoch 1, Batch 468, Loss: 1.323683738708496\n",
      "Epoch 1, Batch 469, Loss: 1.299835205078125\n",
      "Epoch 1, Batch 470, Loss: 1.3078527450561523\n",
      "Epoch 1, Batch 471, Loss: 1.1059787273406982\n",
      "Epoch 1, Batch 472, Loss: 1.106705665588379\n",
      "Epoch 1, Batch 473, Loss: 1.3632886409759521\n",
      "Epoch 1, Batch 474, Loss: 1.1541379690170288\n",
      "Epoch 1, Batch 475, Loss: 1.2208483219146729\n",
      "Epoch 1, Batch 476, Loss: 1.289705514907837\n",
      "Epoch 1, Batch 477, Loss: 1.249682903289795\n",
      "Epoch 1, Batch 478, Loss: 1.116916537284851\n",
      "Epoch 1, Batch 479, Loss: 1.1742793321609497\n",
      "Epoch 1, Batch 480, Loss: 1.3804666996002197\n",
      "Epoch 1, Batch 481, Loss: 1.2624706029891968\n",
      "Epoch 1, Batch 482, Loss: 1.1244595050811768\n",
      "Epoch 1, Batch 483, Loss: 1.1644319295883179\n",
      "Epoch 1, Batch 484, Loss: 1.1885753870010376\n",
      "Epoch 1, Batch 485, Loss: 1.105826735496521\n",
      "Epoch 1, Batch 486, Loss: 1.239375114440918\n",
      "Epoch 1, Batch 487, Loss: 1.1817551851272583\n",
      "Epoch 1, Batch 488, Loss: 1.1922805309295654\n",
      "Epoch 1, Batch 489, Loss: 1.2785848379135132\n",
      "Epoch 1, Batch 490, Loss: 1.2180694341659546\n",
      "Epoch 1, Batch 491, Loss: 1.209611177444458\n",
      "Epoch 1, Batch 492, Loss: 1.262956142425537\n",
      "Epoch 1, Batch 493, Loss: 1.153594732284546\n",
      "Epoch 1, Batch 494, Loss: 1.122354507446289\n",
      "Epoch 1, Batch 495, Loss: 1.0878081321716309\n",
      "Epoch 1, Batch 496, Loss: 1.192994475364685\n",
      "Epoch 1, Batch 497, Loss: 1.2494245767593384\n",
      "Epoch 1, Batch 498, Loss: 1.1096352338790894\n",
      "Epoch 1, Batch 499, Loss: 1.207000970840454\n",
      "Epoch 1, Batch 500, Loss: 1.2715731859207153\n",
      "Epoch 1, Batch 501, Loss: 1.1306387186050415\n",
      "Epoch 1, Batch 502, Loss: 1.12482488155365\n",
      "Epoch 1, Batch 503, Loss: 1.1710795164108276\n",
      "Epoch 1, Batch 504, Loss: 1.1496460437774658\n",
      "Epoch 1, Batch 505, Loss: 1.2090885639190674\n",
      "Epoch 1, Batch 506, Loss: 1.2087171077728271\n",
      "Epoch 1, Batch 507, Loss: 1.167263150215149\n",
      "Epoch 1, Batch 508, Loss: 1.2282137870788574\n",
      "Epoch 1, Batch 509, Loss: 1.2677432298660278\n",
      "Epoch 1, Batch 510, Loss: 1.1957337856292725\n",
      "Epoch 1, Batch 511, Loss: 1.2453209161758423\n",
      "Epoch 1, Batch 512, Loss: 1.0764832496643066\n",
      "Epoch 1, Batch 513, Loss: 1.2014020681381226\n",
      "Epoch 1, Batch 514, Loss: 1.1633802652359009\n",
      "Epoch 1, Batch 515, Loss: 1.1948437690734863\n",
      "Epoch 1, Batch 516, Loss: 1.202176809310913\n",
      "Epoch 1, Batch 517, Loss: 1.191233515739441\n",
      "Epoch 1, Batch 518, Loss: 1.2170361280441284\n",
      "Epoch 1, Batch 519, Loss: 1.1759101152420044\n",
      "Epoch 1, Batch 520, Loss: 1.134018063545227\n",
      "Epoch 1, Batch 521, Loss: 1.0556310415267944\n",
      "Epoch 1, Batch 522, Loss: 1.342505931854248\n",
      "Epoch 1, Batch 523, Loss: 1.1927660703659058\n",
      "Epoch 1, Batch 524, Loss: 1.136775255203247\n",
      "Epoch 1, Batch 525, Loss: 1.146207571029663\n",
      "Epoch 1, Batch 526, Loss: 1.171116828918457\n",
      "Epoch 1, Batch 527, Loss: 1.3084635734558105\n",
      "Epoch 1, Batch 528, Loss: 1.249138593673706\n",
      "Epoch 1, Batch 529, Loss: 1.200415015220642\n",
      "Epoch 1, Batch 530, Loss: 1.0722622871398926\n",
      "Epoch 1, Batch 531, Loss: 1.2767643928527832\n",
      "Epoch 1, Batch 532, Loss: 1.117777943611145\n",
      "Epoch 1, Batch 533, Loss: 1.1573973894119263\n",
      "Epoch 1, Batch 534, Loss: 1.3120478391647339\n",
      "Epoch 1, Batch 535, Loss: 1.0749775171279907\n",
      "Epoch 1, Batch 536, Loss: 1.163559079170227\n",
      "Epoch 1, Batch 537, Loss: 1.1336374282836914\n",
      "Epoch 1, Batch 538, Loss: 1.1940512657165527\n",
      "Epoch 1, Batch 539, Loss: 1.2084190845489502\n",
      "Epoch 1, Batch 540, Loss: 1.11294424533844\n",
      "Epoch 1, Batch 541, Loss: 1.1173101663589478\n",
      "Epoch 1, Batch 542, Loss: 1.2319080829620361\n",
      "Epoch 1, Batch 543, Loss: 1.2393728494644165\n",
      "Epoch 1, Batch 544, Loss: 1.270476222038269\n",
      "Epoch 1, Batch 545, Loss: 1.1662739515304565\n",
      "Epoch 1, Batch 546, Loss: 1.058793306350708\n",
      "Epoch 1, Batch 547, Loss: 0.9752861261367798\n",
      "Epoch 1, Batch 548, Loss: 1.1385927200317383\n",
      "Epoch 1, Batch 549, Loss: 1.2758667469024658\n",
      "Epoch 1, Batch 550, Loss: 1.1894619464874268\n",
      "Epoch 1, Batch 551, Loss: 1.144126057624817\n",
      "Epoch 1, Batch 552, Loss: 1.1307693719863892\n",
      "Epoch 1, Batch 553, Loss: 1.1647319793701172\n",
      "Epoch 1, Batch 554, Loss: 1.2218269109725952\n",
      "Epoch 1, Batch 555, Loss: 1.05131196975708\n",
      "Epoch 1, Batch 556, Loss: 1.1327669620513916\n",
      "Epoch 1, Batch 557, Loss: 1.0819555521011353\n",
      "Epoch 1, Batch 558, Loss: 1.1466166973114014\n",
      "Epoch 1, Batch 559, Loss: 1.0966851711273193\n",
      "Epoch 1, Batch 560, Loss: 1.1977248191833496\n",
      "Epoch 1, Batch 561, Loss: 1.140576720237732\n",
      "Epoch 1, Batch 562, Loss: 1.1740660667419434\n",
      "Epoch 1, Batch 563, Loss: 1.0265986919403076\n",
      "Epoch 1, Batch 564, Loss: 1.1827669143676758\n",
      "Epoch 1, Batch 565, Loss: 1.0937862396240234\n",
      "Epoch 1, Batch 566, Loss: 1.0673906803131104\n",
      "Epoch 1, Batch 567, Loss: 1.258165955543518\n",
      "Epoch 1, Batch 568, Loss: 1.1879318952560425\n",
      "Epoch 1, Batch 569, Loss: 1.0953859090805054\n",
      "Epoch 1, Batch 570, Loss: 1.0718743801116943\n",
      "Epoch 1, Batch 571, Loss: 1.0164787769317627\n",
      "Epoch 1, Batch 572, Loss: 1.1371451616287231\n",
      "Epoch 1, Batch 573, Loss: 1.2430778741836548\n",
      "Epoch 1, Batch 574, Loss: 1.137763261795044\n",
      "Epoch 1, Batch 575, Loss: 1.0902622938156128\n",
      "Epoch 1, Batch 576, Loss: 1.2084957361221313\n",
      "Epoch 1, Batch 577, Loss: 1.0912883281707764\n",
      "Epoch 1, Batch 578, Loss: 1.1152626276016235\n",
      "Epoch 1, Batch 579, Loss: 1.1728376150131226\n",
      "Epoch 1, Batch 580, Loss: 1.1967130899429321\n",
      "Epoch 1, Batch 581, Loss: 1.1108640432357788\n",
      "Epoch 1, Batch 582, Loss: 1.0614864826202393\n",
      "Epoch 1, Batch 583, Loss: 1.0494886636734009\n",
      "Epoch 1, Batch 584, Loss: 1.2068980932235718\n",
      "Epoch 1, Batch 585, Loss: 1.1230013370513916\n",
      "Epoch 1, Batch 586, Loss: 0.9427234530448914\n",
      "Epoch 1, Batch 587, Loss: 1.0824496746063232\n",
      "Epoch 1, Batch 588, Loss: 1.1546194553375244\n",
      "Epoch 1, Batch 589, Loss: 1.1470764875411987\n",
      "Epoch 1, Batch 590, Loss: 1.0294331312179565\n",
      "Epoch 1, Batch 591, Loss: 1.4544429779052734\n",
      "Epoch 1, Batch 592, Loss: 1.151697039604187\n",
      "Epoch 1, Batch 593, Loss: 1.0723727941513062\n",
      "Epoch 1, Batch 594, Loss: 1.1951991319656372\n",
      "Epoch 1, Batch 595, Loss: 1.1311577558517456\n",
      "Epoch 1, Batch 596, Loss: 1.214245319366455\n",
      "Epoch 1, Batch 597, Loss: 1.1506445407867432\n",
      "Epoch 1, Batch 598, Loss: 1.1558454036712646\n",
      "Epoch 1, Batch 599, Loss: 1.1437057256698608\n",
      "Epoch 1, Batch 600, Loss: 1.2512445449829102\n",
      "Epoch 1, Batch 601, Loss: 1.0623142719268799\n",
      "Epoch 1, Batch 602, Loss: 1.1252516508102417\n",
      "Epoch 1, Batch 603, Loss: 1.1491763591766357\n",
      "Epoch 1, Batch 604, Loss: 1.093072533607483\n",
      "Epoch 1, Batch 605, Loss: 1.2836973667144775\n",
      "Epoch 1, Batch 606, Loss: 1.1360046863555908\n",
      "Epoch 1, Batch 607, Loss: 1.1445833444595337\n",
      "Epoch 1, Batch 608, Loss: 1.161909818649292\n",
      "Epoch 1, Batch 609, Loss: 1.175459384918213\n",
      "Epoch 1, Batch 610, Loss: 1.3100614547729492\n",
      "Epoch 1, Batch 611, Loss: 1.1353322267532349\n",
      "Epoch 1, Batch 612, Loss: 1.1869157552719116\n",
      "Epoch 1, Batch 613, Loss: 1.3669682741165161\n",
      "Epoch 1, Batch 614, Loss: 1.1382274627685547\n",
      "Epoch 1, Batch 615, Loss: 1.1810734272003174\n",
      "Epoch 1, Batch 616, Loss: 1.1013214588165283\n",
      "Epoch 1, Batch 617, Loss: 1.2421317100524902\n",
      "Epoch 1, Batch 618, Loss: 1.1916491985321045\n",
      "Epoch 1, Batch 619, Loss: 1.0845866203308105\n",
      "Epoch 1, Batch 620, Loss: 1.1605675220489502\n",
      "Epoch 1, Batch 621, Loss: 1.1607356071472168\n",
      "Epoch 1, Batch 622, Loss: 1.1204967498779297\n",
      "Epoch 1, Batch 623, Loss: 1.0106451511383057\n",
      "Epoch 1, Batch 624, Loss: 1.0883164405822754\n",
      "Epoch 1, Batch 625, Loss: 1.0902212858200073\n",
      "Epoch 1, Batch 626, Loss: 1.1734217405319214\n",
      "Epoch 1, Batch 627, Loss: 1.1718299388885498\n",
      "Epoch 1, Batch 628, Loss: 1.1894251108169556\n",
      "Epoch 1, Batch 629, Loss: 1.0824874639511108\n",
      "Epoch 1, Batch 630, Loss: 1.063392162322998\n",
      "Epoch 1, Batch 631, Loss: 1.3243073225021362\n",
      "Epoch 1, Batch 632, Loss: 1.1995643377304077\n",
      "Epoch 1, Batch 633, Loss: 1.0478065013885498\n",
      "Epoch 1, Batch 634, Loss: 1.049768090248108\n",
      "Epoch 1, Batch 635, Loss: 1.3734468221664429\n",
      "Epoch 1, Batch 636, Loss: 1.1299834251403809\n",
      "Epoch 1, Batch 637, Loss: 1.1736305952072144\n",
      "Epoch 1, Batch 638, Loss: 1.0859687328338623\n",
      "Epoch 1, Batch 639, Loss: 1.1392855644226074\n",
      "Epoch 1, Batch 640, Loss: 1.099366307258606\n",
      "Epoch 1, Batch 641, Loss: 1.2566062211990356\n",
      "Epoch 1, Batch 642, Loss: 1.199102759361267\n",
      "Epoch 1, Batch 643, Loss: 1.2058987617492676\n",
      "Epoch 1, Batch 644, Loss: 1.0387201309204102\n",
      "Epoch 1, Batch 645, Loss: 1.1351597309112549\n",
      "Epoch 1, Batch 646, Loss: 1.1981642246246338\n",
      "Epoch 1, Batch 647, Loss: 1.1133449077606201\n",
      "Epoch 1, Batch 648, Loss: 1.1859490871429443\n",
      "Epoch 1, Batch 649, Loss: 1.1653096675872803\n",
      "Epoch 1, Batch 650, Loss: 1.1663521528244019\n",
      "Epoch 1, Batch 651, Loss: 1.1978962421417236\n",
      "Epoch 1, Batch 652, Loss: 1.0736265182495117\n",
      "Epoch 1, Batch 653, Loss: 1.2779396772384644\n",
      "Epoch 1, Batch 654, Loss: 1.1603009700775146\n",
      "Epoch 1, Batch 655, Loss: 1.03670334815979\n",
      "Epoch 1, Batch 656, Loss: 1.0255022048950195\n",
      "Epoch 1, Batch 657, Loss: 1.1334904432296753\n",
      "Epoch 1, Batch 658, Loss: 1.1995251178741455\n",
      "Epoch 1, Batch 659, Loss: 1.1589871644973755\n",
      "Epoch 1, Batch 660, Loss: 1.0733630657196045\n",
      "Epoch 1, Batch 661, Loss: 1.0452399253845215\n",
      "Epoch 1, Batch 662, Loss: 1.1154810190200806\n",
      "Epoch 1, Batch 663, Loss: 1.190037488937378\n",
      "Epoch 1, Batch 664, Loss: 1.0566484928131104\n",
      "Epoch 1, Batch 665, Loss: 1.0687756538391113\n",
      "Epoch 1, Batch 666, Loss: 1.143580675125122\n",
      "Epoch 1, Batch 667, Loss: 0.9867067337036133\n",
      "Epoch 1, Batch 668, Loss: 1.201559066772461\n",
      "Epoch 1, Batch 669, Loss: 1.0796301364898682\n",
      "Epoch 1, Batch 670, Loss: 1.1274527311325073\n",
      "Epoch 1, Batch 671, Loss: 1.1591014862060547\n",
      "Epoch 1, Batch 672, Loss: 1.172003984451294\n",
      "Epoch 1, Batch 673, Loss: 1.1972689628601074\n",
      "Epoch 1, Batch 674, Loss: 1.0529860258102417\n",
      "Epoch 1, Batch 675, Loss: 1.1143710613250732\n",
      "Epoch 1, Batch 676, Loss: 1.1185637712478638\n",
      "Epoch 1, Batch 677, Loss: 1.048985242843628\n",
      "Epoch 1, Batch 678, Loss: 1.1561014652252197\n",
      "Epoch 1, Batch 679, Loss: 1.0474402904510498\n",
      "Epoch 1, Batch 680, Loss: 1.2524542808532715\n",
      "Epoch 1, Batch 681, Loss: 1.186234712600708\n",
      "Epoch 1, Batch 682, Loss: 0.9517205953598022\n",
      "Epoch 1, Batch 683, Loss: 1.1042290925979614\n",
      "Epoch 1, Batch 684, Loss: 1.149693489074707\n",
      "Epoch 1, Batch 685, Loss: 1.2098060846328735\n",
      "Epoch 1, Batch 686, Loss: 1.0879466533660889\n",
      "Epoch 1, Batch 687, Loss: 1.0479464530944824\n",
      "Epoch 1, Batch 688, Loss: 1.2053091526031494\n",
      "Epoch 1, Batch 689, Loss: 1.0553951263427734\n",
      "Epoch 1, Batch 690, Loss: 1.1509028673171997\n",
      "Epoch 1, Batch 691, Loss: 1.308592438697815\n",
      "Epoch 1, Batch 692, Loss: 1.0129514932632446\n",
      "Epoch 1, Batch 693, Loss: 1.1236664056777954\n",
      "Epoch 1, Batch 694, Loss: 1.1634242534637451\n",
      "Epoch 1, Batch 695, Loss: 1.1154255867004395\n",
      "Epoch 1, Batch 696, Loss: 1.1829450130462646\n",
      "Epoch 1, Batch 697, Loss: 1.0387835502624512\n",
      "Epoch 1, Batch 698, Loss: 1.0091168880462646\n",
      "Epoch 1, Batch 699, Loss: 1.0872615575790405\n",
      "Epoch 1, Batch 700, Loss: 1.057725429534912\n",
      "Epoch 1, Batch 701, Loss: 1.185405969619751\n",
      "Epoch 1, Batch 702, Loss: 1.0124796628952026\n",
      "Epoch 1, Batch 703, Loss: 0.979465663433075\n",
      "Epoch 1, Batch 704, Loss: 1.0503699779510498\n",
      "Epoch 1, Batch 705, Loss: 1.0615277290344238\n",
      "Epoch 1, Batch 706, Loss: 1.0786762237548828\n",
      "Epoch 1, Batch 707, Loss: 1.174344778060913\n",
      "Epoch 1, Batch 708, Loss: 1.0577393770217896\n",
      "Epoch 1, Batch 709, Loss: 1.0313189029693604\n",
      "Epoch 1, Batch 710, Loss: 1.069851040840149\n",
      "Epoch 1, Batch 711, Loss: 1.1393836736679077\n",
      "Epoch 1, Batch 712, Loss: 1.03031325340271\n",
      "Epoch 1, Batch 713, Loss: 0.9790053963661194\n",
      "Epoch 1, Batch 714, Loss: 1.1393200159072876\n",
      "Epoch 1, Batch 715, Loss: 1.1645678281784058\n",
      "Epoch 1, Batch 716, Loss: 1.0185096263885498\n",
      "Epoch 1, Batch 717, Loss: 1.1117398738861084\n",
      "Epoch 1, Batch 718, Loss: 1.2213393449783325\n",
      "Epoch 1, Batch 719, Loss: 1.3314517736434937\n",
      "Epoch 1, Batch 720, Loss: 1.0546648502349854\n",
      "Epoch 1, Batch 721, Loss: 1.048880934715271\n",
      "Epoch 1, Batch 722, Loss: 0.9605112671852112\n",
      "Epoch 1, Batch 723, Loss: 1.0757911205291748\n",
      "Epoch 1, Batch 724, Loss: 1.091011643409729\n",
      "Epoch 1, Batch 725, Loss: 1.0120612382888794\n",
      "Epoch 1, Batch 726, Loss: 1.2312155961990356\n",
      "Epoch 1, Batch 727, Loss: 1.1263208389282227\n",
      "Epoch 1, Batch 728, Loss: 1.1058156490325928\n",
      "Epoch 1, Batch 729, Loss: 1.0068690776824951\n",
      "Epoch 1, Batch 730, Loss: 1.1068111658096313\n",
      "Epoch 1, Batch 731, Loss: 1.137916088104248\n",
      "Epoch 1, Batch 732, Loss: 1.0936750173568726\n",
      "Epoch 1, Batch 733, Loss: 1.2661279439926147\n",
      "Epoch 1, Batch 734, Loss: 1.2664036750793457\n",
      "Epoch 1, Batch 735, Loss: 1.2185368537902832\n",
      "Epoch 1, Batch 736, Loss: 1.0939157009124756\n",
      "Epoch 1, Batch 737, Loss: 1.114789605140686\n",
      "Epoch 1, Batch 738, Loss: 1.0886422395706177\n",
      "Epoch 1, Batch 739, Loss: 1.1446901559829712\n",
      "Epoch 1, Batch 740, Loss: 1.1870853900909424\n",
      "Epoch 1, Batch 741, Loss: 1.0577439069747925\n",
      "Epoch 1, Batch 742, Loss: 1.2494754791259766\n",
      "Epoch 1, Batch 743, Loss: 1.098363995552063\n",
      "Epoch 1, Batch 744, Loss: 1.07558274269104\n",
      "Epoch 1, Batch 745, Loss: 1.0147556066513062\n",
      "Epoch 1, Batch 746, Loss: 1.149054765701294\n",
      "Epoch 1, Batch 747, Loss: 1.033129096031189\n",
      "Epoch 1, Batch 748, Loss: 1.029457926750183\n",
      "Epoch 1, Batch 749, Loss: 1.0984430313110352\n",
      "Epoch 1, Batch 750, Loss: 1.0191707611083984\n",
      "Epoch 1, Batch 751, Loss: 1.1945536136627197\n",
      "Epoch 1, Batch 752, Loss: 1.2224342823028564\n",
      "Epoch 1, Batch 753, Loss: 1.003871202468872\n",
      "Epoch 1, Batch 754, Loss: 1.0262936353683472\n",
      "Epoch 1, Batch 755, Loss: 1.2354800701141357\n",
      "Epoch 1, Batch 756, Loss: 1.045348048210144\n",
      "Epoch 1, Batch 757, Loss: 1.2421889305114746\n",
      "Epoch 1, Batch 758, Loss: 1.2216521501541138\n",
      "Epoch 1, Batch 759, Loss: 1.0495121479034424\n",
      "Epoch 1, Batch 760, Loss: 0.9966204166412354\n",
      "Epoch 1, Batch 761, Loss: 1.0288119316101074\n",
      "Epoch 1, Batch 762, Loss: 1.1231695413589478\n",
      "Epoch 1, Batch 763, Loss: 1.0404092073440552\n",
      "Epoch 1, Batch 764, Loss: 1.0947413444519043\n",
      "Epoch 1, Batch 765, Loss: 0.9997035264968872\n",
      "Epoch 1, Batch 766, Loss: 1.0806429386138916\n",
      "Epoch 1, Batch 767, Loss: 1.146236538887024\n",
      "Epoch 1, Batch 768, Loss: 1.1129279136657715\n",
      "Epoch 1, Batch 769, Loss: 1.2557463645935059\n",
      "Epoch 1, Batch 770, Loss: 1.1693155765533447\n",
      "Epoch 1, Batch 771, Loss: 1.0187735557556152\n",
      "Epoch 1, Batch 772, Loss: 1.071671724319458\n",
      "Epoch 1, Batch 773, Loss: 1.0721107721328735\n",
      "Epoch 1, Batch 774, Loss: 1.1574680805206299\n",
      "Epoch 1, Batch 775, Loss: 1.1113163232803345\n",
      "Epoch 1, Batch 776, Loss: 1.082737684249878\n",
      "Epoch 1, Batch 777, Loss: 1.0280927419662476\n",
      "Epoch 1, Batch 778, Loss: 1.2424598932266235\n",
      "Epoch 1, Batch 779, Loss: 1.0481117963790894\n",
      "Epoch 1, Batch 780, Loss: 1.1263892650604248\n",
      "Epoch 1, Batch 781, Loss: 1.0953325033187866\n",
      "Epoch 1, Batch 782, Loss: 1.0242732763290405\n",
      "Epoch 1, Batch 783, Loss: 0.9683335423469543\n",
      "Epoch 1, Batch 784, Loss: 1.2055925130844116\n",
      "Epoch 1, Batch 785, Loss: 1.2471117973327637\n",
      "Epoch 1, Batch 786, Loss: 1.0610122680664062\n",
      "Epoch 1, Batch 787, Loss: 1.1550984382629395\n",
      "Epoch 1, Batch 788, Loss: 1.1059033870697021\n",
      "Epoch 1, Batch 789, Loss: 1.0668554306030273\n",
      "Epoch 1, Batch 790, Loss: 0.9430841207504272\n",
      "Epoch 1, Batch 791, Loss: 1.1347770690917969\n",
      "Epoch 1, Batch 792, Loss: 1.1641680002212524\n",
      "Epoch 1, Batch 793, Loss: 1.1404305696487427\n",
      "Epoch 1, Batch 794, Loss: 1.0996973514556885\n",
      "Epoch 1, Batch 795, Loss: 1.0998151302337646\n",
      "Epoch 1, Batch 796, Loss: 1.034462571144104\n",
      "Epoch 1, Batch 797, Loss: 1.220695972442627\n",
      "Epoch 1, Batch 798, Loss: 1.0315673351287842\n",
      "Epoch 1, Batch 799, Loss: 1.0849906206130981\n",
      "Epoch 1, Batch 800, Loss: 1.1442434787750244\n",
      "Epoch 1, Batch 801, Loss: 0.9942906498908997\n",
      "Epoch 1, Batch 802, Loss: 1.0686004161834717\n",
      "Epoch 1, Batch 803, Loss: 1.0521684885025024\n",
      "Epoch 1, Batch 804, Loss: 1.0551151037216187\n",
      "Epoch 1, Batch 805, Loss: 1.4135478734970093\n",
      "Epoch 1, Batch 806, Loss: 1.0244516134262085\n",
      "Epoch 1, Batch 807, Loss: 1.0868432521820068\n",
      "Epoch 1, Batch 808, Loss: 1.1296679973602295\n",
      "Epoch 1, Batch 809, Loss: 1.1431268453598022\n",
      "Epoch 1, Batch 810, Loss: 0.992521345615387\n",
      "Epoch 1, Batch 811, Loss: 1.1337991952896118\n",
      "Epoch 1, Batch 812, Loss: 1.2987370491027832\n",
      "Epoch 1, Batch 813, Loss: 0.922352135181427\n",
      "Epoch 1, Batch 814, Loss: 1.1277945041656494\n",
      "Epoch 1, Batch 815, Loss: 1.1074825525283813\n",
      "Epoch 1, Batch 816, Loss: 1.0895538330078125\n",
      "Epoch 1, Batch 817, Loss: 1.085888385772705\n",
      "Epoch 1, Batch 818, Loss: 1.1907209157943726\n",
      "Epoch 1, Batch 819, Loss: 1.0349825620651245\n",
      "Epoch 1, Batch 820, Loss: 0.9851986169815063\n",
      "Epoch 1, Batch 821, Loss: 1.0400464534759521\n",
      "Epoch 1, Batch 822, Loss: 1.1162447929382324\n",
      "Epoch 1, Batch 823, Loss: 1.14113187789917\n",
      "Epoch 1, Batch 824, Loss: 1.043444037437439\n",
      "Epoch 1, Batch 825, Loss: 1.1651798486709595\n",
      "Epoch 1, Batch 826, Loss: 0.96642005443573\n",
      "Epoch 1, Batch 827, Loss: 1.0397014617919922\n",
      "Epoch 1, Batch 828, Loss: 1.0527292490005493\n",
      "Epoch 1, Batch 829, Loss: 1.1559033393859863\n",
      "Epoch 1, Batch 830, Loss: 1.0839710235595703\n",
      "Epoch 1, Batch 831, Loss: 0.993000328540802\n",
      "Epoch 1, Batch 832, Loss: 1.0124154090881348\n",
      "Epoch 1, Batch 833, Loss: 1.055850625038147\n",
      "Epoch 1, Batch 834, Loss: 1.0778976678848267\n",
      "Epoch 1, Batch 835, Loss: 1.0628063678741455\n",
      "Epoch 1, Batch 836, Loss: 1.0502899885177612\n",
      "Epoch 1, Batch 837, Loss: 1.0391085147857666\n",
      "Epoch 1, Batch 838, Loss: 1.0313899517059326\n",
      "Epoch 1, Batch 839, Loss: 1.0666848421096802\n",
      "Epoch 1, Batch 840, Loss: 1.1555842161178589\n",
      "Epoch 1, Batch 841, Loss: 1.0102415084838867\n",
      "Epoch 1, Batch 842, Loss: 1.1097544431686401\n",
      "Epoch 1, Batch 843, Loss: 0.9567515850067139\n",
      "Epoch 1, Batch 844, Loss: 0.9083828926086426\n",
      "Epoch 1, Batch 845, Loss: 1.1082746982574463\n",
      "Epoch 1, Batch 846, Loss: 1.0944479703903198\n",
      "Epoch 1, Batch 847, Loss: 1.0070123672485352\n",
      "Epoch 1, Batch 848, Loss: 1.0281422138214111\n",
      "Epoch 1, Batch 849, Loss: 1.135759711265564\n",
      "Epoch 1, Batch 850, Loss: 0.8692070245742798\n",
      "Epoch 1, Batch 851, Loss: 1.0609407424926758\n",
      "Epoch 1, Batch 852, Loss: 1.1114437580108643\n",
      "Epoch 1, Batch 853, Loss: 1.371328353881836\n",
      "Epoch 1, Batch 854, Loss: 1.0695254802703857\n",
      "Epoch 1, Batch 855, Loss: 1.0085389614105225\n",
      "Epoch 1, Batch 856, Loss: 1.0413813591003418\n",
      "Epoch 1, Batch 857, Loss: 1.1070282459259033\n",
      "Epoch 1, Batch 858, Loss: 1.1583523750305176\n",
      "Epoch 1, Batch 859, Loss: 1.017056941986084\n",
      "Epoch 1, Batch 860, Loss: 0.9974402189254761\n",
      "Epoch 1, Batch 861, Loss: 0.9410122632980347\n",
      "Epoch 1, Batch 862, Loss: 1.2123548984527588\n",
      "Epoch 1, Batch 863, Loss: 1.02276611328125\n",
      "Epoch 1, Batch 864, Loss: 1.0277323722839355\n",
      "Epoch 1, Batch 865, Loss: 0.9775760173797607\n",
      "Epoch 1, Batch 866, Loss: 1.0540518760681152\n",
      "Epoch 1, Batch 867, Loss: 1.1577157974243164\n",
      "Epoch 1, Batch 868, Loss: 1.0497260093688965\n",
      "Epoch 1, Batch 869, Loss: 1.0662380456924438\n",
      "Epoch 1, Batch 870, Loss: 0.9626520276069641\n",
      "Epoch 1, Batch 871, Loss: 0.9984893798828125\n",
      "Epoch 1, Batch 872, Loss: 1.1432609558105469\n",
      "Epoch 1, Batch 873, Loss: 0.9385707378387451\n",
      "Epoch 1, Batch 874, Loss: 1.1173015832901\n",
      "Epoch 1, Batch 875, Loss: 0.9338127970695496\n",
      "Epoch 1, Batch 876, Loss: 1.0357555150985718\n",
      "Epoch 1, Batch 877, Loss: 1.0668553113937378\n",
      "Epoch 1, Batch 878, Loss: 1.2068839073181152\n",
      "Epoch 1, Batch 879, Loss: 1.0033314228057861\n",
      "Epoch 1, Batch 880, Loss: 1.0352442264556885\n",
      "Epoch 1, Batch 881, Loss: 1.1260111331939697\n",
      "Epoch 1, Batch 882, Loss: 1.1024103164672852\n",
      "Epoch 1, Batch 883, Loss: 0.94756019115448\n",
      "Epoch 1, Batch 884, Loss: 0.9101417660713196\n",
      "Epoch 1, Batch 885, Loss: 1.101379156112671\n",
      "Epoch 1, Batch 886, Loss: 1.1352388858795166\n",
      "Epoch 1, Batch 887, Loss: 1.1084531545639038\n",
      "Epoch 1, Batch 888, Loss: 1.1033143997192383\n",
      "Epoch 1, Batch 889, Loss: 0.9981234073638916\n",
      "Epoch 1, Batch 890, Loss: 1.0463130474090576\n",
      "Epoch 1, Batch 891, Loss: 1.1978867053985596\n",
      "Epoch 1, Batch 892, Loss: 1.0331652164459229\n",
      "Epoch 1, Batch 893, Loss: 1.0315370559692383\n",
      "Epoch 1, Batch 894, Loss: 1.098862648010254\n",
      "Epoch 1, Batch 895, Loss: 1.1705951690673828\n",
      "Epoch 1, Batch 896, Loss: 1.0569406747817993\n",
      "Epoch 1, Batch 897, Loss: 1.0447157621383667\n",
      "Epoch 1, Batch 898, Loss: 1.005878210067749\n",
      "Epoch 1, Batch 899, Loss: 1.025984764099121\n",
      "Epoch 1, Batch 900, Loss: 0.9939050674438477\n",
      "Epoch 1, Batch 901, Loss: 1.05268394947052\n",
      "Epoch 1, Batch 902, Loss: 0.8965309858322144\n",
      "Epoch 1, Batch 903, Loss: 1.041239619255066\n",
      "Epoch 1, Batch 904, Loss: 1.0849725008010864\n",
      "Epoch 1, Batch 905, Loss: 0.9475875496864319\n",
      "Epoch 1, Batch 906, Loss: 1.178584098815918\n",
      "Epoch 1, Batch 907, Loss: 1.0216395854949951\n",
      "Epoch 1, Batch 908, Loss: 1.0017940998077393\n",
      "Epoch 1, Batch 909, Loss: 1.0395562648773193\n",
      "Epoch 1, Batch 910, Loss: 0.9539090394973755\n",
      "Epoch 1, Batch 911, Loss: 0.9885220527648926\n",
      "Epoch 1, Batch 912, Loss: 1.1139048337936401\n",
      "Epoch 1, Batch 913, Loss: 0.9784805774688721\n",
      "Epoch 1, Batch 914, Loss: 0.8822882771492004\n",
      "Epoch 1, Batch 915, Loss: 1.1676043272018433\n",
      "Epoch 1, Batch 916, Loss: 1.0393859148025513\n",
      "Epoch 1, Batch 917, Loss: 1.1199531555175781\n",
      "Epoch 1, Batch 918, Loss: 1.107994556427002\n",
      "Epoch 1, Batch 919, Loss: 1.0829708576202393\n",
      "Epoch 1, Batch 920, Loss: 1.2008570432662964\n",
      "Epoch 1, Batch 921, Loss: 1.0633387565612793\n",
      "Epoch 1, Batch 922, Loss: 1.001783013343811\n",
      "Epoch 1, Batch 923, Loss: 1.0157020092010498\n",
      "Epoch 1, Batch 924, Loss: 1.1707813739776611\n",
      "Epoch 1, Batch 925, Loss: 0.927837073802948\n",
      "Epoch 1, Batch 926, Loss: 0.92891526222229\n",
      "Epoch 1, Batch 927, Loss: 1.2573350667953491\n",
      "Epoch 1, Batch 928, Loss: 0.9227132797241211\n",
      "Epoch 1, Batch 929, Loss: 0.9767159819602966\n",
      "Epoch 1, Batch 930, Loss: 1.1210830211639404\n",
      "Epoch 1, Batch 931, Loss: 1.0391618013381958\n",
      "Epoch 1, Batch 932, Loss: 0.9523885846138\n",
      "Epoch 1, Batch 933, Loss: 1.202605128288269\n",
      "Epoch 1, Batch 934, Loss: 1.068009614944458\n",
      "Epoch 1, Batch 935, Loss: 0.9724562764167786\n",
      "Epoch 1, Batch 936, Loss: 1.0027945041656494\n",
      "Epoch 1, Batch 937, Loss: 0.983342707157135\n",
      "Epoch 1, Batch 938, Loss: 1.2536020278930664\n",
      "Accuracy of train set: 0.5201666666666667\n",
      "Epoch 1, Batch 1, Test Loss: 1.1762458086013794\n",
      "Epoch 1, Batch 2, Test Loss: 0.9964752197265625\n",
      "Epoch 1, Batch 3, Test Loss: 1.0122408866882324\n",
      "Epoch 1, Batch 4, Test Loss: 1.0659058094024658\n",
      "Epoch 1, Batch 5, Test Loss: 0.9232500195503235\n",
      "Epoch 1, Batch 6, Test Loss: 1.0864951610565186\n",
      "Epoch 1, Batch 7, Test Loss: 1.2293105125427246\n",
      "Epoch 1, Batch 8, Test Loss: 0.9921145439147949\n",
      "Epoch 1, Batch 9, Test Loss: 1.1807198524475098\n",
      "Epoch 1, Batch 10, Test Loss: 1.00852632522583\n",
      "Epoch 1, Batch 11, Test Loss: 0.9804182052612305\n",
      "Epoch 1, Batch 12, Test Loss: 0.973797082901001\n",
      "Epoch 1, Batch 13, Test Loss: 1.0404284000396729\n",
      "Epoch 1, Batch 14, Test Loss: 0.8917074203491211\n",
      "Epoch 1, Batch 15, Test Loss: 1.1545480489730835\n",
      "Epoch 1, Batch 16, Test Loss: 1.1204913854599\n",
      "Epoch 1, Batch 17, Test Loss: 1.0666855573654175\n",
      "Epoch 1, Batch 18, Test Loss: 1.119539737701416\n",
      "Epoch 1, Batch 19, Test Loss: 1.1952235698699951\n",
      "Epoch 1, Batch 20, Test Loss: 0.9129823446273804\n",
      "Epoch 1, Batch 21, Test Loss: 0.9941570162773132\n",
      "Epoch 1, Batch 22, Test Loss: 0.992569625377655\n",
      "Epoch 1, Batch 23, Test Loss: 1.1457490921020508\n",
      "Epoch 1, Batch 24, Test Loss: 0.8390917778015137\n",
      "Epoch 1, Batch 25, Test Loss: 0.9127232432365417\n",
      "Epoch 1, Batch 26, Test Loss: 1.0908503532409668\n",
      "Epoch 1, Batch 27, Test Loss: 1.0571880340576172\n",
      "Epoch 1, Batch 28, Test Loss: 1.0115463733673096\n",
      "Epoch 1, Batch 29, Test Loss: 1.121383547782898\n",
      "Epoch 1, Batch 30, Test Loss: 1.0356605052947998\n",
      "Epoch 1, Batch 31, Test Loss: 1.0358341932296753\n",
      "Epoch 1, Batch 32, Test Loss: 1.1310352087020874\n",
      "Epoch 1, Batch 33, Test Loss: 0.9428369998931885\n",
      "Epoch 1, Batch 34, Test Loss: 1.0796079635620117\n",
      "Epoch 1, Batch 35, Test Loss: 1.063467264175415\n",
      "Epoch 1, Batch 36, Test Loss: 1.1443569660186768\n",
      "Epoch 1, Batch 37, Test Loss: 1.0279725790023804\n",
      "Epoch 1, Batch 38, Test Loss: 1.0335224866867065\n",
      "Epoch 1, Batch 39, Test Loss: 1.053642988204956\n",
      "Epoch 1, Batch 40, Test Loss: 1.0274133682250977\n",
      "Epoch 1, Batch 41, Test Loss: 1.0602258443832397\n",
      "Epoch 1, Batch 42, Test Loss: 1.1585280895233154\n",
      "Epoch 1, Batch 43, Test Loss: 1.214186668395996\n",
      "Epoch 1, Batch 44, Test Loss: 1.1189323663711548\n",
      "Epoch 1, Batch 45, Test Loss: 0.9647057056427002\n",
      "Epoch 1, Batch 46, Test Loss: 1.2148966789245605\n",
      "Epoch 1, Batch 47, Test Loss: 0.8449676632881165\n",
      "Epoch 1, Batch 48, Test Loss: 0.8713445663452148\n",
      "Epoch 1, Batch 49, Test Loss: 1.2279762029647827\n",
      "Epoch 1, Batch 50, Test Loss: 1.164311408996582\n",
      "Epoch 1, Batch 51, Test Loss: 0.9152143001556396\n",
      "Epoch 1, Batch 52, Test Loss: 0.8888587951660156\n",
      "Epoch 1, Batch 53, Test Loss: 1.0658727884292603\n",
      "Epoch 1, Batch 54, Test Loss: 1.0334724187850952\n",
      "Epoch 1, Batch 55, Test Loss: 0.974668562412262\n",
      "Epoch 1, Batch 56, Test Loss: 1.0798722505569458\n",
      "Epoch 1, Batch 57, Test Loss: 1.0388444662094116\n",
      "Epoch 1, Batch 58, Test Loss: 0.910528302192688\n",
      "Epoch 1, Batch 59, Test Loss: 1.013952374458313\n",
      "Epoch 1, Batch 60, Test Loss: 1.1165128946304321\n",
      "Epoch 1, Batch 61, Test Loss: 1.0565001964569092\n",
      "Epoch 1, Batch 62, Test Loss: 1.0503915548324585\n",
      "Epoch 1, Batch 63, Test Loss: 1.1250354051589966\n",
      "Epoch 1, Batch 64, Test Loss: 1.3186836242675781\n",
      "Epoch 1, Batch 65, Test Loss: 1.1346122026443481\n",
      "Epoch 1, Batch 66, Test Loss: 1.039032220840454\n",
      "Epoch 1, Batch 67, Test Loss: 1.0281600952148438\n",
      "Epoch 1, Batch 68, Test Loss: 1.0677077770233154\n",
      "Epoch 1, Batch 69, Test Loss: 1.2944774627685547\n",
      "Epoch 1, Batch 70, Test Loss: 0.9920322895050049\n",
      "Epoch 1, Batch 71, Test Loss: 1.0155192613601685\n",
      "Epoch 1, Batch 72, Test Loss: 1.0190356969833374\n",
      "Epoch 1, Batch 73, Test Loss: 1.0831217765808105\n",
      "Epoch 1, Batch 74, Test Loss: 1.1700342893600464\n",
      "Epoch 1, Batch 75, Test Loss: 1.1920316219329834\n",
      "Epoch 1, Batch 76, Test Loss: 1.0777276754379272\n",
      "Epoch 1, Batch 77, Test Loss: 1.1106163263320923\n",
      "Epoch 1, Batch 78, Test Loss: 1.034286379814148\n",
      "Epoch 1, Batch 79, Test Loss: 1.13456130027771\n",
      "Epoch 1, Batch 80, Test Loss: 1.0947880744934082\n",
      "Epoch 1, Batch 81, Test Loss: 1.114477515220642\n",
      "Epoch 1, Batch 82, Test Loss: 0.9407144784927368\n",
      "Epoch 1, Batch 83, Test Loss: 1.1007052659988403\n",
      "Epoch 1, Batch 84, Test Loss: 1.2147068977355957\n",
      "Epoch 1, Batch 85, Test Loss: 1.0301661491394043\n",
      "Epoch 1, Batch 86, Test Loss: 1.1860734224319458\n",
      "Epoch 1, Batch 87, Test Loss: 1.0565509796142578\n",
      "Epoch 1, Batch 88, Test Loss: 1.0310752391815186\n",
      "Epoch 1, Batch 89, Test Loss: 0.8163695335388184\n",
      "Epoch 1, Batch 90, Test Loss: 0.9087227582931519\n",
      "Epoch 1, Batch 91, Test Loss: 0.9515210390090942\n",
      "Epoch 1, Batch 92, Test Loss: 1.2124098539352417\n",
      "Epoch 1, Batch 93, Test Loss: 0.9741321802139282\n",
      "Epoch 1, Batch 94, Test Loss: 0.9655925035476685\n",
      "Epoch 1, Batch 95, Test Loss: 1.1145669221878052\n",
      "Epoch 1, Batch 96, Test Loss: 1.0424892902374268\n",
      "Epoch 1, Batch 97, Test Loss: 1.0218340158462524\n",
      "Epoch 1, Batch 98, Test Loss: 0.9898473024368286\n",
      "Epoch 1, Batch 99, Test Loss: 1.214592695236206\n",
      "Epoch 1, Batch 100, Test Loss: 0.9253272414207458\n",
      "Epoch 1, Batch 101, Test Loss: 1.169688105583191\n",
      "Epoch 1, Batch 102, Test Loss: 1.1035234928131104\n",
      "Epoch 1, Batch 103, Test Loss: 0.8935580253601074\n",
      "Epoch 1, Batch 104, Test Loss: 0.9713608622550964\n",
      "Epoch 1, Batch 105, Test Loss: 1.0868630409240723\n",
      "Epoch 1, Batch 106, Test Loss: 1.0922446250915527\n",
      "Epoch 1, Batch 107, Test Loss: 1.0503590106964111\n",
      "Epoch 1, Batch 108, Test Loss: 0.9993494749069214\n",
      "Epoch 1, Batch 109, Test Loss: 1.0334662199020386\n",
      "Epoch 1, Batch 110, Test Loss: 0.9997060298919678\n",
      "Epoch 1, Batch 111, Test Loss: 1.2521324157714844\n",
      "Epoch 1, Batch 112, Test Loss: 1.0026249885559082\n",
      "Epoch 1, Batch 113, Test Loss: 1.1909964084625244\n",
      "Epoch 1, Batch 114, Test Loss: 1.0813243389129639\n",
      "Epoch 1, Batch 115, Test Loss: 1.0747928619384766\n",
      "Epoch 1, Batch 116, Test Loss: 1.0554531812667847\n",
      "Epoch 1, Batch 117, Test Loss: 1.067270040512085\n",
      "Epoch 1, Batch 118, Test Loss: 1.0247656106948853\n",
      "Epoch 1, Batch 119, Test Loss: 0.9077571630477905\n",
      "Epoch 1, Batch 120, Test Loss: 1.099900245666504\n",
      "Epoch 1, Batch 121, Test Loss: 0.9676997661590576\n",
      "Epoch 1, Batch 122, Test Loss: 0.9460339546203613\n",
      "Epoch 1, Batch 123, Test Loss: 0.8689565658569336\n",
      "Epoch 1, Batch 124, Test Loss: 1.0069667100906372\n",
      "Epoch 1, Batch 125, Test Loss: 1.0829119682312012\n",
      "Epoch 1, Batch 126, Test Loss: 1.0291708707809448\n",
      "Epoch 1, Batch 127, Test Loss: 1.0519193410873413\n",
      "Epoch 1, Batch 128, Test Loss: 0.9422512054443359\n",
      "Epoch 1, Batch 129, Test Loss: 1.042733073234558\n",
      "Epoch 1, Batch 130, Test Loss: 1.0541119575500488\n",
      "Epoch 1, Batch 131, Test Loss: 1.0354024171829224\n",
      "Epoch 1, Batch 132, Test Loss: 1.100350022315979\n",
      "Epoch 1, Batch 133, Test Loss: 1.0911954641342163\n",
      "Epoch 1, Batch 134, Test Loss: 1.0061836242675781\n",
      "Epoch 1, Batch 135, Test Loss: 1.042249083518982\n",
      "Epoch 1, Batch 136, Test Loss: 1.1758397817611694\n",
      "Epoch 1, Batch 137, Test Loss: 0.9109611511230469\n",
      "Epoch 1, Batch 138, Test Loss: 1.0477073192596436\n",
      "Epoch 1, Batch 139, Test Loss: 1.1308854818344116\n",
      "Epoch 1, Batch 140, Test Loss: 0.928524911403656\n",
      "Epoch 1, Batch 141, Test Loss: 0.9194413423538208\n",
      "Epoch 1, Batch 142, Test Loss: 0.9836413860321045\n",
      "Epoch 1, Batch 143, Test Loss: 0.9837318658828735\n",
      "Epoch 1, Batch 144, Test Loss: 1.047257423400879\n",
      "Epoch 1, Batch 145, Test Loss: 1.030969262123108\n",
      "Epoch 1, Batch 146, Test Loss: 1.0713703632354736\n",
      "Epoch 1, Batch 147, Test Loss: 0.8533268570899963\n",
      "Epoch 1, Batch 148, Test Loss: 1.232437014579773\n",
      "Epoch 1, Batch 149, Test Loss: 1.145781397819519\n",
      "Epoch 1, Batch 150, Test Loss: 1.0680878162384033\n",
      "Epoch 1, Batch 151, Test Loss: 1.0925989151000977\n",
      "Epoch 1, Batch 152, Test Loss: 1.1963914632797241\n",
      "Epoch 1, Batch 153, Test Loss: 1.0751417875289917\n",
      "Epoch 1, Batch 154, Test Loss: 1.1384669542312622\n",
      "Epoch 1, Batch 155, Test Loss: 1.0870589017868042\n",
      "Epoch 1, Batch 156, Test Loss: 0.9371051788330078\n",
      "Epoch 1, Batch 157, Test Loss: 1.1247587203979492\n",
      "Epoch 1, Batch 158, Test Loss: 1.0953110456466675\n",
      "Epoch 1, Batch 159, Test Loss: 0.9339334964752197\n",
      "Epoch 1, Batch 160, Test Loss: 1.1380715370178223\n",
      "Epoch 1, Batch 161, Test Loss: 1.0404571294784546\n",
      "Epoch 1, Batch 162, Test Loss: 0.9503850340843201\n",
      "Epoch 1, Batch 163, Test Loss: 0.912071168422699\n",
      "Epoch 1, Batch 164, Test Loss: 1.1256264448165894\n",
      "Epoch 1, Batch 165, Test Loss: 1.0301681756973267\n",
      "Epoch 1, Batch 166, Test Loss: 1.2142181396484375\n",
      "Epoch 1, Batch 167, Test Loss: 1.029012680053711\n",
      "Epoch 1, Batch 168, Test Loss: 1.0519412755966187\n",
      "Epoch 1, Batch 169, Test Loss: 0.9551188945770264\n",
      "Epoch 1, Batch 170, Test Loss: 0.9819649457931519\n",
      "Epoch 1, Batch 171, Test Loss: 1.0078858137130737\n",
      "Epoch 1, Batch 172, Test Loss: 1.0087395906448364\n",
      "Epoch 1, Batch 173, Test Loss: 1.1915347576141357\n",
      "Epoch 1, Batch 174, Test Loss: 0.9384025931358337\n",
      "Epoch 1, Batch 175, Test Loss: 1.09342360496521\n",
      "Epoch 1, Batch 176, Test Loss: 1.1014854907989502\n",
      "Epoch 1, Batch 177, Test Loss: 1.2989146709442139\n",
      "Epoch 1, Batch 178, Test Loss: 1.0299030542373657\n",
      "Epoch 1, Batch 179, Test Loss: 1.0053426027297974\n",
      "Epoch 1, Batch 180, Test Loss: 1.050246000289917\n",
      "Epoch 1, Batch 181, Test Loss: 1.1203327178955078\n",
      "Epoch 1, Batch 182, Test Loss: 1.137251853942871\n",
      "Epoch 1, Batch 183, Test Loss: 0.9507035613059998\n",
      "Epoch 1, Batch 184, Test Loss: 1.2899565696716309\n",
      "Epoch 1, Batch 185, Test Loss: 1.0859400033950806\n",
      "Epoch 1, Batch 186, Test Loss: 1.061548113822937\n",
      "Epoch 1, Batch 187, Test Loss: 1.1140564680099487\n",
      "Epoch 1, Batch 188, Test Loss: 0.9297783970832825\n",
      "Epoch 1, Batch 189, Test Loss: 1.1098604202270508\n",
      "Epoch 1, Batch 190, Test Loss: 1.0948619842529297\n",
      "Epoch 1, Batch 191, Test Loss: 0.9962248206138611\n",
      "Epoch 1, Batch 192, Test Loss: 0.8793814182281494\n",
      "Epoch 1, Batch 193, Test Loss: 0.936324954032898\n",
      "Epoch 1, Batch 194, Test Loss: 1.122542142868042\n",
      "Epoch 1, Batch 195, Test Loss: 1.1385180950164795\n",
      "Epoch 1, Batch 196, Test Loss: 1.1074937582015991\n",
      "Epoch 1, Batch 197, Test Loss: 0.9248515963554382\n",
      "Epoch 1, Batch 198, Test Loss: 1.0796068906784058\n",
      "Epoch 1, Batch 199, Test Loss: 1.0798780918121338\n",
      "Epoch 1, Batch 200, Test Loss: 1.0172020196914673\n",
      "Epoch 1, Batch 201, Test Loss: 1.0229322910308838\n",
      "Epoch 1, Batch 202, Test Loss: 1.150384783744812\n",
      "Epoch 1, Batch 203, Test Loss: 0.7856380939483643\n",
      "Epoch 1, Batch 204, Test Loss: 1.2724348306655884\n",
      "Epoch 1, Batch 205, Test Loss: 1.1838356256484985\n",
      "Epoch 1, Batch 206, Test Loss: 1.2168210744857788\n",
      "Epoch 1, Batch 207, Test Loss: 1.0726900100708008\n",
      "Epoch 1, Batch 208, Test Loss: 1.0013766288757324\n",
      "Epoch 1, Batch 209, Test Loss: 0.9621642827987671\n",
      "Epoch 1, Batch 210, Test Loss: 0.9349310398101807\n",
      "Epoch 1, Batch 211, Test Loss: 1.083824634552002\n",
      "Epoch 1, Batch 212, Test Loss: 1.176108956336975\n",
      "Epoch 1, Batch 213, Test Loss: 1.1101173162460327\n",
      "Epoch 1, Batch 214, Test Loss: 1.2664964199066162\n",
      "Epoch 1, Batch 215, Test Loss: 1.0155937671661377\n",
      "Epoch 1, Batch 216, Test Loss: 0.9342131614685059\n",
      "Epoch 1, Batch 217, Test Loss: 1.160256266593933\n",
      "Epoch 1, Batch 218, Test Loss: 1.0509604215621948\n",
      "Epoch 1, Batch 219, Test Loss: 1.115917444229126\n",
      "Epoch 1, Batch 220, Test Loss: 1.1129388809204102\n",
      "Epoch 1, Batch 221, Test Loss: 1.0175979137420654\n",
      "Epoch 1, Batch 222, Test Loss: 1.0154767036437988\n",
      "Epoch 1, Batch 223, Test Loss: 1.160707712173462\n",
      "Epoch 1, Batch 224, Test Loss: 1.052926778793335\n",
      "Epoch 1, Batch 225, Test Loss: 1.026764988899231\n",
      "Epoch 1, Batch 226, Test Loss: 1.0236551761627197\n",
      "Epoch 1, Batch 227, Test Loss: 0.9223129749298096\n",
      "Epoch 1, Batch 228, Test Loss: 0.9377661347389221\n",
      "Epoch 1, Batch 229, Test Loss: 1.1412173509597778\n",
      "Epoch 1, Batch 230, Test Loss: 0.9354735612869263\n",
      "Epoch 1, Batch 231, Test Loss: 1.217193603515625\n",
      "Epoch 1, Batch 232, Test Loss: 1.1425999402999878\n",
      "Epoch 1, Batch 233, Test Loss: 1.0674015283584595\n",
      "Epoch 1, Batch 234, Test Loss: 1.058086633682251\n",
      "Epoch 1, Batch 235, Test Loss: 1.0540070533752441\n",
      "Epoch 1, Batch 236, Test Loss: 1.153836965560913\n",
      "Epoch 1, Batch 237, Test Loss: 1.0381730794906616\n",
      "Epoch 1, Batch 238, Test Loss: 1.0674329996109009\n",
      "Epoch 1, Batch 239, Test Loss: 1.0795907974243164\n",
      "Epoch 1, Batch 240, Test Loss: 1.0521870851516724\n",
      "Epoch 1, Batch 241, Test Loss: 1.0929360389709473\n",
      "Epoch 1, Batch 242, Test Loss: 1.2465133666992188\n",
      "Epoch 1, Batch 243, Test Loss: 1.1116318702697754\n",
      "Epoch 1, Batch 244, Test Loss: 0.9452898502349854\n",
      "Epoch 1, Batch 245, Test Loss: 1.0383436679840088\n",
      "Epoch 1, Batch 246, Test Loss: 1.1553058624267578\n",
      "Epoch 1, Batch 247, Test Loss: 1.246678113937378\n",
      "Epoch 1, Batch 248, Test Loss: 0.946022629737854\n",
      "Epoch 1, Batch 249, Test Loss: 0.9985133409500122\n",
      "Epoch 1, Batch 250, Test Loss: 1.424344778060913\n",
      "Epoch 1, Batch 251, Test Loss: 1.0505168437957764\n",
      "Epoch 1, Batch 252, Test Loss: 0.8920112252235413\n",
      "Epoch 1, Batch 253, Test Loss: 0.9215088486671448\n",
      "Epoch 1, Batch 254, Test Loss: 1.076757550239563\n",
      "Epoch 1, Batch 255, Test Loss: 1.0788615942001343\n",
      "Epoch 1, Batch 256, Test Loss: 1.0378954410552979\n",
      "Epoch 1, Batch 257, Test Loss: 1.0600053071975708\n",
      "Epoch 1, Batch 258, Test Loss: 1.0995713472366333\n",
      "Epoch 1, Batch 259, Test Loss: 1.2523010969161987\n",
      "Epoch 1, Batch 260, Test Loss: 1.0070732831954956\n",
      "Epoch 1, Batch 261, Test Loss: 1.0368521213531494\n",
      "Epoch 1, Batch 262, Test Loss: 1.122633457183838\n",
      "Epoch 1, Batch 263, Test Loss: 1.0132970809936523\n",
      "Epoch 1, Batch 264, Test Loss: 1.1362777948379517\n",
      "Epoch 1, Batch 265, Test Loss: 1.137683629989624\n",
      "Epoch 1, Batch 266, Test Loss: 1.0972188711166382\n",
      "Epoch 1, Batch 267, Test Loss: 1.222217321395874\n",
      "Epoch 1, Batch 268, Test Loss: 0.9590561985969543\n",
      "Epoch 1, Batch 269, Test Loss: 0.9667659401893616\n",
      "Epoch 1, Batch 270, Test Loss: 1.0646581649780273\n",
      "Epoch 1, Batch 271, Test Loss: 1.2257307767868042\n",
      "Epoch 1, Batch 272, Test Loss: 1.1726568937301636\n",
      "Epoch 1, Batch 273, Test Loss: 1.0558593273162842\n",
      "Epoch 1, Batch 274, Test Loss: 1.034153938293457\n",
      "Epoch 1, Batch 275, Test Loss: 1.0028785467147827\n",
      "Epoch 1, Batch 276, Test Loss: 1.1148446798324585\n",
      "Epoch 1, Batch 277, Test Loss: 1.0958830118179321\n",
      "Epoch 1, Batch 278, Test Loss: 1.1593067646026611\n",
      "Epoch 1, Batch 279, Test Loss: 1.0952352285385132\n",
      "Epoch 1, Batch 280, Test Loss: 1.0522981882095337\n",
      "Epoch 1, Batch 281, Test Loss: 1.15126371383667\n",
      "Epoch 1, Batch 282, Test Loss: 0.9856648445129395\n",
      "Epoch 1, Batch 283, Test Loss: 1.1244009733200073\n",
      "Epoch 1, Batch 284, Test Loss: 1.205562949180603\n",
      "Epoch 1, Batch 285, Test Loss: 1.0307130813598633\n",
      "Epoch 1, Batch 286, Test Loss: 1.1074631214141846\n",
      "Epoch 1, Batch 287, Test Loss: 0.9953646063804626\n",
      "Epoch 1, Batch 288, Test Loss: 0.9877672791481018\n",
      "Epoch 1, Batch 289, Test Loss: 1.0031074285507202\n",
      "Epoch 1, Batch 290, Test Loss: 0.9454845786094666\n",
      "Epoch 1, Batch 291, Test Loss: 1.1593220233917236\n",
      "Epoch 1, Batch 292, Test Loss: 1.1596189737319946\n",
      "Epoch 1, Batch 293, Test Loss: 1.0002391338348389\n",
      "Epoch 1, Batch 294, Test Loss: 1.045655369758606\n",
      "Epoch 1, Batch 295, Test Loss: 1.0346266031265259\n",
      "Epoch 1, Batch 296, Test Loss: 1.0821356773376465\n",
      "Epoch 1, Batch 297, Test Loss: 1.0852867364883423\n",
      "Epoch 1, Batch 298, Test Loss: 1.0954344272613525\n",
      "Epoch 1, Batch 299, Test Loss: 1.0517032146453857\n",
      "Epoch 1, Batch 300, Test Loss: 1.103765845298767\n",
      "Epoch 1, Batch 301, Test Loss: 1.0482454299926758\n",
      "Epoch 1, Batch 302, Test Loss: 1.1523754596710205\n",
      "Epoch 1, Batch 303, Test Loss: 1.0580406188964844\n",
      "Epoch 1, Batch 304, Test Loss: 0.9309317469596863\n",
      "Epoch 1, Batch 305, Test Loss: 0.944631814956665\n",
      "Epoch 1, Batch 306, Test Loss: 1.2020320892333984\n",
      "Epoch 1, Batch 307, Test Loss: 1.1672594547271729\n",
      "Epoch 1, Batch 308, Test Loss: 1.0293850898742676\n",
      "Epoch 1, Batch 309, Test Loss: 1.1746416091918945\n",
      "Epoch 1, Batch 310, Test Loss: 1.163224458694458\n",
      "Epoch 1, Batch 311, Test Loss: 1.0215204954147339\n",
      "Epoch 1, Batch 312, Test Loss: 1.039168119430542\n",
      "Epoch 1, Batch 313, Test Loss: 1.0793551206588745\n",
      "Epoch 1, Batch 314, Test Loss: 1.062993049621582\n",
      "Epoch 1, Batch 315, Test Loss: 1.009876012802124\n",
      "Epoch 1, Batch 316, Test Loss: 1.0219669342041016\n",
      "Epoch 1, Batch 317, Test Loss: 0.9872364401817322\n",
      "Epoch 1, Batch 318, Test Loss: 1.1014779806137085\n",
      "Epoch 1, Batch 319, Test Loss: 1.0306894779205322\n",
      "Epoch 1, Batch 320, Test Loss: 1.0744309425354004\n",
      "Epoch 1, Batch 321, Test Loss: 1.1565155982971191\n",
      "Epoch 1, Batch 322, Test Loss: 1.032975673675537\n",
      "Epoch 1, Batch 323, Test Loss: 1.1819369792938232\n",
      "Epoch 1, Batch 324, Test Loss: 1.104225754737854\n",
      "Epoch 1, Batch 325, Test Loss: 1.0908842086791992\n",
      "Epoch 1, Batch 326, Test Loss: 1.012509822845459\n",
      "Epoch 1, Batch 327, Test Loss: 1.0158456563949585\n",
      "Epoch 1, Batch 328, Test Loss: 1.2131397724151611\n",
      "Epoch 1, Batch 329, Test Loss: 1.01931893825531\n",
      "Epoch 1, Batch 330, Test Loss: 0.9656862020492554\n",
      "Epoch 1, Batch 331, Test Loss: 1.1650633811950684\n",
      "Epoch 1, Batch 332, Test Loss: 0.8867794871330261\n",
      "Epoch 1, Batch 333, Test Loss: 0.9945032596588135\n",
      "Epoch 1, Batch 334, Test Loss: 1.262174129486084\n",
      "Epoch 1, Batch 335, Test Loss: 1.1430847644805908\n",
      "Epoch 1, Batch 336, Test Loss: 0.9478271007537842\n",
      "Epoch 1, Batch 337, Test Loss: 1.0655133724212646\n",
      "Epoch 1, Batch 338, Test Loss: 0.9885417222976685\n",
      "Epoch 1, Batch 339, Test Loss: 1.0466628074645996\n",
      "Epoch 1, Batch 340, Test Loss: 1.0415079593658447\n",
      "Epoch 1, Batch 341, Test Loss: 1.1990933418273926\n",
      "Epoch 1, Batch 342, Test Loss: 1.0302023887634277\n",
      "Epoch 1, Batch 343, Test Loss: 1.0189043283462524\n",
      "Epoch 1, Batch 344, Test Loss: 0.9908620119094849\n",
      "Epoch 1, Batch 345, Test Loss: 1.1020724773406982\n",
      "Epoch 1, Batch 346, Test Loss: 1.1464673280715942\n",
      "Epoch 1, Batch 347, Test Loss: 1.0185867547988892\n",
      "Epoch 1, Batch 348, Test Loss: 1.2434526681900024\n",
      "Epoch 1, Batch 349, Test Loss: 1.0281661748886108\n",
      "Epoch 1, Batch 350, Test Loss: 0.978550374507904\n",
      "Epoch 1, Batch 351, Test Loss: 1.1083474159240723\n",
      "Epoch 1, Batch 352, Test Loss: 0.9328233003616333\n",
      "Epoch 1, Batch 353, Test Loss: 1.1187061071395874\n",
      "Epoch 1, Batch 354, Test Loss: 1.06221342086792\n",
      "Epoch 1, Batch 355, Test Loss: 0.8136147260665894\n",
      "Epoch 1, Batch 356, Test Loss: 1.0521689653396606\n",
      "Epoch 1, Batch 357, Test Loss: 1.1672462224960327\n",
      "Epoch 1, Batch 358, Test Loss: 0.9494984149932861\n",
      "Epoch 1, Batch 359, Test Loss: 1.1149262189865112\n",
      "Epoch 1, Batch 360, Test Loss: 1.152185082435608\n",
      "Epoch 1, Batch 361, Test Loss: 1.0819224119186401\n",
      "Epoch 1, Batch 362, Test Loss: 1.0813798904418945\n",
      "Epoch 1, Batch 363, Test Loss: 1.083983302116394\n",
      "Epoch 1, Batch 364, Test Loss: 1.3198754787445068\n",
      "Epoch 1, Batch 365, Test Loss: 1.077864408493042\n",
      "Epoch 1, Batch 366, Test Loss: 1.0058088302612305\n",
      "Epoch 1, Batch 367, Test Loss: 0.9830483794212341\n",
      "Epoch 1, Batch 368, Test Loss: 1.0144399404525757\n",
      "Epoch 1, Batch 369, Test Loss: 1.0210107564926147\n",
      "Epoch 1, Batch 370, Test Loss: 1.1508573293685913\n",
      "Epoch 1, Batch 371, Test Loss: 0.987086832523346\n",
      "Epoch 1, Batch 372, Test Loss: 1.1981253623962402\n",
      "Epoch 1, Batch 373, Test Loss: 1.1947126388549805\n",
      "Epoch 1, Batch 374, Test Loss: 1.0672364234924316\n",
      "Epoch 1, Batch 375, Test Loss: 1.1489853858947754\n",
      "Epoch 1, Batch 376, Test Loss: 0.8641661405563354\n",
      "Epoch 1, Batch 377, Test Loss: 1.0401332378387451\n",
      "Epoch 1, Batch 378, Test Loss: 1.0609620809555054\n",
      "Epoch 1, Batch 379, Test Loss: 1.0766098499298096\n",
      "Epoch 1, Batch 380, Test Loss: 0.9572590589523315\n",
      "Epoch 1, Batch 381, Test Loss: 1.1565046310424805\n",
      "Epoch 1, Batch 382, Test Loss: 1.1099680662155151\n",
      "Epoch 1, Batch 383, Test Loss: 1.085850477218628\n",
      "Epoch 1, Batch 384, Test Loss: 0.9714083671569824\n",
      "Epoch 1, Batch 385, Test Loss: 1.0690865516662598\n",
      "Epoch 1, Batch 386, Test Loss: 1.1432114839553833\n",
      "Epoch 1, Batch 387, Test Loss: 1.1360827684402466\n",
      "Epoch 1, Batch 388, Test Loss: 1.009110927581787\n",
      "Epoch 1, Batch 389, Test Loss: 1.0773792266845703\n",
      "Epoch 1, Batch 390, Test Loss: 1.0796592235565186\n",
      "Epoch 1, Batch 391, Test Loss: 1.1873286962509155\n",
      "Epoch 1, Batch 392, Test Loss: 1.401255488395691\n",
      "Epoch 1, Batch 393, Test Loss: 1.2635093927383423\n",
      "Epoch 1, Batch 394, Test Loss: 1.0515451431274414\n",
      "Epoch 1, Batch 395, Test Loss: 1.0018227100372314\n",
      "Epoch 1, Batch 396, Test Loss: 1.0854727029800415\n",
      "Epoch 1, Batch 397, Test Loss: 1.1514668464660645\n",
      "Epoch 1, Batch 398, Test Loss: 1.0738592147827148\n",
      "Epoch 1, Batch 399, Test Loss: 0.9044065475463867\n",
      "Epoch 1, Batch 400, Test Loss: 1.0715395212173462\n",
      "Epoch 1, Batch 401, Test Loss: 1.09377121925354\n",
      "Epoch 1, Batch 402, Test Loss: 1.2732728719711304\n",
      "Epoch 1, Batch 403, Test Loss: 0.9785304069519043\n",
      "Epoch 1, Batch 404, Test Loss: 1.042268991470337\n",
      "Epoch 1, Batch 405, Test Loss: 0.9183861613273621\n",
      "Epoch 1, Batch 406, Test Loss: 0.912276566028595\n",
      "Epoch 1, Batch 407, Test Loss: 1.0258808135986328\n",
      "Epoch 1, Batch 408, Test Loss: 1.0599231719970703\n",
      "Epoch 1, Batch 409, Test Loss: 0.9298387765884399\n",
      "Epoch 1, Batch 410, Test Loss: 1.2829902172088623\n",
      "Epoch 1, Batch 411, Test Loss: 1.0675973892211914\n",
      "Epoch 1, Batch 412, Test Loss: 1.0047767162322998\n",
      "Epoch 1, Batch 413, Test Loss: 0.9496661424636841\n",
      "Epoch 1, Batch 414, Test Loss: 1.0179246664047241\n",
      "Epoch 1, Batch 415, Test Loss: 1.0971019268035889\n",
      "Epoch 1, Batch 416, Test Loss: 1.2340670824050903\n",
      "Epoch 1, Batch 417, Test Loss: 1.5340436697006226\n",
      "Epoch 1, Batch 418, Test Loss: 1.044930100440979\n",
      "Epoch 1, Batch 419, Test Loss: 0.9816914200782776\n",
      "Epoch 1, Batch 420, Test Loss: 1.1218417882919312\n",
      "Epoch 1, Batch 421, Test Loss: 1.2766478061676025\n",
      "Epoch 1, Batch 422, Test Loss: 1.191409707069397\n",
      "Epoch 1, Batch 423, Test Loss: 1.1206070184707642\n",
      "Epoch 1, Batch 424, Test Loss: 0.8683110475540161\n",
      "Epoch 1, Batch 425, Test Loss: 1.0649900436401367\n",
      "Epoch 1, Batch 426, Test Loss: 0.8439264297485352\n",
      "Epoch 1, Batch 427, Test Loss: 1.075804352760315\n",
      "Epoch 1, Batch 428, Test Loss: 1.0080947875976562\n",
      "Epoch 1, Batch 429, Test Loss: 0.9448556900024414\n",
      "Epoch 1, Batch 430, Test Loss: 1.0879753828048706\n",
      "Epoch 1, Batch 431, Test Loss: 1.0257134437561035\n",
      "Epoch 1, Batch 432, Test Loss: 0.9158499836921692\n",
      "Epoch 1, Batch 433, Test Loss: 1.0999013185501099\n",
      "Epoch 1, Batch 434, Test Loss: 1.1218020915985107\n",
      "Epoch 1, Batch 435, Test Loss: 1.0260255336761475\n",
      "Epoch 1, Batch 436, Test Loss: 1.1820273399353027\n",
      "Epoch 1, Batch 437, Test Loss: 1.0525442361831665\n",
      "Epoch 1, Batch 438, Test Loss: 1.0052130222320557\n",
      "Epoch 1, Batch 439, Test Loss: 1.0123417377471924\n",
      "Epoch 1, Batch 440, Test Loss: 1.1159193515777588\n",
      "Epoch 1, Batch 441, Test Loss: 1.0535978078842163\n",
      "Epoch 1, Batch 442, Test Loss: 1.0362759828567505\n",
      "Epoch 1, Batch 443, Test Loss: 0.9054421782493591\n",
      "Epoch 1, Batch 444, Test Loss: 0.9684566259384155\n",
      "Epoch 1, Batch 445, Test Loss: 1.0772716999053955\n",
      "Epoch 1, Batch 446, Test Loss: 1.037524938583374\n",
      "Epoch 1, Batch 447, Test Loss: 1.1786328554153442\n",
      "Epoch 1, Batch 448, Test Loss: 1.2649062871932983\n",
      "Epoch 1, Batch 449, Test Loss: 1.220411777496338\n",
      "Epoch 1, Batch 450, Test Loss: 1.0114789009094238\n",
      "Epoch 1, Batch 451, Test Loss: 1.1240947246551514\n",
      "Epoch 1, Batch 452, Test Loss: 1.1412783861160278\n",
      "Epoch 1, Batch 453, Test Loss: 0.9595224857330322\n",
      "Epoch 1, Batch 454, Test Loss: 0.9236322045326233\n",
      "Epoch 1, Batch 455, Test Loss: 1.0505783557891846\n",
      "Epoch 1, Batch 456, Test Loss: 1.035622239112854\n",
      "Epoch 1, Batch 457, Test Loss: 1.033487319946289\n",
      "Epoch 1, Batch 458, Test Loss: 1.0801682472229004\n",
      "Epoch 1, Batch 459, Test Loss: 1.095350980758667\n",
      "Epoch 1, Batch 460, Test Loss: 1.2271653413772583\n",
      "Epoch 1, Batch 461, Test Loss: 1.2307326793670654\n",
      "Epoch 1, Batch 462, Test Loss: 1.0213720798492432\n",
      "Epoch 1, Batch 463, Test Loss: 1.1043522357940674\n",
      "Epoch 1, Batch 464, Test Loss: 1.1095932722091675\n",
      "Epoch 1, Batch 465, Test Loss: 1.0524065494537354\n",
      "Epoch 1, Batch 466, Test Loss: 1.1505684852600098\n",
      "Epoch 1, Batch 467, Test Loss: 1.0415198802947998\n",
      "Epoch 1, Batch 468, Test Loss: 1.1358438730239868\n",
      "Epoch 1, Batch 469, Test Loss: 0.9661059379577637\n",
      "Epoch 1, Batch 470, Test Loss: 1.0217478275299072\n",
      "Epoch 1, Batch 471, Test Loss: 1.04449462890625\n",
      "Epoch 1, Batch 472, Test Loss: 0.8374989032745361\n",
      "Epoch 1, Batch 473, Test Loss: 1.0901527404785156\n",
      "Epoch 1, Batch 474, Test Loss: 1.3680309057235718\n",
      "Epoch 1, Batch 475, Test Loss: 1.0841081142425537\n",
      "Epoch 1, Batch 476, Test Loss: 1.3898279666900635\n",
      "Epoch 1, Batch 477, Test Loss: 1.0753811597824097\n",
      "Epoch 1, Batch 478, Test Loss: 1.1330093145370483\n",
      "Epoch 1, Batch 479, Test Loss: 1.2557282447814941\n",
      "Epoch 1, Batch 480, Test Loss: 0.954979419708252\n",
      "Epoch 1, Batch 481, Test Loss: 1.0535765886306763\n",
      "Epoch 1, Batch 482, Test Loss: 1.0066156387329102\n",
      "Epoch 1, Batch 483, Test Loss: 0.9487695693969727\n",
      "Epoch 1, Batch 484, Test Loss: 1.045276403427124\n",
      "Epoch 1, Batch 485, Test Loss: 1.131837248802185\n",
      "Epoch 1, Batch 486, Test Loss: 1.1035925149917603\n",
      "Epoch 1, Batch 487, Test Loss: 1.2443996667861938\n",
      "Epoch 1, Batch 488, Test Loss: 1.0199064016342163\n",
      "Epoch 1, Batch 489, Test Loss: 1.227095603942871\n",
      "Epoch 1, Batch 490, Test Loss: 1.050244688987732\n",
      "Epoch 1, Batch 491, Test Loss: 0.9455502033233643\n",
      "Epoch 1, Batch 492, Test Loss: 1.0086820125579834\n",
      "Epoch 1, Batch 493, Test Loss: 1.0526418685913086\n",
      "Epoch 1, Batch 494, Test Loss: 1.0290486812591553\n",
      "Epoch 1, Batch 495, Test Loss: 1.0547804832458496\n",
      "Epoch 1, Batch 496, Test Loss: 0.9676715731620789\n",
      "Epoch 1, Batch 497, Test Loss: 0.9271541833877563\n",
      "Epoch 1, Batch 498, Test Loss: 1.0572645664215088\n",
      "Epoch 1, Batch 499, Test Loss: 0.9113003015518188\n",
      "Epoch 1, Batch 500, Test Loss: 0.9503219127655029\n",
      "Epoch 1, Batch 501, Test Loss: 0.9449827671051025\n",
      "Epoch 1, Batch 502, Test Loss: 1.0228309631347656\n",
      "Epoch 1, Batch 503, Test Loss: 1.0336267948150635\n",
      "Epoch 1, Batch 504, Test Loss: 1.1209704875946045\n",
      "Epoch 1, Batch 505, Test Loss: 1.1042420864105225\n",
      "Epoch 1, Batch 506, Test Loss: 0.8587184548377991\n",
      "Epoch 1, Batch 507, Test Loss: 1.0018397569656372\n",
      "Epoch 1, Batch 508, Test Loss: 1.164408564567566\n",
      "Epoch 1, Batch 509, Test Loss: 0.9838130474090576\n",
      "Epoch 1, Batch 510, Test Loss: 0.9825314283370972\n",
      "Epoch 1, Batch 511, Test Loss: 0.9942493438720703\n",
      "Epoch 1, Batch 512, Test Loss: 1.108060359954834\n",
      "Epoch 1, Batch 513, Test Loss: 1.0054208040237427\n",
      "Epoch 1, Batch 514, Test Loss: 1.0371993780136108\n",
      "Epoch 1, Batch 515, Test Loss: 1.1288219690322876\n",
      "Epoch 1, Batch 516, Test Loss: 1.0622143745422363\n",
      "Epoch 1, Batch 517, Test Loss: 1.1068781614303589\n",
      "Epoch 1, Batch 518, Test Loss: 1.0601387023925781\n",
      "Epoch 1, Batch 519, Test Loss: 0.9649036526679993\n",
      "Epoch 1, Batch 520, Test Loss: 1.0953751802444458\n",
      "Epoch 1, Batch 521, Test Loss: 1.2377865314483643\n",
      "Epoch 1, Batch 522, Test Loss: 0.9831315279006958\n",
      "Epoch 1, Batch 523, Test Loss: 1.0625088214874268\n",
      "Epoch 1, Batch 524, Test Loss: 1.233542799949646\n",
      "Epoch 1, Batch 525, Test Loss: 0.9800618290901184\n",
      "Epoch 1, Batch 526, Test Loss: 1.166752815246582\n",
      "Epoch 1, Batch 527, Test Loss: 1.0765310525894165\n",
      "Epoch 1, Batch 528, Test Loss: 1.0754045248031616\n",
      "Epoch 1, Batch 529, Test Loss: 0.9950859546661377\n",
      "Epoch 1, Batch 530, Test Loss: 1.0248843431472778\n",
      "Epoch 1, Batch 531, Test Loss: 1.0810085535049438\n",
      "Epoch 1, Batch 532, Test Loss: 1.2030938863754272\n",
      "Epoch 1, Batch 533, Test Loss: 0.9372406005859375\n",
      "Epoch 1, Batch 534, Test Loss: 1.0799626111984253\n",
      "Epoch 1, Batch 535, Test Loss: 1.2265702486038208\n",
      "Epoch 1, Batch 536, Test Loss: 0.965793251991272\n",
      "Epoch 1, Batch 537, Test Loss: 1.0482627153396606\n",
      "Epoch 1, Batch 538, Test Loss: 0.9623927474021912\n",
      "Epoch 1, Batch 539, Test Loss: 1.0349966287612915\n",
      "Epoch 1, Batch 540, Test Loss: 1.0071806907653809\n",
      "Epoch 1, Batch 541, Test Loss: 1.1484874486923218\n",
      "Epoch 1, Batch 542, Test Loss: 1.0930774211883545\n",
      "Epoch 1, Batch 543, Test Loss: 1.317368507385254\n",
      "Epoch 1, Batch 544, Test Loss: 1.0534659624099731\n",
      "Epoch 1, Batch 545, Test Loss: 1.0075876712799072\n",
      "Epoch 1, Batch 546, Test Loss: 0.9991287589073181\n",
      "Epoch 1, Batch 547, Test Loss: 0.9737252593040466\n",
      "Epoch 1, Batch 548, Test Loss: 1.0020939111709595\n",
      "Epoch 1, Batch 549, Test Loss: 0.9491247534751892\n",
      "Epoch 1, Batch 550, Test Loss: 1.2690980434417725\n",
      "Epoch 1, Batch 551, Test Loss: 1.0980418920516968\n",
      "Epoch 1, Batch 552, Test Loss: 0.9313303828239441\n",
      "Epoch 1, Batch 553, Test Loss: 1.0169711112976074\n",
      "Epoch 1, Batch 554, Test Loss: 0.969446063041687\n",
      "Epoch 1, Batch 555, Test Loss: 0.9319047927856445\n",
      "Epoch 1, Batch 556, Test Loss: 1.1598275899887085\n",
      "Epoch 1, Batch 557, Test Loss: 1.0910511016845703\n",
      "Epoch 1, Batch 558, Test Loss: 1.1298545598983765\n",
      "Epoch 1, Batch 559, Test Loss: 1.0985817909240723\n",
      "Epoch 1, Batch 560, Test Loss: 0.928861141204834\n",
      "Epoch 1, Batch 561, Test Loss: 1.0037868022918701\n",
      "Epoch 1, Batch 562, Test Loss: 1.0322993993759155\n",
      "Epoch 1, Batch 563, Test Loss: 0.8745598793029785\n",
      "Epoch 1, Batch 564, Test Loss: 1.1528500318527222\n",
      "Epoch 1, Batch 565, Test Loss: 0.9893938302993774\n",
      "Epoch 1, Batch 566, Test Loss: 0.9620770215988159\n",
      "Epoch 1, Batch 567, Test Loss: 1.3924766778945923\n",
      "Epoch 1, Batch 568, Test Loss: 0.9932456016540527\n",
      "Epoch 1, Batch 569, Test Loss: 1.1015880107879639\n",
      "Epoch 1, Batch 570, Test Loss: 0.9179118275642395\n",
      "Epoch 1, Batch 571, Test Loss: 1.0267117023468018\n",
      "Epoch 1, Batch 572, Test Loss: 0.8678675889968872\n",
      "Epoch 1, Batch 573, Test Loss: 1.0159640312194824\n",
      "Epoch 1, Batch 574, Test Loss: 1.174988865852356\n",
      "Epoch 1, Batch 575, Test Loss: 0.9536563158035278\n",
      "Epoch 1, Batch 576, Test Loss: 1.1092246770858765\n",
      "Epoch 1, Batch 577, Test Loss: 0.9826778769493103\n",
      "Epoch 1, Batch 578, Test Loss: 1.230772852897644\n",
      "Epoch 1, Batch 579, Test Loss: 1.0933951139450073\n",
      "Epoch 1, Batch 580, Test Loss: 0.9799237251281738\n",
      "Epoch 1, Batch 581, Test Loss: 1.1589620113372803\n",
      "Epoch 1, Batch 582, Test Loss: 1.0241864919662476\n",
      "Epoch 1, Batch 583, Test Loss: 1.220402479171753\n",
      "Epoch 1, Batch 584, Test Loss: 0.870895266532898\n",
      "Epoch 1, Batch 585, Test Loss: 1.0215295553207397\n",
      "Epoch 1, Batch 586, Test Loss: 1.0139133930206299\n",
      "Epoch 1, Batch 587, Test Loss: 0.8706496357917786\n",
      "Epoch 1, Batch 588, Test Loss: 1.0119189023971558\n",
      "Epoch 1, Batch 589, Test Loss: 0.9735999703407288\n",
      "Epoch 1, Batch 590, Test Loss: 1.0455504655838013\n",
      "Epoch 1, Batch 591, Test Loss: 1.0581527948379517\n",
      "Epoch 1, Batch 592, Test Loss: 1.0696557760238647\n",
      "Epoch 1, Batch 593, Test Loss: 0.9754329323768616\n",
      "Epoch 1, Batch 594, Test Loss: 1.1773910522460938\n",
      "Epoch 1, Batch 595, Test Loss: 1.1086244583129883\n",
      "Epoch 1, Batch 596, Test Loss: 0.9846034049987793\n",
      "Epoch 1, Batch 597, Test Loss: 1.127363681793213\n",
      "Epoch 1, Batch 598, Test Loss: 1.1036932468414307\n",
      "Epoch 1, Batch 599, Test Loss: 1.0111875534057617\n",
      "Epoch 1, Batch 600, Test Loss: 1.1659523248672485\n",
      "Epoch 1, Batch 601, Test Loss: 1.0865713357925415\n",
      "Epoch 1, Batch 602, Test Loss: 0.9982495307922363\n",
      "Epoch 1, Batch 603, Test Loss: 1.0472298860549927\n",
      "Epoch 1, Batch 604, Test Loss: 1.1458796262741089\n",
      "Epoch 1, Batch 605, Test Loss: 0.9980398416519165\n",
      "Epoch 1, Batch 606, Test Loss: 1.1631724834442139\n",
      "Epoch 1, Batch 607, Test Loss: 1.0697184801101685\n",
      "Epoch 1, Batch 608, Test Loss: 1.124342918395996\n",
      "Epoch 1, Batch 609, Test Loss: 0.9101937413215637\n",
      "Epoch 1, Batch 610, Test Loss: 1.1068005561828613\n",
      "Epoch 1, Batch 611, Test Loss: 1.1939725875854492\n",
      "Epoch 1, Batch 612, Test Loss: 0.9781980514526367\n",
      "Epoch 1, Batch 613, Test Loss: 0.9474362134933472\n",
      "Epoch 1, Batch 614, Test Loss: 1.0387717485427856\n",
      "Epoch 1, Batch 615, Test Loss: 0.9889678359031677\n",
      "Epoch 1, Batch 616, Test Loss: 1.0407814979553223\n",
      "Epoch 1, Batch 617, Test Loss: 1.0273951292037964\n",
      "Epoch 1, Batch 618, Test Loss: 1.0749061107635498\n",
      "Epoch 1, Batch 619, Test Loss: 1.0730807781219482\n",
      "Epoch 1, Batch 620, Test Loss: 1.0084047317504883\n",
      "Epoch 1, Batch 621, Test Loss: 1.130265712738037\n",
      "Epoch 1, Batch 622, Test Loss: 1.0544956922531128\n",
      "Epoch 1, Batch 623, Test Loss: 1.040029525756836\n",
      "Epoch 1, Batch 624, Test Loss: 1.0347261428833008\n",
      "Epoch 1, Batch 625, Test Loss: 1.0176105499267578\n",
      "Epoch 1, Batch 626, Test Loss: 1.0021437406539917\n",
      "Epoch 1, Batch 627, Test Loss: 1.0320143699645996\n",
      "Epoch 1, Batch 628, Test Loss: 1.007575273513794\n",
      "Epoch 1, Batch 629, Test Loss: 0.8715868592262268\n",
      "Epoch 1, Batch 630, Test Loss: 1.0156359672546387\n",
      "Epoch 1, Batch 631, Test Loss: 1.108924388885498\n",
      "Epoch 1, Batch 632, Test Loss: 0.9859203100204468\n",
      "Epoch 1, Batch 633, Test Loss: 1.1792511940002441\n",
      "Epoch 1, Batch 634, Test Loss: 1.0779929161071777\n",
      "Epoch 1, Batch 635, Test Loss: 1.030174732208252\n",
      "Epoch 1, Batch 636, Test Loss: 1.0715198516845703\n",
      "Epoch 1, Batch 637, Test Loss: 1.0025670528411865\n",
      "Epoch 1, Batch 638, Test Loss: 1.2441730499267578\n",
      "Epoch 1, Batch 639, Test Loss: 0.9572204351425171\n",
      "Epoch 1, Batch 640, Test Loss: 1.069562554359436\n",
      "Epoch 1, Batch 641, Test Loss: 1.026525855064392\n",
      "Epoch 1, Batch 642, Test Loss: 1.0247652530670166\n",
      "Epoch 1, Batch 643, Test Loss: 1.1305789947509766\n",
      "Epoch 1, Batch 644, Test Loss: 1.1782747507095337\n",
      "Epoch 1, Batch 645, Test Loss: 0.9042483568191528\n",
      "Epoch 1, Batch 646, Test Loss: 1.2716925144195557\n",
      "Epoch 1, Batch 647, Test Loss: 1.0555745363235474\n",
      "Epoch 1, Batch 648, Test Loss: 0.9594749808311462\n",
      "Epoch 1, Batch 649, Test Loss: 1.0211893320083618\n",
      "Epoch 1, Batch 650, Test Loss: 1.0295603275299072\n",
      "Epoch 1, Batch 651, Test Loss: 0.8926911354064941\n",
      "Epoch 1, Batch 652, Test Loss: 1.0327067375183105\n",
      "Epoch 1, Batch 653, Test Loss: 1.0278599262237549\n",
      "Epoch 1, Batch 654, Test Loss: 1.0253534317016602\n",
      "Epoch 1, Batch 655, Test Loss: 1.2047719955444336\n",
      "Epoch 1, Batch 656, Test Loss: 0.8887649774551392\n",
      "Epoch 1, Batch 657, Test Loss: 1.0036048889160156\n",
      "Epoch 1, Batch 658, Test Loss: 1.0740448236465454\n",
      "Epoch 1, Batch 659, Test Loss: 0.871343731880188\n",
      "Epoch 1, Batch 660, Test Loss: 1.0759170055389404\n",
      "Epoch 1, Batch 661, Test Loss: 1.2402995824813843\n",
      "Epoch 1, Batch 662, Test Loss: 1.060304045677185\n",
      "Epoch 1, Batch 663, Test Loss: 1.0788636207580566\n",
      "Epoch 1, Batch 664, Test Loss: 1.0733065605163574\n",
      "Epoch 1, Batch 665, Test Loss: 0.9084830284118652\n",
      "Epoch 1, Batch 666, Test Loss: 1.172150731086731\n",
      "Epoch 1, Batch 667, Test Loss: 1.1712123155593872\n",
      "Epoch 1, Batch 668, Test Loss: 1.1391197443008423\n",
      "Epoch 1, Batch 669, Test Loss: 1.061510682106018\n",
      "Epoch 1, Batch 670, Test Loss: 1.1840095520019531\n",
      "Epoch 1, Batch 671, Test Loss: 1.08564293384552\n",
      "Epoch 1, Batch 672, Test Loss: 0.9907771944999695\n",
      "Epoch 1, Batch 673, Test Loss: 1.0449042320251465\n",
      "Epoch 1, Batch 674, Test Loss: 1.0561037063598633\n",
      "Epoch 1, Batch 675, Test Loss: 1.027098536491394\n",
      "Epoch 1, Batch 676, Test Loss: 1.1042006015777588\n",
      "Epoch 1, Batch 677, Test Loss: 1.0918850898742676\n",
      "Epoch 1, Batch 678, Test Loss: 0.9940816164016724\n",
      "Epoch 1, Batch 679, Test Loss: 1.1354478597640991\n",
      "Epoch 1, Batch 680, Test Loss: 0.9932202100753784\n",
      "Epoch 1, Batch 681, Test Loss: 1.0984339714050293\n",
      "Epoch 1, Batch 682, Test Loss: 1.216594934463501\n",
      "Epoch 1, Batch 683, Test Loss: 0.9443424940109253\n",
      "Epoch 1, Batch 684, Test Loss: 1.078768253326416\n",
      "Epoch 1, Batch 685, Test Loss: 1.0540399551391602\n",
      "Epoch 1, Batch 686, Test Loss: 1.1241743564605713\n",
      "Epoch 1, Batch 687, Test Loss: 0.888274610042572\n",
      "Epoch 1, Batch 688, Test Loss: 1.1196202039718628\n",
      "Epoch 1, Batch 689, Test Loss: 1.2881380319595337\n",
      "Epoch 1, Batch 690, Test Loss: 1.1842870712280273\n",
      "Epoch 1, Batch 691, Test Loss: 0.9565582275390625\n",
      "Epoch 1, Batch 692, Test Loss: 1.1840437650680542\n",
      "Epoch 1, Batch 693, Test Loss: 1.0044677257537842\n",
      "Epoch 1, Batch 694, Test Loss: 1.2077981233596802\n",
      "Epoch 1, Batch 695, Test Loss: 1.0218013525009155\n",
      "Epoch 1, Batch 696, Test Loss: 1.0345735549926758\n",
      "Epoch 1, Batch 697, Test Loss: 1.1521567106246948\n",
      "Epoch 1, Batch 698, Test Loss: 1.1640174388885498\n",
      "Epoch 1, Batch 699, Test Loss: 1.126011610031128\n",
      "Epoch 1, Batch 700, Test Loss: 0.9854733943939209\n",
      "Epoch 1, Batch 701, Test Loss: 1.0632154941558838\n",
      "Epoch 1, Batch 702, Test Loss: 1.0661462545394897\n",
      "Epoch 1, Batch 703, Test Loss: 1.1480268239974976\n",
      "Epoch 1, Batch 704, Test Loss: 1.1266337633132935\n",
      "Epoch 1, Batch 705, Test Loss: 0.9668784141540527\n",
      "Epoch 1, Batch 706, Test Loss: 1.19062340259552\n",
      "Epoch 1, Batch 707, Test Loss: 1.0524479150772095\n",
      "Epoch 1, Batch 708, Test Loss: 0.9991156458854675\n",
      "Epoch 1, Batch 709, Test Loss: 1.0208264589309692\n",
      "Epoch 1, Batch 710, Test Loss: 1.033277153968811\n",
      "Epoch 1, Batch 711, Test Loss: 1.020385503768921\n",
      "Epoch 1, Batch 712, Test Loss: 1.0871622562408447\n",
      "Epoch 1, Batch 713, Test Loss: 1.0771818161010742\n",
      "Epoch 1, Batch 714, Test Loss: 1.0059614181518555\n",
      "Epoch 1, Batch 715, Test Loss: 1.247850775718689\n",
      "Epoch 1, Batch 716, Test Loss: 1.272731065750122\n",
      "Epoch 1, Batch 717, Test Loss: 0.9571012854576111\n",
      "Epoch 1, Batch 718, Test Loss: 1.1660728454589844\n",
      "Epoch 1, Batch 719, Test Loss: 1.1503779888153076\n",
      "Epoch 1, Batch 720, Test Loss: 1.1576933860778809\n",
      "Epoch 1, Batch 721, Test Loss: 1.0680365562438965\n",
      "Epoch 1, Batch 722, Test Loss: 1.2586077451705933\n",
      "Epoch 1, Batch 723, Test Loss: 1.1039692163467407\n",
      "Epoch 1, Batch 724, Test Loss: 1.1412986516952515\n",
      "Epoch 1, Batch 725, Test Loss: 0.9541881084442139\n",
      "Epoch 1, Batch 726, Test Loss: 1.0012197494506836\n",
      "Epoch 1, Batch 727, Test Loss: 1.052010178565979\n",
      "Epoch 1, Batch 728, Test Loss: 1.0995417833328247\n",
      "Epoch 1, Batch 729, Test Loss: 1.1296619176864624\n",
      "Epoch 1, Batch 730, Test Loss: 1.1064083576202393\n",
      "Epoch 1, Batch 731, Test Loss: 1.0318684577941895\n",
      "Epoch 1, Batch 732, Test Loss: 1.1436710357666016\n",
      "Epoch 1, Batch 733, Test Loss: 0.9963740110397339\n",
      "Epoch 1, Batch 734, Test Loss: 1.019148588180542\n",
      "Epoch 1, Batch 735, Test Loss: 0.9509733319282532\n",
      "Epoch 1, Batch 736, Test Loss: 1.0172733068466187\n",
      "Epoch 1, Batch 737, Test Loss: 0.9912517666816711\n",
      "Epoch 1, Batch 738, Test Loss: 1.0712648630142212\n",
      "Epoch 1, Batch 739, Test Loss: 1.126944899559021\n",
      "Epoch 1, Batch 740, Test Loss: 1.0433175563812256\n",
      "Epoch 1, Batch 741, Test Loss: 0.8716171383857727\n",
      "Epoch 1, Batch 742, Test Loss: 1.0013878345489502\n",
      "Epoch 1, Batch 743, Test Loss: 1.1478610038757324\n",
      "Epoch 1, Batch 744, Test Loss: 1.0609453916549683\n",
      "Epoch 1, Batch 745, Test Loss: 1.2480626106262207\n",
      "Epoch 1, Batch 746, Test Loss: 0.9412032961845398\n",
      "Epoch 1, Batch 747, Test Loss: 1.0091421604156494\n",
      "Epoch 1, Batch 748, Test Loss: 1.1173685789108276\n",
      "Epoch 1, Batch 749, Test Loss: 1.1334431171417236\n",
      "Epoch 1, Batch 750, Test Loss: 1.0469539165496826\n",
      "Epoch 1, Batch 751, Test Loss: 1.0887492895126343\n",
      "Epoch 1, Batch 752, Test Loss: 1.04239022731781\n",
      "Epoch 1, Batch 753, Test Loss: 1.2318413257598877\n",
      "Epoch 1, Batch 754, Test Loss: 1.1128028631210327\n",
      "Epoch 1, Batch 755, Test Loss: 0.8669310212135315\n",
      "Epoch 1, Batch 756, Test Loss: 1.1109148263931274\n",
      "Epoch 1, Batch 757, Test Loss: 0.9733729362487793\n",
      "Epoch 1, Batch 758, Test Loss: 1.0403566360473633\n",
      "Epoch 1, Batch 759, Test Loss: 0.9603998064994812\n",
      "Epoch 1, Batch 760, Test Loss: 0.9128369688987732\n",
      "Epoch 1, Batch 761, Test Loss: 0.9567373394966125\n",
      "Epoch 1, Batch 762, Test Loss: 0.9442232847213745\n",
      "Epoch 1, Batch 763, Test Loss: 1.2217991352081299\n",
      "Epoch 1, Batch 764, Test Loss: 1.1347867250442505\n",
      "Epoch 1, Batch 765, Test Loss: 0.9813501238822937\n",
      "Epoch 1, Batch 766, Test Loss: 0.9987931251525879\n",
      "Epoch 1, Batch 767, Test Loss: 1.2420402765274048\n",
      "Epoch 1, Batch 768, Test Loss: 1.0901832580566406\n",
      "Epoch 1, Batch 769, Test Loss: 0.9839308857917786\n",
      "Epoch 1, Batch 770, Test Loss: 1.1489267349243164\n",
      "Epoch 1, Batch 771, Test Loss: 1.073082447052002\n",
      "Epoch 1, Batch 772, Test Loss: 1.0060741901397705\n",
      "Epoch 1, Batch 773, Test Loss: 1.0572131872177124\n",
      "Epoch 1, Batch 774, Test Loss: 1.1009231805801392\n",
      "Epoch 1, Batch 775, Test Loss: 1.1116975545883179\n",
      "Epoch 1, Batch 776, Test Loss: 0.9881762266159058\n",
      "Epoch 1, Batch 777, Test Loss: 1.036731243133545\n",
      "Epoch 1, Batch 778, Test Loss: 1.0979844331741333\n",
      "Epoch 1, Batch 779, Test Loss: 0.9786366820335388\n",
      "Epoch 1, Batch 780, Test Loss: 1.0463677644729614\n",
      "Epoch 1, Batch 781, Test Loss: 1.0553642511367798\n",
      "Epoch 1, Batch 782, Test Loss: 0.8908829092979431\n",
      "Epoch 1, Batch 783, Test Loss: 0.9945623874664307\n",
      "Epoch 1, Batch 784, Test Loss: 1.0090032815933228\n",
      "Epoch 1, Batch 785, Test Loss: 1.0980582237243652\n",
      "Epoch 1, Batch 786, Test Loss: 1.0699801445007324\n",
      "Epoch 1, Batch 787, Test Loss: 0.7434516549110413\n",
      "Epoch 1, Batch 788, Test Loss: 0.9611719846725464\n",
      "Epoch 1, Batch 789, Test Loss: 1.0767349004745483\n",
      "Epoch 1, Batch 790, Test Loss: 1.1123911142349243\n",
      "Epoch 1, Batch 791, Test Loss: 1.0707069635391235\n",
      "Epoch 1, Batch 792, Test Loss: 1.1384916305541992\n",
      "Epoch 1, Batch 793, Test Loss: 1.0960662364959717\n",
      "Epoch 1, Batch 794, Test Loss: 0.9810081720352173\n",
      "Epoch 1, Batch 795, Test Loss: 1.1502074003219604\n",
      "Epoch 1, Batch 796, Test Loss: 0.9305528402328491\n",
      "Epoch 1, Batch 797, Test Loss: 1.0615684986114502\n",
      "Epoch 1, Batch 798, Test Loss: 0.9851369857788086\n",
      "Epoch 1, Batch 799, Test Loss: 1.0237963199615479\n",
      "Epoch 1, Batch 800, Test Loss: 1.024402141571045\n",
      "Epoch 1, Batch 801, Test Loss: 0.9633055329322815\n",
      "Epoch 1, Batch 802, Test Loss: 1.2502260208129883\n",
      "Epoch 1, Batch 803, Test Loss: 1.1036746501922607\n",
      "Epoch 1, Batch 804, Test Loss: 1.1825417280197144\n",
      "Epoch 1, Batch 805, Test Loss: 0.9626858234405518\n",
      "Epoch 1, Batch 806, Test Loss: 1.0264732837677002\n",
      "Epoch 1, Batch 807, Test Loss: 1.0495564937591553\n",
      "Epoch 1, Batch 808, Test Loss: 1.0482789278030396\n",
      "Epoch 1, Batch 809, Test Loss: 1.0670135021209717\n",
      "Epoch 1, Batch 810, Test Loss: 0.9356426000595093\n",
      "Epoch 1, Batch 811, Test Loss: 1.120504379272461\n",
      "Epoch 1, Batch 812, Test Loss: 1.2760452032089233\n",
      "Epoch 1, Batch 813, Test Loss: 1.1762665510177612\n",
      "Epoch 1, Batch 814, Test Loss: 0.9945694804191589\n",
      "Epoch 1, Batch 815, Test Loss: 1.2365761995315552\n",
      "Epoch 1, Batch 816, Test Loss: 1.0962421894073486\n",
      "Epoch 1, Batch 817, Test Loss: 0.984944224357605\n",
      "Epoch 1, Batch 818, Test Loss: 1.0471681356430054\n",
      "Epoch 1, Batch 819, Test Loss: 0.9945554733276367\n",
      "Epoch 1, Batch 820, Test Loss: 0.9756002426147461\n",
      "Epoch 1, Batch 821, Test Loss: 1.063286304473877\n",
      "Epoch 1, Batch 822, Test Loss: 0.9720227122306824\n",
      "Epoch 1, Batch 823, Test Loss: 1.032533049583435\n",
      "Epoch 1, Batch 824, Test Loss: 0.845431923866272\n",
      "Epoch 1, Batch 825, Test Loss: 1.2235227823257446\n",
      "Epoch 1, Batch 826, Test Loss: 1.0414867401123047\n",
      "Epoch 1, Batch 827, Test Loss: 1.4959216117858887\n",
      "Epoch 1, Batch 828, Test Loss: 0.9981781244277954\n",
      "Epoch 1, Batch 829, Test Loss: 0.9063424468040466\n",
      "Epoch 1, Batch 830, Test Loss: 1.1716930866241455\n",
      "Epoch 1, Batch 831, Test Loss: 1.112769365310669\n",
      "Epoch 1, Batch 832, Test Loss: 1.036637783050537\n",
      "Epoch 1, Batch 833, Test Loss: 1.06325364112854\n",
      "Epoch 1, Batch 834, Test Loss: 1.171394944190979\n",
      "Epoch 1, Batch 835, Test Loss: 1.067272424697876\n",
      "Epoch 1, Batch 836, Test Loss: 0.9002633094787598\n",
      "Epoch 1, Batch 837, Test Loss: 1.17268967628479\n",
      "Epoch 1, Batch 838, Test Loss: 1.1069691181182861\n",
      "Epoch 1, Batch 839, Test Loss: 0.9245796203613281\n",
      "Epoch 1, Batch 840, Test Loss: 1.0129280090332031\n",
      "Epoch 1, Batch 841, Test Loss: 1.0928354263305664\n",
      "Epoch 1, Batch 842, Test Loss: 1.0773499011993408\n",
      "Epoch 1, Batch 843, Test Loss: 0.9080913066864014\n",
      "Epoch 1, Batch 844, Test Loss: 1.0159984827041626\n",
      "Epoch 1, Batch 845, Test Loss: 1.2980544567108154\n",
      "Epoch 1, Batch 846, Test Loss: 1.0482563972473145\n",
      "Epoch 1, Batch 847, Test Loss: 0.9722734093666077\n",
      "Epoch 1, Batch 848, Test Loss: 0.99403977394104\n",
      "Epoch 1, Batch 849, Test Loss: 1.1247787475585938\n",
      "Epoch 1, Batch 850, Test Loss: 1.1753191947937012\n",
      "Epoch 1, Batch 851, Test Loss: 1.0062154531478882\n",
      "Epoch 1, Batch 852, Test Loss: 1.1383370161056519\n",
      "Epoch 1, Batch 853, Test Loss: 1.2357640266418457\n",
      "Epoch 1, Batch 854, Test Loss: 1.0845105648040771\n",
      "Epoch 1, Batch 855, Test Loss: 1.1907669305801392\n",
      "Epoch 1, Batch 856, Test Loss: 1.1617954969406128\n",
      "Epoch 1, Batch 857, Test Loss: 1.0396064519882202\n",
      "Epoch 1, Batch 858, Test Loss: 1.2528431415557861\n",
      "Epoch 1, Batch 859, Test Loss: 1.2611161470413208\n",
      "Epoch 1, Batch 860, Test Loss: 1.3545808792114258\n",
      "Epoch 1, Batch 861, Test Loss: 1.0520265102386475\n",
      "Epoch 1, Batch 862, Test Loss: 1.0016010999679565\n",
      "Epoch 1, Batch 863, Test Loss: 0.9073222875595093\n",
      "Epoch 1, Batch 864, Test Loss: 1.3659671545028687\n",
      "Epoch 1, Batch 865, Test Loss: 1.0010215044021606\n",
      "Epoch 1, Batch 866, Test Loss: 1.0159441232681274\n",
      "Epoch 1, Batch 867, Test Loss: 1.0190341472625732\n",
      "Epoch 1, Batch 868, Test Loss: 1.2387324571609497\n",
      "Epoch 1, Batch 869, Test Loss: 1.1390882730484009\n",
      "Epoch 1, Batch 870, Test Loss: 0.9241051077842712\n",
      "Epoch 1, Batch 871, Test Loss: 0.9806759357452393\n",
      "Epoch 1, Batch 872, Test Loss: 0.8984440565109253\n",
      "Epoch 1, Batch 873, Test Loss: 1.0107555389404297\n",
      "Epoch 1, Batch 874, Test Loss: 1.156607747077942\n",
      "Epoch 1, Batch 875, Test Loss: 1.1645233631134033\n",
      "Epoch 1, Batch 876, Test Loss: 0.9933042526245117\n",
      "Epoch 1, Batch 877, Test Loss: 1.172365427017212\n",
      "Epoch 1, Batch 878, Test Loss: 1.0655341148376465\n",
      "Epoch 1, Batch 879, Test Loss: 1.0732908248901367\n",
      "Epoch 1, Batch 880, Test Loss: 0.9026931524276733\n",
      "Epoch 1, Batch 881, Test Loss: 1.140017032623291\n",
      "Epoch 1, Batch 882, Test Loss: 0.9665292501449585\n",
      "Epoch 1, Batch 883, Test Loss: 1.0000566244125366\n",
      "Epoch 1, Batch 884, Test Loss: 1.2606053352355957\n",
      "Epoch 1, Batch 885, Test Loss: 1.2223381996154785\n",
      "Epoch 1, Batch 886, Test Loss: 0.9144672155380249\n",
      "Epoch 1, Batch 887, Test Loss: 1.068871021270752\n",
      "Epoch 1, Batch 888, Test Loss: 0.9204782247543335\n",
      "Epoch 1, Batch 889, Test Loss: 1.1156772375106812\n",
      "Epoch 1, Batch 890, Test Loss: 1.0291270017623901\n",
      "Epoch 1, Batch 891, Test Loss: 1.19671630859375\n",
      "Epoch 1, Batch 892, Test Loss: 1.2075765132904053\n",
      "Epoch 1, Batch 893, Test Loss: 1.0275893211364746\n",
      "Epoch 1, Batch 894, Test Loss: 1.031760334968567\n",
      "Epoch 1, Batch 895, Test Loss: 1.1145778894424438\n",
      "Epoch 1, Batch 896, Test Loss: 0.9862486124038696\n",
      "Epoch 1, Batch 897, Test Loss: 1.1333471536636353\n",
      "Epoch 1, Batch 898, Test Loss: 1.1036664247512817\n",
      "Epoch 1, Batch 899, Test Loss: 0.9614209532737732\n",
      "Epoch 1, Batch 900, Test Loss: 1.0295542478561401\n",
      "Epoch 1, Batch 901, Test Loss: 1.3187408447265625\n",
      "Epoch 1, Batch 902, Test Loss: 1.0634677410125732\n",
      "Epoch 1, Batch 903, Test Loss: 0.9340486526489258\n",
      "Epoch 1, Batch 904, Test Loss: 1.0000027418136597\n",
      "Epoch 1, Batch 905, Test Loss: 1.156745433807373\n",
      "Epoch 1, Batch 906, Test Loss: 1.1729570627212524\n",
      "Epoch 1, Batch 907, Test Loss: 1.0508867502212524\n",
      "Epoch 1, Batch 908, Test Loss: 1.1267039775848389\n",
      "Epoch 1, Batch 909, Test Loss: 0.9908843636512756\n",
      "Epoch 1, Batch 910, Test Loss: 0.9469053745269775\n",
      "Epoch 1, Batch 911, Test Loss: 1.345156192779541\n",
      "Epoch 1, Batch 912, Test Loss: 1.1160433292388916\n",
      "Epoch 1, Batch 913, Test Loss: 1.068459391593933\n",
      "Epoch 1, Batch 914, Test Loss: 1.0796029567718506\n",
      "Epoch 1, Batch 915, Test Loss: 1.2232328653335571\n",
      "Epoch 1, Batch 916, Test Loss: 1.1952788829803467\n",
      "Epoch 1, Batch 917, Test Loss: 0.9796210527420044\n",
      "Epoch 1, Batch 918, Test Loss: 1.038784146308899\n",
      "Epoch 1, Batch 919, Test Loss: 1.0429677963256836\n",
      "Epoch 1, Batch 920, Test Loss: 0.9814451932907104\n",
      "Epoch 1, Batch 921, Test Loss: 1.11323881149292\n",
      "Epoch 1, Batch 922, Test Loss: 1.0352896451950073\n",
      "Epoch 1, Batch 923, Test Loss: 1.106808066368103\n",
      "Epoch 1, Batch 924, Test Loss: 1.1718864440917969\n",
      "Epoch 1, Batch 925, Test Loss: 1.2451841831207275\n",
      "Epoch 1, Batch 926, Test Loss: 0.8703827857971191\n",
      "Epoch 1, Batch 927, Test Loss: 1.0070886611938477\n",
      "Epoch 1, Batch 928, Test Loss: 1.0696173906326294\n",
      "Epoch 1, Batch 929, Test Loss: 1.1255202293395996\n",
      "Epoch 1, Batch 930, Test Loss: 1.0545084476470947\n",
      "Epoch 1, Batch 931, Test Loss: 0.9729790687561035\n",
      "Epoch 1, Batch 932, Test Loss: 1.1078706979751587\n",
      "Epoch 1, Batch 933, Test Loss: 0.9543620944023132\n",
      "Epoch 1, Batch 934, Test Loss: 0.9486011266708374\n",
      "Epoch 1, Batch 935, Test Loss: 1.0040026903152466\n",
      "Epoch 1, Batch 936, Test Loss: 1.1601537466049194\n",
      "Epoch 1, Batch 937, Test Loss: 1.1496762037277222\n",
      "Epoch 1, Batch 938, Test Loss: 1.1266717910766602\n",
      "Accuracy of Test set: 0.61875\n",
      "Epoch 2, Batch 1, Loss: 1.0020220279693604\n",
      "Epoch 2, Batch 2, Loss: 0.9941875338554382\n",
      "Epoch 2, Batch 3, Loss: 1.0567264556884766\n",
      "Epoch 2, Batch 4, Loss: 0.9920003414154053\n",
      "Epoch 2, Batch 5, Loss: 0.944939374923706\n",
      "Epoch 2, Batch 6, Loss: 0.9549645781517029\n",
      "Epoch 2, Batch 7, Loss: 1.0382472276687622\n",
      "Epoch 2, Batch 8, Loss: 0.982628583908081\n",
      "Epoch 2, Batch 9, Loss: 0.9216940402984619\n",
      "Epoch 2, Batch 10, Loss: 0.9345902800559998\n",
      "Epoch 2, Batch 11, Loss: 0.9905415773391724\n",
      "Epoch 2, Batch 12, Loss: 1.0022783279418945\n",
      "Epoch 2, Batch 13, Loss: 0.8560885787010193\n",
      "Epoch 2, Batch 14, Loss: 0.9186195135116577\n",
      "Epoch 2, Batch 15, Loss: 1.1079949140548706\n",
      "Epoch 2, Batch 16, Loss: 1.0762856006622314\n",
      "Epoch 2, Batch 17, Loss: 0.8630792498588562\n",
      "Epoch 2, Batch 18, Loss: 0.9514151215553284\n",
      "Epoch 2, Batch 19, Loss: 0.9633623361587524\n",
      "Epoch 2, Batch 20, Loss: 0.9938647747039795\n",
      "Epoch 2, Batch 21, Loss: 1.0876948833465576\n",
      "Epoch 2, Batch 22, Loss: 1.0037041902542114\n",
      "Epoch 2, Batch 23, Loss: 1.0919513702392578\n",
      "Epoch 2, Batch 24, Loss: 0.9871400594711304\n",
      "Epoch 2, Batch 25, Loss: 0.8327428698539734\n",
      "Epoch 2, Batch 26, Loss: 1.3053258657455444\n",
      "Epoch 2, Batch 27, Loss: 1.1967378854751587\n",
      "Epoch 2, Batch 28, Loss: 1.0388755798339844\n",
      "Epoch 2, Batch 29, Loss: 0.9577732086181641\n",
      "Epoch 2, Batch 30, Loss: 0.9000252485275269\n",
      "Epoch 2, Batch 31, Loss: 0.9316505193710327\n",
      "Epoch 2, Batch 32, Loss: 1.0165799856185913\n",
      "Epoch 2, Batch 33, Loss: 1.1050293445587158\n",
      "Epoch 2, Batch 34, Loss: 1.13275146484375\n",
      "Epoch 2, Batch 35, Loss: 1.00752592086792\n",
      "Epoch 2, Batch 36, Loss: 1.0989853143692017\n",
      "Epoch 2, Batch 37, Loss: 1.1145397424697876\n",
      "Epoch 2, Batch 38, Loss: 1.0400937795639038\n",
      "Epoch 2, Batch 39, Loss: 0.9326841235160828\n",
      "Epoch 2, Batch 40, Loss: 1.1036564111709595\n",
      "Epoch 2, Batch 41, Loss: 1.09562087059021\n",
      "Epoch 2, Batch 42, Loss: 0.9969112873077393\n",
      "Epoch 2, Batch 43, Loss: 1.0465612411499023\n",
      "Epoch 2, Batch 44, Loss: 1.0347412824630737\n",
      "Epoch 2, Batch 45, Loss: 1.3759061098098755\n",
      "Epoch 2, Batch 46, Loss: 0.9531847238540649\n",
      "Epoch 2, Batch 47, Loss: 1.0251359939575195\n",
      "Epoch 2, Batch 48, Loss: 1.153582215309143\n",
      "Epoch 2, Batch 49, Loss: 0.8421952128410339\n",
      "Epoch 2, Batch 50, Loss: 0.9152463674545288\n",
      "Epoch 2, Batch 51, Loss: 1.0468205213546753\n",
      "Epoch 2, Batch 52, Loss: 0.8969781398773193\n",
      "Epoch 2, Batch 53, Loss: 1.1259654760360718\n",
      "Epoch 2, Batch 54, Loss: 0.8849212527275085\n",
      "Epoch 2, Batch 55, Loss: 0.9106487035751343\n",
      "Epoch 2, Batch 56, Loss: 0.9474136233329773\n",
      "Epoch 2, Batch 57, Loss: 1.009511947631836\n",
      "Epoch 2, Batch 58, Loss: 0.7892194986343384\n",
      "Epoch 2, Batch 59, Loss: 1.0355312824249268\n",
      "Epoch 2, Batch 60, Loss: 1.1177871227264404\n",
      "Epoch 2, Batch 61, Loss: 1.0982214212417603\n",
      "Epoch 2, Batch 62, Loss: 1.1123554706573486\n",
      "Epoch 2, Batch 63, Loss: 0.9257161617279053\n",
      "Epoch 2, Batch 64, Loss: 0.8588060140609741\n",
      "Epoch 2, Batch 65, Loss: 1.0029457807540894\n",
      "Epoch 2, Batch 66, Loss: 0.9086704850196838\n",
      "Epoch 2, Batch 67, Loss: 1.0215857028961182\n",
      "Epoch 2, Batch 68, Loss: 1.1286602020263672\n",
      "Epoch 2, Batch 69, Loss: 0.857628583908081\n",
      "Epoch 2, Batch 70, Loss: 1.1737107038497925\n",
      "Epoch 2, Batch 71, Loss: 1.0012638568878174\n",
      "Epoch 2, Batch 72, Loss: 1.1823147535324097\n",
      "Epoch 2, Batch 73, Loss: 1.0666685104370117\n",
      "Epoch 2, Batch 74, Loss: 1.1127954721450806\n",
      "Epoch 2, Batch 75, Loss: 0.9076457619667053\n",
      "Epoch 2, Batch 76, Loss: 1.2510602474212646\n",
      "Epoch 2, Batch 77, Loss: 0.9368534088134766\n",
      "Epoch 2, Batch 78, Loss: 1.0966717004776\n",
      "Epoch 2, Batch 79, Loss: 1.1706933975219727\n",
      "Epoch 2, Batch 80, Loss: 1.0193393230438232\n",
      "Epoch 2, Batch 81, Loss: 0.9638797640800476\n",
      "Epoch 2, Batch 82, Loss: 1.1197168827056885\n",
      "Epoch 2, Batch 83, Loss: 0.9309599995613098\n",
      "Epoch 2, Batch 84, Loss: 0.9176698923110962\n",
      "Epoch 2, Batch 85, Loss: 0.9549311399459839\n",
      "Epoch 2, Batch 86, Loss: 1.128568172454834\n",
      "Epoch 2, Batch 87, Loss: 1.0762133598327637\n",
      "Epoch 2, Batch 88, Loss: 1.0402419567108154\n",
      "Epoch 2, Batch 89, Loss: 0.9045485258102417\n",
      "Epoch 2, Batch 90, Loss: 1.0541993379592896\n",
      "Epoch 2, Batch 91, Loss: 0.9568634629249573\n",
      "Epoch 2, Batch 92, Loss: 0.9470254778862\n",
      "Epoch 2, Batch 93, Loss: 1.0814533233642578\n",
      "Epoch 2, Batch 94, Loss: 1.0450677871704102\n",
      "Epoch 2, Batch 95, Loss: 1.0311384201049805\n",
      "Epoch 2, Batch 96, Loss: 0.9195735454559326\n",
      "Epoch 2, Batch 97, Loss: 1.0394853353500366\n",
      "Epoch 2, Batch 98, Loss: 1.0254013538360596\n",
      "Epoch 2, Batch 99, Loss: 1.0234739780426025\n",
      "Epoch 2, Batch 100, Loss: 0.9296690821647644\n",
      "Epoch 2, Batch 101, Loss: 1.0758423805236816\n",
      "Epoch 2, Batch 102, Loss: 1.1988301277160645\n",
      "Epoch 2, Batch 103, Loss: 0.9782878756523132\n",
      "Epoch 2, Batch 104, Loss: 1.0544145107269287\n",
      "Epoch 2, Batch 105, Loss: 1.306011438369751\n",
      "Epoch 2, Batch 106, Loss: 1.059338927268982\n",
      "Epoch 2, Batch 107, Loss: 1.1130117177963257\n",
      "Epoch 2, Batch 108, Loss: 1.0778520107269287\n",
      "Epoch 2, Batch 109, Loss: 1.0972472429275513\n",
      "Epoch 2, Batch 110, Loss: 0.8910821676254272\n",
      "Epoch 2, Batch 111, Loss: 1.0698624849319458\n",
      "Epoch 2, Batch 112, Loss: 0.9274014234542847\n",
      "Epoch 2, Batch 113, Loss: 0.786571204662323\n",
      "Epoch 2, Batch 114, Loss: 1.0744575262069702\n",
      "Epoch 2, Batch 115, Loss: 0.9792638421058655\n",
      "Epoch 2, Batch 116, Loss: 1.0043513774871826\n",
      "Epoch 2, Batch 117, Loss: 0.9989446401596069\n",
      "Epoch 2, Batch 118, Loss: 1.0915614366531372\n",
      "Epoch 2, Batch 119, Loss: 1.2588576078414917\n",
      "Epoch 2, Batch 120, Loss: 1.00810968875885\n",
      "Epoch 2, Batch 121, Loss: 0.9400396347045898\n",
      "Epoch 2, Batch 122, Loss: 1.000016689300537\n",
      "Epoch 2, Batch 123, Loss: 1.1699334383010864\n",
      "Epoch 2, Batch 124, Loss: 1.0906709432601929\n",
      "Epoch 2, Batch 125, Loss: 0.8429505825042725\n",
      "Epoch 2, Batch 126, Loss: 1.034837007522583\n",
      "Epoch 2, Batch 127, Loss: 1.0677865743637085\n",
      "Epoch 2, Batch 128, Loss: 0.9394750595092773\n",
      "Epoch 2, Batch 129, Loss: 1.1312904357910156\n",
      "Epoch 2, Batch 130, Loss: 0.9814841151237488\n",
      "Epoch 2, Batch 131, Loss: 1.0872758626937866\n",
      "Epoch 2, Batch 132, Loss: 0.9369261860847473\n",
      "Epoch 2, Batch 133, Loss: 1.0690841674804688\n",
      "Epoch 2, Batch 134, Loss: 0.9235398173332214\n",
      "Epoch 2, Batch 135, Loss: 0.9469772577285767\n",
      "Epoch 2, Batch 136, Loss: 0.9885707497596741\n",
      "Epoch 2, Batch 137, Loss: 0.8867978453636169\n",
      "Epoch 2, Batch 138, Loss: 1.079984188079834\n",
      "Epoch 2, Batch 139, Loss: 1.0390629768371582\n",
      "Epoch 2, Batch 140, Loss: 0.9995876550674438\n",
      "Epoch 2, Batch 141, Loss: 1.1749131679534912\n",
      "Epoch 2, Batch 142, Loss: 1.0299533605575562\n",
      "Epoch 2, Batch 143, Loss: 1.0573920011520386\n",
      "Epoch 2, Batch 144, Loss: 0.9819610714912415\n",
      "Epoch 2, Batch 145, Loss: 0.9445927739143372\n",
      "Epoch 2, Batch 146, Loss: 1.1140416860580444\n",
      "Epoch 2, Batch 147, Loss: 0.9482190608978271\n",
      "Epoch 2, Batch 148, Loss: 0.8576122522354126\n",
      "Epoch 2, Batch 149, Loss: 0.9556108713150024\n",
      "Epoch 2, Batch 150, Loss: 1.0026506185531616\n",
      "Epoch 2, Batch 151, Loss: 0.872796356678009\n",
      "Epoch 2, Batch 152, Loss: 0.8425666093826294\n",
      "Epoch 2, Batch 153, Loss: 0.8869184851646423\n",
      "Epoch 2, Batch 154, Loss: 0.8775026202201843\n",
      "Epoch 2, Batch 155, Loss: 0.9275896549224854\n",
      "Epoch 2, Batch 156, Loss: 0.9943299293518066\n",
      "Epoch 2, Batch 157, Loss: 1.0062947273254395\n",
      "Epoch 2, Batch 158, Loss: 1.1271405220031738\n",
      "Epoch 2, Batch 159, Loss: 0.9458709359169006\n",
      "Epoch 2, Batch 160, Loss: 1.0650861263275146\n",
      "Epoch 2, Batch 161, Loss: 0.9371335506439209\n",
      "Epoch 2, Batch 162, Loss: 0.9684274792671204\n",
      "Epoch 2, Batch 163, Loss: 0.9925639033317566\n",
      "Epoch 2, Batch 164, Loss: 0.9540387988090515\n",
      "Epoch 2, Batch 165, Loss: 0.8940433263778687\n",
      "Epoch 2, Batch 166, Loss: 0.9420508146286011\n",
      "Epoch 2, Batch 167, Loss: 0.9738298058509827\n",
      "Epoch 2, Batch 168, Loss: 0.968862771987915\n",
      "Epoch 2, Batch 169, Loss: 0.9878140091896057\n",
      "Epoch 2, Batch 170, Loss: 0.9078311920166016\n",
      "Epoch 2, Batch 171, Loss: 0.9424164891242981\n",
      "Epoch 2, Batch 172, Loss: 0.963229238986969\n",
      "Epoch 2, Batch 173, Loss: 0.8577812910079956\n",
      "Epoch 2, Batch 174, Loss: 0.9736616611480713\n",
      "Epoch 2, Batch 175, Loss: 1.0371094942092896\n",
      "Epoch 2, Batch 176, Loss: 0.9312937259674072\n",
      "Epoch 2, Batch 177, Loss: 0.8144164681434631\n",
      "Epoch 2, Batch 178, Loss: 1.0084666013717651\n",
      "Epoch 2, Batch 179, Loss: 0.9490766525268555\n",
      "Epoch 2, Batch 180, Loss: 0.9361845850944519\n",
      "Epoch 2, Batch 181, Loss: 1.0665948390960693\n",
      "Epoch 2, Batch 182, Loss: 0.9487532377243042\n",
      "Epoch 2, Batch 183, Loss: 1.257218599319458\n",
      "Epoch 2, Batch 184, Loss: 1.0224871635437012\n",
      "Epoch 2, Batch 185, Loss: 0.9569830894470215\n",
      "Epoch 2, Batch 186, Loss: 0.9411894083023071\n",
      "Epoch 2, Batch 187, Loss: 0.8638356924057007\n",
      "Epoch 2, Batch 188, Loss: 1.0443111658096313\n",
      "Epoch 2, Batch 189, Loss: 1.1174978017807007\n",
      "Epoch 2, Batch 190, Loss: 0.775661826133728\n",
      "Epoch 2, Batch 191, Loss: 1.028788685798645\n",
      "Epoch 2, Batch 192, Loss: 0.9820401668548584\n",
      "Epoch 2, Batch 193, Loss: 1.0243958234786987\n",
      "Epoch 2, Batch 194, Loss: 0.8846241235733032\n",
      "Epoch 2, Batch 195, Loss: 1.1216975450515747\n",
      "Epoch 2, Batch 196, Loss: 0.9529544711112976\n",
      "Epoch 2, Batch 197, Loss: 0.9766877293586731\n",
      "Epoch 2, Batch 198, Loss: 1.0907537937164307\n",
      "Epoch 2, Batch 199, Loss: 1.0090612173080444\n",
      "Epoch 2, Batch 200, Loss: 1.0073031187057495\n",
      "Epoch 2, Batch 201, Loss: 1.104366660118103\n",
      "Epoch 2, Batch 202, Loss: 0.9099059104919434\n",
      "Epoch 2, Batch 203, Loss: 1.055582880973816\n",
      "Epoch 2, Batch 204, Loss: 0.8312897682189941\n",
      "Epoch 2, Batch 205, Loss: 1.185866117477417\n",
      "Epoch 2, Batch 206, Loss: 0.8778918981552124\n",
      "Epoch 2, Batch 207, Loss: 0.8447490930557251\n",
      "Epoch 2, Batch 208, Loss: 1.122018575668335\n",
      "Epoch 2, Batch 209, Loss: 0.909234344959259\n",
      "Epoch 2, Batch 210, Loss: 1.0508100986480713\n",
      "Epoch 2, Batch 211, Loss: 0.8608852624893188\n",
      "Epoch 2, Batch 212, Loss: 0.863598108291626\n",
      "Epoch 2, Batch 213, Loss: 1.1701833009719849\n",
      "Epoch 2, Batch 214, Loss: 0.8732296824455261\n",
      "Epoch 2, Batch 215, Loss: 0.8716588020324707\n",
      "Epoch 2, Batch 216, Loss: 0.9094745516777039\n",
      "Epoch 2, Batch 217, Loss: 1.1091636419296265\n",
      "Epoch 2, Batch 218, Loss: 0.8425337672233582\n",
      "Epoch 2, Batch 219, Loss: 1.032286524772644\n",
      "Epoch 2, Batch 220, Loss: 1.077467441558838\n",
      "Epoch 2, Batch 221, Loss: 1.1618516445159912\n",
      "Epoch 2, Batch 222, Loss: 1.1048940420150757\n",
      "Epoch 2, Batch 223, Loss: 0.9802069067955017\n",
      "Epoch 2, Batch 224, Loss: 1.0591404438018799\n",
      "Epoch 2, Batch 225, Loss: 0.9154406785964966\n",
      "Epoch 2, Batch 226, Loss: 1.0089123249053955\n",
      "Epoch 2, Batch 227, Loss: 1.0236711502075195\n",
      "Epoch 2, Batch 228, Loss: 0.9165799617767334\n",
      "Epoch 2, Batch 229, Loss: 1.0032061338424683\n",
      "Epoch 2, Batch 230, Loss: 0.9803292751312256\n",
      "Epoch 2, Batch 231, Loss: 0.8287152051925659\n",
      "Epoch 2, Batch 232, Loss: 0.9602941274642944\n",
      "Epoch 2, Batch 233, Loss: 0.9925063848495483\n",
      "Epoch 2, Batch 234, Loss: 0.895075798034668\n",
      "Epoch 2, Batch 235, Loss: 1.0144426822662354\n",
      "Epoch 2, Batch 236, Loss: 0.9569756984710693\n",
      "Epoch 2, Batch 237, Loss: 0.8926182389259338\n",
      "Epoch 2, Batch 238, Loss: 0.9568011164665222\n",
      "Epoch 2, Batch 239, Loss: 0.9148577451705933\n",
      "Epoch 2, Batch 240, Loss: 1.1971888542175293\n",
      "Epoch 2, Batch 241, Loss: 0.949381947517395\n",
      "Epoch 2, Batch 242, Loss: 0.8601333498954773\n",
      "Epoch 2, Batch 243, Loss: 0.8889835476875305\n",
      "Epoch 2, Batch 244, Loss: 0.9522976875305176\n",
      "Epoch 2, Batch 245, Loss: 0.993193507194519\n",
      "Epoch 2, Batch 246, Loss: 1.214512825012207\n",
      "Epoch 2, Batch 247, Loss: 0.931046724319458\n",
      "Epoch 2, Batch 248, Loss: 0.8216791152954102\n",
      "Epoch 2, Batch 249, Loss: 0.7989805340766907\n",
      "Epoch 2, Batch 250, Loss: 0.9841219186782837\n",
      "Epoch 2, Batch 251, Loss: 1.0369222164154053\n",
      "Epoch 2, Batch 252, Loss: 0.9248126149177551\n",
      "Epoch 2, Batch 253, Loss: 0.932914674282074\n",
      "Epoch 2, Batch 254, Loss: 0.9561127424240112\n",
      "Epoch 2, Batch 255, Loss: 0.9849280118942261\n",
      "Epoch 2, Batch 256, Loss: 0.9460664391517639\n",
      "Epoch 2, Batch 257, Loss: 1.0500789880752563\n",
      "Epoch 2, Batch 258, Loss: 1.1617190837860107\n",
      "Epoch 2, Batch 259, Loss: 1.0175509452819824\n",
      "Epoch 2, Batch 260, Loss: 0.9657082557678223\n",
      "Epoch 2, Batch 261, Loss: 1.023158311843872\n",
      "Epoch 2, Batch 262, Loss: 1.0260517597198486\n",
      "Epoch 2, Batch 263, Loss: 1.03949773311615\n",
      "Epoch 2, Batch 264, Loss: 1.1199638843536377\n",
      "Epoch 2, Batch 265, Loss: 0.9476974606513977\n",
      "Epoch 2, Batch 266, Loss: 1.0704796314239502\n",
      "Epoch 2, Batch 267, Loss: 1.2043719291687012\n",
      "Epoch 2, Batch 268, Loss: 1.0019160509109497\n",
      "Epoch 2, Batch 269, Loss: 1.201172113418579\n",
      "Epoch 2, Batch 270, Loss: 0.9925628900527954\n",
      "Epoch 2, Batch 271, Loss: 0.9184218049049377\n",
      "Epoch 2, Batch 272, Loss: 0.8673946857452393\n",
      "Epoch 2, Batch 273, Loss: 0.9345795512199402\n",
      "Epoch 2, Batch 274, Loss: 1.107383370399475\n",
      "Epoch 2, Batch 275, Loss: 0.9272732734680176\n",
      "Epoch 2, Batch 276, Loss: 0.9732601642608643\n",
      "Epoch 2, Batch 277, Loss: 1.186790943145752\n",
      "Epoch 2, Batch 278, Loss: 0.9813893437385559\n",
      "Epoch 2, Batch 279, Loss: 0.9820934534072876\n",
      "Epoch 2, Batch 280, Loss: 0.9385862350463867\n",
      "Epoch 2, Batch 281, Loss: 0.9357276558876038\n",
      "Epoch 2, Batch 282, Loss: 0.9962958097457886\n",
      "Epoch 2, Batch 283, Loss: 1.052601933479309\n",
      "Epoch 2, Batch 284, Loss: 0.8606324195861816\n",
      "Epoch 2, Batch 285, Loss: 0.9139981865882874\n",
      "Epoch 2, Batch 286, Loss: 0.9534817934036255\n",
      "Epoch 2, Batch 287, Loss: 0.9372588396072388\n",
      "Epoch 2, Batch 288, Loss: 0.9576380252838135\n",
      "Epoch 2, Batch 289, Loss: 0.9922149181365967\n",
      "Epoch 2, Batch 290, Loss: 0.8073204159736633\n",
      "Epoch 2, Batch 291, Loss: 0.9312624335289001\n",
      "Epoch 2, Batch 292, Loss: 1.0033056735992432\n",
      "Epoch 2, Batch 293, Loss: 0.8918784856796265\n",
      "Epoch 2, Batch 294, Loss: 0.7624364495277405\n",
      "Epoch 2, Batch 295, Loss: 0.9485763311386108\n",
      "Epoch 2, Batch 296, Loss: 0.9752882122993469\n",
      "Epoch 2, Batch 297, Loss: 0.893634021282196\n",
      "Epoch 2, Batch 298, Loss: 0.8156358003616333\n",
      "Epoch 2, Batch 299, Loss: 1.1464645862579346\n",
      "Epoch 2, Batch 300, Loss: 0.8847702145576477\n",
      "Epoch 2, Batch 301, Loss: 0.9771290421485901\n",
      "Epoch 2, Batch 302, Loss: 0.9059197902679443\n",
      "Epoch 2, Batch 303, Loss: 0.9549203515052795\n",
      "Epoch 2, Batch 304, Loss: 1.2591367959976196\n",
      "Epoch 2, Batch 305, Loss: 0.997394859790802\n",
      "Epoch 2, Batch 306, Loss: 0.8137789964675903\n",
      "Epoch 2, Batch 307, Loss: 0.9755908250808716\n",
      "Epoch 2, Batch 308, Loss: 1.0953398942947388\n",
      "Epoch 2, Batch 309, Loss: 0.9631078243255615\n",
      "Epoch 2, Batch 310, Loss: 1.041860818862915\n",
      "Epoch 2, Batch 311, Loss: 1.0708434581756592\n",
      "Epoch 2, Batch 312, Loss: 0.926095187664032\n",
      "Epoch 2, Batch 313, Loss: 1.004064917564392\n",
      "Epoch 2, Batch 314, Loss: 1.065863013267517\n",
      "Epoch 2, Batch 315, Loss: 1.4055448770523071\n",
      "Epoch 2, Batch 316, Loss: 0.8605130910873413\n",
      "Epoch 2, Batch 317, Loss: 0.9332355260848999\n",
      "Epoch 2, Batch 318, Loss: 1.0617893934249878\n",
      "Epoch 2, Batch 319, Loss: 0.8278599977493286\n",
      "Epoch 2, Batch 320, Loss: 0.9559552073478699\n",
      "Epoch 2, Batch 321, Loss: 0.8983160257339478\n",
      "Epoch 2, Batch 322, Loss: 1.1760330200195312\n",
      "Epoch 2, Batch 323, Loss: 1.0070940256118774\n",
      "Epoch 2, Batch 324, Loss: 0.791224479675293\n",
      "Epoch 2, Batch 325, Loss: 0.8639208078384399\n",
      "Epoch 2, Batch 326, Loss: 1.0948671102523804\n",
      "Epoch 2, Batch 327, Loss: 1.2264529466629028\n",
      "Epoch 2, Batch 328, Loss: 0.9189326167106628\n",
      "Epoch 2, Batch 329, Loss: 0.8220061659812927\n",
      "Epoch 2, Batch 330, Loss: 1.1308681964874268\n",
      "Epoch 2, Batch 331, Loss: 0.836338222026825\n",
      "Epoch 2, Batch 332, Loss: 1.0141701698303223\n",
      "Epoch 2, Batch 333, Loss: 1.0250800848007202\n",
      "Epoch 2, Batch 334, Loss: 0.8789821267127991\n",
      "Epoch 2, Batch 335, Loss: 1.0300254821777344\n",
      "Epoch 2, Batch 336, Loss: 1.0576813220977783\n",
      "Epoch 2, Batch 337, Loss: 0.9264355301856995\n",
      "Epoch 2, Batch 338, Loss: 0.9914423227310181\n",
      "Epoch 2, Batch 339, Loss: 1.010005235671997\n",
      "Epoch 2, Batch 340, Loss: 0.9483963251113892\n",
      "Epoch 2, Batch 341, Loss: 1.0900763273239136\n",
      "Epoch 2, Batch 342, Loss: 0.8152670860290527\n",
      "Epoch 2, Batch 343, Loss: 0.8171075582504272\n",
      "Epoch 2, Batch 344, Loss: 0.9789494276046753\n",
      "Epoch 2, Batch 345, Loss: 0.8881934285163879\n",
      "Epoch 2, Batch 346, Loss: 0.7682813405990601\n",
      "Epoch 2, Batch 347, Loss: 0.9837173223495483\n",
      "Epoch 2, Batch 348, Loss: 0.9227601885795593\n",
      "Epoch 2, Batch 349, Loss: 0.8767465949058533\n",
      "Epoch 2, Batch 350, Loss: 1.0340114831924438\n",
      "Epoch 2, Batch 351, Loss: 0.8332525491714478\n",
      "Epoch 2, Batch 352, Loss: 1.0018857717514038\n",
      "Epoch 2, Batch 353, Loss: 0.795881986618042\n",
      "Epoch 2, Batch 354, Loss: 0.8830589652061462\n",
      "Epoch 2, Batch 355, Loss: 1.1180301904678345\n",
      "Epoch 2, Batch 356, Loss: 0.9354920387268066\n",
      "Epoch 2, Batch 357, Loss: 0.898361086845398\n",
      "Epoch 2, Batch 358, Loss: 0.7489957809448242\n",
      "Epoch 2, Batch 359, Loss: 0.8793377876281738\n",
      "Epoch 2, Batch 360, Loss: 0.9392800331115723\n",
      "Epoch 2, Batch 361, Loss: 0.8818545341491699\n",
      "Epoch 2, Batch 362, Loss: 0.8974440097808838\n",
      "Epoch 2, Batch 363, Loss: 1.1654605865478516\n",
      "Epoch 2, Batch 364, Loss: 0.9144158959388733\n",
      "Epoch 2, Batch 365, Loss: 1.0192065238952637\n",
      "Epoch 2, Batch 366, Loss: 0.9697991609573364\n",
      "Epoch 2, Batch 367, Loss: 0.9450271129608154\n",
      "Epoch 2, Batch 368, Loss: 0.9624496102333069\n",
      "Epoch 2, Batch 369, Loss: 0.7394202351570129\n",
      "Epoch 2, Batch 370, Loss: 1.0195270776748657\n",
      "Epoch 2, Batch 371, Loss: 0.9413474202156067\n",
      "Epoch 2, Batch 372, Loss: 1.0131679773330688\n",
      "Epoch 2, Batch 373, Loss: 1.0638980865478516\n",
      "Epoch 2, Batch 374, Loss: 0.7642297148704529\n",
      "Epoch 2, Batch 375, Loss: 0.9742895364761353\n",
      "Epoch 2, Batch 376, Loss: 0.8551242351531982\n",
      "Epoch 2, Batch 377, Loss: 1.0811349153518677\n",
      "Epoch 2, Batch 378, Loss: 0.9183064699172974\n",
      "Epoch 2, Batch 379, Loss: 0.9662230014801025\n",
      "Epoch 2, Batch 380, Loss: 0.911344051361084\n",
      "Epoch 2, Batch 381, Loss: 0.8728042840957642\n",
      "Epoch 2, Batch 382, Loss: 1.096740484237671\n",
      "Epoch 2, Batch 383, Loss: 0.8860747814178467\n",
      "Epoch 2, Batch 384, Loss: 0.9140241742134094\n",
      "Epoch 2, Batch 385, Loss: 0.823133111000061\n",
      "Epoch 2, Batch 386, Loss: 0.9239778518676758\n",
      "Epoch 2, Batch 387, Loss: 0.9032520651817322\n",
      "Epoch 2, Batch 388, Loss: 0.9574841856956482\n",
      "Epoch 2, Batch 389, Loss: 0.8426646590232849\n",
      "Epoch 2, Batch 390, Loss: 0.7778583765029907\n",
      "Epoch 2, Batch 391, Loss: 0.8535606861114502\n",
      "Epoch 2, Batch 392, Loss: 0.7298074960708618\n",
      "Epoch 2, Batch 393, Loss: 1.094994068145752\n",
      "Epoch 2, Batch 394, Loss: 0.9110738039016724\n",
      "Epoch 2, Batch 395, Loss: 1.0121324062347412\n",
      "Epoch 2, Batch 396, Loss: 0.9022232294082642\n",
      "Epoch 2, Batch 397, Loss: 0.9711967706680298\n",
      "Epoch 2, Batch 398, Loss: 0.9524252414703369\n",
      "Epoch 2, Batch 399, Loss: 0.9171422719955444\n",
      "Epoch 2, Batch 400, Loss: 1.008482575416565\n",
      "Epoch 2, Batch 401, Loss: 0.9574863910675049\n",
      "Epoch 2, Batch 402, Loss: 0.8287171721458435\n",
      "Epoch 2, Batch 403, Loss: 0.8611147403717041\n",
      "Epoch 2, Batch 404, Loss: 0.8303394317626953\n",
      "Epoch 2, Batch 405, Loss: 0.8884152173995972\n",
      "Epoch 2, Batch 406, Loss: 0.8530736565589905\n",
      "Epoch 2, Batch 407, Loss: 0.9436571598052979\n",
      "Epoch 2, Batch 408, Loss: 1.0241972208023071\n",
      "Epoch 2, Batch 409, Loss: 1.0792999267578125\n",
      "Epoch 2, Batch 410, Loss: 0.9879029393196106\n",
      "Epoch 2, Batch 411, Loss: 0.8411898016929626\n",
      "Epoch 2, Batch 412, Loss: 0.8695551753044128\n",
      "Epoch 2, Batch 413, Loss: 0.9579364657402039\n",
      "Epoch 2, Batch 414, Loss: 0.9210212826728821\n",
      "Epoch 2, Batch 415, Loss: 1.0223779678344727\n",
      "Epoch 2, Batch 416, Loss: 0.9105291366577148\n",
      "Epoch 2, Batch 417, Loss: 0.9405207633972168\n",
      "Epoch 2, Batch 418, Loss: 1.0364471673965454\n",
      "Epoch 2, Batch 419, Loss: 0.9263125658035278\n",
      "Epoch 2, Batch 420, Loss: 0.9911014437675476\n",
      "Epoch 2, Batch 421, Loss: 0.8408522605895996\n",
      "Epoch 2, Batch 422, Loss: 0.9847228527069092\n",
      "Epoch 2, Batch 423, Loss: 0.7363375425338745\n",
      "Epoch 2, Batch 424, Loss: 0.9375165700912476\n",
      "Epoch 2, Batch 425, Loss: 1.0361043214797974\n",
      "Epoch 2, Batch 426, Loss: 0.9651483297348022\n",
      "Epoch 2, Batch 427, Loss: 1.0126800537109375\n",
      "Epoch 2, Batch 428, Loss: 0.8330633640289307\n",
      "Epoch 2, Batch 429, Loss: 0.8922804594039917\n",
      "Epoch 2, Batch 430, Loss: 0.985828161239624\n",
      "Epoch 2, Batch 431, Loss: 0.7917419075965881\n",
      "Epoch 2, Batch 432, Loss: 0.8936131596565247\n",
      "Epoch 2, Batch 433, Loss: 0.8506848216056824\n",
      "Epoch 2, Batch 434, Loss: 1.005310297012329\n",
      "Epoch 2, Batch 435, Loss: 0.9000617265701294\n",
      "Epoch 2, Batch 436, Loss: 0.8708913326263428\n",
      "Epoch 2, Batch 437, Loss: 0.867400050163269\n",
      "Epoch 2, Batch 438, Loss: 0.9160541892051697\n",
      "Epoch 2, Batch 439, Loss: 0.7344624400138855\n",
      "Epoch 2, Batch 440, Loss: 0.9675962924957275\n",
      "Epoch 2, Batch 441, Loss: 0.779924750328064\n",
      "Epoch 2, Batch 442, Loss: 0.9321442246437073\n",
      "Epoch 2, Batch 443, Loss: 0.8570504188537598\n",
      "Epoch 2, Batch 444, Loss: 0.9400690197944641\n",
      "Epoch 2, Batch 445, Loss: 1.0263779163360596\n",
      "Epoch 2, Batch 446, Loss: 0.750597357749939\n",
      "Epoch 2, Batch 447, Loss: 0.7814016342163086\n",
      "Epoch 2, Batch 448, Loss: 1.1320428848266602\n",
      "Epoch 2, Batch 449, Loss: 0.8490453958511353\n",
      "Epoch 2, Batch 450, Loss: 0.9267266988754272\n",
      "Epoch 2, Batch 451, Loss: 0.99506676197052\n",
      "Epoch 2, Batch 452, Loss: 0.8144036531448364\n",
      "Epoch 2, Batch 453, Loss: 0.8886473178863525\n",
      "Epoch 2, Batch 454, Loss: 0.8834385275840759\n",
      "Epoch 2, Batch 455, Loss: 0.9990515112876892\n",
      "Epoch 2, Batch 456, Loss: 0.9920663237571716\n",
      "Epoch 2, Batch 457, Loss: 0.7797766327857971\n",
      "Epoch 2, Batch 458, Loss: 0.9826744794845581\n",
      "Epoch 2, Batch 459, Loss: 0.7565999031066895\n",
      "Epoch 2, Batch 460, Loss: 1.012087106704712\n",
      "Epoch 2, Batch 461, Loss: 0.7560293078422546\n",
      "Epoch 2, Batch 462, Loss: 0.8301233649253845\n",
      "Epoch 2, Batch 463, Loss: 0.7028917074203491\n",
      "Epoch 2, Batch 464, Loss: 0.9830167293548584\n",
      "Epoch 2, Batch 465, Loss: 0.9665721654891968\n",
      "Epoch 2, Batch 466, Loss: 0.9995554089546204\n",
      "Epoch 2, Batch 467, Loss: 0.9414647817611694\n",
      "Epoch 2, Batch 468, Loss: 0.7939127683639526\n",
      "Epoch 2, Batch 469, Loss: 0.7832334637641907\n",
      "Epoch 2, Batch 470, Loss: 0.8152582049369812\n",
      "Epoch 2, Batch 471, Loss: 0.9552178382873535\n",
      "Epoch 2, Batch 472, Loss: 1.0276397466659546\n",
      "Epoch 2, Batch 473, Loss: 0.9205467104911804\n",
      "Epoch 2, Batch 474, Loss: 0.873737096786499\n",
      "Epoch 2, Batch 475, Loss: 0.9075849056243896\n",
      "Epoch 2, Batch 476, Loss: 1.0021727085113525\n",
      "Epoch 2, Batch 477, Loss: 0.784157395362854\n",
      "Epoch 2, Batch 478, Loss: 0.9229166507720947\n",
      "Epoch 2, Batch 479, Loss: 0.8699716329574585\n",
      "Epoch 2, Batch 480, Loss: 0.6909796595573425\n",
      "Epoch 2, Batch 481, Loss: 0.7712810635566711\n",
      "Epoch 2, Batch 482, Loss: 0.8122681975364685\n",
      "Epoch 2, Batch 483, Loss: 0.9114837646484375\n",
      "Epoch 2, Batch 484, Loss: 0.8268682956695557\n",
      "Epoch 2, Batch 485, Loss: 1.2240031957626343\n",
      "Epoch 2, Batch 486, Loss: 1.0485652685165405\n",
      "Epoch 2, Batch 487, Loss: 0.8314570188522339\n",
      "Epoch 2, Batch 488, Loss: 0.8288050889968872\n",
      "Epoch 2, Batch 489, Loss: 1.0199695825576782\n",
      "Epoch 2, Batch 490, Loss: 1.022650122642517\n",
      "Epoch 2, Batch 491, Loss: 0.7107401490211487\n",
      "Epoch 2, Batch 492, Loss: 0.9354052543640137\n",
      "Epoch 2, Batch 493, Loss: 0.8665478229522705\n",
      "Epoch 2, Batch 494, Loss: 0.8139069676399231\n",
      "Epoch 2, Batch 495, Loss: 0.9490655064582825\n",
      "Epoch 2, Batch 496, Loss: 0.7350851893424988\n",
      "Epoch 2, Batch 497, Loss: 1.128340721130371\n",
      "Epoch 2, Batch 498, Loss: 0.9695107936859131\n",
      "Epoch 2, Batch 499, Loss: 0.9093143343925476\n",
      "Epoch 2, Batch 500, Loss: 0.822293221950531\n",
      "Epoch 2, Batch 501, Loss: 0.8563517928123474\n",
      "Epoch 2, Batch 502, Loss: 0.7744371294975281\n",
      "Epoch 2, Batch 503, Loss: 0.8903799057006836\n",
      "Epoch 2, Batch 504, Loss: 0.6709467172622681\n",
      "Epoch 2, Batch 505, Loss: 1.0094293355941772\n",
      "Epoch 2, Batch 506, Loss: 1.1458781957626343\n",
      "Epoch 2, Batch 507, Loss: 0.862335741519928\n",
      "Epoch 2, Batch 508, Loss: 0.9124234318733215\n",
      "Epoch 2, Batch 509, Loss: 0.8304246664047241\n",
      "Epoch 2, Batch 510, Loss: 0.9839383959770203\n",
      "Epoch 2, Batch 511, Loss: 0.8340796232223511\n",
      "Epoch 2, Batch 512, Loss: 0.8873052000999451\n",
      "Epoch 2, Batch 513, Loss: 0.9942758679389954\n",
      "Epoch 2, Batch 514, Loss: 0.698469340801239\n",
      "Epoch 2, Batch 515, Loss: 0.9134828448295593\n",
      "Epoch 2, Batch 516, Loss: 0.9850221276283264\n",
      "Epoch 2, Batch 517, Loss: 0.8048701286315918\n",
      "Epoch 2, Batch 518, Loss: 0.9241865277290344\n",
      "Epoch 2, Batch 519, Loss: 1.0603435039520264\n",
      "Epoch 2, Batch 520, Loss: 0.7525368928909302\n",
      "Epoch 2, Batch 521, Loss: 0.992016077041626\n",
      "Epoch 2, Batch 522, Loss: 0.9197014570236206\n",
      "Epoch 2, Batch 523, Loss: 0.939568042755127\n",
      "Epoch 2, Batch 524, Loss: 0.9012290835380554\n",
      "Epoch 2, Batch 525, Loss: 1.0562645196914673\n",
      "Epoch 2, Batch 526, Loss: 1.031502604484558\n",
      "Epoch 2, Batch 527, Loss: 0.9458726048469543\n",
      "Epoch 2, Batch 528, Loss: 0.780602216720581\n",
      "Epoch 2, Batch 529, Loss: 0.8344936370849609\n",
      "Epoch 2, Batch 530, Loss: 0.9296118021011353\n",
      "Epoch 2, Batch 531, Loss: 0.921750545501709\n",
      "Epoch 2, Batch 532, Loss: 0.9483686685562134\n",
      "Epoch 2, Batch 533, Loss: 0.857670783996582\n",
      "Epoch 2, Batch 534, Loss: 0.8276932835578918\n",
      "Epoch 2, Batch 535, Loss: 0.8236068487167358\n",
      "Epoch 2, Batch 536, Loss: 0.8667616248130798\n",
      "Epoch 2, Batch 537, Loss: 0.8571035861968994\n",
      "Epoch 2, Batch 538, Loss: 0.8420382142066956\n",
      "Epoch 2, Batch 539, Loss: 0.7796347141265869\n",
      "Epoch 2, Batch 540, Loss: 0.9533069133758545\n",
      "Epoch 2, Batch 541, Loss: 0.7962940335273743\n",
      "Epoch 2, Batch 542, Loss: 0.9608543515205383\n",
      "Epoch 2, Batch 543, Loss: 0.839195728302002\n",
      "Epoch 2, Batch 544, Loss: 0.7009875774383545\n",
      "Epoch 2, Batch 545, Loss: 0.7417960166931152\n",
      "Epoch 2, Batch 546, Loss: 0.8097183108329773\n",
      "Epoch 2, Batch 547, Loss: 0.8657382130622864\n",
      "Epoch 2, Batch 548, Loss: 0.9370414018630981\n",
      "Epoch 2, Batch 549, Loss: 0.7147009968757629\n",
      "Epoch 2, Batch 550, Loss: 0.8657668232917786\n",
      "Epoch 2, Batch 551, Loss: 0.9270564913749695\n",
      "Epoch 2, Batch 552, Loss: 1.0602200031280518\n",
      "Epoch 2, Batch 553, Loss: 0.7450904846191406\n",
      "Epoch 2, Batch 554, Loss: 0.9346566200256348\n",
      "Epoch 2, Batch 555, Loss: 0.8770082592964172\n",
      "Epoch 2, Batch 556, Loss: 0.8275583982467651\n",
      "Epoch 2, Batch 557, Loss: 0.8556131720542908\n",
      "Epoch 2, Batch 558, Loss: 0.8415060639381409\n",
      "Epoch 2, Batch 559, Loss: 0.7590599060058594\n",
      "Epoch 2, Batch 560, Loss: 1.0678534507751465\n",
      "Epoch 2, Batch 561, Loss: 0.9092384576797485\n",
      "Epoch 2, Batch 562, Loss: 0.869044303894043\n",
      "Epoch 2, Batch 563, Loss: 0.9022940397262573\n",
      "Epoch 2, Batch 564, Loss: 0.8429065942764282\n",
      "Epoch 2, Batch 565, Loss: 0.8699027895927429\n",
      "Epoch 2, Batch 566, Loss: 1.1123353242874146\n",
      "Epoch 2, Batch 567, Loss: 1.2532052993774414\n",
      "Epoch 2, Batch 568, Loss: 0.8021416068077087\n",
      "Epoch 2, Batch 569, Loss: 0.8265029788017273\n",
      "Epoch 2, Batch 570, Loss: 0.6693456172943115\n",
      "Epoch 2, Batch 571, Loss: 0.7076706290245056\n",
      "Epoch 2, Batch 572, Loss: 0.9321498870849609\n",
      "Epoch 2, Batch 573, Loss: 0.8102205991744995\n",
      "Epoch 2, Batch 574, Loss: 0.72010338306427\n",
      "Epoch 2, Batch 575, Loss: 0.8415614366531372\n",
      "Epoch 2, Batch 576, Loss: 0.7652614116668701\n",
      "Epoch 2, Batch 577, Loss: 0.7812680006027222\n",
      "Epoch 2, Batch 578, Loss: 0.90997713804245\n",
      "Epoch 2, Batch 579, Loss: 0.8818179368972778\n",
      "Epoch 2, Batch 580, Loss: 0.8859346508979797\n",
      "Epoch 2, Batch 581, Loss: 0.8244131207466125\n",
      "Epoch 2, Batch 582, Loss: 0.9378266334533691\n",
      "Epoch 2, Batch 583, Loss: 0.8683977127075195\n",
      "Epoch 2, Batch 584, Loss: 0.769065260887146\n",
      "Epoch 2, Batch 585, Loss: 0.9994330406188965\n",
      "Epoch 2, Batch 586, Loss: 0.8319694995880127\n",
      "Epoch 2, Batch 587, Loss: 0.6888775825500488\n",
      "Epoch 2, Batch 588, Loss: 1.0508418083190918\n",
      "Epoch 2, Batch 589, Loss: 0.8553835153579712\n",
      "Epoch 2, Batch 590, Loss: 0.8683172464370728\n",
      "Epoch 2, Batch 591, Loss: 0.9166901707649231\n",
      "Epoch 2, Batch 592, Loss: 0.9012507796287537\n",
      "Epoch 2, Batch 593, Loss: 0.8656489849090576\n",
      "Epoch 2, Batch 594, Loss: 0.8533135652542114\n",
      "Epoch 2, Batch 595, Loss: 0.7161474823951721\n",
      "Epoch 2, Batch 596, Loss: 1.0413578748703003\n",
      "Epoch 2, Batch 597, Loss: 0.9611996412277222\n",
      "Epoch 2, Batch 598, Loss: 0.8864412903785706\n",
      "Epoch 2, Batch 599, Loss: 0.8527134656906128\n",
      "Epoch 2, Batch 600, Loss: 0.8756944537162781\n",
      "Epoch 2, Batch 601, Loss: 0.9027559757232666\n",
      "Epoch 2, Batch 602, Loss: 0.8341612815856934\n",
      "Epoch 2, Batch 603, Loss: 0.8868411183357239\n",
      "Epoch 2, Batch 604, Loss: 0.9070978164672852\n",
      "Epoch 2, Batch 605, Loss: 0.8730165958404541\n",
      "Epoch 2, Batch 606, Loss: 1.0447584390640259\n",
      "Epoch 2, Batch 607, Loss: 0.9273117184638977\n",
      "Epoch 2, Batch 608, Loss: 0.8594998121261597\n",
      "Epoch 2, Batch 609, Loss: 0.8857560157775879\n",
      "Epoch 2, Batch 610, Loss: 0.8966789841651917\n",
      "Epoch 2, Batch 611, Loss: 0.8928032517433167\n",
      "Epoch 2, Batch 612, Loss: 0.9692226648330688\n",
      "Epoch 2, Batch 613, Loss: 0.8054863810539246\n",
      "Epoch 2, Batch 614, Loss: 0.9892777800559998\n",
      "Epoch 2, Batch 615, Loss: 0.8148170709609985\n",
      "Epoch 2, Batch 616, Loss: 0.8512879610061646\n",
      "Epoch 2, Batch 617, Loss: 0.723774790763855\n",
      "Epoch 2, Batch 618, Loss: 0.8997160196304321\n",
      "Epoch 2, Batch 619, Loss: 0.8610957860946655\n",
      "Epoch 2, Batch 620, Loss: 0.9146736860275269\n",
      "Epoch 2, Batch 621, Loss: 0.8989963531494141\n",
      "Epoch 2, Batch 622, Loss: 1.0480537414550781\n",
      "Epoch 2, Batch 623, Loss: 0.8230789303779602\n",
      "Epoch 2, Batch 624, Loss: 1.075295329093933\n",
      "Epoch 2, Batch 625, Loss: 0.7932384014129639\n",
      "Epoch 2, Batch 626, Loss: 0.960669994354248\n",
      "Epoch 2, Batch 627, Loss: 0.8694791197776794\n",
      "Epoch 2, Batch 628, Loss: 0.6961575746536255\n",
      "Epoch 2, Batch 629, Loss: 0.8937748074531555\n",
      "Epoch 2, Batch 630, Loss: 0.81238853931427\n",
      "Epoch 2, Batch 631, Loss: 0.9953898191452026\n",
      "Epoch 2, Batch 632, Loss: 0.9275699257850647\n",
      "Epoch 2, Batch 633, Loss: 0.9263623952865601\n",
      "Epoch 2, Batch 634, Loss: 0.7722103595733643\n",
      "Epoch 2, Batch 635, Loss: 0.8084409832954407\n",
      "Epoch 2, Batch 636, Loss: 0.8304727077484131\n",
      "Epoch 2, Batch 637, Loss: 0.9023049473762512\n",
      "Epoch 2, Batch 638, Loss: 1.1039804220199585\n",
      "Epoch 2, Batch 639, Loss: 1.1147987842559814\n",
      "Epoch 2, Batch 640, Loss: 0.9028465747833252\n",
      "Epoch 2, Batch 641, Loss: 0.7713857293128967\n",
      "Epoch 2, Batch 642, Loss: 0.8619600534439087\n",
      "Epoch 2, Batch 643, Loss: 0.915984034538269\n",
      "Epoch 2, Batch 644, Loss: 0.9629160165786743\n",
      "Epoch 2, Batch 645, Loss: 0.8461242318153381\n",
      "Epoch 2, Batch 646, Loss: 0.8413249254226685\n",
      "Epoch 2, Batch 647, Loss: 0.8319541811943054\n",
      "Epoch 2, Batch 648, Loss: 0.9366159439086914\n",
      "Epoch 2, Batch 649, Loss: 0.8969770669937134\n",
      "Epoch 2, Batch 650, Loss: 0.7561393976211548\n",
      "Epoch 2, Batch 651, Loss: 0.9421254992485046\n",
      "Epoch 2, Batch 652, Loss: 0.7773655652999878\n",
      "Epoch 2, Batch 653, Loss: 0.7533970475196838\n",
      "Epoch 2, Batch 654, Loss: 0.8461357951164246\n",
      "Epoch 2, Batch 655, Loss: 0.9016134738922119\n",
      "Epoch 2, Batch 656, Loss: 0.7349980473518372\n",
      "Epoch 2, Batch 657, Loss: 0.7742870450019836\n",
      "Epoch 2, Batch 658, Loss: 0.727892279624939\n",
      "Epoch 2, Batch 659, Loss: 0.9767842888832092\n",
      "Epoch 2, Batch 660, Loss: 0.968678891658783\n",
      "Epoch 2, Batch 661, Loss: 0.9486194849014282\n",
      "Epoch 2, Batch 662, Loss: 0.9767711758613586\n",
      "Epoch 2, Batch 663, Loss: 0.8543544411659241\n",
      "Epoch 2, Batch 664, Loss: 0.8861386775970459\n",
      "Epoch 2, Batch 665, Loss: 0.8922615051269531\n",
      "Epoch 2, Batch 666, Loss: 0.6827420592308044\n",
      "Epoch 2, Batch 667, Loss: 0.6639756560325623\n",
      "Epoch 2, Batch 668, Loss: 0.9722957015037537\n",
      "Epoch 2, Batch 669, Loss: 0.8951107263565063\n",
      "Epoch 2, Batch 670, Loss: 0.8004912734031677\n",
      "Epoch 2, Batch 671, Loss: 0.6528390049934387\n",
      "Epoch 2, Batch 672, Loss: 0.6950281858444214\n",
      "Epoch 2, Batch 673, Loss: 0.8943016529083252\n",
      "Epoch 2, Batch 674, Loss: 0.9003696441650391\n",
      "Epoch 2, Batch 675, Loss: 0.996381402015686\n",
      "Epoch 2, Batch 676, Loss: 0.8629233837127686\n",
      "Epoch 2, Batch 677, Loss: 0.9344961047172546\n",
      "Epoch 2, Batch 678, Loss: 0.9450729489326477\n",
      "Epoch 2, Batch 679, Loss: 0.9632390141487122\n",
      "Epoch 2, Batch 680, Loss: 0.8731479048728943\n",
      "Epoch 2, Batch 681, Loss: 0.8322812914848328\n",
      "Epoch 2, Batch 682, Loss: 0.6195321083068848\n",
      "Epoch 2, Batch 683, Loss: 0.8190145492553711\n",
      "Epoch 2, Batch 684, Loss: 0.6116148233413696\n",
      "Epoch 2, Batch 685, Loss: 0.9190981388092041\n",
      "Epoch 2, Batch 686, Loss: 0.8910946846008301\n",
      "Epoch 2, Batch 687, Loss: 0.8467221856117249\n",
      "Epoch 2, Batch 688, Loss: 0.9149027466773987\n",
      "Epoch 2, Batch 689, Loss: 1.026649832725525\n",
      "Epoch 2, Batch 690, Loss: 0.631615400314331\n",
      "Epoch 2, Batch 691, Loss: 0.8875712752342224\n",
      "Epoch 2, Batch 692, Loss: 0.9046902656555176\n",
      "Epoch 2, Batch 693, Loss: 0.7440738677978516\n",
      "Epoch 2, Batch 694, Loss: 0.7885609865188599\n",
      "Epoch 2, Batch 695, Loss: 0.9190046191215515\n",
      "Epoch 2, Batch 696, Loss: 1.0771905183792114\n",
      "Epoch 2, Batch 697, Loss: 0.8304253220558167\n",
      "Epoch 2, Batch 698, Loss: 0.9914913177490234\n",
      "Epoch 2, Batch 699, Loss: 0.7695844173431396\n",
      "Epoch 2, Batch 700, Loss: 0.8013150691986084\n",
      "Epoch 2, Batch 701, Loss: 0.9726815819740295\n",
      "Epoch 2, Batch 702, Loss: 0.8473275303840637\n",
      "Epoch 2, Batch 703, Loss: 0.8368546962738037\n",
      "Epoch 2, Batch 704, Loss: 0.9323334097862244\n",
      "Epoch 2, Batch 705, Loss: 1.1977483034133911\n",
      "Epoch 2, Batch 706, Loss: 0.8101766705513\n",
      "Epoch 2, Batch 707, Loss: 0.7849300503730774\n",
      "Epoch 2, Batch 708, Loss: 0.705414891242981\n",
      "Epoch 2, Batch 709, Loss: 1.152574062347412\n",
      "Epoch 2, Batch 710, Loss: 0.8475192785263062\n",
      "Epoch 2, Batch 711, Loss: 0.9204540252685547\n",
      "Epoch 2, Batch 712, Loss: 0.7540701627731323\n",
      "Epoch 2, Batch 713, Loss: 0.7936186194419861\n",
      "Epoch 2, Batch 714, Loss: 0.7728208303451538\n",
      "Epoch 2, Batch 715, Loss: 0.7701839804649353\n",
      "Epoch 2, Batch 716, Loss: 0.6831074357032776\n",
      "Epoch 2, Batch 717, Loss: 0.8411402106285095\n",
      "Epoch 2, Batch 718, Loss: 0.925373375415802\n",
      "Epoch 2, Batch 719, Loss: 0.9203720688819885\n",
      "Epoch 2, Batch 720, Loss: 1.3976500034332275\n",
      "Epoch 2, Batch 721, Loss: 0.8647081851959229\n",
      "Epoch 2, Batch 722, Loss: 0.9506679773330688\n",
      "Epoch 2, Batch 723, Loss: 0.8170569539070129\n",
      "Epoch 2, Batch 724, Loss: 0.778501033782959\n",
      "Epoch 2, Batch 725, Loss: 0.930570125579834\n",
      "Epoch 2, Batch 726, Loss: 0.8029272556304932\n",
      "Epoch 2, Batch 727, Loss: 0.8088827133178711\n",
      "Epoch 2, Batch 728, Loss: 0.8128272294998169\n",
      "Epoch 2, Batch 729, Loss: 0.758568286895752\n",
      "Epoch 2, Batch 730, Loss: 0.8996258974075317\n",
      "Epoch 2, Batch 731, Loss: 0.9431126713752747\n",
      "Epoch 2, Batch 732, Loss: 0.8919295072555542\n",
      "Epoch 2, Batch 733, Loss: 0.7757127285003662\n",
      "Epoch 2, Batch 734, Loss: 0.8551126718521118\n",
      "Epoch 2, Batch 735, Loss: 0.7333214282989502\n",
      "Epoch 2, Batch 736, Loss: 0.8713213205337524\n",
      "Epoch 2, Batch 737, Loss: 0.8432253003120422\n",
      "Epoch 2, Batch 738, Loss: 0.683589518070221\n",
      "Epoch 2, Batch 739, Loss: 0.9816616773605347\n",
      "Epoch 2, Batch 740, Loss: 0.9672590494155884\n",
      "Epoch 2, Batch 741, Loss: 0.8032914400100708\n",
      "Epoch 2, Batch 742, Loss: 0.7751892805099487\n",
      "Epoch 2, Batch 743, Loss: 0.961540162563324\n",
      "Epoch 2, Batch 744, Loss: 0.8978400826454163\n",
      "Epoch 2, Batch 745, Loss: 0.7366320490837097\n",
      "Epoch 2, Batch 746, Loss: 0.8191015124320984\n",
      "Epoch 2, Batch 747, Loss: 0.8863462209701538\n",
      "Epoch 2, Batch 748, Loss: 0.6678677201271057\n",
      "Epoch 2, Batch 749, Loss: 0.7086068391799927\n",
      "Epoch 2, Batch 750, Loss: 0.8069806694984436\n",
      "Epoch 2, Batch 751, Loss: 0.7533395886421204\n",
      "Epoch 2, Batch 752, Loss: 0.8337925672531128\n",
      "Epoch 2, Batch 753, Loss: 0.7412071228027344\n",
      "Epoch 2, Batch 754, Loss: 0.9508103132247925\n",
      "Epoch 2, Batch 755, Loss: 0.8037921190261841\n",
      "Epoch 2, Batch 756, Loss: 0.7148560285568237\n",
      "Epoch 2, Batch 757, Loss: 0.7960994243621826\n",
      "Epoch 2, Batch 758, Loss: 0.7372589707374573\n",
      "Epoch 2, Batch 759, Loss: 0.9580843448638916\n",
      "Epoch 2, Batch 760, Loss: 0.6990017890930176\n",
      "Epoch 2, Batch 761, Loss: 0.9733282923698425\n",
      "Epoch 2, Batch 762, Loss: 0.7063631415367126\n",
      "Epoch 2, Batch 763, Loss: 0.6575396656990051\n",
      "Epoch 2, Batch 764, Loss: 0.8169717192649841\n",
      "Epoch 2, Batch 765, Loss: 0.8537889122962952\n",
      "Epoch 2, Batch 766, Loss: 0.6800343990325928\n",
      "Epoch 2, Batch 767, Loss: 0.6641035079956055\n",
      "Epoch 2, Batch 768, Loss: 0.8654727935791016\n",
      "Epoch 2, Batch 769, Loss: 0.913176953792572\n",
      "Epoch 2, Batch 770, Loss: 0.9104825258255005\n",
      "Epoch 2, Batch 771, Loss: 0.9345235228538513\n",
      "Epoch 2, Batch 772, Loss: 0.8799731731414795\n",
      "Epoch 2, Batch 773, Loss: 1.1120109558105469\n",
      "Epoch 2, Batch 774, Loss: 0.6042758822441101\n",
      "Epoch 2, Batch 775, Loss: 0.704010546207428\n",
      "Epoch 2, Batch 776, Loss: 0.6997913718223572\n",
      "Epoch 2, Batch 777, Loss: 0.8665960431098938\n",
      "Epoch 2, Batch 778, Loss: 1.0854823589324951\n",
      "Epoch 2, Batch 779, Loss: 0.8940881490707397\n",
      "Epoch 2, Batch 780, Loss: 0.7962997555732727\n",
      "Epoch 2, Batch 781, Loss: 0.7223930358886719\n",
      "Epoch 2, Batch 782, Loss: 0.8968318700790405\n",
      "Epoch 2, Batch 783, Loss: 0.9211786389350891\n",
      "Epoch 2, Batch 784, Loss: 0.7478079795837402\n",
      "Epoch 2, Batch 785, Loss: 1.0906624794006348\n",
      "Epoch 2, Batch 786, Loss: 0.80670166015625\n",
      "Epoch 2, Batch 787, Loss: 1.0937026739120483\n",
      "Epoch 2, Batch 788, Loss: 0.8410088419914246\n",
      "Epoch 2, Batch 789, Loss: 0.6896248459815979\n",
      "Epoch 2, Batch 790, Loss: 0.8334189653396606\n",
      "Epoch 2, Batch 791, Loss: 0.7920527458190918\n",
      "Epoch 2, Batch 792, Loss: 0.7362807989120483\n",
      "Epoch 2, Batch 793, Loss: 0.8388692736625671\n",
      "Epoch 2, Batch 794, Loss: 0.8438873887062073\n",
      "Epoch 2, Batch 795, Loss: 0.9100037813186646\n",
      "Epoch 2, Batch 796, Loss: 0.8567012548446655\n",
      "Epoch 2, Batch 797, Loss: 0.797498881816864\n",
      "Epoch 2, Batch 798, Loss: 0.7414303421974182\n",
      "Epoch 2, Batch 799, Loss: 0.891380786895752\n",
      "Epoch 2, Batch 800, Loss: 0.7725551128387451\n",
      "Epoch 2, Batch 801, Loss: 0.794647216796875\n",
      "Epoch 2, Batch 802, Loss: 0.624625027179718\n",
      "Epoch 2, Batch 803, Loss: 0.7602940201759338\n",
      "Epoch 2, Batch 804, Loss: 0.7471535801887512\n",
      "Epoch 2, Batch 805, Loss: 0.6639969348907471\n",
      "Epoch 2, Batch 806, Loss: 0.8046215772628784\n",
      "Epoch 2, Batch 807, Loss: 0.9179236888885498\n",
      "Epoch 2, Batch 808, Loss: 0.8190228343009949\n",
      "Epoch 2, Batch 809, Loss: 1.0427641868591309\n",
      "Epoch 2, Batch 810, Loss: 0.7800945043563843\n",
      "Epoch 2, Batch 811, Loss: 1.243109107017517\n",
      "Epoch 2, Batch 812, Loss: 0.8142206072807312\n",
      "Epoch 2, Batch 813, Loss: 0.9468531012535095\n",
      "Epoch 2, Batch 814, Loss: 0.792003333568573\n",
      "Epoch 2, Batch 815, Loss: 0.8449080586433411\n",
      "Epoch 2, Batch 816, Loss: 0.8198635578155518\n",
      "Epoch 2, Batch 817, Loss: 1.0218479633331299\n",
      "Epoch 2, Batch 818, Loss: 0.8371070623397827\n",
      "Epoch 2, Batch 819, Loss: 0.8656373620033264\n",
      "Epoch 2, Batch 820, Loss: 0.9223194718360901\n",
      "Epoch 2, Batch 821, Loss: 0.6559528708457947\n",
      "Epoch 2, Batch 822, Loss: 0.8053532838821411\n",
      "Epoch 2, Batch 823, Loss: 0.6375460028648376\n",
      "Epoch 2, Batch 824, Loss: 0.9137626886367798\n",
      "Epoch 2, Batch 825, Loss: 0.773514986038208\n",
      "Epoch 2, Batch 826, Loss: 0.9140514135360718\n",
      "Epoch 2, Batch 827, Loss: 0.7649474143981934\n",
      "Epoch 2, Batch 828, Loss: 1.1173675060272217\n",
      "Epoch 2, Batch 829, Loss: 0.849907398223877\n",
      "Epoch 2, Batch 830, Loss: 0.8383859992027283\n",
      "Epoch 2, Batch 831, Loss: 0.8822808265686035\n",
      "Epoch 2, Batch 832, Loss: 0.8900197744369507\n",
      "Epoch 2, Batch 833, Loss: 0.814202606678009\n",
      "Epoch 2, Batch 834, Loss: 0.6910005807876587\n",
      "Epoch 2, Batch 835, Loss: 0.7918702363967896\n",
      "Epoch 2, Batch 836, Loss: 0.7930045127868652\n",
      "Epoch 2, Batch 837, Loss: 0.9543754458427429\n",
      "Epoch 2, Batch 838, Loss: 0.8563038110733032\n",
      "Epoch 2, Batch 839, Loss: 0.8057646155357361\n",
      "Epoch 2, Batch 840, Loss: 0.7034545540809631\n",
      "Epoch 2, Batch 841, Loss: 0.7931272387504578\n",
      "Epoch 2, Batch 842, Loss: 0.752109706401825\n",
      "Epoch 2, Batch 843, Loss: 0.8671138286590576\n",
      "Epoch 2, Batch 844, Loss: 0.8559603691101074\n",
      "Epoch 2, Batch 845, Loss: 0.8963491320610046\n",
      "Epoch 2, Batch 846, Loss: 0.8950344920158386\n",
      "Epoch 2, Batch 847, Loss: 0.8607365489006042\n",
      "Epoch 2, Batch 848, Loss: 0.8844680190086365\n",
      "Epoch 2, Batch 849, Loss: 0.9139449596405029\n",
      "Epoch 2, Batch 850, Loss: 0.8361537456512451\n",
      "Epoch 2, Batch 851, Loss: 0.9000423550605774\n",
      "Epoch 2, Batch 852, Loss: 0.8233815431594849\n",
      "Epoch 2, Batch 853, Loss: 0.7795959711074829\n",
      "Epoch 2, Batch 854, Loss: 0.7997232675552368\n",
      "Epoch 2, Batch 855, Loss: 0.7451339960098267\n",
      "Epoch 2, Batch 856, Loss: 0.8196285963058472\n",
      "Epoch 2, Batch 857, Loss: 0.8826873302459717\n",
      "Epoch 2, Batch 858, Loss: 0.7222714424133301\n",
      "Epoch 2, Batch 859, Loss: 0.665241539478302\n",
      "Epoch 2, Batch 860, Loss: 0.854889452457428\n",
      "Epoch 2, Batch 861, Loss: 0.9323546886444092\n",
      "Epoch 2, Batch 862, Loss: 0.9332443475723267\n",
      "Epoch 2, Batch 863, Loss: 0.9386938214302063\n",
      "Epoch 2, Batch 864, Loss: 1.1292927265167236\n",
      "Epoch 2, Batch 865, Loss: 0.8173995614051819\n",
      "Epoch 2, Batch 866, Loss: 0.8701278567314148\n",
      "Epoch 2, Batch 867, Loss: 0.828296959400177\n",
      "Epoch 2, Batch 868, Loss: 0.7591519951820374\n",
      "Epoch 2, Batch 869, Loss: 0.7073619365692139\n",
      "Epoch 2, Batch 870, Loss: 0.8302134275436401\n",
      "Epoch 2, Batch 871, Loss: 0.8047676086425781\n",
      "Epoch 2, Batch 872, Loss: 0.832136332988739\n",
      "Epoch 2, Batch 873, Loss: 0.9210675954818726\n",
      "Epoch 2, Batch 874, Loss: 0.9719015955924988\n",
      "Epoch 2, Batch 875, Loss: 0.8916712999343872\n",
      "Epoch 2, Batch 876, Loss: 0.704378604888916\n",
      "Epoch 2, Batch 877, Loss: 0.851826012134552\n",
      "Epoch 2, Batch 878, Loss: 0.7845640182495117\n",
      "Epoch 2, Batch 879, Loss: 0.976989209651947\n",
      "Epoch 2, Batch 880, Loss: 0.8235452175140381\n",
      "Epoch 2, Batch 881, Loss: 0.8530362844467163\n",
      "Epoch 2, Batch 882, Loss: 0.6838048100471497\n",
      "Epoch 2, Batch 883, Loss: 0.7993844151496887\n",
      "Epoch 2, Batch 884, Loss: 0.9211851954460144\n",
      "Epoch 2, Batch 885, Loss: 0.8987295031547546\n",
      "Epoch 2, Batch 886, Loss: 0.7328457236289978\n",
      "Epoch 2, Batch 887, Loss: 0.9230181574821472\n",
      "Epoch 2, Batch 888, Loss: 0.7889440059661865\n",
      "Epoch 2, Batch 889, Loss: 0.9246482849121094\n",
      "Epoch 2, Batch 890, Loss: 0.7414354681968689\n",
      "Epoch 2, Batch 891, Loss: 0.8817472457885742\n",
      "Epoch 2, Batch 892, Loss: 0.9226396083831787\n",
      "Epoch 2, Batch 893, Loss: 0.7142936587333679\n",
      "Epoch 2, Batch 894, Loss: 0.8199984431266785\n",
      "Epoch 2, Batch 895, Loss: 0.9831488132476807\n",
      "Epoch 2, Batch 896, Loss: 0.6924107074737549\n",
      "Epoch 2, Batch 897, Loss: 0.9852148294448853\n",
      "Epoch 2, Batch 898, Loss: 0.9216790795326233\n",
      "Epoch 2, Batch 899, Loss: 0.7803077697753906\n",
      "Epoch 2, Batch 900, Loss: 0.7707680463790894\n",
      "Epoch 2, Batch 901, Loss: 0.7395015358924866\n",
      "Epoch 2, Batch 902, Loss: 0.7221419215202332\n",
      "Epoch 2, Batch 903, Loss: 0.9860409498214722\n",
      "Epoch 2, Batch 904, Loss: 0.9055232405662537\n",
      "Epoch 2, Batch 905, Loss: 0.7990565896034241\n",
      "Epoch 2, Batch 906, Loss: 0.838458776473999\n",
      "Epoch 2, Batch 907, Loss: 0.8064146041870117\n",
      "Epoch 2, Batch 908, Loss: 0.8091303706169128\n",
      "Epoch 2, Batch 909, Loss: 0.8808859586715698\n",
      "Epoch 2, Batch 910, Loss: 0.7809160947799683\n",
      "Epoch 2, Batch 911, Loss: 0.9194313287734985\n",
      "Epoch 2, Batch 912, Loss: 1.1123987436294556\n",
      "Epoch 2, Batch 913, Loss: 0.8300183415412903\n",
      "Epoch 2, Batch 914, Loss: 0.7166123390197754\n",
      "Epoch 2, Batch 915, Loss: 0.7268680930137634\n",
      "Epoch 2, Batch 916, Loss: 0.776299238204956\n",
      "Epoch 2, Batch 917, Loss: 0.7876163721084595\n",
      "Epoch 2, Batch 918, Loss: 0.9959672093391418\n",
      "Epoch 2, Batch 919, Loss: 0.8785898685455322\n",
      "Epoch 2, Batch 920, Loss: 1.0321029424667358\n",
      "Epoch 2, Batch 921, Loss: 0.8238503932952881\n",
      "Epoch 2, Batch 922, Loss: 0.7198431491851807\n",
      "Epoch 2, Batch 923, Loss: 0.7631088495254517\n",
      "Epoch 2, Batch 924, Loss: 0.8706934452056885\n",
      "Epoch 2, Batch 925, Loss: 0.844506561756134\n",
      "Epoch 2, Batch 926, Loss: 1.0871577262878418\n",
      "Epoch 2, Batch 927, Loss: 0.8137159943580627\n",
      "Epoch 2, Batch 928, Loss: 0.750070333480835\n",
      "Epoch 2, Batch 929, Loss: 0.7158653140068054\n",
      "Epoch 2, Batch 930, Loss: 0.6442669630050659\n",
      "Epoch 2, Batch 931, Loss: 0.7521328926086426\n",
      "Epoch 2, Batch 932, Loss: 0.7451992034912109\n",
      "Epoch 2, Batch 933, Loss: 1.1254311800003052\n",
      "Epoch 2, Batch 934, Loss: 0.8190557360649109\n",
      "Epoch 2, Batch 935, Loss: 0.8347861170768738\n",
      "Epoch 2, Batch 936, Loss: 0.689446210861206\n",
      "Epoch 2, Batch 937, Loss: 0.7818886637687683\n",
      "Epoch 2, Batch 938, Loss: 0.8085923194885254\n",
      "Accuracy of train set: 0.64745\n",
      "Epoch 2, Batch 1, Test Loss: 0.8109389543533325\n",
      "Epoch 2, Batch 2, Test Loss: 0.7908704280853271\n",
      "Epoch 2, Batch 3, Test Loss: 0.7193204760551453\n",
      "Epoch 2, Batch 4, Test Loss: 0.7056012749671936\n",
      "Epoch 2, Batch 5, Test Loss: 0.7943531274795532\n",
      "Epoch 2, Batch 6, Test Loss: 0.7600746154785156\n",
      "Epoch 2, Batch 7, Test Loss: 0.76100093126297\n",
      "Epoch 2, Batch 8, Test Loss: 0.8110027313232422\n",
      "Epoch 2, Batch 9, Test Loss: 0.9660120010375977\n",
      "Epoch 2, Batch 10, Test Loss: 1.203213095664978\n",
      "Epoch 2, Batch 11, Test Loss: 0.7501142024993896\n",
      "Epoch 2, Batch 12, Test Loss: 0.6935800313949585\n",
      "Epoch 2, Batch 13, Test Loss: 1.0530860424041748\n",
      "Epoch 2, Batch 14, Test Loss: 0.9424283504486084\n",
      "Epoch 2, Batch 15, Test Loss: 0.9134621620178223\n",
      "Epoch 2, Batch 16, Test Loss: 0.8561570048332214\n",
      "Epoch 2, Batch 17, Test Loss: 0.8975087404251099\n",
      "Epoch 2, Batch 18, Test Loss: 0.8064092993736267\n",
      "Epoch 2, Batch 19, Test Loss: 0.8467833399772644\n",
      "Epoch 2, Batch 20, Test Loss: 0.7782564163208008\n",
      "Epoch 2, Batch 21, Test Loss: 1.0186333656311035\n",
      "Epoch 2, Batch 22, Test Loss: 0.7763150930404663\n",
      "Epoch 2, Batch 23, Test Loss: 1.0179966688156128\n",
      "Epoch 2, Batch 24, Test Loss: 1.0060620307922363\n",
      "Epoch 2, Batch 25, Test Loss: 0.8187878727912903\n",
      "Epoch 2, Batch 26, Test Loss: 0.9143365025520325\n",
      "Epoch 2, Batch 27, Test Loss: 1.0356403589248657\n",
      "Epoch 2, Batch 28, Test Loss: 0.9827619791030884\n",
      "Epoch 2, Batch 29, Test Loss: 0.909428060054779\n",
      "Epoch 2, Batch 30, Test Loss: 0.7732996940612793\n",
      "Epoch 2, Batch 31, Test Loss: 0.6773412823677063\n",
      "Epoch 2, Batch 32, Test Loss: 0.6820049285888672\n",
      "Epoch 2, Batch 33, Test Loss: 0.6465155482292175\n",
      "Epoch 2, Batch 34, Test Loss: 0.8857906460762024\n",
      "Epoch 2, Batch 35, Test Loss: 0.8484258055686951\n",
      "Epoch 2, Batch 36, Test Loss: 0.7470099925994873\n",
      "Epoch 2, Batch 37, Test Loss: 0.7008177638053894\n",
      "Epoch 2, Batch 38, Test Loss: 0.8797217011451721\n",
      "Epoch 2, Batch 39, Test Loss: 0.8944578766822815\n",
      "Epoch 2, Batch 40, Test Loss: 0.9844032526016235\n",
      "Epoch 2, Batch 41, Test Loss: 0.8049730658531189\n",
      "Epoch 2, Batch 42, Test Loss: 0.7103051543235779\n",
      "Epoch 2, Batch 43, Test Loss: 1.0122448205947876\n",
      "Epoch 2, Batch 44, Test Loss: 0.7955335378646851\n",
      "Epoch 2, Batch 45, Test Loss: 0.8065657615661621\n",
      "Epoch 2, Batch 46, Test Loss: 0.8000522255897522\n",
      "Epoch 2, Batch 47, Test Loss: 0.9567011594772339\n",
      "Epoch 2, Batch 48, Test Loss: 0.8011209964752197\n",
      "Epoch 2, Batch 49, Test Loss: 0.7116366028785706\n",
      "Epoch 2, Batch 50, Test Loss: 0.7847657203674316\n",
      "Epoch 2, Batch 51, Test Loss: 0.8277702927589417\n",
      "Epoch 2, Batch 52, Test Loss: 0.6625325679779053\n",
      "Epoch 2, Batch 53, Test Loss: 0.770174503326416\n",
      "Epoch 2, Batch 54, Test Loss: 0.9914529323577881\n",
      "Epoch 2, Batch 55, Test Loss: 0.8513913154602051\n",
      "Epoch 2, Batch 56, Test Loss: 0.6544788479804993\n",
      "Epoch 2, Batch 57, Test Loss: 0.8976389169692993\n",
      "Epoch 2, Batch 58, Test Loss: 0.8474491834640503\n",
      "Epoch 2, Batch 59, Test Loss: 0.8932863473892212\n",
      "Epoch 2, Batch 60, Test Loss: 0.9192231297492981\n",
      "Epoch 2, Batch 61, Test Loss: 0.8565003871917725\n",
      "Epoch 2, Batch 62, Test Loss: 0.8755513429641724\n",
      "Epoch 2, Batch 63, Test Loss: 0.8469035625457764\n",
      "Epoch 2, Batch 64, Test Loss: 0.9116735458374023\n",
      "Epoch 2, Batch 65, Test Loss: 0.8902642130851746\n",
      "Epoch 2, Batch 66, Test Loss: 0.8583665490150452\n",
      "Epoch 2, Batch 67, Test Loss: 0.9460541605949402\n",
      "Epoch 2, Batch 68, Test Loss: 0.7796483635902405\n",
      "Epoch 2, Batch 69, Test Loss: 0.9943458437919617\n",
      "Epoch 2, Batch 70, Test Loss: 0.7403008341789246\n",
      "Epoch 2, Batch 71, Test Loss: 0.8407782316207886\n",
      "Epoch 2, Batch 72, Test Loss: 0.6721615791320801\n",
      "Epoch 2, Batch 73, Test Loss: 0.9531357288360596\n",
      "Epoch 2, Batch 74, Test Loss: 0.8210744261741638\n",
      "Epoch 2, Batch 75, Test Loss: 0.8790041208267212\n",
      "Epoch 2, Batch 76, Test Loss: 0.7357017397880554\n",
      "Epoch 2, Batch 77, Test Loss: 0.7952630519866943\n",
      "Epoch 2, Batch 78, Test Loss: 1.1031675338745117\n",
      "Epoch 2, Batch 79, Test Loss: 0.7512360215187073\n",
      "Epoch 2, Batch 80, Test Loss: 0.7175334095954895\n",
      "Epoch 2, Batch 81, Test Loss: 0.7673957347869873\n",
      "Epoch 2, Batch 82, Test Loss: 0.7038288116455078\n",
      "Epoch 2, Batch 83, Test Loss: 0.8540726900100708\n",
      "Epoch 2, Batch 84, Test Loss: 0.8125783205032349\n",
      "Epoch 2, Batch 85, Test Loss: 0.8648113012313843\n",
      "Epoch 2, Batch 86, Test Loss: 0.9291741251945496\n",
      "Epoch 2, Batch 87, Test Loss: 0.8243118524551392\n",
      "Epoch 2, Batch 88, Test Loss: 0.7368396520614624\n",
      "Epoch 2, Batch 89, Test Loss: 0.8116787672042847\n",
      "Epoch 2, Batch 90, Test Loss: 0.6928737163543701\n",
      "Epoch 2, Batch 91, Test Loss: 0.7479538917541504\n",
      "Epoch 2, Batch 92, Test Loss: 0.8569760918617249\n",
      "Epoch 2, Batch 93, Test Loss: 0.9198259115219116\n",
      "Epoch 2, Batch 94, Test Loss: 0.8958807587623596\n",
      "Epoch 2, Batch 95, Test Loss: 0.9942169785499573\n",
      "Epoch 2, Batch 96, Test Loss: 0.8547174334526062\n",
      "Epoch 2, Batch 97, Test Loss: 0.9203437566757202\n",
      "Epoch 2, Batch 98, Test Loss: 0.7660248875617981\n",
      "Epoch 2, Batch 99, Test Loss: 0.8757230043411255\n",
      "Epoch 2, Batch 100, Test Loss: 0.662580668926239\n",
      "Epoch 2, Batch 101, Test Loss: 0.8628402948379517\n",
      "Epoch 2, Batch 102, Test Loss: 0.8572061657905579\n",
      "Epoch 2, Batch 103, Test Loss: 0.8341976404190063\n",
      "Epoch 2, Batch 104, Test Loss: 1.1057742834091187\n",
      "Epoch 2, Batch 105, Test Loss: 0.901482343673706\n",
      "Epoch 2, Batch 106, Test Loss: 0.8110777735710144\n",
      "Epoch 2, Batch 107, Test Loss: 0.8610228896141052\n",
      "Epoch 2, Batch 108, Test Loss: 0.9574134349822998\n",
      "Epoch 2, Batch 109, Test Loss: 1.0096406936645508\n",
      "Epoch 2, Batch 110, Test Loss: 0.8663938641548157\n",
      "Epoch 2, Batch 111, Test Loss: 0.7664499282836914\n",
      "Epoch 2, Batch 112, Test Loss: 0.7617896199226379\n",
      "Epoch 2, Batch 113, Test Loss: 0.8377780318260193\n",
      "Epoch 2, Batch 114, Test Loss: 0.8687868118286133\n",
      "Epoch 2, Batch 115, Test Loss: 0.7721548080444336\n",
      "Epoch 2, Batch 116, Test Loss: 0.6914469003677368\n",
      "Epoch 2, Batch 117, Test Loss: 0.7417027950286865\n",
      "Epoch 2, Batch 118, Test Loss: 1.0458362102508545\n",
      "Epoch 2, Batch 119, Test Loss: 0.7633873820304871\n",
      "Epoch 2, Batch 120, Test Loss: 0.7801309823989868\n",
      "Epoch 2, Batch 121, Test Loss: 1.020918607711792\n",
      "Epoch 2, Batch 122, Test Loss: 0.9031774997711182\n",
      "Epoch 2, Batch 123, Test Loss: 0.7770193219184875\n",
      "Epoch 2, Batch 124, Test Loss: 0.8242635726928711\n",
      "Epoch 2, Batch 125, Test Loss: 0.8327370882034302\n",
      "Epoch 2, Batch 126, Test Loss: 1.000942587852478\n",
      "Epoch 2, Batch 127, Test Loss: 0.8056662082672119\n",
      "Epoch 2, Batch 128, Test Loss: 0.7681015729904175\n",
      "Epoch 2, Batch 129, Test Loss: 0.9162997007369995\n",
      "Epoch 2, Batch 130, Test Loss: 0.8141341805458069\n",
      "Epoch 2, Batch 131, Test Loss: 0.6554527878761292\n",
      "Epoch 2, Batch 132, Test Loss: 0.8079410791397095\n",
      "Epoch 2, Batch 133, Test Loss: 0.7274197340011597\n",
      "Epoch 2, Batch 134, Test Loss: 0.9740031957626343\n",
      "Epoch 2, Batch 135, Test Loss: 0.8488155007362366\n",
      "Epoch 2, Batch 136, Test Loss: 0.9697163105010986\n",
      "Epoch 2, Batch 137, Test Loss: 0.8975829482078552\n",
      "Epoch 2, Batch 138, Test Loss: 0.7029730081558228\n",
      "Epoch 2, Batch 139, Test Loss: 0.7772707939147949\n",
      "Epoch 2, Batch 140, Test Loss: 0.7732350826263428\n",
      "Epoch 2, Batch 141, Test Loss: 0.8028861284255981\n",
      "Epoch 2, Batch 142, Test Loss: 0.8835735321044922\n",
      "Epoch 2, Batch 143, Test Loss: 0.7134001851081848\n",
      "Epoch 2, Batch 144, Test Loss: 0.6714024543762207\n",
      "Epoch 2, Batch 145, Test Loss: 0.8769462704658508\n",
      "Epoch 2, Batch 146, Test Loss: 0.9473950862884521\n",
      "Epoch 2, Batch 147, Test Loss: 0.8251912593841553\n",
      "Epoch 2, Batch 148, Test Loss: 0.7756146788597107\n",
      "Epoch 2, Batch 149, Test Loss: 0.9255497455596924\n",
      "Epoch 2, Batch 150, Test Loss: 0.7140309810638428\n",
      "Epoch 2, Batch 151, Test Loss: 0.7866513729095459\n",
      "Epoch 2, Batch 152, Test Loss: 0.8209316730499268\n",
      "Epoch 2, Batch 153, Test Loss: 0.7659250497817993\n",
      "Epoch 2, Batch 154, Test Loss: 0.8487510085105896\n",
      "Epoch 2, Batch 155, Test Loss: 0.6910967826843262\n",
      "Epoch 2, Batch 156, Test Loss: 0.8357235193252563\n",
      "Epoch 2, Batch 157, Test Loss: 0.8734360337257385\n",
      "Epoch 2, Batch 158, Test Loss: 0.9734252691268921\n",
      "Epoch 2, Batch 159, Test Loss: 0.9454795122146606\n",
      "Epoch 2, Batch 160, Test Loss: 0.8064310550689697\n",
      "Epoch 2, Batch 161, Test Loss: 0.774821937084198\n",
      "Epoch 2, Batch 162, Test Loss: 0.9882307648658752\n",
      "Epoch 2, Batch 163, Test Loss: 0.7383042573928833\n",
      "Epoch 2, Batch 164, Test Loss: 0.8229458332061768\n",
      "Epoch 2, Batch 165, Test Loss: 0.7522830367088318\n",
      "Epoch 2, Batch 166, Test Loss: 0.8956785202026367\n",
      "Epoch 2, Batch 167, Test Loss: 0.9390783905982971\n",
      "Epoch 2, Batch 168, Test Loss: 0.7977691888809204\n",
      "Epoch 2, Batch 169, Test Loss: 0.6896172165870667\n",
      "Epoch 2, Batch 170, Test Loss: 0.8582521677017212\n",
      "Epoch 2, Batch 171, Test Loss: 0.6954211592674255\n",
      "Epoch 2, Batch 172, Test Loss: 0.7619677186012268\n",
      "Epoch 2, Batch 173, Test Loss: 1.1659824848175049\n",
      "Epoch 2, Batch 174, Test Loss: 0.9540340304374695\n",
      "Epoch 2, Batch 175, Test Loss: 0.83354651927948\n",
      "Epoch 2, Batch 176, Test Loss: 1.123016595840454\n",
      "Epoch 2, Batch 177, Test Loss: 0.8422229290008545\n",
      "Epoch 2, Batch 178, Test Loss: 0.7994046807289124\n",
      "Epoch 2, Batch 179, Test Loss: 0.7352180480957031\n",
      "Epoch 2, Batch 180, Test Loss: 0.6089032888412476\n",
      "Epoch 2, Batch 181, Test Loss: 0.8063375949859619\n",
      "Epoch 2, Batch 182, Test Loss: 0.8311653733253479\n",
      "Epoch 2, Batch 183, Test Loss: 0.8434480428695679\n",
      "Epoch 2, Batch 184, Test Loss: 0.8728457689285278\n",
      "Epoch 2, Batch 185, Test Loss: 1.0261502265930176\n",
      "Epoch 2, Batch 186, Test Loss: 0.7756543159484863\n",
      "Epoch 2, Batch 187, Test Loss: 0.8525420427322388\n",
      "Epoch 2, Batch 188, Test Loss: 0.8730837106704712\n",
      "Epoch 2, Batch 189, Test Loss: 0.8079354763031006\n",
      "Epoch 2, Batch 190, Test Loss: 0.8288298845291138\n",
      "Epoch 2, Batch 191, Test Loss: 0.748652994632721\n",
      "Epoch 2, Batch 192, Test Loss: 0.844581663608551\n",
      "Epoch 2, Batch 193, Test Loss: 0.806475043296814\n",
      "Epoch 2, Batch 194, Test Loss: 0.7807978391647339\n",
      "Epoch 2, Batch 195, Test Loss: 0.9933329224586487\n",
      "Epoch 2, Batch 196, Test Loss: 0.8195229768753052\n",
      "Epoch 2, Batch 197, Test Loss: 0.8025358319282532\n",
      "Epoch 2, Batch 198, Test Loss: 0.7256317138671875\n",
      "Epoch 2, Batch 199, Test Loss: 0.7665494084358215\n",
      "Epoch 2, Batch 200, Test Loss: 1.1879171133041382\n",
      "Epoch 2, Batch 201, Test Loss: 0.7258513569831848\n",
      "Epoch 2, Batch 202, Test Loss: 0.9119952917098999\n",
      "Epoch 2, Batch 203, Test Loss: 1.089827060699463\n",
      "Epoch 2, Batch 204, Test Loss: 0.8457463979721069\n",
      "Epoch 2, Batch 205, Test Loss: 0.9015931487083435\n",
      "Epoch 2, Batch 206, Test Loss: 0.8725969195365906\n",
      "Epoch 2, Batch 207, Test Loss: 1.1765000820159912\n",
      "Epoch 2, Batch 208, Test Loss: 0.8662899136543274\n",
      "Epoch 2, Batch 209, Test Loss: 0.8933526873588562\n",
      "Epoch 2, Batch 210, Test Loss: 0.9597316980361938\n",
      "Epoch 2, Batch 211, Test Loss: 0.877627432346344\n",
      "Epoch 2, Batch 212, Test Loss: 0.7741881608963013\n",
      "Epoch 2, Batch 213, Test Loss: 0.7231510281562805\n",
      "Epoch 2, Batch 214, Test Loss: 0.8088147044181824\n",
      "Epoch 2, Batch 215, Test Loss: 1.056114912033081\n",
      "Epoch 2, Batch 216, Test Loss: 0.7492940425872803\n",
      "Epoch 2, Batch 217, Test Loss: 0.7415801286697388\n",
      "Epoch 2, Batch 218, Test Loss: 0.7754338979721069\n",
      "Epoch 2, Batch 219, Test Loss: 0.7403299808502197\n",
      "Epoch 2, Batch 220, Test Loss: 0.9440187215805054\n",
      "Epoch 2, Batch 221, Test Loss: 0.790224552154541\n",
      "Epoch 2, Batch 222, Test Loss: 0.9124948382377625\n",
      "Epoch 2, Batch 223, Test Loss: 0.9349015951156616\n",
      "Epoch 2, Batch 224, Test Loss: 0.8910600543022156\n",
      "Epoch 2, Batch 225, Test Loss: 0.9015578627586365\n",
      "Epoch 2, Batch 226, Test Loss: 0.8384717702865601\n",
      "Epoch 2, Batch 227, Test Loss: 0.8747093677520752\n",
      "Epoch 2, Batch 228, Test Loss: 0.902795672416687\n",
      "Epoch 2, Batch 229, Test Loss: 0.7041875123977661\n",
      "Epoch 2, Batch 230, Test Loss: 0.8470439910888672\n",
      "Epoch 2, Batch 231, Test Loss: 0.7457718849182129\n",
      "Epoch 2, Batch 232, Test Loss: 0.7065111398696899\n",
      "Epoch 2, Batch 233, Test Loss: 0.8190723061561584\n",
      "Epoch 2, Batch 234, Test Loss: 0.7664521336555481\n",
      "Epoch 2, Batch 235, Test Loss: 0.8708622455596924\n",
      "Epoch 2, Batch 236, Test Loss: 0.7454482316970825\n",
      "Epoch 2, Batch 237, Test Loss: 0.9856683015823364\n",
      "Epoch 2, Batch 238, Test Loss: 0.7308335304260254\n",
      "Epoch 2, Batch 239, Test Loss: 1.0632967948913574\n",
      "Epoch 2, Batch 240, Test Loss: 0.6793621778488159\n",
      "Epoch 2, Batch 241, Test Loss: 0.9664582014083862\n",
      "Epoch 2, Batch 242, Test Loss: 0.7401813268661499\n",
      "Epoch 2, Batch 243, Test Loss: 0.7196283340454102\n",
      "Epoch 2, Batch 244, Test Loss: 0.7913826704025269\n",
      "Epoch 2, Batch 245, Test Loss: 0.7628619074821472\n",
      "Epoch 2, Batch 246, Test Loss: 0.773219108581543\n",
      "Epoch 2, Batch 247, Test Loss: 0.9687581062316895\n",
      "Epoch 2, Batch 248, Test Loss: 0.8754033446311951\n",
      "Epoch 2, Batch 249, Test Loss: 0.8902715444564819\n",
      "Epoch 2, Batch 250, Test Loss: 0.911105215549469\n",
      "Epoch 2, Batch 251, Test Loss: 0.7321657538414001\n",
      "Epoch 2, Batch 252, Test Loss: 0.6884955167770386\n",
      "Epoch 2, Batch 253, Test Loss: 1.0836538076400757\n",
      "Epoch 2, Batch 254, Test Loss: 0.9790483713150024\n",
      "Epoch 2, Batch 255, Test Loss: 0.6614186763763428\n",
      "Epoch 2, Batch 256, Test Loss: 1.0829349756240845\n",
      "Epoch 2, Batch 257, Test Loss: 0.9857648611068726\n",
      "Epoch 2, Batch 258, Test Loss: 0.9270007610321045\n",
      "Epoch 2, Batch 259, Test Loss: 0.9428852200508118\n",
      "Epoch 2, Batch 260, Test Loss: 0.7575915455818176\n",
      "Epoch 2, Batch 261, Test Loss: 0.9138122200965881\n",
      "Epoch 2, Batch 262, Test Loss: 0.8729457259178162\n",
      "Epoch 2, Batch 263, Test Loss: 0.6695252656936646\n",
      "Epoch 2, Batch 264, Test Loss: 0.729581892490387\n",
      "Epoch 2, Batch 265, Test Loss: 0.7947125434875488\n",
      "Epoch 2, Batch 266, Test Loss: 0.9114185571670532\n",
      "Epoch 2, Batch 267, Test Loss: 0.7568916082382202\n",
      "Epoch 2, Batch 268, Test Loss: 0.8524531126022339\n",
      "Epoch 2, Batch 269, Test Loss: 1.0896859169006348\n",
      "Epoch 2, Batch 270, Test Loss: 0.7669336199760437\n",
      "Epoch 2, Batch 271, Test Loss: 0.9696344137191772\n",
      "Epoch 2, Batch 272, Test Loss: 0.9739142656326294\n",
      "Epoch 2, Batch 273, Test Loss: 0.6659543514251709\n",
      "Epoch 2, Batch 274, Test Loss: 0.7948595881462097\n",
      "Epoch 2, Batch 275, Test Loss: 0.8368374109268188\n",
      "Epoch 2, Batch 276, Test Loss: 0.6691850423812866\n",
      "Epoch 2, Batch 277, Test Loss: 0.929506778717041\n",
      "Epoch 2, Batch 278, Test Loss: 0.8312232494354248\n",
      "Epoch 2, Batch 279, Test Loss: 0.797438383102417\n",
      "Epoch 2, Batch 280, Test Loss: 0.9091120958328247\n",
      "Epoch 2, Batch 281, Test Loss: 0.6906217336654663\n",
      "Epoch 2, Batch 282, Test Loss: 0.8066250085830688\n",
      "Epoch 2, Batch 283, Test Loss: 0.7099524736404419\n",
      "Epoch 2, Batch 284, Test Loss: 0.6983957290649414\n",
      "Epoch 2, Batch 285, Test Loss: 0.7019583582878113\n",
      "Epoch 2, Batch 286, Test Loss: 0.8518751859664917\n",
      "Epoch 2, Batch 287, Test Loss: 0.8107882738113403\n",
      "Epoch 2, Batch 288, Test Loss: 0.8604217767715454\n",
      "Epoch 2, Batch 289, Test Loss: 0.6829655170440674\n",
      "Epoch 2, Batch 290, Test Loss: 0.736647367477417\n",
      "Epoch 2, Batch 291, Test Loss: 0.7871111631393433\n",
      "Epoch 2, Batch 292, Test Loss: 0.8692440390586853\n",
      "Epoch 2, Batch 293, Test Loss: 0.8105046153068542\n",
      "Epoch 2, Batch 294, Test Loss: 0.7760133743286133\n",
      "Epoch 2, Batch 295, Test Loss: 1.0559746026992798\n",
      "Epoch 2, Batch 296, Test Loss: 0.8322333097457886\n",
      "Epoch 2, Batch 297, Test Loss: 0.7301695942878723\n",
      "Epoch 2, Batch 298, Test Loss: 0.6844998002052307\n",
      "Epoch 2, Batch 299, Test Loss: 0.9275022149085999\n",
      "Epoch 2, Batch 300, Test Loss: 0.7450838685035706\n",
      "Epoch 2, Batch 301, Test Loss: 0.796214759349823\n",
      "Epoch 2, Batch 302, Test Loss: 0.879016101360321\n",
      "Epoch 2, Batch 303, Test Loss: 1.1375595331192017\n",
      "Epoch 2, Batch 304, Test Loss: 0.7461737990379333\n",
      "Epoch 2, Batch 305, Test Loss: 0.7247381806373596\n",
      "Epoch 2, Batch 306, Test Loss: 0.7369687557220459\n",
      "Epoch 2, Batch 307, Test Loss: 0.8479002118110657\n",
      "Epoch 2, Batch 308, Test Loss: 0.7914555668830872\n",
      "Epoch 2, Batch 309, Test Loss: 0.8088943958282471\n",
      "Epoch 2, Batch 310, Test Loss: 0.691465437412262\n",
      "Epoch 2, Batch 311, Test Loss: 0.6224105358123779\n",
      "Epoch 2, Batch 312, Test Loss: 0.8555480241775513\n",
      "Epoch 2, Batch 313, Test Loss: 0.7863264679908752\n",
      "Epoch 2, Batch 314, Test Loss: 0.8000242114067078\n",
      "Epoch 2, Batch 315, Test Loss: 0.7062804698944092\n",
      "Epoch 2, Batch 316, Test Loss: 0.7589200139045715\n",
      "Epoch 2, Batch 317, Test Loss: 1.1772189140319824\n",
      "Epoch 2, Batch 318, Test Loss: 0.8893535137176514\n",
      "Epoch 2, Batch 319, Test Loss: 0.8930476903915405\n",
      "Epoch 2, Batch 320, Test Loss: 0.7584272027015686\n",
      "Epoch 2, Batch 321, Test Loss: 0.8458601236343384\n",
      "Epoch 2, Batch 322, Test Loss: 1.0778628587722778\n",
      "Epoch 2, Batch 323, Test Loss: 0.9796456694602966\n",
      "Epoch 2, Batch 324, Test Loss: 0.7598142623901367\n",
      "Epoch 2, Batch 325, Test Loss: 0.8886642456054688\n",
      "Epoch 2, Batch 326, Test Loss: 1.0390183925628662\n",
      "Epoch 2, Batch 327, Test Loss: 0.7570405006408691\n",
      "Epoch 2, Batch 328, Test Loss: 0.8548502922058105\n",
      "Epoch 2, Batch 329, Test Loss: 0.7631818056106567\n",
      "Epoch 2, Batch 330, Test Loss: 0.9694302082061768\n",
      "Epoch 2, Batch 331, Test Loss: 0.6868064403533936\n",
      "Epoch 2, Batch 332, Test Loss: 0.6757838129997253\n",
      "Epoch 2, Batch 333, Test Loss: 0.8174206018447876\n",
      "Epoch 2, Batch 334, Test Loss: 0.9076901078224182\n",
      "Epoch 2, Batch 335, Test Loss: 0.8776265382766724\n",
      "Epoch 2, Batch 336, Test Loss: 0.8401656746864319\n",
      "Epoch 2, Batch 337, Test Loss: 0.9173778295516968\n",
      "Epoch 2, Batch 338, Test Loss: 0.8339893817901611\n",
      "Epoch 2, Batch 339, Test Loss: 0.9917211532592773\n",
      "Epoch 2, Batch 340, Test Loss: 0.8219300508499146\n",
      "Epoch 2, Batch 341, Test Loss: 0.7725402116775513\n",
      "Epoch 2, Batch 342, Test Loss: 0.7989560961723328\n",
      "Epoch 2, Batch 343, Test Loss: 0.816887378692627\n",
      "Epoch 2, Batch 344, Test Loss: 1.0029120445251465\n",
      "Epoch 2, Batch 345, Test Loss: 0.9035533666610718\n",
      "Epoch 2, Batch 346, Test Loss: 0.7720277905464172\n",
      "Epoch 2, Batch 347, Test Loss: 0.6486385464668274\n",
      "Epoch 2, Batch 348, Test Loss: 0.7058519721031189\n",
      "Epoch 2, Batch 349, Test Loss: 0.6386987566947937\n",
      "Epoch 2, Batch 350, Test Loss: 0.9010984897613525\n",
      "Epoch 2, Batch 351, Test Loss: 0.8791216015815735\n",
      "Epoch 2, Batch 352, Test Loss: 0.7871757745742798\n",
      "Epoch 2, Batch 353, Test Loss: 0.8159322142601013\n",
      "Epoch 2, Batch 354, Test Loss: 0.7251951694488525\n",
      "Epoch 2, Batch 355, Test Loss: 0.6890479922294617\n",
      "Epoch 2, Batch 356, Test Loss: 0.7303526401519775\n",
      "Epoch 2, Batch 357, Test Loss: 0.7366320490837097\n",
      "Epoch 2, Batch 358, Test Loss: 0.8031294345855713\n",
      "Epoch 2, Batch 359, Test Loss: 0.8198675513267517\n",
      "Epoch 2, Batch 360, Test Loss: 0.7927106618881226\n",
      "Epoch 2, Batch 361, Test Loss: 0.8392581939697266\n",
      "Epoch 2, Batch 362, Test Loss: 0.7933462858200073\n",
      "Epoch 2, Batch 363, Test Loss: 0.8664306402206421\n",
      "Epoch 2, Batch 364, Test Loss: 0.788638174533844\n",
      "Epoch 2, Batch 365, Test Loss: 1.019632339477539\n",
      "Epoch 2, Batch 366, Test Loss: 0.8590949773788452\n",
      "Epoch 2, Batch 367, Test Loss: 0.8724962472915649\n",
      "Epoch 2, Batch 368, Test Loss: 1.0111021995544434\n",
      "Epoch 2, Batch 369, Test Loss: 0.8144216537475586\n",
      "Epoch 2, Batch 370, Test Loss: 0.7982334494590759\n",
      "Epoch 2, Batch 371, Test Loss: 0.8202385306358337\n",
      "Epoch 2, Batch 372, Test Loss: 0.7508275508880615\n",
      "Epoch 2, Batch 373, Test Loss: 0.6689412593841553\n",
      "Epoch 2, Batch 374, Test Loss: 0.7456943988800049\n",
      "Epoch 2, Batch 375, Test Loss: 0.9329646229743958\n",
      "Epoch 2, Batch 376, Test Loss: 0.7330787777900696\n",
      "Epoch 2, Batch 377, Test Loss: 0.7900561094284058\n",
      "Epoch 2, Batch 378, Test Loss: 0.844345211982727\n",
      "Epoch 2, Batch 379, Test Loss: 0.7724795341491699\n",
      "Epoch 2, Batch 380, Test Loss: 0.8864182829856873\n",
      "Epoch 2, Batch 381, Test Loss: 0.6672123074531555\n",
      "Epoch 2, Batch 382, Test Loss: 0.7440906763076782\n",
      "Epoch 2, Batch 383, Test Loss: 0.7312569618225098\n",
      "Epoch 2, Batch 384, Test Loss: 0.9432255625724792\n",
      "Epoch 2, Batch 385, Test Loss: 0.729350745677948\n",
      "Epoch 2, Batch 386, Test Loss: 0.9720019698143005\n",
      "Epoch 2, Batch 387, Test Loss: 0.7480385303497314\n",
      "Epoch 2, Batch 388, Test Loss: 0.997619092464447\n",
      "Epoch 2, Batch 389, Test Loss: 0.8963303565979004\n",
      "Epoch 2, Batch 390, Test Loss: 0.8943560123443604\n",
      "Epoch 2, Batch 391, Test Loss: 0.8025952577590942\n",
      "Epoch 2, Batch 392, Test Loss: 0.8554038405418396\n",
      "Epoch 2, Batch 393, Test Loss: 0.8750696778297424\n",
      "Epoch 2, Batch 394, Test Loss: 0.7618002891540527\n",
      "Epoch 2, Batch 395, Test Loss: 1.0857669115066528\n",
      "Epoch 2, Batch 396, Test Loss: 0.9269800186157227\n",
      "Epoch 2, Batch 397, Test Loss: 0.9148264527320862\n",
      "Epoch 2, Batch 398, Test Loss: 1.1701562404632568\n",
      "Epoch 2, Batch 399, Test Loss: 0.719514787197113\n",
      "Epoch 2, Batch 400, Test Loss: 0.950567364692688\n",
      "Epoch 2, Batch 401, Test Loss: 0.8094820976257324\n",
      "Epoch 2, Batch 402, Test Loss: 0.8821464776992798\n",
      "Epoch 2, Batch 403, Test Loss: 0.8212270736694336\n",
      "Epoch 2, Batch 404, Test Loss: 0.7301570177078247\n",
      "Epoch 2, Batch 405, Test Loss: 0.6711547374725342\n",
      "Epoch 2, Batch 406, Test Loss: 0.9755209684371948\n",
      "Epoch 2, Batch 407, Test Loss: 0.9803604483604431\n",
      "Epoch 2, Batch 408, Test Loss: 0.9902471303939819\n",
      "Epoch 2, Batch 409, Test Loss: 0.6521527171134949\n",
      "Epoch 2, Batch 410, Test Loss: 0.7465310096740723\n",
      "Epoch 2, Batch 411, Test Loss: 0.9336146712303162\n",
      "Epoch 2, Batch 412, Test Loss: 0.8633379340171814\n",
      "Epoch 2, Batch 413, Test Loss: 1.0025798082351685\n",
      "Epoch 2, Batch 414, Test Loss: 1.0191519260406494\n",
      "Epoch 2, Batch 415, Test Loss: 0.840546190738678\n",
      "Epoch 2, Batch 416, Test Loss: 0.9967144727706909\n",
      "Epoch 2, Batch 417, Test Loss: 0.7208340167999268\n",
      "Epoch 2, Batch 418, Test Loss: 0.8712455630302429\n",
      "Epoch 2, Batch 419, Test Loss: 0.7966564893722534\n",
      "Epoch 2, Batch 420, Test Loss: 0.8001038432121277\n",
      "Epoch 2, Batch 421, Test Loss: 0.7678609490394592\n",
      "Epoch 2, Batch 422, Test Loss: 0.8732841610908508\n",
      "Epoch 2, Batch 423, Test Loss: 0.8938807249069214\n",
      "Epoch 2, Batch 424, Test Loss: 0.8470413088798523\n",
      "Epoch 2, Batch 425, Test Loss: 0.7947035431861877\n",
      "Epoch 2, Batch 426, Test Loss: 1.094659447669983\n",
      "Epoch 2, Batch 427, Test Loss: 0.8495401740074158\n",
      "Epoch 2, Batch 428, Test Loss: 0.8705311417579651\n",
      "Epoch 2, Batch 429, Test Loss: 0.6675153970718384\n",
      "Epoch 2, Batch 430, Test Loss: 0.8817719221115112\n",
      "Epoch 2, Batch 431, Test Loss: 0.9872879385948181\n",
      "Epoch 2, Batch 432, Test Loss: 0.732109785079956\n",
      "Epoch 2, Batch 433, Test Loss: 0.9176779985427856\n",
      "Epoch 2, Batch 434, Test Loss: 0.8269723057746887\n",
      "Epoch 2, Batch 435, Test Loss: 0.79411780834198\n",
      "Epoch 2, Batch 436, Test Loss: 0.919363260269165\n",
      "Epoch 2, Batch 437, Test Loss: 1.0322833061218262\n",
      "Epoch 2, Batch 438, Test Loss: 0.7790637612342834\n",
      "Epoch 2, Batch 439, Test Loss: 0.937885046005249\n",
      "Epoch 2, Batch 440, Test Loss: 0.7730165719985962\n",
      "Epoch 2, Batch 441, Test Loss: 0.9330543279647827\n",
      "Epoch 2, Batch 442, Test Loss: 0.7700250744819641\n",
      "Epoch 2, Batch 443, Test Loss: 0.8320134282112122\n",
      "Epoch 2, Batch 444, Test Loss: 0.7860588431358337\n",
      "Epoch 2, Batch 445, Test Loss: 0.6343578100204468\n",
      "Epoch 2, Batch 446, Test Loss: 0.8840435743331909\n",
      "Epoch 2, Batch 447, Test Loss: 0.7893528342247009\n",
      "Epoch 2, Batch 448, Test Loss: 0.8861808180809021\n",
      "Epoch 2, Batch 449, Test Loss: 0.7318254709243774\n",
      "Epoch 2, Batch 450, Test Loss: 1.0293406248092651\n",
      "Epoch 2, Batch 451, Test Loss: 0.7514139413833618\n",
      "Epoch 2, Batch 452, Test Loss: 0.837877094745636\n",
      "Epoch 2, Batch 453, Test Loss: 0.8253396153450012\n",
      "Epoch 2, Batch 454, Test Loss: 0.8133775591850281\n",
      "Epoch 2, Batch 455, Test Loss: 0.6910442113876343\n",
      "Epoch 2, Batch 456, Test Loss: 0.9738373756408691\n",
      "Epoch 2, Batch 457, Test Loss: 0.8521933555603027\n",
      "Epoch 2, Batch 458, Test Loss: 0.8721938133239746\n",
      "Epoch 2, Batch 459, Test Loss: 0.651894748210907\n",
      "Epoch 2, Batch 460, Test Loss: 0.6700934767723083\n",
      "Epoch 2, Batch 461, Test Loss: 0.8416609764099121\n",
      "Epoch 2, Batch 462, Test Loss: 0.7303091287612915\n",
      "Epoch 2, Batch 463, Test Loss: 0.7289780974388123\n",
      "Epoch 2, Batch 464, Test Loss: 0.9697094559669495\n",
      "Epoch 2, Batch 465, Test Loss: 0.8521518111228943\n",
      "Epoch 2, Batch 466, Test Loss: 0.8539214730262756\n",
      "Epoch 2, Batch 467, Test Loss: 0.854872465133667\n",
      "Epoch 2, Batch 468, Test Loss: 0.8306599855422974\n",
      "Epoch 2, Batch 469, Test Loss: 0.6755779385566711\n",
      "Epoch 2, Batch 470, Test Loss: 1.0311285257339478\n",
      "Epoch 2, Batch 471, Test Loss: 0.6971717476844788\n",
      "Epoch 2, Batch 472, Test Loss: 0.8471159934997559\n",
      "Epoch 2, Batch 473, Test Loss: 0.7890564799308777\n",
      "Epoch 2, Batch 474, Test Loss: 1.2003726959228516\n",
      "Epoch 2, Batch 475, Test Loss: 0.776185154914856\n",
      "Epoch 2, Batch 476, Test Loss: 0.9065199494361877\n",
      "Epoch 2, Batch 477, Test Loss: 0.7829089164733887\n",
      "Epoch 2, Batch 478, Test Loss: 0.9991717338562012\n",
      "Epoch 2, Batch 479, Test Loss: 0.9590782523155212\n",
      "Epoch 2, Batch 480, Test Loss: 0.6522705554962158\n",
      "Epoch 2, Batch 481, Test Loss: 1.0066412687301636\n",
      "Epoch 2, Batch 482, Test Loss: 0.7392421960830688\n",
      "Epoch 2, Batch 483, Test Loss: 0.9003783464431763\n",
      "Epoch 2, Batch 484, Test Loss: 0.9063050150871277\n",
      "Epoch 2, Batch 485, Test Loss: 0.7892706394195557\n",
      "Epoch 2, Batch 486, Test Loss: 0.8586931228637695\n",
      "Epoch 2, Batch 487, Test Loss: 0.8066073656082153\n",
      "Epoch 2, Batch 488, Test Loss: 0.8323456645011902\n",
      "Epoch 2, Batch 489, Test Loss: 0.8380337953567505\n",
      "Epoch 2, Batch 490, Test Loss: 0.8807401657104492\n",
      "Epoch 2, Batch 491, Test Loss: 0.8006812334060669\n",
      "Epoch 2, Batch 492, Test Loss: 0.8236578702926636\n",
      "Epoch 2, Batch 493, Test Loss: 0.983661949634552\n",
      "Epoch 2, Batch 494, Test Loss: 0.8863623142242432\n",
      "Epoch 2, Batch 495, Test Loss: 0.7865160703659058\n",
      "Epoch 2, Batch 496, Test Loss: 0.8568301796913147\n",
      "Epoch 2, Batch 497, Test Loss: 0.9206924438476562\n",
      "Epoch 2, Batch 498, Test Loss: 0.8422811031341553\n",
      "Epoch 2, Batch 499, Test Loss: 0.5633182525634766\n",
      "Epoch 2, Batch 500, Test Loss: 0.7560697793960571\n",
      "Epoch 2, Batch 501, Test Loss: 1.029274582862854\n",
      "Epoch 2, Batch 502, Test Loss: 0.8659086227416992\n",
      "Epoch 2, Batch 503, Test Loss: 0.7614628076553345\n",
      "Epoch 2, Batch 504, Test Loss: 0.7412832975387573\n",
      "Epoch 2, Batch 505, Test Loss: 0.9830415844917297\n",
      "Epoch 2, Batch 506, Test Loss: 0.8876363039016724\n",
      "Epoch 2, Batch 507, Test Loss: 0.8378284573554993\n",
      "Epoch 2, Batch 508, Test Loss: 0.8935977220535278\n",
      "Epoch 2, Batch 509, Test Loss: 0.5921223759651184\n",
      "Epoch 2, Batch 510, Test Loss: 0.7804616093635559\n",
      "Epoch 2, Batch 511, Test Loss: 0.6996365189552307\n",
      "Epoch 2, Batch 512, Test Loss: 0.9447985887527466\n",
      "Epoch 2, Batch 513, Test Loss: 0.8470336198806763\n",
      "Epoch 2, Batch 514, Test Loss: 0.7786632776260376\n",
      "Epoch 2, Batch 515, Test Loss: 0.7605069875717163\n",
      "Epoch 2, Batch 516, Test Loss: 1.004923939704895\n",
      "Epoch 2, Batch 517, Test Loss: 0.9212659597396851\n",
      "Epoch 2, Batch 518, Test Loss: 0.799489438533783\n",
      "Epoch 2, Batch 519, Test Loss: 0.827184796333313\n",
      "Epoch 2, Batch 520, Test Loss: 0.8849543333053589\n",
      "Epoch 2, Batch 521, Test Loss: 0.8175525069236755\n",
      "Epoch 2, Batch 522, Test Loss: 0.8931326270103455\n",
      "Epoch 2, Batch 523, Test Loss: 0.7048974633216858\n",
      "Epoch 2, Batch 524, Test Loss: 0.7215899229049683\n",
      "Epoch 2, Batch 525, Test Loss: 0.6856645941734314\n",
      "Epoch 2, Batch 526, Test Loss: 0.6846046447753906\n",
      "Epoch 2, Batch 527, Test Loss: 0.862618625164032\n",
      "Epoch 2, Batch 528, Test Loss: 0.997860312461853\n",
      "Epoch 2, Batch 529, Test Loss: 0.874301552772522\n",
      "Epoch 2, Batch 530, Test Loss: 0.8166354298591614\n",
      "Epoch 2, Batch 531, Test Loss: 0.8916113376617432\n",
      "Epoch 2, Batch 532, Test Loss: 0.7567664384841919\n",
      "Epoch 2, Batch 533, Test Loss: 0.9288384914398193\n",
      "Epoch 2, Batch 534, Test Loss: 0.7854689359664917\n",
      "Epoch 2, Batch 535, Test Loss: 0.7750573754310608\n",
      "Epoch 2, Batch 536, Test Loss: 0.8402945399284363\n",
      "Epoch 2, Batch 537, Test Loss: 0.8523588180541992\n",
      "Epoch 2, Batch 538, Test Loss: 0.9389230608940125\n",
      "Epoch 2, Batch 539, Test Loss: 0.7670104503631592\n",
      "Epoch 2, Batch 540, Test Loss: 0.8016389012336731\n",
      "Epoch 2, Batch 541, Test Loss: 0.5249669551849365\n",
      "Epoch 2, Batch 542, Test Loss: 0.8926467895507812\n",
      "Epoch 2, Batch 543, Test Loss: 0.7197846174240112\n",
      "Epoch 2, Batch 544, Test Loss: 0.710751473903656\n",
      "Epoch 2, Batch 545, Test Loss: 0.9014852046966553\n",
      "Epoch 2, Batch 546, Test Loss: 0.7867322564125061\n",
      "Epoch 2, Batch 547, Test Loss: 0.7369814515113831\n",
      "Epoch 2, Batch 548, Test Loss: 0.7769866585731506\n",
      "Epoch 2, Batch 549, Test Loss: 0.6789606809616089\n",
      "Epoch 2, Batch 550, Test Loss: 0.6513242125511169\n",
      "Epoch 2, Batch 551, Test Loss: 0.8167108297348022\n",
      "Epoch 2, Batch 552, Test Loss: 0.9433541297912598\n",
      "Epoch 2, Batch 553, Test Loss: 0.8957422971725464\n",
      "Epoch 2, Batch 554, Test Loss: 1.0621325969696045\n",
      "Epoch 2, Batch 555, Test Loss: 0.9190594553947449\n",
      "Epoch 2, Batch 556, Test Loss: 0.9341716766357422\n",
      "Epoch 2, Batch 557, Test Loss: 0.9278618693351746\n",
      "Epoch 2, Batch 558, Test Loss: 0.7627937197685242\n",
      "Epoch 2, Batch 559, Test Loss: 0.9005507230758667\n",
      "Epoch 2, Batch 560, Test Loss: 0.9384501576423645\n",
      "Epoch 2, Batch 561, Test Loss: 1.0023596286773682\n",
      "Epoch 2, Batch 562, Test Loss: 0.783233106136322\n",
      "Epoch 2, Batch 563, Test Loss: 0.910778284072876\n",
      "Epoch 2, Batch 564, Test Loss: 0.8337996006011963\n",
      "Epoch 2, Batch 565, Test Loss: 0.9128285050392151\n",
      "Epoch 2, Batch 566, Test Loss: 0.7379101514816284\n",
      "Epoch 2, Batch 567, Test Loss: 1.1297897100448608\n",
      "Epoch 2, Batch 568, Test Loss: 0.6519545316696167\n",
      "Epoch 2, Batch 569, Test Loss: 0.8854409456253052\n",
      "Epoch 2, Batch 570, Test Loss: 0.7386437654495239\n",
      "Epoch 2, Batch 571, Test Loss: 0.7143164873123169\n",
      "Epoch 2, Batch 572, Test Loss: 0.6680529713630676\n",
      "Epoch 2, Batch 573, Test Loss: 0.9312574863433838\n",
      "Epoch 2, Batch 574, Test Loss: 0.8531947135925293\n",
      "Epoch 2, Batch 575, Test Loss: 0.7851842641830444\n",
      "Epoch 2, Batch 576, Test Loss: 0.7743038535118103\n",
      "Epoch 2, Batch 577, Test Loss: 0.8977874517440796\n",
      "Epoch 2, Batch 578, Test Loss: 0.7149908542633057\n",
      "Epoch 2, Batch 579, Test Loss: 0.9166823625564575\n",
      "Epoch 2, Batch 580, Test Loss: 0.9213129281997681\n",
      "Epoch 2, Batch 581, Test Loss: 0.9027401804924011\n",
      "Epoch 2, Batch 582, Test Loss: 0.8701910972595215\n",
      "Epoch 2, Batch 583, Test Loss: 0.8014863729476929\n",
      "Epoch 2, Batch 584, Test Loss: 0.8104527592658997\n",
      "Epoch 2, Batch 585, Test Loss: 0.7567094564437866\n",
      "Epoch 2, Batch 586, Test Loss: 0.9014087319374084\n",
      "Epoch 2, Batch 587, Test Loss: 0.9403265118598938\n",
      "Epoch 2, Batch 588, Test Loss: 0.7779897451400757\n",
      "Epoch 2, Batch 589, Test Loss: 0.8346982002258301\n",
      "Epoch 2, Batch 590, Test Loss: 0.7901343107223511\n",
      "Epoch 2, Batch 591, Test Loss: 0.6575210690498352\n",
      "Epoch 2, Batch 592, Test Loss: 0.666449785232544\n",
      "Epoch 2, Batch 593, Test Loss: 0.9064743518829346\n",
      "Epoch 2, Batch 594, Test Loss: 0.6912376880645752\n",
      "Epoch 2, Batch 595, Test Loss: 0.9206418991088867\n",
      "Epoch 2, Batch 596, Test Loss: 0.6847613453865051\n",
      "Epoch 2, Batch 597, Test Loss: 0.7634183168411255\n",
      "Epoch 2, Batch 598, Test Loss: 0.8547864556312561\n",
      "Epoch 2, Batch 599, Test Loss: 0.7520841956138611\n",
      "Epoch 2, Batch 600, Test Loss: 1.3236417770385742\n",
      "Epoch 2, Batch 601, Test Loss: 0.8325728178024292\n",
      "Epoch 2, Batch 602, Test Loss: 0.8928371667861938\n",
      "Epoch 2, Batch 603, Test Loss: 0.7949240207672119\n",
      "Epoch 2, Batch 604, Test Loss: 0.9324235916137695\n",
      "Epoch 2, Batch 605, Test Loss: 0.7234482765197754\n",
      "Epoch 2, Batch 606, Test Loss: 0.7297049760818481\n",
      "Epoch 2, Batch 607, Test Loss: 0.8760859966278076\n",
      "Epoch 2, Batch 608, Test Loss: 0.7202757596969604\n",
      "Epoch 2, Batch 609, Test Loss: 0.949897050857544\n",
      "Epoch 2, Batch 610, Test Loss: 0.9161412715911865\n",
      "Epoch 2, Batch 611, Test Loss: 0.7663980722427368\n",
      "Epoch 2, Batch 612, Test Loss: 0.9806325435638428\n",
      "Epoch 2, Batch 613, Test Loss: 0.7378824353218079\n",
      "Epoch 2, Batch 614, Test Loss: 0.9574138522148132\n",
      "Epoch 2, Batch 615, Test Loss: 0.7351288199424744\n",
      "Epoch 2, Batch 616, Test Loss: 0.7212130427360535\n",
      "Epoch 2, Batch 617, Test Loss: 0.9137855768203735\n",
      "Epoch 2, Batch 618, Test Loss: 0.9769263863563538\n",
      "Epoch 2, Batch 619, Test Loss: 0.9118147492408752\n",
      "Epoch 2, Batch 620, Test Loss: 1.0935194492340088\n",
      "Epoch 2, Batch 621, Test Loss: 0.7576731443405151\n",
      "Epoch 2, Batch 622, Test Loss: 0.6399565935134888\n",
      "Epoch 2, Batch 623, Test Loss: 0.7617907524108887\n",
      "Epoch 2, Batch 624, Test Loss: 0.6781101822853088\n",
      "Epoch 2, Batch 625, Test Loss: 0.752914547920227\n",
      "Epoch 2, Batch 626, Test Loss: 0.7329710721969604\n",
      "Epoch 2, Batch 627, Test Loss: 1.1034845113754272\n",
      "Epoch 2, Batch 628, Test Loss: 0.9236399531364441\n",
      "Epoch 2, Batch 629, Test Loss: 0.7712574005126953\n",
      "Epoch 2, Batch 630, Test Loss: 0.9246940016746521\n",
      "Epoch 2, Batch 631, Test Loss: 1.1063296794891357\n",
      "Epoch 2, Batch 632, Test Loss: 0.9740493893623352\n",
      "Epoch 2, Batch 633, Test Loss: 0.9581314325332642\n",
      "Epoch 2, Batch 634, Test Loss: 0.8867185711860657\n",
      "Epoch 2, Batch 635, Test Loss: 0.8764886260032654\n",
      "Epoch 2, Batch 636, Test Loss: 0.9045917987823486\n",
      "Epoch 2, Batch 637, Test Loss: 0.9089432954788208\n",
      "Epoch 2, Batch 638, Test Loss: 0.7975782752037048\n",
      "Epoch 2, Batch 639, Test Loss: 0.5793595314025879\n",
      "Epoch 2, Batch 640, Test Loss: 0.8029075264930725\n",
      "Epoch 2, Batch 641, Test Loss: 0.932365894317627\n",
      "Epoch 2, Batch 642, Test Loss: 1.0029338598251343\n",
      "Epoch 2, Batch 643, Test Loss: 0.8148421049118042\n",
      "Epoch 2, Batch 644, Test Loss: 0.7033252120018005\n",
      "Epoch 2, Batch 645, Test Loss: 0.8819925785064697\n",
      "Epoch 2, Batch 646, Test Loss: 0.6630821228027344\n",
      "Epoch 2, Batch 647, Test Loss: 0.787320077419281\n",
      "Epoch 2, Batch 648, Test Loss: 0.7776169776916504\n",
      "Epoch 2, Batch 649, Test Loss: 1.0346689224243164\n",
      "Epoch 2, Batch 650, Test Loss: 0.8901020884513855\n",
      "Epoch 2, Batch 651, Test Loss: 0.836921215057373\n",
      "Epoch 2, Batch 652, Test Loss: 0.856818437576294\n",
      "Epoch 2, Batch 653, Test Loss: 0.8077034950256348\n",
      "Epoch 2, Batch 654, Test Loss: 0.9949516654014587\n",
      "Epoch 2, Batch 655, Test Loss: 0.7517815828323364\n",
      "Epoch 2, Batch 656, Test Loss: 0.9286757111549377\n",
      "Epoch 2, Batch 657, Test Loss: 0.7169338464736938\n",
      "Epoch 2, Batch 658, Test Loss: 0.6902459859848022\n",
      "Epoch 2, Batch 659, Test Loss: 0.7476340532302856\n",
      "Epoch 2, Batch 660, Test Loss: 0.763325572013855\n",
      "Epoch 2, Batch 661, Test Loss: 0.8943015336990356\n",
      "Epoch 2, Batch 662, Test Loss: 0.9003242254257202\n",
      "Epoch 2, Batch 663, Test Loss: 0.8909608721733093\n",
      "Epoch 2, Batch 664, Test Loss: 0.7966954708099365\n",
      "Epoch 2, Batch 665, Test Loss: 0.9236897230148315\n",
      "Epoch 2, Batch 666, Test Loss: 0.8337647914886475\n",
      "Epoch 2, Batch 667, Test Loss: 0.9352532625198364\n",
      "Epoch 2, Batch 668, Test Loss: 0.8436297178268433\n",
      "Epoch 2, Batch 669, Test Loss: 0.9612287282943726\n",
      "Epoch 2, Batch 670, Test Loss: 0.9950838088989258\n",
      "Epoch 2, Batch 671, Test Loss: 0.7841614484786987\n",
      "Epoch 2, Batch 672, Test Loss: 1.0382617712020874\n",
      "Epoch 2, Batch 673, Test Loss: 0.8776189684867859\n",
      "Epoch 2, Batch 674, Test Loss: 0.8858255743980408\n",
      "Epoch 2, Batch 675, Test Loss: 0.6616921424865723\n",
      "Epoch 2, Batch 676, Test Loss: 0.7595783472061157\n",
      "Epoch 2, Batch 677, Test Loss: 0.6753593683242798\n",
      "Epoch 2, Batch 678, Test Loss: 0.9568567276000977\n",
      "Epoch 2, Batch 679, Test Loss: 0.8047184348106384\n",
      "Epoch 2, Batch 680, Test Loss: 0.730442464351654\n",
      "Epoch 2, Batch 681, Test Loss: 0.851693332195282\n",
      "Epoch 2, Batch 682, Test Loss: 0.8803317546844482\n",
      "Epoch 2, Batch 683, Test Loss: 0.8858914971351624\n",
      "Epoch 2, Batch 684, Test Loss: 0.848648190498352\n",
      "Epoch 2, Batch 685, Test Loss: 0.8656386137008667\n",
      "Epoch 2, Batch 686, Test Loss: 0.8826418519020081\n",
      "Epoch 2, Batch 687, Test Loss: 0.6863158941268921\n",
      "Epoch 2, Batch 688, Test Loss: 1.0264402627944946\n",
      "Epoch 2, Batch 689, Test Loss: 0.8487028479576111\n",
      "Epoch 2, Batch 690, Test Loss: 0.8888914585113525\n",
      "Epoch 2, Batch 691, Test Loss: 0.8195186853408813\n",
      "Epoch 2, Batch 692, Test Loss: 0.7373321056365967\n",
      "Epoch 2, Batch 693, Test Loss: 0.7228456735610962\n",
      "Epoch 2, Batch 694, Test Loss: 0.9183727502822876\n",
      "Epoch 2, Batch 695, Test Loss: 0.8269678950309753\n",
      "Epoch 2, Batch 696, Test Loss: 0.9117916226387024\n",
      "Epoch 2, Batch 697, Test Loss: 0.8764513731002808\n",
      "Epoch 2, Batch 698, Test Loss: 0.6539961695671082\n",
      "Epoch 2, Batch 699, Test Loss: 0.9012548923492432\n",
      "Epoch 2, Batch 700, Test Loss: 0.7989205718040466\n",
      "Epoch 2, Batch 701, Test Loss: 0.8099931478500366\n",
      "Epoch 2, Batch 702, Test Loss: 0.8466240763664246\n",
      "Epoch 2, Batch 703, Test Loss: 0.5451722741127014\n",
      "Epoch 2, Batch 704, Test Loss: 0.775885820388794\n",
      "Epoch 2, Batch 705, Test Loss: 0.8083467483520508\n",
      "Epoch 2, Batch 706, Test Loss: 0.8229659199714661\n",
      "Epoch 2, Batch 707, Test Loss: 0.8803276419639587\n",
      "Epoch 2, Batch 708, Test Loss: 0.8676250576972961\n",
      "Epoch 2, Batch 709, Test Loss: 0.8620307445526123\n",
      "Epoch 2, Batch 710, Test Loss: 0.7661277651786804\n",
      "Epoch 2, Batch 711, Test Loss: 0.8287367820739746\n",
      "Epoch 2, Batch 712, Test Loss: 0.8153868913650513\n",
      "Epoch 2, Batch 713, Test Loss: 0.7819591760635376\n",
      "Epoch 2, Batch 714, Test Loss: 1.0236786603927612\n",
      "Epoch 2, Batch 715, Test Loss: 0.909135639667511\n",
      "Epoch 2, Batch 716, Test Loss: 0.8601410984992981\n",
      "Epoch 2, Batch 717, Test Loss: 0.8024448156356812\n",
      "Epoch 2, Batch 718, Test Loss: 0.8762866258621216\n",
      "Epoch 2, Batch 719, Test Loss: 0.9425128698348999\n",
      "Epoch 2, Batch 720, Test Loss: 0.7276805639266968\n",
      "Epoch 2, Batch 721, Test Loss: 0.7359173893928528\n",
      "Epoch 2, Batch 722, Test Loss: 0.8139510750770569\n",
      "Epoch 2, Batch 723, Test Loss: 0.8074657320976257\n",
      "Epoch 2, Batch 724, Test Loss: 0.9466158151626587\n",
      "Epoch 2, Batch 725, Test Loss: 0.8034992218017578\n",
      "Epoch 2, Batch 726, Test Loss: 0.7889352440834045\n",
      "Epoch 2, Batch 727, Test Loss: 0.9063934683799744\n",
      "Epoch 2, Batch 728, Test Loss: 0.8314539194107056\n",
      "Epoch 2, Batch 729, Test Loss: 0.9338891506195068\n",
      "Epoch 2, Batch 730, Test Loss: 0.8924853205680847\n",
      "Epoch 2, Batch 731, Test Loss: 0.7827253937721252\n",
      "Epoch 2, Batch 732, Test Loss: 0.7890470027923584\n",
      "Epoch 2, Batch 733, Test Loss: 0.8062939047813416\n",
      "Epoch 2, Batch 734, Test Loss: 0.8467766046524048\n",
      "Epoch 2, Batch 735, Test Loss: 1.060192584991455\n",
      "Epoch 2, Batch 736, Test Loss: 0.9268966317176819\n",
      "Epoch 2, Batch 737, Test Loss: 0.8630903959274292\n",
      "Epoch 2, Batch 738, Test Loss: 0.8613089323043823\n",
      "Epoch 2, Batch 739, Test Loss: 0.7589097023010254\n",
      "Epoch 2, Batch 740, Test Loss: 1.0795562267303467\n",
      "Epoch 2, Batch 741, Test Loss: 0.8268275856971741\n",
      "Epoch 2, Batch 742, Test Loss: 0.905890166759491\n",
      "Epoch 2, Batch 743, Test Loss: 0.6964470744132996\n",
      "Epoch 2, Batch 744, Test Loss: 0.7913417816162109\n",
      "Epoch 2, Batch 745, Test Loss: 0.7132992744445801\n",
      "Epoch 2, Batch 746, Test Loss: 0.9228757619857788\n",
      "Epoch 2, Batch 747, Test Loss: 0.7508490085601807\n",
      "Epoch 2, Batch 748, Test Loss: 0.7034071683883667\n",
      "Epoch 2, Batch 749, Test Loss: 0.6361724734306335\n",
      "Epoch 2, Batch 750, Test Loss: 0.8260849118232727\n",
      "Epoch 2, Batch 751, Test Loss: 0.8778380751609802\n",
      "Epoch 2, Batch 752, Test Loss: 0.9462953805923462\n",
      "Epoch 2, Batch 753, Test Loss: 0.6920148134231567\n",
      "Epoch 2, Batch 754, Test Loss: 0.7836378812789917\n",
      "Epoch 2, Batch 755, Test Loss: 0.7521088719367981\n",
      "Epoch 2, Batch 756, Test Loss: 0.9786897301673889\n",
      "Epoch 2, Batch 757, Test Loss: 0.8142420649528503\n",
      "Epoch 2, Batch 758, Test Loss: 0.7871412634849548\n",
      "Epoch 2, Batch 759, Test Loss: 0.7832427620887756\n",
      "Epoch 2, Batch 760, Test Loss: 0.7334244847297668\n",
      "Epoch 2, Batch 761, Test Loss: 0.9954982995986938\n",
      "Epoch 2, Batch 762, Test Loss: 0.810158908367157\n",
      "Epoch 2, Batch 763, Test Loss: 0.9209446310997009\n",
      "Epoch 2, Batch 764, Test Loss: 0.8032827377319336\n",
      "Epoch 2, Batch 765, Test Loss: 0.8635877370834351\n",
      "Epoch 2, Batch 766, Test Loss: 0.8109248280525208\n",
      "Epoch 2, Batch 767, Test Loss: 0.8892030119895935\n",
      "Epoch 2, Batch 768, Test Loss: 0.8256152272224426\n",
      "Epoch 2, Batch 769, Test Loss: 0.6825193166732788\n",
      "Epoch 2, Batch 770, Test Loss: 0.7596323490142822\n",
      "Epoch 2, Batch 771, Test Loss: 0.633091390132904\n",
      "Epoch 2, Batch 772, Test Loss: 0.6830270290374756\n",
      "Epoch 2, Batch 773, Test Loss: 1.2486815452575684\n",
      "Epoch 2, Batch 774, Test Loss: 0.8307733535766602\n",
      "Epoch 2, Batch 775, Test Loss: 0.9061791896820068\n",
      "Epoch 2, Batch 776, Test Loss: 0.8990901708602905\n",
      "Epoch 2, Batch 777, Test Loss: 0.882932722568512\n",
      "Epoch 2, Batch 778, Test Loss: 1.0242074728012085\n",
      "Epoch 2, Batch 779, Test Loss: 0.7980738282203674\n",
      "Epoch 2, Batch 780, Test Loss: 0.693276047706604\n",
      "Epoch 2, Batch 781, Test Loss: 0.9058758020401001\n",
      "Epoch 2, Batch 782, Test Loss: 0.8931666612625122\n",
      "Epoch 2, Batch 783, Test Loss: 0.8140734434127808\n",
      "Epoch 2, Batch 784, Test Loss: 0.6785121560096741\n",
      "Epoch 2, Batch 785, Test Loss: 0.8303459882736206\n",
      "Epoch 2, Batch 786, Test Loss: 0.7773252725601196\n",
      "Epoch 2, Batch 787, Test Loss: 0.8378428220748901\n",
      "Epoch 2, Batch 788, Test Loss: 0.7292466759681702\n",
      "Epoch 2, Batch 789, Test Loss: 0.906334638595581\n",
      "Epoch 2, Batch 790, Test Loss: 0.8333507776260376\n",
      "Epoch 2, Batch 791, Test Loss: 0.7509167194366455\n",
      "Epoch 2, Batch 792, Test Loss: 0.6815674901008606\n",
      "Epoch 2, Batch 793, Test Loss: 0.7784216403961182\n",
      "Epoch 2, Batch 794, Test Loss: 0.7301167249679565\n",
      "Epoch 2, Batch 795, Test Loss: 0.9381303191184998\n",
      "Epoch 2, Batch 796, Test Loss: 0.8027501702308655\n",
      "Epoch 2, Batch 797, Test Loss: 0.7160979509353638\n",
      "Epoch 2, Batch 798, Test Loss: 0.9734362959861755\n",
      "Epoch 2, Batch 799, Test Loss: 0.9455006122589111\n",
      "Epoch 2, Batch 800, Test Loss: 0.9071811437606812\n",
      "Epoch 2, Batch 801, Test Loss: 0.7151399254798889\n",
      "Epoch 2, Batch 802, Test Loss: 0.8288359642028809\n",
      "Epoch 2, Batch 803, Test Loss: 0.832535445690155\n",
      "Epoch 2, Batch 804, Test Loss: 0.8503773808479309\n",
      "Epoch 2, Batch 805, Test Loss: 1.1613671779632568\n",
      "Epoch 2, Batch 806, Test Loss: 0.8181354999542236\n",
      "Epoch 2, Batch 807, Test Loss: 0.6340367197990417\n",
      "Epoch 2, Batch 808, Test Loss: 1.0508967638015747\n",
      "Epoch 2, Batch 809, Test Loss: 0.7668706178665161\n",
      "Epoch 2, Batch 810, Test Loss: 0.7825121283531189\n",
      "Epoch 2, Batch 811, Test Loss: 0.8216398358345032\n",
      "Epoch 2, Batch 812, Test Loss: 0.7238843441009521\n",
      "Epoch 2, Batch 813, Test Loss: 1.065736174583435\n",
      "Epoch 2, Batch 814, Test Loss: 0.9709047079086304\n",
      "Epoch 2, Batch 815, Test Loss: 0.9098058938980103\n",
      "Epoch 2, Batch 816, Test Loss: 0.7988889813423157\n",
      "Epoch 2, Batch 817, Test Loss: 0.9332024455070496\n",
      "Epoch 2, Batch 818, Test Loss: 0.8537537455558777\n",
      "Epoch 2, Batch 819, Test Loss: 0.8687166571617126\n",
      "Epoch 2, Batch 820, Test Loss: 0.9121514558792114\n",
      "Epoch 2, Batch 821, Test Loss: 0.7966200113296509\n",
      "Epoch 2, Batch 822, Test Loss: 0.7141963243484497\n",
      "Epoch 2, Batch 823, Test Loss: 1.0082182884216309\n",
      "Epoch 2, Batch 824, Test Loss: 0.9552662968635559\n",
      "Epoch 2, Batch 825, Test Loss: 0.7212291955947876\n",
      "Epoch 2, Batch 826, Test Loss: 0.8873748183250427\n",
      "Epoch 2, Batch 827, Test Loss: 0.7617788314819336\n",
      "Epoch 2, Batch 828, Test Loss: 0.8721081018447876\n",
      "Epoch 2, Batch 829, Test Loss: 0.8368198275566101\n",
      "Epoch 2, Batch 830, Test Loss: 0.7835940718650818\n",
      "Epoch 2, Batch 831, Test Loss: 0.8030063509941101\n",
      "Epoch 2, Batch 832, Test Loss: 1.0381790399551392\n",
      "Epoch 2, Batch 833, Test Loss: 0.7662450075149536\n",
      "Epoch 2, Batch 834, Test Loss: 0.9246528744697571\n",
      "Epoch 2, Batch 835, Test Loss: 1.0346214771270752\n",
      "Epoch 2, Batch 836, Test Loss: 0.852378785610199\n",
      "Epoch 2, Batch 837, Test Loss: 0.7758685350418091\n",
      "Epoch 2, Batch 838, Test Loss: 0.9760902523994446\n",
      "Epoch 2, Batch 839, Test Loss: 0.94292151927948\n",
      "Epoch 2, Batch 840, Test Loss: 0.840374231338501\n",
      "Epoch 2, Batch 841, Test Loss: 0.9075532555580139\n",
      "Epoch 2, Batch 842, Test Loss: 0.8733627200126648\n",
      "Epoch 2, Batch 843, Test Loss: 0.6824367046356201\n",
      "Epoch 2, Batch 844, Test Loss: 0.9256980419158936\n",
      "Epoch 2, Batch 845, Test Loss: 0.7183321714401245\n",
      "Epoch 2, Batch 846, Test Loss: 0.7203099727630615\n",
      "Epoch 2, Batch 847, Test Loss: 0.7237645387649536\n",
      "Epoch 2, Batch 848, Test Loss: 0.8752928376197815\n",
      "Epoch 2, Batch 849, Test Loss: 1.154986023902893\n",
      "Epoch 2, Batch 850, Test Loss: 0.8539235591888428\n",
      "Epoch 2, Batch 851, Test Loss: 0.6191991567611694\n",
      "Epoch 2, Batch 852, Test Loss: 0.8224949240684509\n",
      "Epoch 2, Batch 853, Test Loss: 0.8595880270004272\n",
      "Epoch 2, Batch 854, Test Loss: 0.6884019374847412\n",
      "Epoch 2, Batch 855, Test Loss: 0.6917313933372498\n",
      "Epoch 2, Batch 856, Test Loss: 0.767754077911377\n",
      "Epoch 2, Batch 857, Test Loss: 0.7262564301490784\n",
      "Epoch 2, Batch 858, Test Loss: 0.8716554641723633\n",
      "Epoch 2, Batch 859, Test Loss: 0.8981328010559082\n",
      "Epoch 2, Batch 860, Test Loss: 0.8887584805488586\n",
      "Epoch 2, Batch 861, Test Loss: 0.8157227635383606\n",
      "Epoch 2, Batch 862, Test Loss: 0.975438117980957\n",
      "Epoch 2, Batch 863, Test Loss: 0.6932305097579956\n",
      "Epoch 2, Batch 864, Test Loss: 0.9666285514831543\n",
      "Epoch 2, Batch 865, Test Loss: 0.9593467116355896\n",
      "Epoch 2, Batch 866, Test Loss: 0.8056498765945435\n",
      "Epoch 2, Batch 867, Test Loss: 0.717203676700592\n",
      "Epoch 2, Batch 868, Test Loss: 0.8864067792892456\n",
      "Epoch 2, Batch 869, Test Loss: 0.9012935161590576\n",
      "Epoch 2, Batch 870, Test Loss: 0.8708525896072388\n",
      "Epoch 2, Batch 871, Test Loss: 1.2570067644119263\n",
      "Epoch 2, Batch 872, Test Loss: 0.8702728748321533\n",
      "Epoch 2, Batch 873, Test Loss: 0.7093169093132019\n",
      "Epoch 2, Batch 874, Test Loss: 0.9738320708274841\n",
      "Epoch 2, Batch 875, Test Loss: 0.8617741465568542\n",
      "Epoch 2, Batch 876, Test Loss: 0.7595286965370178\n",
      "Epoch 2, Batch 877, Test Loss: 0.9297020435333252\n",
      "Epoch 2, Batch 878, Test Loss: 0.907616913318634\n",
      "Epoch 2, Batch 879, Test Loss: 0.8931301832199097\n",
      "Epoch 2, Batch 880, Test Loss: 0.7958192229270935\n",
      "Epoch 2, Batch 881, Test Loss: 1.1278672218322754\n",
      "Epoch 2, Batch 882, Test Loss: 0.8420904874801636\n",
      "Epoch 2, Batch 883, Test Loss: 0.8337963223457336\n",
      "Epoch 2, Batch 884, Test Loss: 0.848172664642334\n",
      "Epoch 2, Batch 885, Test Loss: 0.7250117659568787\n",
      "Epoch 2, Batch 886, Test Loss: 0.7801818251609802\n",
      "Epoch 2, Batch 887, Test Loss: 0.6246472001075745\n",
      "Epoch 2, Batch 888, Test Loss: 0.7965112924575806\n",
      "Epoch 2, Batch 889, Test Loss: 0.7576753497123718\n",
      "Epoch 2, Batch 890, Test Loss: 0.8553681373596191\n",
      "Epoch 2, Batch 891, Test Loss: 0.8278670310974121\n",
      "Epoch 2, Batch 892, Test Loss: 0.8360787630081177\n",
      "Epoch 2, Batch 893, Test Loss: 0.9127353429794312\n",
      "Epoch 2, Batch 894, Test Loss: 0.9694608449935913\n",
      "Epoch 2, Batch 895, Test Loss: 0.9447227120399475\n",
      "Epoch 2, Batch 896, Test Loss: 0.7333866357803345\n",
      "Epoch 2, Batch 897, Test Loss: 0.894396185874939\n",
      "Epoch 2, Batch 898, Test Loss: 0.7552019953727722\n",
      "Epoch 2, Batch 899, Test Loss: 0.6905301213264465\n",
      "Epoch 2, Batch 900, Test Loss: 0.8245648741722107\n",
      "Epoch 2, Batch 901, Test Loss: 0.8070149421691895\n",
      "Epoch 2, Batch 902, Test Loss: 0.7695550322532654\n",
      "Epoch 2, Batch 903, Test Loss: 0.8654084801673889\n",
      "Epoch 2, Batch 904, Test Loss: 0.828761100769043\n",
      "Epoch 2, Batch 905, Test Loss: 0.7820255160331726\n",
      "Epoch 2, Batch 906, Test Loss: 0.7615275382995605\n",
      "Epoch 2, Batch 907, Test Loss: 0.8289781808853149\n",
      "Epoch 2, Batch 908, Test Loss: 0.7534065842628479\n",
      "Epoch 2, Batch 909, Test Loss: 0.9869955778121948\n",
      "Epoch 2, Batch 910, Test Loss: 0.6968991160392761\n",
      "Epoch 2, Batch 911, Test Loss: 0.8372663855552673\n",
      "Epoch 2, Batch 912, Test Loss: 1.060503602027893\n",
      "Epoch 2, Batch 913, Test Loss: 0.8179072737693787\n",
      "Epoch 2, Batch 914, Test Loss: 0.8098440170288086\n",
      "Epoch 2, Batch 915, Test Loss: 0.8517409563064575\n",
      "Epoch 2, Batch 916, Test Loss: 0.753989577293396\n",
      "Epoch 2, Batch 917, Test Loss: 0.7356351613998413\n",
      "Epoch 2, Batch 918, Test Loss: 0.8617556095123291\n",
      "Epoch 2, Batch 919, Test Loss: 0.7430247068405151\n",
      "Epoch 2, Batch 920, Test Loss: 1.0866386890411377\n",
      "Epoch 2, Batch 921, Test Loss: 0.9018025398254395\n",
      "Epoch 2, Batch 922, Test Loss: 0.9621616005897522\n",
      "Epoch 2, Batch 923, Test Loss: 0.8579776287078857\n",
      "Epoch 2, Batch 924, Test Loss: 0.9785555601119995\n",
      "Epoch 2, Batch 925, Test Loss: 0.7338473796844482\n",
      "Epoch 2, Batch 926, Test Loss: 0.8605689406394958\n",
      "Epoch 2, Batch 927, Test Loss: 0.9482097625732422\n",
      "Epoch 2, Batch 928, Test Loss: 0.8236576318740845\n",
      "Epoch 2, Batch 929, Test Loss: 0.7372999787330627\n",
      "Epoch 2, Batch 930, Test Loss: 0.8510538935661316\n",
      "Epoch 2, Batch 931, Test Loss: 1.0606695413589478\n",
      "Epoch 2, Batch 932, Test Loss: 0.9165917634963989\n",
      "Epoch 2, Batch 933, Test Loss: 0.9394035339355469\n",
      "Epoch 2, Batch 934, Test Loss: 0.9038962721824646\n",
      "Epoch 2, Batch 935, Test Loss: 0.7201895713806152\n",
      "Epoch 2, Batch 936, Test Loss: 0.9738238453865051\n",
      "Epoch 2, Batch 937, Test Loss: 0.6694880127906799\n",
      "Epoch 2, Batch 938, Test Loss: 0.6393734812736511\n",
      "Accuracy of Test set: 0.6283666666666666\n",
      "Epoch 3, Batch 1, Loss: 0.87038254737854\n",
      "Epoch 3, Batch 2, Loss: 0.7365928888320923\n",
      "Epoch 3, Batch 3, Loss: 0.7921656370162964\n",
      "Epoch 3, Batch 4, Loss: 0.8046913146972656\n",
      "Epoch 3, Batch 5, Loss: 0.7287682294845581\n",
      "Epoch 3, Batch 6, Loss: 0.8062593936920166\n",
      "Epoch 3, Batch 7, Loss: 0.7217105627059937\n",
      "Epoch 3, Batch 8, Loss: 0.8707582354545593\n",
      "Epoch 3, Batch 9, Loss: 0.6727191805839539\n",
      "Epoch 3, Batch 10, Loss: 0.6505951881408691\n",
      "Epoch 3, Batch 11, Loss: 0.7542905807495117\n",
      "Epoch 3, Batch 12, Loss: 0.8359020948410034\n",
      "Epoch 3, Batch 13, Loss: 0.8612227439880371\n",
      "Epoch 3, Batch 14, Loss: 0.6383224725723267\n",
      "Epoch 3, Batch 15, Loss: 0.716424822807312\n",
      "Epoch 3, Batch 16, Loss: 0.8858014941215515\n",
      "Epoch 3, Batch 17, Loss: 0.7549996972084045\n",
      "Epoch 3, Batch 18, Loss: 0.9539528489112854\n",
      "Epoch 3, Batch 19, Loss: 0.7577106356620789\n",
      "Epoch 3, Batch 20, Loss: 0.869657039642334\n",
      "Epoch 3, Batch 21, Loss: 0.8741857409477234\n",
      "Epoch 3, Batch 22, Loss: 0.8722909688949585\n",
      "Epoch 3, Batch 23, Loss: 0.7886072397232056\n",
      "Epoch 3, Batch 24, Loss: 0.6651094555854797\n",
      "Epoch 3, Batch 25, Loss: 0.729920506477356\n",
      "Epoch 3, Batch 26, Loss: 0.7009469866752625\n",
      "Epoch 3, Batch 27, Loss: 0.682608425617218\n",
      "Epoch 3, Batch 28, Loss: 0.8247107267379761\n",
      "Epoch 3, Batch 29, Loss: 0.8623417019844055\n",
      "Epoch 3, Batch 30, Loss: 1.002920150756836\n",
      "Epoch 3, Batch 31, Loss: 1.007108449935913\n",
      "Epoch 3, Batch 32, Loss: 0.8488736152648926\n",
      "Epoch 3, Batch 33, Loss: 0.8446823358535767\n",
      "Epoch 3, Batch 34, Loss: 0.7204146385192871\n",
      "Epoch 3, Batch 35, Loss: 0.9725090265274048\n",
      "Epoch 3, Batch 36, Loss: 0.732759952545166\n",
      "Epoch 3, Batch 37, Loss: 0.7567477822303772\n",
      "Epoch 3, Batch 38, Loss: 0.8578871488571167\n",
      "Epoch 3, Batch 39, Loss: 0.757747232913971\n",
      "Epoch 3, Batch 40, Loss: 0.740111231803894\n",
      "Epoch 3, Batch 41, Loss: 0.8458373546600342\n",
      "Epoch 3, Batch 42, Loss: 0.9369874000549316\n",
      "Epoch 3, Batch 43, Loss: 0.7234588861465454\n",
      "Epoch 3, Batch 44, Loss: 0.7875941395759583\n",
      "Epoch 3, Batch 45, Loss: 0.687670111656189\n",
      "Epoch 3, Batch 46, Loss: 0.8510308265686035\n",
      "Epoch 3, Batch 47, Loss: 0.7272971868515015\n",
      "Epoch 3, Batch 48, Loss: 0.8613742589950562\n",
      "Epoch 3, Batch 49, Loss: 0.9053386449813843\n",
      "Epoch 3, Batch 50, Loss: 0.7772918939590454\n",
      "Epoch 3, Batch 51, Loss: 0.7469571828842163\n",
      "Epoch 3, Batch 52, Loss: 0.9096079468727112\n",
      "Epoch 3, Batch 53, Loss: 0.7562373280525208\n",
      "Epoch 3, Batch 54, Loss: 0.9333691596984863\n",
      "Epoch 3, Batch 55, Loss: 0.7344458103179932\n",
      "Epoch 3, Batch 56, Loss: 0.84894859790802\n",
      "Epoch 3, Batch 57, Loss: 0.8234505653381348\n",
      "Epoch 3, Batch 58, Loss: 0.7749229669570923\n",
      "Epoch 3, Batch 59, Loss: 0.662568986415863\n",
      "Epoch 3, Batch 60, Loss: 0.8942176699638367\n",
      "Epoch 3, Batch 61, Loss: 0.6519755721092224\n",
      "Epoch 3, Batch 62, Loss: 0.9282033443450928\n",
      "Epoch 3, Batch 63, Loss: 0.6941918730735779\n",
      "Epoch 3, Batch 64, Loss: 0.6307125687599182\n",
      "Epoch 3, Batch 65, Loss: 0.8617033958435059\n",
      "Epoch 3, Batch 66, Loss: 0.753301739692688\n",
      "Epoch 3, Batch 67, Loss: 0.9268831610679626\n",
      "Epoch 3, Batch 68, Loss: 0.7773953676223755\n",
      "Epoch 3, Batch 69, Loss: 0.7129864692687988\n",
      "Epoch 3, Batch 70, Loss: 0.6148961782455444\n",
      "Epoch 3, Batch 71, Loss: 0.851212203502655\n",
      "Epoch 3, Batch 72, Loss: 0.8763476610183716\n",
      "Epoch 3, Batch 73, Loss: 0.6707981824874878\n",
      "Epoch 3, Batch 74, Loss: 0.783658504486084\n",
      "Epoch 3, Batch 75, Loss: 0.7659186720848083\n",
      "Epoch 3, Batch 76, Loss: 0.6533219218254089\n",
      "Epoch 3, Batch 77, Loss: 0.6406915187835693\n",
      "Epoch 3, Batch 78, Loss: 0.8482150435447693\n",
      "Epoch 3, Batch 79, Loss: 0.786965548992157\n",
      "Epoch 3, Batch 80, Loss: 0.8538223505020142\n",
      "Epoch 3, Batch 81, Loss: 0.8375483155250549\n",
      "Epoch 3, Batch 82, Loss: 0.7920554280281067\n",
      "Epoch 3, Batch 83, Loss: 0.9440150260925293\n",
      "Epoch 3, Batch 84, Loss: 0.9260416030883789\n",
      "Epoch 3, Batch 85, Loss: 0.9569259285926819\n",
      "Epoch 3, Batch 86, Loss: 0.8437814116477966\n",
      "Epoch 3, Batch 87, Loss: 0.6713201999664307\n",
      "Epoch 3, Batch 88, Loss: 0.7333081364631653\n",
      "Epoch 3, Batch 89, Loss: 0.7628785371780396\n",
      "Epoch 3, Batch 90, Loss: 0.7777320146560669\n",
      "Epoch 3, Batch 91, Loss: 0.695040762424469\n",
      "Epoch 3, Batch 92, Loss: 0.8297343254089355\n",
      "Epoch 3, Batch 93, Loss: 0.7785248160362244\n",
      "Epoch 3, Batch 94, Loss: 0.7891348004341125\n",
      "Epoch 3, Batch 95, Loss: 0.9429377913475037\n",
      "Epoch 3, Batch 96, Loss: 0.9240666627883911\n",
      "Epoch 3, Batch 97, Loss: 0.8155854344367981\n",
      "Epoch 3, Batch 98, Loss: 0.6322346925735474\n",
      "Epoch 3, Batch 99, Loss: 0.8476487994194031\n",
      "Epoch 3, Batch 100, Loss: 0.8037744760513306\n",
      "Epoch 3, Batch 101, Loss: 0.7283370494842529\n",
      "Epoch 3, Batch 102, Loss: 0.5973324775695801\n",
      "Epoch 3, Batch 103, Loss: 0.7963452339172363\n",
      "Epoch 3, Batch 104, Loss: 0.6933715343475342\n",
      "Epoch 3, Batch 105, Loss: 0.8894080519676208\n",
      "Epoch 3, Batch 106, Loss: 0.6618824005126953\n",
      "Epoch 3, Batch 107, Loss: 0.7853964567184448\n",
      "Epoch 3, Batch 108, Loss: 0.6366467475891113\n",
      "Epoch 3, Batch 109, Loss: 0.894292950630188\n",
      "Epoch 3, Batch 110, Loss: 0.7501037120819092\n",
      "Epoch 3, Batch 111, Loss: 0.7045525312423706\n",
      "Epoch 3, Batch 112, Loss: 0.6801944971084595\n",
      "Epoch 3, Batch 113, Loss: 0.8765572309494019\n",
      "Epoch 3, Batch 114, Loss: 0.8313249349594116\n",
      "Epoch 3, Batch 115, Loss: 0.7971373200416565\n",
      "Epoch 3, Batch 116, Loss: 0.819140613079071\n",
      "Epoch 3, Batch 117, Loss: 0.8564059138298035\n",
      "Epoch 3, Batch 118, Loss: 0.6779946684837341\n",
      "Epoch 3, Batch 119, Loss: 0.8055204749107361\n",
      "Epoch 3, Batch 120, Loss: 0.8455269932746887\n",
      "Epoch 3, Batch 121, Loss: 0.7908803224563599\n",
      "Epoch 3, Batch 122, Loss: 0.7886642217636108\n",
      "Epoch 3, Batch 123, Loss: 0.8169514536857605\n",
      "Epoch 3, Batch 124, Loss: 0.7196765542030334\n",
      "Epoch 3, Batch 125, Loss: 0.7085578441619873\n",
      "Epoch 3, Batch 126, Loss: 0.7122960686683655\n",
      "Epoch 3, Batch 127, Loss: 0.9803465008735657\n",
      "Epoch 3, Batch 128, Loss: 0.7289405465126038\n",
      "Epoch 3, Batch 129, Loss: 0.6954542398452759\n",
      "Epoch 3, Batch 130, Loss: 0.8072278499603271\n",
      "Epoch 3, Batch 131, Loss: 0.8957988619804382\n",
      "Epoch 3, Batch 132, Loss: 0.8444191813468933\n",
      "Epoch 3, Batch 133, Loss: 0.822378933429718\n",
      "Epoch 3, Batch 134, Loss: 0.6989779472351074\n",
      "Epoch 3, Batch 135, Loss: 0.7117740511894226\n",
      "Epoch 3, Batch 136, Loss: 0.8554018139839172\n",
      "Epoch 3, Batch 137, Loss: 0.7485442757606506\n",
      "Epoch 3, Batch 138, Loss: 0.8397689461708069\n",
      "Epoch 3, Batch 139, Loss: 0.7959616184234619\n",
      "Epoch 3, Batch 140, Loss: 0.7013643980026245\n",
      "Epoch 3, Batch 141, Loss: 0.8440462946891785\n",
      "Epoch 3, Batch 142, Loss: 0.7947617769241333\n",
      "Epoch 3, Batch 143, Loss: 1.1477863788604736\n",
      "Epoch 3, Batch 144, Loss: 0.8769590854644775\n",
      "Epoch 3, Batch 145, Loss: 0.8464838266372681\n",
      "Epoch 3, Batch 146, Loss: 0.821104884147644\n",
      "Epoch 3, Batch 147, Loss: 0.8067071437835693\n",
      "Epoch 3, Batch 148, Loss: 0.6442953944206238\n",
      "Epoch 3, Batch 149, Loss: 0.7825207114219666\n",
      "Epoch 3, Batch 150, Loss: 0.9371780753135681\n",
      "Epoch 3, Batch 151, Loss: 0.6144885420799255\n",
      "Epoch 3, Batch 152, Loss: 0.8879694938659668\n",
      "Epoch 3, Batch 153, Loss: 0.8782460689544678\n",
      "Epoch 3, Batch 154, Loss: 0.6663787364959717\n",
      "Epoch 3, Batch 155, Loss: 0.8011392951011658\n",
      "Epoch 3, Batch 156, Loss: 0.7737731337547302\n",
      "Epoch 3, Batch 157, Loss: 0.8611113429069519\n",
      "Epoch 3, Batch 158, Loss: 0.9527372717857361\n",
      "Epoch 3, Batch 159, Loss: 0.6892387866973877\n",
      "Epoch 3, Batch 160, Loss: 0.775640070438385\n",
      "Epoch 3, Batch 161, Loss: 0.803378701210022\n",
      "Epoch 3, Batch 162, Loss: 0.8126556873321533\n",
      "Epoch 3, Batch 163, Loss: 0.8028967380523682\n",
      "Epoch 3, Batch 164, Loss: 0.6538603901863098\n",
      "Epoch 3, Batch 165, Loss: 0.7481588125228882\n",
      "Epoch 3, Batch 166, Loss: 0.6477435231208801\n",
      "Epoch 3, Batch 167, Loss: 0.7280134558677673\n",
      "Epoch 3, Batch 168, Loss: 0.9000651836395264\n",
      "Epoch 3, Batch 169, Loss: 0.8980932235717773\n",
      "Epoch 3, Batch 170, Loss: 0.6150609254837036\n",
      "Epoch 3, Batch 171, Loss: 0.7664511799812317\n",
      "Epoch 3, Batch 172, Loss: 0.7651935815811157\n",
      "Epoch 3, Batch 173, Loss: 0.8040342330932617\n",
      "Epoch 3, Batch 174, Loss: 0.7242081165313721\n",
      "Epoch 3, Batch 175, Loss: 0.7866369485855103\n",
      "Epoch 3, Batch 176, Loss: 0.6134995818138123\n",
      "Epoch 3, Batch 177, Loss: 0.8731452822685242\n",
      "Epoch 3, Batch 178, Loss: 0.7343086004257202\n",
      "Epoch 3, Batch 179, Loss: 0.719654381275177\n",
      "Epoch 3, Batch 180, Loss: 0.8220962285995483\n",
      "Epoch 3, Batch 181, Loss: 0.6248838305473328\n",
      "Epoch 3, Batch 182, Loss: 0.9245837330818176\n",
      "Epoch 3, Batch 183, Loss: 0.7876015901565552\n",
      "Epoch 3, Batch 184, Loss: 0.763871431350708\n",
      "Epoch 3, Batch 185, Loss: 0.954193651676178\n",
      "Epoch 3, Batch 186, Loss: 0.7585554122924805\n",
      "Epoch 3, Batch 187, Loss: 0.83810955286026\n",
      "Epoch 3, Batch 188, Loss: 0.672539234161377\n",
      "Epoch 3, Batch 189, Loss: 0.914776623249054\n",
      "Epoch 3, Batch 190, Loss: 0.6343780159950256\n",
      "Epoch 3, Batch 191, Loss: 0.6620770692825317\n",
      "Epoch 3, Batch 192, Loss: 0.7546908259391785\n",
      "Epoch 3, Batch 193, Loss: 0.8470516800880432\n",
      "Epoch 3, Batch 194, Loss: 0.6417762637138367\n",
      "Epoch 3, Batch 195, Loss: 0.7029933929443359\n",
      "Epoch 3, Batch 196, Loss: 0.7925460934638977\n",
      "Epoch 3, Batch 197, Loss: 0.7736722230911255\n",
      "Epoch 3, Batch 198, Loss: 0.7249279022216797\n",
      "Epoch 3, Batch 199, Loss: 0.8230911493301392\n",
      "Epoch 3, Batch 200, Loss: 0.8055679798126221\n",
      "Epoch 3, Batch 201, Loss: 0.9394214153289795\n",
      "Epoch 3, Batch 202, Loss: 0.7127887010574341\n",
      "Epoch 3, Batch 203, Loss: 0.786490797996521\n",
      "Epoch 3, Batch 204, Loss: 0.7047388553619385\n",
      "Epoch 3, Batch 205, Loss: 0.8622903227806091\n",
      "Epoch 3, Batch 206, Loss: 0.6825896501541138\n",
      "Epoch 3, Batch 207, Loss: 0.990781843662262\n",
      "Epoch 3, Batch 208, Loss: 0.8738387227058411\n",
      "Epoch 3, Batch 209, Loss: 0.7180382013320923\n",
      "Epoch 3, Batch 210, Loss: 0.8255262970924377\n",
      "Epoch 3, Batch 211, Loss: 0.7483962178230286\n",
      "Epoch 3, Batch 212, Loss: 0.7130563855171204\n",
      "Epoch 3, Batch 213, Loss: 0.6422892808914185\n",
      "Epoch 3, Batch 214, Loss: 0.8082603812217712\n",
      "Epoch 3, Batch 215, Loss: 1.0530085563659668\n",
      "Epoch 3, Batch 216, Loss: 0.7906478643417358\n",
      "Epoch 3, Batch 217, Loss: 0.7711724042892456\n",
      "Epoch 3, Batch 218, Loss: 0.7358746528625488\n",
      "Epoch 3, Batch 219, Loss: 0.7852385640144348\n",
      "Epoch 3, Batch 220, Loss: 0.8400883674621582\n",
      "Epoch 3, Batch 221, Loss: 0.8351361751556396\n",
      "Epoch 3, Batch 222, Loss: 0.7330692410469055\n",
      "Epoch 3, Batch 223, Loss: 0.7064433097839355\n",
      "Epoch 3, Batch 224, Loss: 1.0350501537322998\n",
      "Epoch 3, Batch 225, Loss: 0.6409008502960205\n",
      "Epoch 3, Batch 226, Loss: 0.9376481175422668\n",
      "Epoch 3, Batch 227, Loss: 0.8317173719406128\n",
      "Epoch 3, Batch 228, Loss: 0.773496150970459\n",
      "Epoch 3, Batch 229, Loss: 1.0290032625198364\n",
      "Epoch 3, Batch 230, Loss: 0.7041653394699097\n",
      "Epoch 3, Batch 231, Loss: 0.8147459626197815\n",
      "Epoch 3, Batch 232, Loss: 0.718023419380188\n",
      "Epoch 3, Batch 233, Loss: 0.5913464426994324\n",
      "Epoch 3, Batch 234, Loss: 0.6301183700561523\n",
      "Epoch 3, Batch 235, Loss: 0.6601234674453735\n",
      "Epoch 3, Batch 236, Loss: 0.7519199252128601\n",
      "Epoch 3, Batch 237, Loss: 0.7822829484939575\n",
      "Epoch 3, Batch 238, Loss: 0.8199712038040161\n",
      "Epoch 3, Batch 239, Loss: 0.8680163621902466\n",
      "Epoch 3, Batch 240, Loss: 0.7133944630622864\n",
      "Epoch 3, Batch 241, Loss: 0.7269310355186462\n",
      "Epoch 3, Batch 242, Loss: 0.7602869868278503\n",
      "Epoch 3, Batch 243, Loss: 0.8053630590438843\n",
      "Epoch 3, Batch 244, Loss: 0.6469549536705017\n",
      "Epoch 3, Batch 245, Loss: 0.7815735936164856\n",
      "Epoch 3, Batch 246, Loss: 0.6256757974624634\n",
      "Epoch 3, Batch 247, Loss: 0.7167727947235107\n",
      "Epoch 3, Batch 248, Loss: 0.5925046801567078\n",
      "Epoch 3, Batch 249, Loss: 0.6812789440155029\n",
      "Epoch 3, Batch 250, Loss: 0.8724417686462402\n",
      "Epoch 3, Batch 251, Loss: 0.6994240283966064\n",
      "Epoch 3, Batch 252, Loss: 0.8852952718734741\n",
      "Epoch 3, Batch 253, Loss: 0.7692177295684814\n",
      "Epoch 3, Batch 254, Loss: 0.7596368789672852\n",
      "Epoch 3, Batch 255, Loss: 0.7918007373809814\n",
      "Epoch 3, Batch 256, Loss: 0.7333070039749146\n",
      "Epoch 3, Batch 257, Loss: 0.5845481753349304\n",
      "Epoch 3, Batch 258, Loss: 0.7814733386039734\n",
      "Epoch 3, Batch 259, Loss: 0.8069941997528076\n",
      "Epoch 3, Batch 260, Loss: 0.7396444082260132\n",
      "Epoch 3, Batch 261, Loss: 0.6524440050125122\n",
      "Epoch 3, Batch 262, Loss: 0.7586744427680969\n",
      "Epoch 3, Batch 263, Loss: 0.8576586842536926\n",
      "Epoch 3, Batch 264, Loss: 0.7815998196601868\n",
      "Epoch 3, Batch 265, Loss: 0.7782477140426636\n",
      "Epoch 3, Batch 266, Loss: 0.7778640985488892\n",
      "Epoch 3, Batch 267, Loss: 0.8299169540405273\n",
      "Epoch 3, Batch 268, Loss: 0.8835476040840149\n",
      "Epoch 3, Batch 269, Loss: 1.0268782377243042\n",
      "Epoch 3, Batch 270, Loss: 0.8767345547676086\n",
      "Epoch 3, Batch 271, Loss: 0.7470976710319519\n",
      "Epoch 3, Batch 272, Loss: 0.9507349729537964\n",
      "Epoch 3, Batch 273, Loss: 0.7352559566497803\n",
      "Epoch 3, Batch 274, Loss: 0.8000134229660034\n",
      "Epoch 3, Batch 275, Loss: 0.7755084037780762\n",
      "Epoch 3, Batch 276, Loss: 0.5856634378433228\n",
      "Epoch 3, Batch 277, Loss: 0.8092179894447327\n",
      "Epoch 3, Batch 278, Loss: 0.8657277822494507\n",
      "Epoch 3, Batch 279, Loss: 0.6546446084976196\n",
      "Epoch 3, Batch 280, Loss: 0.9357749223709106\n",
      "Epoch 3, Batch 281, Loss: 0.7988590002059937\n",
      "Epoch 3, Batch 282, Loss: 0.9304707646369934\n",
      "Epoch 3, Batch 283, Loss: 0.5410630702972412\n",
      "Epoch 3, Batch 284, Loss: 1.0075846910476685\n",
      "Epoch 3, Batch 285, Loss: 0.46015986800193787\n",
      "Epoch 3, Batch 286, Loss: 0.5811494588851929\n",
      "Epoch 3, Batch 287, Loss: 0.8407109975814819\n",
      "Epoch 3, Batch 288, Loss: 0.721148669719696\n",
      "Epoch 3, Batch 289, Loss: 0.706953763961792\n",
      "Epoch 3, Batch 290, Loss: 0.7729274034500122\n",
      "Epoch 3, Batch 291, Loss: 0.7542840242385864\n",
      "Epoch 3, Batch 292, Loss: 0.6930333375930786\n",
      "Epoch 3, Batch 293, Loss: 0.6541315317153931\n",
      "Epoch 3, Batch 294, Loss: 1.0360133647918701\n",
      "Epoch 3, Batch 295, Loss: 0.9425626993179321\n",
      "Epoch 3, Batch 296, Loss: 0.846283495426178\n",
      "Epoch 3, Batch 297, Loss: 0.7904438972473145\n",
      "Epoch 3, Batch 298, Loss: 0.770393967628479\n",
      "Epoch 3, Batch 299, Loss: 1.0707334280014038\n",
      "Epoch 3, Batch 300, Loss: 0.7275716066360474\n",
      "Epoch 3, Batch 301, Loss: 0.8935068249702454\n",
      "Epoch 3, Batch 302, Loss: 0.7643850445747375\n",
      "Epoch 3, Batch 303, Loss: 0.6571952104568481\n",
      "Epoch 3, Batch 304, Loss: 0.7224939465522766\n",
      "Epoch 3, Batch 305, Loss: 0.7049700021743774\n",
      "Epoch 3, Batch 306, Loss: 0.7307558655738831\n",
      "Epoch 3, Batch 307, Loss: 0.8503423929214478\n",
      "Epoch 3, Batch 308, Loss: 0.6770711541175842\n",
      "Epoch 3, Batch 309, Loss: 0.666039764881134\n",
      "Epoch 3, Batch 310, Loss: 0.9668340682983398\n",
      "Epoch 3, Batch 311, Loss: 0.8121626973152161\n",
      "Epoch 3, Batch 312, Loss: 0.6816859245300293\n",
      "Epoch 3, Batch 313, Loss: 0.6659669876098633\n",
      "Epoch 3, Batch 314, Loss: 0.7408749461174011\n",
      "Epoch 3, Batch 315, Loss: 0.896329402923584\n",
      "Epoch 3, Batch 316, Loss: 0.645643949508667\n",
      "Epoch 3, Batch 317, Loss: 0.7742416262626648\n",
      "Epoch 3, Batch 318, Loss: 0.8599065542221069\n",
      "Epoch 3, Batch 319, Loss: 1.0395121574401855\n",
      "Epoch 3, Batch 320, Loss: 0.6489136219024658\n",
      "Epoch 3, Batch 321, Loss: 0.7251673340797424\n",
      "Epoch 3, Batch 322, Loss: 0.654022753238678\n",
      "Epoch 3, Batch 323, Loss: 0.7871085405349731\n",
      "Epoch 3, Batch 324, Loss: 0.7438859939575195\n",
      "Epoch 3, Batch 325, Loss: 0.7101768255233765\n",
      "Epoch 3, Batch 326, Loss: 0.8926891088485718\n",
      "Epoch 3, Batch 327, Loss: 0.6761472225189209\n",
      "Epoch 3, Batch 328, Loss: 0.7926682829856873\n",
      "Epoch 3, Batch 329, Loss: 0.5811046361923218\n",
      "Epoch 3, Batch 330, Loss: 0.9471642374992371\n",
      "Epoch 3, Batch 331, Loss: 0.717084527015686\n",
      "Epoch 3, Batch 332, Loss: 0.779674232006073\n",
      "Epoch 3, Batch 333, Loss: 0.8528201580047607\n",
      "Epoch 3, Batch 334, Loss: 0.735895037651062\n",
      "Epoch 3, Batch 335, Loss: 0.7575870752334595\n",
      "Epoch 3, Batch 336, Loss: 0.70367431640625\n",
      "Epoch 3, Batch 337, Loss: 0.7805268168449402\n",
      "Epoch 3, Batch 338, Loss: 0.8379499912261963\n",
      "Epoch 3, Batch 339, Loss: 0.804614245891571\n",
      "Epoch 3, Batch 340, Loss: 0.8570411801338196\n",
      "Epoch 3, Batch 341, Loss: 1.0597083568572998\n",
      "Epoch 3, Batch 342, Loss: 0.816976010799408\n",
      "Epoch 3, Batch 343, Loss: 1.0030698776245117\n",
      "Epoch 3, Batch 344, Loss: 0.6850032806396484\n",
      "Epoch 3, Batch 345, Loss: 0.7884912490844727\n",
      "Epoch 3, Batch 346, Loss: 0.8647741675376892\n",
      "Epoch 3, Batch 347, Loss: 0.825124204158783\n",
      "Epoch 3, Batch 348, Loss: 0.6228812336921692\n",
      "Epoch 3, Batch 349, Loss: 0.7411674857139587\n",
      "Epoch 3, Batch 350, Loss: 0.9917665719985962\n",
      "Epoch 3, Batch 351, Loss: 0.8365286588668823\n",
      "Epoch 3, Batch 352, Loss: 0.7287684679031372\n",
      "Epoch 3, Batch 353, Loss: 1.0217386484146118\n",
      "Epoch 3, Batch 354, Loss: 0.6407612562179565\n",
      "Epoch 3, Batch 355, Loss: 0.6713948845863342\n",
      "Epoch 3, Batch 356, Loss: 0.7416583299636841\n",
      "Epoch 3, Batch 357, Loss: 0.7964810729026794\n",
      "Epoch 3, Batch 358, Loss: 0.7575576901435852\n",
      "Epoch 3, Batch 359, Loss: 0.983133852481842\n",
      "Epoch 3, Batch 360, Loss: 0.814555287361145\n",
      "Epoch 3, Batch 361, Loss: 0.6284584999084473\n",
      "Epoch 3, Batch 362, Loss: 0.8969947099685669\n",
      "Epoch 3, Batch 363, Loss: 0.709545910358429\n",
      "Epoch 3, Batch 364, Loss: 0.8475059866905212\n",
      "Epoch 3, Batch 365, Loss: 0.888786792755127\n",
      "Epoch 3, Batch 366, Loss: 1.0375045537948608\n",
      "Epoch 3, Batch 367, Loss: 0.6993241906166077\n",
      "Epoch 3, Batch 368, Loss: 0.7655540704727173\n",
      "Epoch 3, Batch 369, Loss: 0.8116534352302551\n",
      "Epoch 3, Batch 370, Loss: 0.8414021730422974\n",
      "Epoch 3, Batch 371, Loss: 0.6093412637710571\n",
      "Epoch 3, Batch 372, Loss: 0.6613268852233887\n",
      "Epoch 3, Batch 373, Loss: 0.7274335026741028\n",
      "Epoch 3, Batch 374, Loss: 0.7692682147026062\n",
      "Epoch 3, Batch 375, Loss: 0.8193811774253845\n",
      "Epoch 3, Batch 376, Loss: 0.5525437593460083\n",
      "Epoch 3, Batch 377, Loss: 0.6443701386451721\n",
      "Epoch 3, Batch 378, Loss: 0.8036196231842041\n",
      "Epoch 3, Batch 379, Loss: 0.6953388452529907\n",
      "Epoch 3, Batch 380, Loss: 0.726945161819458\n",
      "Epoch 3, Batch 381, Loss: 0.7579010725021362\n",
      "Epoch 3, Batch 382, Loss: 0.8346503376960754\n",
      "Epoch 3, Batch 383, Loss: 0.7089477777481079\n",
      "Epoch 3, Batch 384, Loss: 0.7032809257507324\n",
      "Epoch 3, Batch 385, Loss: 0.789583146572113\n",
      "Epoch 3, Batch 386, Loss: 0.6398969888687134\n",
      "Epoch 3, Batch 387, Loss: 0.7007113099098206\n",
      "Epoch 3, Batch 388, Loss: 0.8907614946365356\n",
      "Epoch 3, Batch 389, Loss: 0.6993711590766907\n",
      "Epoch 3, Batch 390, Loss: 0.802111804485321\n",
      "Epoch 3, Batch 391, Loss: 0.8122106790542603\n",
      "Epoch 3, Batch 392, Loss: 0.6512207388877869\n",
      "Epoch 3, Batch 393, Loss: 0.6302189230918884\n",
      "Epoch 3, Batch 394, Loss: 0.5139251351356506\n",
      "Epoch 3, Batch 395, Loss: 0.6989688873291016\n",
      "Epoch 3, Batch 396, Loss: 0.7047983407974243\n",
      "Epoch 3, Batch 397, Loss: 0.6764437556266785\n",
      "Epoch 3, Batch 398, Loss: 0.7200906872749329\n",
      "Epoch 3, Batch 399, Loss: 0.8061217069625854\n",
      "Epoch 3, Batch 400, Loss: 0.8022467494010925\n",
      "Epoch 3, Batch 401, Loss: 0.6668934226036072\n",
      "Epoch 3, Batch 402, Loss: 0.8399239778518677\n",
      "Epoch 3, Batch 403, Loss: 0.6943888664245605\n",
      "Epoch 3, Batch 404, Loss: 0.762356162071228\n",
      "Epoch 3, Batch 405, Loss: 0.7907644510269165\n",
      "Epoch 3, Batch 406, Loss: 0.9167319536209106\n",
      "Epoch 3, Batch 407, Loss: 0.7975894808769226\n",
      "Epoch 3, Batch 408, Loss: 0.648456335067749\n",
      "Epoch 3, Batch 409, Loss: 0.8160144686698914\n",
      "Epoch 3, Batch 410, Loss: 0.7888903617858887\n",
      "Epoch 3, Batch 411, Loss: 0.9710073471069336\n",
      "Epoch 3, Batch 412, Loss: 0.7137860655784607\n",
      "Epoch 3, Batch 413, Loss: 0.7177621722221375\n",
      "Epoch 3, Batch 414, Loss: 0.8148508667945862\n",
      "Epoch 3, Batch 415, Loss: 0.7353256344795227\n",
      "Epoch 3, Batch 416, Loss: 1.0105981826782227\n",
      "Epoch 3, Batch 417, Loss: 0.8452037572860718\n",
      "Epoch 3, Batch 418, Loss: 0.7148674726486206\n",
      "Epoch 3, Batch 419, Loss: 0.927179753780365\n",
      "Epoch 3, Batch 420, Loss: 0.9935569167137146\n",
      "Epoch 3, Batch 421, Loss: 1.0686094760894775\n",
      "Epoch 3, Batch 422, Loss: 0.9367821216583252\n",
      "Epoch 3, Batch 423, Loss: 0.7189055681228638\n",
      "Epoch 3, Batch 424, Loss: 0.6981680989265442\n",
      "Epoch 3, Batch 425, Loss: 0.6706690788269043\n",
      "Epoch 3, Batch 426, Loss: 0.7105585932731628\n",
      "Epoch 3, Batch 427, Loss: 0.6498230695724487\n",
      "Epoch 3, Batch 428, Loss: 0.8282660245895386\n",
      "Epoch 3, Batch 429, Loss: 0.8276437520980835\n",
      "Epoch 3, Batch 430, Loss: 0.8211720585823059\n",
      "Epoch 3, Batch 431, Loss: 0.8200908899307251\n",
      "Epoch 3, Batch 432, Loss: 0.6978608965873718\n",
      "Epoch 3, Batch 433, Loss: 1.0397237539291382\n",
      "Epoch 3, Batch 434, Loss: 0.7999899387359619\n",
      "Epoch 3, Batch 435, Loss: 0.8869131803512573\n",
      "Epoch 3, Batch 436, Loss: 0.8739466667175293\n",
      "Epoch 3, Batch 437, Loss: 0.747260570526123\n",
      "Epoch 3, Batch 438, Loss: 0.6583409905433655\n",
      "Epoch 3, Batch 439, Loss: 0.7244905829429626\n",
      "Epoch 3, Batch 440, Loss: 0.7885980010032654\n",
      "Epoch 3, Batch 441, Loss: 0.9562618732452393\n",
      "Epoch 3, Batch 442, Loss: 0.7205119132995605\n",
      "Epoch 3, Batch 443, Loss: 0.6856622695922852\n",
      "Epoch 3, Batch 444, Loss: 0.7218002676963806\n",
      "Epoch 3, Batch 445, Loss: 0.6734580993652344\n",
      "Epoch 3, Batch 446, Loss: 0.835429847240448\n",
      "Epoch 3, Batch 447, Loss: 0.7824544906616211\n",
      "Epoch 3, Batch 448, Loss: 0.7445631623268127\n",
      "Epoch 3, Batch 449, Loss: 0.8124175667762756\n",
      "Epoch 3, Batch 450, Loss: 1.0347272157669067\n",
      "Epoch 3, Batch 451, Loss: 0.588446319103241\n",
      "Epoch 3, Batch 452, Loss: 0.6475831270217896\n",
      "Epoch 3, Batch 453, Loss: 0.6443630456924438\n",
      "Epoch 3, Batch 454, Loss: 0.6578567028045654\n",
      "Epoch 3, Batch 455, Loss: 0.8547679781913757\n",
      "Epoch 3, Batch 456, Loss: 0.8847982883453369\n",
      "Epoch 3, Batch 457, Loss: 0.6402319669723511\n",
      "Epoch 3, Batch 458, Loss: 0.8079174160957336\n",
      "Epoch 3, Batch 459, Loss: 0.700904905796051\n",
      "Epoch 3, Batch 460, Loss: 0.7396700382232666\n",
      "Epoch 3, Batch 461, Loss: 0.8492914438247681\n",
      "Epoch 3, Batch 462, Loss: 0.8173270225524902\n",
      "Epoch 3, Batch 463, Loss: 0.7266625165939331\n",
      "Epoch 3, Batch 464, Loss: 0.7871267199516296\n",
      "Epoch 3, Batch 465, Loss: 0.8651280999183655\n",
      "Epoch 3, Batch 466, Loss: 0.6846808195114136\n",
      "Epoch 3, Batch 467, Loss: 0.6576412320137024\n",
      "Epoch 3, Batch 468, Loss: 0.7559770345687866\n",
      "Epoch 3, Batch 469, Loss: 0.8144899606704712\n",
      "Epoch 3, Batch 470, Loss: 0.9043878316879272\n",
      "Epoch 3, Batch 471, Loss: 0.7819046974182129\n",
      "Epoch 3, Batch 472, Loss: 0.8014626502990723\n",
      "Epoch 3, Batch 473, Loss: 0.7092786431312561\n",
      "Epoch 3, Batch 474, Loss: 0.6727994680404663\n",
      "Epoch 3, Batch 475, Loss: 0.7332059144973755\n",
      "Epoch 3, Batch 476, Loss: 0.8196609020233154\n",
      "Epoch 3, Batch 477, Loss: 0.6855982542037964\n",
      "Epoch 3, Batch 478, Loss: 0.9344889521598816\n",
      "Epoch 3, Batch 479, Loss: 0.6532458066940308\n",
      "Epoch 3, Batch 480, Loss: 0.8025427460670471\n",
      "Epoch 3, Batch 481, Loss: 0.9489789009094238\n",
      "Epoch 3, Batch 482, Loss: 0.7672477960586548\n",
      "Epoch 3, Batch 483, Loss: 0.7141625881195068\n",
      "Epoch 3, Batch 484, Loss: 0.6929025650024414\n",
      "Epoch 3, Batch 485, Loss: 0.7685709595680237\n",
      "Epoch 3, Batch 486, Loss: 0.7091983556747437\n",
      "Epoch 3, Batch 487, Loss: 0.8708844780921936\n",
      "Epoch 3, Batch 488, Loss: 0.6839542388916016\n",
      "Epoch 3, Batch 489, Loss: 0.9223334789276123\n",
      "Epoch 3, Batch 490, Loss: 0.6377682089805603\n",
      "Epoch 3, Batch 491, Loss: 0.646879255771637\n",
      "Epoch 3, Batch 492, Loss: 0.7738431692123413\n",
      "Epoch 3, Batch 493, Loss: 0.777351438999176\n",
      "Epoch 3, Batch 494, Loss: 0.7773779630661011\n",
      "Epoch 3, Batch 495, Loss: 0.7258046269416809\n",
      "Epoch 3, Batch 496, Loss: 0.7391464710235596\n",
      "Epoch 3, Batch 497, Loss: 0.6352293491363525\n",
      "Epoch 3, Batch 498, Loss: 0.9048179984092712\n",
      "Epoch 3, Batch 499, Loss: 0.8349735140800476\n",
      "Epoch 3, Batch 500, Loss: 0.8584429025650024\n",
      "Epoch 3, Batch 501, Loss: 0.675157368183136\n",
      "Epoch 3, Batch 502, Loss: 0.8660850524902344\n",
      "Epoch 3, Batch 503, Loss: 0.7223161458969116\n",
      "Epoch 3, Batch 504, Loss: 0.775809109210968\n",
      "Epoch 3, Batch 505, Loss: 0.8293826580047607\n",
      "Epoch 3, Batch 506, Loss: 0.8299412131309509\n",
      "Epoch 3, Batch 507, Loss: 0.8259125351905823\n",
      "Epoch 3, Batch 508, Loss: 0.5083274245262146\n",
      "Epoch 3, Batch 509, Loss: 0.8670320510864258\n",
      "Epoch 3, Batch 510, Loss: 1.08446204662323\n",
      "Epoch 3, Batch 511, Loss: 0.6860535740852356\n",
      "Epoch 3, Batch 512, Loss: 0.7791882157325745\n",
      "Epoch 3, Batch 513, Loss: 0.6003999710083008\n",
      "Epoch 3, Batch 514, Loss: 0.8197647333145142\n",
      "Epoch 3, Batch 515, Loss: 0.7835335731506348\n",
      "Epoch 3, Batch 516, Loss: 0.945056676864624\n",
      "Epoch 3, Batch 517, Loss: 0.7882251739501953\n",
      "Epoch 3, Batch 518, Loss: 0.9545157551765442\n",
      "Epoch 3, Batch 519, Loss: 0.7235537171363831\n",
      "Epoch 3, Batch 520, Loss: 0.9820010662078857\n",
      "Epoch 3, Batch 521, Loss: 0.7725561857223511\n",
      "Epoch 3, Batch 522, Loss: 0.5027920007705688\n",
      "Epoch 3, Batch 523, Loss: 0.8361948132514954\n",
      "Epoch 3, Batch 524, Loss: 0.7753735780715942\n",
      "Epoch 3, Batch 525, Loss: 0.9038974046707153\n",
      "Epoch 3, Batch 526, Loss: 0.7536540031433105\n",
      "Epoch 3, Batch 527, Loss: 0.6886386871337891\n",
      "Epoch 3, Batch 528, Loss: 0.6988182067871094\n",
      "Epoch 3, Batch 529, Loss: 0.6561552882194519\n",
      "Epoch 3, Batch 530, Loss: 0.6345374584197998\n",
      "Epoch 3, Batch 531, Loss: 0.7874939441680908\n",
      "Epoch 3, Batch 532, Loss: 0.7913281917572021\n",
      "Epoch 3, Batch 533, Loss: 0.682282567024231\n",
      "Epoch 3, Batch 534, Loss: 0.8306964635848999\n",
      "Epoch 3, Batch 535, Loss: 0.5996826887130737\n",
      "Epoch 3, Batch 536, Loss: 0.6689112186431885\n",
      "Epoch 3, Batch 537, Loss: 0.7982695698738098\n",
      "Epoch 3, Batch 538, Loss: 0.8439539074897766\n",
      "Epoch 3, Batch 539, Loss: 0.6746464967727661\n",
      "Epoch 3, Batch 540, Loss: 0.9372868537902832\n",
      "Epoch 3, Batch 541, Loss: 0.6488167643547058\n",
      "Epoch 3, Batch 542, Loss: 0.7148481607437134\n",
      "Epoch 3, Batch 543, Loss: 0.6913881301879883\n",
      "Epoch 3, Batch 544, Loss: 0.660274863243103\n",
      "Epoch 3, Batch 545, Loss: 0.7790696024894714\n",
      "Epoch 3, Batch 546, Loss: 0.6854540109634399\n",
      "Epoch 3, Batch 547, Loss: 0.9104889631271362\n",
      "Epoch 3, Batch 548, Loss: 0.6070845723152161\n",
      "Epoch 3, Batch 549, Loss: 0.690188467502594\n",
      "Epoch 3, Batch 550, Loss: 0.9255004525184631\n",
      "Epoch 3, Batch 551, Loss: 0.7477660775184631\n",
      "Epoch 3, Batch 552, Loss: 0.8980984687805176\n",
      "Epoch 3, Batch 553, Loss: 0.9398751258850098\n",
      "Epoch 3, Batch 554, Loss: 0.6313344836235046\n",
      "Epoch 3, Batch 555, Loss: 0.7564530968666077\n",
      "Epoch 3, Batch 556, Loss: 0.7285290360450745\n",
      "Epoch 3, Batch 557, Loss: 0.7411938309669495\n",
      "Epoch 3, Batch 558, Loss: 0.7238147258758545\n",
      "Epoch 3, Batch 559, Loss: 0.7486101388931274\n",
      "Epoch 3, Batch 560, Loss: 0.8684526085853577\n",
      "Epoch 3, Batch 561, Loss: 0.616368293762207\n",
      "Epoch 3, Batch 562, Loss: 0.6603203415870667\n",
      "Epoch 3, Batch 563, Loss: 0.6983715295791626\n",
      "Epoch 3, Batch 564, Loss: 0.7441731691360474\n",
      "Epoch 3, Batch 565, Loss: 1.0130385160446167\n",
      "Epoch 3, Batch 566, Loss: 0.5638332962989807\n",
      "Epoch 3, Batch 567, Loss: 1.0512878894805908\n",
      "Epoch 3, Batch 568, Loss: 0.6545898914337158\n",
      "Epoch 3, Batch 569, Loss: 0.7218523621559143\n",
      "Epoch 3, Batch 570, Loss: 0.8722068667411804\n",
      "Epoch 3, Batch 571, Loss: 0.6545828580856323\n",
      "Epoch 3, Batch 572, Loss: 0.7872447371482849\n",
      "Epoch 3, Batch 573, Loss: 0.9971656203269958\n",
      "Epoch 3, Batch 574, Loss: 0.7523303627967834\n",
      "Epoch 3, Batch 575, Loss: 0.6763908863067627\n",
      "Epoch 3, Batch 576, Loss: 0.8887060880661011\n",
      "Epoch 3, Batch 577, Loss: 0.8790795803070068\n",
      "Epoch 3, Batch 578, Loss: 0.5888915657997131\n",
      "Epoch 3, Batch 579, Loss: 0.6690695285797119\n",
      "Epoch 3, Batch 580, Loss: 0.7774480581283569\n",
      "Epoch 3, Batch 581, Loss: 0.605675458908081\n",
      "Epoch 3, Batch 582, Loss: 0.6487098932266235\n",
      "Epoch 3, Batch 583, Loss: 0.7359206676483154\n",
      "Epoch 3, Batch 584, Loss: 0.6871561408042908\n",
      "Epoch 3, Batch 585, Loss: 0.574206531047821\n",
      "Epoch 3, Batch 586, Loss: 0.6547375321388245\n",
      "Epoch 3, Batch 587, Loss: 0.9254686236381531\n",
      "Epoch 3, Batch 588, Loss: 0.7575272917747498\n",
      "Epoch 3, Batch 589, Loss: 0.9436795115470886\n",
      "Epoch 3, Batch 590, Loss: 0.6795989274978638\n",
      "Epoch 3, Batch 591, Loss: 0.8131300210952759\n",
      "Epoch 3, Batch 592, Loss: 0.5959832072257996\n",
      "Epoch 3, Batch 593, Loss: 0.7246279120445251\n",
      "Epoch 3, Batch 594, Loss: 0.7149556875228882\n",
      "Epoch 3, Batch 595, Loss: 0.6651493906974792\n",
      "Epoch 3, Batch 596, Loss: 0.5524044036865234\n",
      "Epoch 3, Batch 597, Loss: 0.531397819519043\n",
      "Epoch 3, Batch 598, Loss: 0.8978895545005798\n",
      "Epoch 3, Batch 599, Loss: 0.6541510820388794\n",
      "Epoch 3, Batch 600, Loss: 0.6154620051383972\n",
      "Epoch 3, Batch 601, Loss: 0.5850030779838562\n",
      "Epoch 3, Batch 602, Loss: 1.0165492296218872\n",
      "Epoch 3, Batch 603, Loss: 0.8761875033378601\n",
      "Epoch 3, Batch 604, Loss: 0.680955171585083\n",
      "Epoch 3, Batch 605, Loss: 1.150913119316101\n",
      "Epoch 3, Batch 606, Loss: 0.864690363407135\n",
      "Epoch 3, Batch 607, Loss: 0.6755380630493164\n",
      "Epoch 3, Batch 608, Loss: 0.6055651903152466\n",
      "Epoch 3, Batch 609, Loss: 0.6783524751663208\n",
      "Epoch 3, Batch 610, Loss: 0.7071503400802612\n",
      "Epoch 3, Batch 611, Loss: 0.612907886505127\n",
      "Epoch 3, Batch 612, Loss: 0.7936004996299744\n",
      "Epoch 3, Batch 613, Loss: 0.8221828937530518\n",
      "Epoch 3, Batch 614, Loss: 0.6219083070755005\n",
      "Epoch 3, Batch 615, Loss: 1.0046101808547974\n",
      "Epoch 3, Batch 616, Loss: 0.790790319442749\n",
      "Epoch 3, Batch 617, Loss: 0.7973375916481018\n",
      "Epoch 3, Batch 618, Loss: 0.8697516918182373\n",
      "Epoch 3, Batch 619, Loss: 0.7482731938362122\n",
      "Epoch 3, Batch 620, Loss: 0.7626724243164062\n",
      "Epoch 3, Batch 621, Loss: 0.6333813071250916\n",
      "Epoch 3, Batch 622, Loss: 0.6969160437583923\n",
      "Epoch 3, Batch 623, Loss: 0.8619730472564697\n",
      "Epoch 3, Batch 624, Loss: 0.7493609189987183\n",
      "Epoch 3, Batch 625, Loss: 0.9749788045883179\n",
      "Epoch 3, Batch 626, Loss: 0.6989520788192749\n",
      "Epoch 3, Batch 627, Loss: 0.7326112389564514\n",
      "Epoch 3, Batch 628, Loss: 0.6993990540504456\n",
      "Epoch 3, Batch 629, Loss: 0.6889371871948242\n",
      "Epoch 3, Batch 630, Loss: 0.6305410861968994\n",
      "Epoch 3, Batch 631, Loss: 0.6992354989051819\n",
      "Epoch 3, Batch 632, Loss: 0.7484641075134277\n",
      "Epoch 3, Batch 633, Loss: 0.6609712839126587\n",
      "Epoch 3, Batch 634, Loss: 0.6219812631607056\n",
      "Epoch 3, Batch 635, Loss: 0.9083783626556396\n",
      "Epoch 3, Batch 636, Loss: 0.5797634720802307\n",
      "Epoch 3, Batch 637, Loss: 0.7553659677505493\n",
      "Epoch 3, Batch 638, Loss: 0.7540063261985779\n",
      "Epoch 3, Batch 639, Loss: 0.6947155594825745\n",
      "Epoch 3, Batch 640, Loss: 0.7364145517349243\n",
      "Epoch 3, Batch 641, Loss: 0.6597892045974731\n",
      "Epoch 3, Batch 642, Loss: 0.8220900893211365\n",
      "Epoch 3, Batch 643, Loss: 0.6582646369934082\n",
      "Epoch 3, Batch 644, Loss: 0.5920753479003906\n",
      "Epoch 3, Batch 645, Loss: 0.8210984468460083\n",
      "Epoch 3, Batch 646, Loss: 0.7037690281867981\n",
      "Epoch 3, Batch 647, Loss: 0.8212260603904724\n",
      "Epoch 3, Batch 648, Loss: 0.6755063533782959\n",
      "Epoch 3, Batch 649, Loss: 0.8370324373245239\n",
      "Epoch 3, Batch 650, Loss: 0.7667298913002014\n",
      "Epoch 3, Batch 651, Loss: 0.6971568465232849\n",
      "Epoch 3, Batch 652, Loss: 0.7159170508384705\n",
      "Epoch 3, Batch 653, Loss: 0.75621497631073\n",
      "Epoch 3, Batch 654, Loss: 0.699657678604126\n",
      "Epoch 3, Batch 655, Loss: 0.7594608664512634\n",
      "Epoch 3, Batch 656, Loss: 0.5893495082855225\n",
      "Epoch 3, Batch 657, Loss: 0.830573558807373\n",
      "Epoch 3, Batch 658, Loss: 0.6653792858123779\n",
      "Epoch 3, Batch 659, Loss: 0.71654212474823\n",
      "Epoch 3, Batch 660, Loss: 0.6355746388435364\n",
      "Epoch 3, Batch 661, Loss: 0.7912536859512329\n",
      "Epoch 3, Batch 662, Loss: 0.6962181329727173\n",
      "Epoch 3, Batch 663, Loss: 0.8470299243927002\n",
      "Epoch 3, Batch 664, Loss: 0.8860716819763184\n",
      "Epoch 3, Batch 665, Loss: 0.689592719078064\n",
      "Epoch 3, Batch 666, Loss: 0.7335337400436401\n",
      "Epoch 3, Batch 667, Loss: 0.6902675628662109\n",
      "Epoch 3, Batch 668, Loss: 1.1281839609146118\n",
      "Epoch 3, Batch 669, Loss: 0.6999883651733398\n",
      "Epoch 3, Batch 670, Loss: 0.6117050051689148\n",
      "Epoch 3, Batch 671, Loss: 0.7443917393684387\n",
      "Epoch 3, Batch 672, Loss: 0.7147270441055298\n",
      "Epoch 3, Batch 673, Loss: 0.808882474899292\n",
      "Epoch 3, Batch 674, Loss: 0.7689164876937866\n",
      "Epoch 3, Batch 675, Loss: 0.7211878895759583\n",
      "Epoch 3, Batch 676, Loss: 0.7087881565093994\n",
      "Epoch 3, Batch 677, Loss: 0.616590678691864\n",
      "Epoch 3, Batch 678, Loss: 0.696993887424469\n",
      "Epoch 3, Batch 679, Loss: 0.6016307473182678\n",
      "Epoch 3, Batch 680, Loss: 0.9149336814880371\n",
      "Epoch 3, Batch 681, Loss: 0.7473382949829102\n",
      "Epoch 3, Batch 682, Loss: 0.5561606884002686\n",
      "Epoch 3, Batch 683, Loss: 0.8127586841583252\n",
      "Epoch 3, Batch 684, Loss: 0.8014805912971497\n",
      "Epoch 3, Batch 685, Loss: 0.8938242197036743\n",
      "Epoch 3, Batch 686, Loss: 0.7315304279327393\n",
      "Epoch 3, Batch 687, Loss: 0.9296121001243591\n",
      "Epoch 3, Batch 688, Loss: 0.6552311778068542\n",
      "Epoch 3, Batch 689, Loss: 0.7704507112503052\n",
      "Epoch 3, Batch 690, Loss: 0.7313247919082642\n",
      "Epoch 3, Batch 691, Loss: 0.6950420141220093\n",
      "Epoch 3, Batch 692, Loss: 0.8497446179389954\n",
      "Epoch 3, Batch 693, Loss: 0.7092581391334534\n",
      "Epoch 3, Batch 694, Loss: 0.6965097188949585\n",
      "Epoch 3, Batch 695, Loss: 0.660851776599884\n",
      "Epoch 3, Batch 696, Loss: 0.8024075031280518\n",
      "Epoch 3, Batch 697, Loss: 0.6973282098770142\n",
      "Epoch 3, Batch 698, Loss: 0.7548626661300659\n",
      "Epoch 3, Batch 699, Loss: 1.046044111251831\n",
      "Epoch 3, Batch 700, Loss: 0.6491549015045166\n",
      "Epoch 3, Batch 701, Loss: 0.8632234334945679\n",
      "Epoch 3, Batch 702, Loss: 0.9297189712524414\n",
      "Epoch 3, Batch 703, Loss: 0.5154284834861755\n",
      "Epoch 3, Batch 704, Loss: 0.7220124006271362\n",
      "Epoch 3, Batch 705, Loss: 0.784102201461792\n",
      "Epoch 3, Batch 706, Loss: 0.7431482672691345\n",
      "Epoch 3, Batch 707, Loss: 0.8525773286819458\n",
      "Epoch 3, Batch 708, Loss: 0.6267104744911194\n",
      "Epoch 3, Batch 709, Loss: 0.6130150556564331\n",
      "Epoch 3, Batch 710, Loss: 0.763740062713623\n",
      "Epoch 3, Batch 711, Loss: 0.8286644816398621\n",
      "Epoch 3, Batch 712, Loss: 0.6716775894165039\n",
      "Epoch 3, Batch 713, Loss: 0.7505057454109192\n",
      "Epoch 3, Batch 714, Loss: 0.7736585736274719\n",
      "Epoch 3, Batch 715, Loss: 0.8665425181388855\n",
      "Epoch 3, Batch 716, Loss: 0.6546555161476135\n",
      "Epoch 3, Batch 717, Loss: 0.9636009931564331\n",
      "Epoch 3, Batch 718, Loss: 0.6445906162261963\n",
      "Epoch 3, Batch 719, Loss: 0.633453905582428\n",
      "Epoch 3, Batch 720, Loss: 0.6742928624153137\n",
      "Epoch 3, Batch 721, Loss: 0.6918909549713135\n",
      "Epoch 3, Batch 722, Loss: 0.7240051627159119\n",
      "Epoch 3, Batch 723, Loss: 0.7893196940422058\n",
      "Epoch 3, Batch 724, Loss: 0.6994438171386719\n",
      "Epoch 3, Batch 725, Loss: 0.8886964321136475\n",
      "Epoch 3, Batch 726, Loss: 0.6764355301856995\n",
      "Epoch 3, Batch 727, Loss: 0.7078858613967896\n",
      "Epoch 3, Batch 728, Loss: 0.6490511298179626\n",
      "Epoch 3, Batch 729, Loss: 0.659788191318512\n",
      "Epoch 3, Batch 730, Loss: 0.7768242359161377\n",
      "Epoch 3, Batch 731, Loss: 1.1306267976760864\n",
      "Epoch 3, Batch 732, Loss: 0.6547279357910156\n",
      "Epoch 3, Batch 733, Loss: 0.7867424488067627\n",
      "Epoch 3, Batch 734, Loss: 0.6155844926834106\n",
      "Epoch 3, Batch 735, Loss: 0.779424250125885\n",
      "Epoch 3, Batch 736, Loss: 0.9602770209312439\n",
      "Epoch 3, Batch 737, Loss: 0.7316144704818726\n",
      "Epoch 3, Batch 738, Loss: 0.8383201956748962\n",
      "Epoch 3, Batch 739, Loss: 0.6699971556663513\n",
      "Epoch 3, Batch 740, Loss: 0.6761376857757568\n",
      "Epoch 3, Batch 741, Loss: 1.0729784965515137\n",
      "Epoch 3, Batch 742, Loss: 0.681850790977478\n",
      "Epoch 3, Batch 743, Loss: 0.8541951179504395\n",
      "Epoch 3, Batch 744, Loss: 0.9024271965026855\n",
      "Epoch 3, Batch 745, Loss: 0.734695553779602\n",
      "Epoch 3, Batch 746, Loss: 0.8462859988212585\n",
      "Epoch 3, Batch 747, Loss: 0.7217240333557129\n",
      "Epoch 3, Batch 748, Loss: 0.734855592250824\n",
      "Epoch 3, Batch 749, Loss: 0.6542105674743652\n",
      "Epoch 3, Batch 750, Loss: 0.7283182144165039\n",
      "Epoch 3, Batch 751, Loss: 0.6231240034103394\n",
      "Epoch 3, Batch 752, Loss: 0.5900943875312805\n",
      "Epoch 3, Batch 753, Loss: 0.7396506667137146\n",
      "Epoch 3, Batch 754, Loss: 0.5995847582817078\n",
      "Epoch 3, Batch 755, Loss: 0.8159676790237427\n",
      "Epoch 3, Batch 756, Loss: 0.5710031390190125\n",
      "Epoch 3, Batch 757, Loss: 0.6204462647438049\n",
      "Epoch 3, Batch 758, Loss: 0.7616667151451111\n",
      "Epoch 3, Batch 759, Loss: 0.7954556941986084\n",
      "Epoch 3, Batch 760, Loss: 0.8327382802963257\n",
      "Epoch 3, Batch 761, Loss: 0.8199926018714905\n",
      "Epoch 3, Batch 762, Loss: 0.745427668094635\n",
      "Epoch 3, Batch 763, Loss: 0.6151669025421143\n",
      "Epoch 3, Batch 764, Loss: 0.6531263589859009\n",
      "Epoch 3, Batch 765, Loss: 0.8995410799980164\n",
      "Epoch 3, Batch 766, Loss: 0.8243268132209778\n",
      "Epoch 3, Batch 767, Loss: 0.791318416595459\n",
      "Epoch 3, Batch 768, Loss: 0.8804616332054138\n",
      "Epoch 3, Batch 769, Loss: 0.6942352056503296\n",
      "Epoch 3, Batch 770, Loss: 0.6539076566696167\n",
      "Epoch 3, Batch 771, Loss: 0.9432590007781982\n",
      "Epoch 3, Batch 772, Loss: 0.8525148630142212\n",
      "Epoch 3, Batch 773, Loss: 0.7168049812316895\n",
      "Epoch 3, Batch 774, Loss: 0.6348738074302673\n",
      "Epoch 3, Batch 775, Loss: 0.7289563417434692\n",
      "Epoch 3, Batch 776, Loss: 1.0586885213851929\n",
      "Epoch 3, Batch 777, Loss: 0.7542104721069336\n",
      "Epoch 3, Batch 778, Loss: 0.8305563926696777\n",
      "Epoch 3, Batch 779, Loss: 0.8074306845664978\n",
      "Epoch 3, Batch 780, Loss: 0.7152882814407349\n",
      "Epoch 3, Batch 781, Loss: 0.7039778232574463\n",
      "Epoch 3, Batch 782, Loss: 0.6828681826591492\n",
      "Epoch 3, Batch 783, Loss: 0.7261950969696045\n",
      "Epoch 3, Batch 784, Loss: 0.7234978675842285\n",
      "Epoch 3, Batch 785, Loss: 1.0487380027770996\n",
      "Epoch 3, Batch 786, Loss: 0.9599655866622925\n",
      "Epoch 3, Batch 787, Loss: 0.6294227242469788\n",
      "Epoch 3, Batch 788, Loss: 0.5896916389465332\n",
      "Epoch 3, Batch 789, Loss: 1.099440336227417\n",
      "Epoch 3, Batch 790, Loss: 0.7169601917266846\n",
      "Epoch 3, Batch 791, Loss: 0.8414844274520874\n",
      "Epoch 3, Batch 792, Loss: 0.7684963345527649\n",
      "Epoch 3, Batch 793, Loss: 0.858453094959259\n",
      "Epoch 3, Batch 794, Loss: 0.6917220950126648\n",
      "Epoch 3, Batch 795, Loss: 0.6646159291267395\n",
      "Epoch 3, Batch 796, Loss: 0.6336355805397034\n",
      "Epoch 3, Batch 797, Loss: 0.8194181323051453\n",
      "Epoch 3, Batch 798, Loss: 0.6957366466522217\n",
      "Epoch 3, Batch 799, Loss: 0.6787930727005005\n",
      "Epoch 3, Batch 800, Loss: 0.7667070627212524\n",
      "Epoch 3, Batch 801, Loss: 0.8496531248092651\n",
      "Epoch 3, Batch 802, Loss: 0.8292322158813477\n",
      "Epoch 3, Batch 803, Loss: 0.6409186720848083\n",
      "Epoch 3, Batch 804, Loss: 0.7077791094779968\n",
      "Epoch 3, Batch 805, Loss: 0.7896191477775574\n",
      "Epoch 3, Batch 806, Loss: 0.7979143857955933\n",
      "Epoch 3, Batch 807, Loss: 0.7023544311523438\n",
      "Epoch 3, Batch 808, Loss: 0.5154073238372803\n",
      "Epoch 3, Batch 809, Loss: 0.7010421752929688\n",
      "Epoch 3, Batch 810, Loss: 0.798006534576416\n",
      "Epoch 3, Batch 811, Loss: 0.6805245280265808\n",
      "Epoch 3, Batch 812, Loss: 0.6454982161521912\n",
      "Epoch 3, Batch 813, Loss: 0.7178174257278442\n",
      "Epoch 3, Batch 814, Loss: 0.6273899078369141\n",
      "Epoch 3, Batch 815, Loss: 0.6123638153076172\n",
      "Epoch 3, Batch 816, Loss: 0.6317836046218872\n",
      "Epoch 3, Batch 817, Loss: 0.6537969708442688\n",
      "Epoch 3, Batch 818, Loss: 0.7826801538467407\n",
      "Epoch 3, Batch 819, Loss: 0.6962558627128601\n",
      "Epoch 3, Batch 820, Loss: 0.5797524452209473\n",
      "Epoch 3, Batch 821, Loss: 0.7700897455215454\n",
      "Epoch 3, Batch 822, Loss: 0.663285493850708\n",
      "Epoch 3, Batch 823, Loss: 0.7750177383422852\n",
      "Epoch 3, Batch 824, Loss: 0.9015454649925232\n",
      "Epoch 3, Batch 825, Loss: 0.8042537569999695\n",
      "Epoch 3, Batch 826, Loss: 0.8007323741912842\n",
      "Epoch 3, Batch 827, Loss: 0.6764811277389526\n",
      "Epoch 3, Batch 828, Loss: 0.7936064600944519\n",
      "Epoch 3, Batch 829, Loss: 0.6460973620414734\n",
      "Epoch 3, Batch 830, Loss: 0.7394596934318542\n",
      "Epoch 3, Batch 831, Loss: 0.7726498246192932\n",
      "Epoch 3, Batch 832, Loss: 0.6548470258712769\n",
      "Epoch 3, Batch 833, Loss: 0.5995571613311768\n",
      "Epoch 3, Batch 834, Loss: 0.6263970732688904\n",
      "Epoch 3, Batch 835, Loss: 0.8140541315078735\n",
      "Epoch 3, Batch 836, Loss: 0.6599379181861877\n",
      "Epoch 3, Batch 837, Loss: 0.7442278265953064\n",
      "Epoch 3, Batch 838, Loss: 0.8192460536956787\n",
      "Epoch 3, Batch 839, Loss: 0.6172316670417786\n",
      "Epoch 3, Batch 840, Loss: 0.7021027207374573\n",
      "Epoch 3, Batch 841, Loss: 0.6867724061012268\n",
      "Epoch 3, Batch 842, Loss: 0.7984236478805542\n",
      "Epoch 3, Batch 843, Loss: 0.7136529684066772\n",
      "Epoch 3, Batch 844, Loss: 0.8815313577651978\n",
      "Epoch 3, Batch 845, Loss: 0.7133728861808777\n",
      "Epoch 3, Batch 846, Loss: 0.6898589134216309\n",
      "Epoch 3, Batch 847, Loss: 0.7834631204605103\n",
      "Epoch 3, Batch 848, Loss: 0.686896800994873\n",
      "Epoch 3, Batch 849, Loss: 0.7893190979957581\n",
      "Epoch 3, Batch 850, Loss: 0.6364349126815796\n",
      "Epoch 3, Batch 851, Loss: 0.7634525895118713\n",
      "Epoch 3, Batch 852, Loss: 0.7326540350914001\n",
      "Epoch 3, Batch 853, Loss: 0.6963022947311401\n",
      "Epoch 3, Batch 854, Loss: 0.8963724970817566\n",
      "Epoch 3, Batch 855, Loss: 0.5332483649253845\n",
      "Epoch 3, Batch 856, Loss: 0.6254295110702515\n",
      "Epoch 3, Batch 857, Loss: 0.5587062835693359\n",
      "Epoch 3, Batch 858, Loss: 0.870222270488739\n",
      "Epoch 3, Batch 859, Loss: 0.641472339630127\n",
      "Epoch 3, Batch 860, Loss: 0.6816035509109497\n",
      "Epoch 3, Batch 861, Loss: 0.7344783544540405\n",
      "Epoch 3, Batch 862, Loss: 0.5648921132087708\n",
      "Epoch 3, Batch 863, Loss: 0.6603467464447021\n",
      "Epoch 3, Batch 864, Loss: 0.5579225420951843\n",
      "Epoch 3, Batch 865, Loss: 0.633013904094696\n",
      "Epoch 3, Batch 866, Loss: 0.6744375824928284\n",
      "Epoch 3, Batch 867, Loss: 0.872870147228241\n",
      "Epoch 3, Batch 868, Loss: 0.6572221517562866\n",
      "Epoch 3, Batch 869, Loss: 0.689647912979126\n",
      "Epoch 3, Batch 870, Loss: 0.5068351030349731\n",
      "Epoch 3, Batch 871, Loss: 0.5049376487731934\n",
      "Epoch 3, Batch 872, Loss: 0.8362933993339539\n",
      "Epoch 3, Batch 873, Loss: 0.7176582217216492\n",
      "Epoch 3, Batch 874, Loss: 0.6332223415374756\n",
      "Epoch 3, Batch 875, Loss: 0.804548442363739\n",
      "Epoch 3, Batch 876, Loss: 0.707273006439209\n",
      "Epoch 3, Batch 877, Loss: 0.6908893585205078\n",
      "Epoch 3, Batch 878, Loss: 0.6716830134391785\n",
      "Epoch 3, Batch 879, Loss: 0.560062825679779\n",
      "Epoch 3, Batch 880, Loss: 0.7694004774093628\n",
      "Epoch 3, Batch 881, Loss: 0.663578987121582\n",
      "Epoch 3, Batch 882, Loss: 0.5198856592178345\n",
      "Epoch 3, Batch 883, Loss: 0.793558657169342\n",
      "Epoch 3, Batch 884, Loss: 0.8478730916976929\n",
      "Epoch 3, Batch 885, Loss: 0.6418120861053467\n",
      "Epoch 3, Batch 886, Loss: 0.6620416045188904\n",
      "Epoch 3, Batch 887, Loss: 0.7796424627304077\n",
      "Epoch 3, Batch 888, Loss: 0.5952168703079224\n",
      "Epoch 3, Batch 889, Loss: 0.7323163747787476\n",
      "Epoch 3, Batch 890, Loss: 0.6955723166465759\n",
      "Epoch 3, Batch 891, Loss: 0.6219300627708435\n",
      "Epoch 3, Batch 892, Loss: 0.8209642171859741\n",
      "Epoch 3, Batch 893, Loss: 0.7225704789161682\n",
      "Epoch 3, Batch 894, Loss: 0.7877784371376038\n",
      "Epoch 3, Batch 895, Loss: 0.7897041440010071\n",
      "Epoch 3, Batch 896, Loss: 0.5806204676628113\n",
      "Epoch 3, Batch 897, Loss: 0.5797852277755737\n",
      "Epoch 3, Batch 898, Loss: 0.8655940294265747\n",
      "Epoch 3, Batch 899, Loss: 0.5778250694274902\n",
      "Epoch 3, Batch 900, Loss: 0.7349037528038025\n",
      "Epoch 3, Batch 901, Loss: 0.7422827482223511\n",
      "Epoch 3, Batch 902, Loss: 0.6864163875579834\n",
      "Epoch 3, Batch 903, Loss: 0.724374532699585\n",
      "Epoch 3, Batch 904, Loss: 0.6777793169021606\n",
      "Epoch 3, Batch 905, Loss: 0.6321790218353271\n",
      "Epoch 3, Batch 906, Loss: 0.5141632556915283\n",
      "Epoch 3, Batch 907, Loss: 0.599518895149231\n",
      "Epoch 3, Batch 908, Loss: 0.619585394859314\n",
      "Epoch 3, Batch 909, Loss: 0.6378788948059082\n",
      "Epoch 3, Batch 910, Loss: 0.8523160815238953\n",
      "Epoch 3, Batch 911, Loss: 0.6502038240432739\n",
      "Epoch 3, Batch 912, Loss: 0.6487376689910889\n",
      "Epoch 3, Batch 913, Loss: 0.6367625594139099\n",
      "Epoch 3, Batch 914, Loss: 0.7466402649879456\n",
      "Epoch 3, Batch 915, Loss: 0.781408429145813\n",
      "Epoch 3, Batch 916, Loss: 0.904349684715271\n",
      "Epoch 3, Batch 917, Loss: 0.7222720384597778\n",
      "Epoch 3, Batch 918, Loss: 0.6817576885223389\n",
      "Epoch 3, Batch 919, Loss: 0.6882257461547852\n",
      "Epoch 3, Batch 920, Loss: 0.8272838592529297\n",
      "Epoch 3, Batch 921, Loss: 0.7544244527816772\n",
      "Epoch 3, Batch 922, Loss: 0.7455801367759705\n",
      "Epoch 3, Batch 923, Loss: 0.8158108592033386\n",
      "Epoch 3, Batch 924, Loss: 0.615652322769165\n",
      "Epoch 3, Batch 925, Loss: 0.7962892651557922\n",
      "Epoch 3, Batch 926, Loss: 0.7907798886299133\n",
      "Epoch 3, Batch 927, Loss: 0.6302343010902405\n",
      "Epoch 3, Batch 928, Loss: 0.7416512370109558\n",
      "Epoch 3, Batch 929, Loss: 0.5882009267807007\n",
      "Epoch 3, Batch 930, Loss: 0.5628765821456909\n",
      "Epoch 3, Batch 931, Loss: 0.5906253457069397\n",
      "Epoch 3, Batch 932, Loss: 0.8709044456481934\n",
      "Epoch 3, Batch 933, Loss: 0.7442046403884888\n",
      "Epoch 3, Batch 934, Loss: 0.6776837110519409\n",
      "Epoch 3, Batch 935, Loss: 0.7164373993873596\n",
      "Epoch 3, Batch 936, Loss: 0.7754825949668884\n",
      "Epoch 3, Batch 937, Loss: 0.6361117362976074\n",
      "Epoch 3, Batch 938, Loss: 0.518403947353363\n",
      "Accuracy of train set: 0.7120166666666666\n",
      "Epoch 3, Batch 1, Test Loss: 0.702280580997467\n",
      "Epoch 3, Batch 2, Test Loss: 0.5996600389480591\n",
      "Epoch 3, Batch 3, Test Loss: 0.7915193438529968\n",
      "Epoch 3, Batch 4, Test Loss: 0.6101040244102478\n",
      "Epoch 3, Batch 5, Test Loss: 0.6174301505088806\n",
      "Epoch 3, Batch 6, Test Loss: 0.7485034465789795\n",
      "Epoch 3, Batch 7, Test Loss: 0.8157630562782288\n",
      "Epoch 3, Batch 8, Test Loss: 0.935029923915863\n",
      "Epoch 3, Batch 9, Test Loss: 0.537378191947937\n",
      "Epoch 3, Batch 10, Test Loss: 0.7367309927940369\n",
      "Epoch 3, Batch 11, Test Loss: 0.6119484901428223\n",
      "Epoch 3, Batch 12, Test Loss: 0.9688867926597595\n",
      "Epoch 3, Batch 13, Test Loss: 0.5973931550979614\n",
      "Epoch 3, Batch 14, Test Loss: 0.6820310354232788\n",
      "Epoch 3, Batch 15, Test Loss: 0.7115758657455444\n",
      "Epoch 3, Batch 16, Test Loss: 0.8168830275535583\n",
      "Epoch 3, Batch 17, Test Loss: 0.6110059022903442\n",
      "Epoch 3, Batch 18, Test Loss: 0.6743606328964233\n",
      "Epoch 3, Batch 19, Test Loss: 0.6172651648521423\n",
      "Epoch 3, Batch 20, Test Loss: 0.7685059905052185\n",
      "Epoch 3, Batch 21, Test Loss: 0.5860297679901123\n",
      "Epoch 3, Batch 22, Test Loss: 0.6913872361183167\n",
      "Epoch 3, Batch 23, Test Loss: 0.8020849227905273\n",
      "Epoch 3, Batch 24, Test Loss: 0.8667197227478027\n",
      "Epoch 3, Batch 25, Test Loss: 0.5831462740898132\n",
      "Epoch 3, Batch 26, Test Loss: 0.5645794868469238\n",
      "Epoch 3, Batch 27, Test Loss: 0.563022792339325\n",
      "Epoch 3, Batch 28, Test Loss: 0.7566205263137817\n",
      "Epoch 3, Batch 29, Test Loss: 0.7589859366416931\n",
      "Epoch 3, Batch 30, Test Loss: 0.6715821027755737\n",
      "Epoch 3, Batch 31, Test Loss: 0.6429166197776794\n",
      "Epoch 3, Batch 32, Test Loss: 0.6874887347221375\n",
      "Epoch 3, Batch 33, Test Loss: 0.6954711675643921\n",
      "Epoch 3, Batch 34, Test Loss: 0.6005052924156189\n",
      "Epoch 3, Batch 35, Test Loss: 0.7072393298149109\n",
      "Epoch 3, Batch 36, Test Loss: 0.7209799885749817\n",
      "Epoch 3, Batch 37, Test Loss: 0.7314878702163696\n",
      "Epoch 3, Batch 38, Test Loss: 0.6705334186553955\n",
      "Epoch 3, Batch 39, Test Loss: 0.8114819526672363\n",
      "Epoch 3, Batch 40, Test Loss: 0.6694303750991821\n",
      "Epoch 3, Batch 41, Test Loss: 0.658825159072876\n",
      "Epoch 3, Batch 42, Test Loss: 1.0384362936019897\n",
      "Epoch 3, Batch 43, Test Loss: 0.7583521604537964\n",
      "Epoch 3, Batch 44, Test Loss: 0.7780608534812927\n",
      "Epoch 3, Batch 45, Test Loss: 0.9294105768203735\n",
      "Epoch 3, Batch 46, Test Loss: 0.7314397692680359\n",
      "Epoch 3, Batch 47, Test Loss: 0.7237050533294678\n",
      "Epoch 3, Batch 48, Test Loss: 0.6326295137405396\n",
      "Epoch 3, Batch 49, Test Loss: 0.8277314305305481\n",
      "Epoch 3, Batch 50, Test Loss: 0.6864104270935059\n",
      "Epoch 3, Batch 51, Test Loss: 0.6657347679138184\n",
      "Epoch 3, Batch 52, Test Loss: 0.6529743671417236\n",
      "Epoch 3, Batch 53, Test Loss: 0.6804276704788208\n",
      "Epoch 3, Batch 54, Test Loss: 0.6405655741691589\n",
      "Epoch 3, Batch 55, Test Loss: 0.7581577301025391\n",
      "Epoch 3, Batch 56, Test Loss: 0.5869605541229248\n",
      "Epoch 3, Batch 57, Test Loss: 0.7052111625671387\n",
      "Epoch 3, Batch 58, Test Loss: 0.7141402363777161\n",
      "Epoch 3, Batch 59, Test Loss: 0.6991415619850159\n",
      "Epoch 3, Batch 60, Test Loss: 0.6377089023590088\n",
      "Epoch 3, Batch 61, Test Loss: 0.6934155225753784\n",
      "Epoch 3, Batch 62, Test Loss: 0.5738646984100342\n",
      "Epoch 3, Batch 63, Test Loss: 0.6471987962722778\n",
      "Epoch 3, Batch 64, Test Loss: 0.8545840978622437\n",
      "Epoch 3, Batch 65, Test Loss: 0.6793586015701294\n",
      "Epoch 3, Batch 66, Test Loss: 0.6318061947822571\n",
      "Epoch 3, Batch 67, Test Loss: 0.7093250155448914\n",
      "Epoch 3, Batch 68, Test Loss: 0.7472590208053589\n",
      "Epoch 3, Batch 69, Test Loss: 0.7264041900634766\n",
      "Epoch 3, Batch 70, Test Loss: 0.76875239610672\n",
      "Epoch 3, Batch 71, Test Loss: 0.6178545951843262\n",
      "Epoch 3, Batch 72, Test Loss: 0.6177473068237305\n",
      "Epoch 3, Batch 73, Test Loss: 0.7463623285293579\n",
      "Epoch 3, Batch 74, Test Loss: 0.8011536598205566\n",
      "Epoch 3, Batch 75, Test Loss: 0.5345315337181091\n",
      "Epoch 3, Batch 76, Test Loss: 0.7128604054450989\n",
      "Epoch 3, Batch 77, Test Loss: 0.5864039659500122\n",
      "Epoch 3, Batch 78, Test Loss: 0.7174205183982849\n",
      "Epoch 3, Batch 79, Test Loss: 0.6320757865905762\n",
      "Epoch 3, Batch 80, Test Loss: 0.7389098405838013\n",
      "Epoch 3, Batch 81, Test Loss: 0.6018518209457397\n",
      "Epoch 3, Batch 82, Test Loss: 0.5581462383270264\n",
      "Epoch 3, Batch 83, Test Loss: 0.7117929458618164\n",
      "Epoch 3, Batch 84, Test Loss: 0.7395516633987427\n",
      "Epoch 3, Batch 85, Test Loss: 0.5586056709289551\n",
      "Epoch 3, Batch 86, Test Loss: 0.685817301273346\n",
      "Epoch 3, Batch 87, Test Loss: 0.663074791431427\n",
      "Epoch 3, Batch 88, Test Loss: 0.5789164304733276\n",
      "Epoch 3, Batch 89, Test Loss: 0.7351382374763489\n",
      "Epoch 3, Batch 90, Test Loss: 1.0128085613250732\n",
      "Epoch 3, Batch 91, Test Loss: 0.6212249994277954\n",
      "Epoch 3, Batch 92, Test Loss: 0.9722033739089966\n",
      "Epoch 3, Batch 93, Test Loss: 0.7694331407546997\n",
      "Epoch 3, Batch 94, Test Loss: 0.6370978355407715\n",
      "Epoch 3, Batch 95, Test Loss: 0.5459422469139099\n",
      "Epoch 3, Batch 96, Test Loss: 0.7020353078842163\n",
      "Epoch 3, Batch 97, Test Loss: 1.0326151847839355\n",
      "Epoch 3, Batch 98, Test Loss: 0.8428844213485718\n",
      "Epoch 3, Batch 99, Test Loss: 0.9471979141235352\n",
      "Epoch 3, Batch 100, Test Loss: 0.7632585167884827\n",
      "Epoch 3, Batch 101, Test Loss: 0.7637802362442017\n",
      "Epoch 3, Batch 102, Test Loss: 0.5688568353652954\n",
      "Epoch 3, Batch 103, Test Loss: 0.6720201373100281\n",
      "Epoch 3, Batch 104, Test Loss: 0.5785242915153503\n",
      "Epoch 3, Batch 105, Test Loss: 0.8056725859642029\n",
      "Epoch 3, Batch 106, Test Loss: 0.757208526134491\n",
      "Epoch 3, Batch 107, Test Loss: 0.7558810710906982\n",
      "Epoch 3, Batch 108, Test Loss: 0.6895710229873657\n",
      "Epoch 3, Batch 109, Test Loss: 0.7628285884857178\n",
      "Epoch 3, Batch 110, Test Loss: 0.7231305837631226\n",
      "Epoch 3, Batch 111, Test Loss: 0.5453924536705017\n",
      "Epoch 3, Batch 112, Test Loss: 0.8967140316963196\n",
      "Epoch 3, Batch 113, Test Loss: 0.7035468220710754\n",
      "Epoch 3, Batch 114, Test Loss: 0.7218630909919739\n",
      "Epoch 3, Batch 115, Test Loss: 1.1058722734451294\n",
      "Epoch 3, Batch 116, Test Loss: 0.6047459840774536\n",
      "Epoch 3, Batch 117, Test Loss: 0.7910842299461365\n",
      "Epoch 3, Batch 118, Test Loss: 0.6206763982772827\n",
      "Epoch 3, Batch 119, Test Loss: 0.6094913482666016\n",
      "Epoch 3, Batch 120, Test Loss: 0.653184711933136\n",
      "Epoch 3, Batch 121, Test Loss: 0.6766445636749268\n",
      "Epoch 3, Batch 122, Test Loss: 0.8610182404518127\n",
      "Epoch 3, Batch 123, Test Loss: 0.800331175327301\n",
      "Epoch 3, Batch 124, Test Loss: 0.7067748308181763\n",
      "Epoch 3, Batch 125, Test Loss: 0.6427611708641052\n",
      "Epoch 3, Batch 126, Test Loss: 0.6518836617469788\n",
      "Epoch 3, Batch 127, Test Loss: 0.6856521368026733\n",
      "Epoch 3, Batch 128, Test Loss: 0.6802474856376648\n",
      "Epoch 3, Batch 129, Test Loss: 0.5408802032470703\n",
      "Epoch 3, Batch 130, Test Loss: 0.6919177174568176\n",
      "Epoch 3, Batch 131, Test Loss: 0.5966710448265076\n",
      "Epoch 3, Batch 132, Test Loss: 0.7743250131607056\n",
      "Epoch 3, Batch 133, Test Loss: 0.6895022392272949\n",
      "Epoch 3, Batch 134, Test Loss: 0.7366239428520203\n",
      "Epoch 3, Batch 135, Test Loss: 0.7613096237182617\n",
      "Epoch 3, Batch 136, Test Loss: 0.7989006638526917\n",
      "Epoch 3, Batch 137, Test Loss: 0.7424249649047852\n",
      "Epoch 3, Batch 138, Test Loss: 0.6313666105270386\n",
      "Epoch 3, Batch 139, Test Loss: 1.0109325647354126\n",
      "Epoch 3, Batch 140, Test Loss: 0.6846227049827576\n",
      "Epoch 3, Batch 141, Test Loss: 0.7096167802810669\n",
      "Epoch 3, Batch 142, Test Loss: 1.0034270286560059\n",
      "Epoch 3, Batch 143, Test Loss: 0.7755556106567383\n",
      "Epoch 3, Batch 144, Test Loss: 0.9223244190216064\n",
      "Epoch 3, Batch 145, Test Loss: 0.6412971019744873\n",
      "Epoch 3, Batch 146, Test Loss: 0.8535539507865906\n",
      "Epoch 3, Batch 147, Test Loss: 0.6880899667739868\n",
      "Epoch 3, Batch 148, Test Loss: 0.7734500169754028\n",
      "Epoch 3, Batch 149, Test Loss: 0.7354478240013123\n",
      "Epoch 3, Batch 150, Test Loss: 0.9314960837364197\n",
      "Epoch 3, Batch 151, Test Loss: 0.9469249844551086\n",
      "Epoch 3, Batch 152, Test Loss: 0.603208601474762\n",
      "Epoch 3, Batch 153, Test Loss: 0.639401912689209\n",
      "Epoch 3, Batch 154, Test Loss: 0.9371076822280884\n",
      "Epoch 3, Batch 155, Test Loss: 0.5562688112258911\n",
      "Epoch 3, Batch 156, Test Loss: 0.6696808338165283\n",
      "Epoch 3, Batch 157, Test Loss: 0.7537033557891846\n",
      "Epoch 3, Batch 158, Test Loss: 0.6286503672599792\n",
      "Epoch 3, Batch 159, Test Loss: 0.7108452320098877\n",
      "Epoch 3, Batch 160, Test Loss: 0.6526579260826111\n",
      "Epoch 3, Batch 161, Test Loss: 0.7998068332672119\n",
      "Epoch 3, Batch 162, Test Loss: 0.7613580226898193\n",
      "Epoch 3, Batch 163, Test Loss: 0.6694050431251526\n",
      "Epoch 3, Batch 164, Test Loss: 0.8051478266716003\n",
      "Epoch 3, Batch 165, Test Loss: 0.5182536244392395\n",
      "Epoch 3, Batch 166, Test Loss: 0.7757828831672668\n",
      "Epoch 3, Batch 167, Test Loss: 0.5400994420051575\n",
      "Epoch 3, Batch 168, Test Loss: 0.6069408059120178\n",
      "Epoch 3, Batch 169, Test Loss: 0.6498965620994568\n",
      "Epoch 3, Batch 170, Test Loss: 0.928697943687439\n",
      "Epoch 3, Batch 171, Test Loss: 0.5882920622825623\n",
      "Epoch 3, Batch 172, Test Loss: 0.6053256988525391\n",
      "Epoch 3, Batch 173, Test Loss: 0.6282832622528076\n",
      "Epoch 3, Batch 174, Test Loss: 0.7630410194396973\n",
      "Epoch 3, Batch 175, Test Loss: 0.698093056678772\n",
      "Epoch 3, Batch 176, Test Loss: 0.5656772255897522\n",
      "Epoch 3, Batch 177, Test Loss: 0.7509660720825195\n",
      "Epoch 3, Batch 178, Test Loss: 0.6290509700775146\n",
      "Epoch 3, Batch 179, Test Loss: 0.8473997116088867\n",
      "Epoch 3, Batch 180, Test Loss: 0.6699154376983643\n",
      "Epoch 3, Batch 181, Test Loss: 0.7183777093887329\n",
      "Epoch 3, Batch 182, Test Loss: 0.6638349890708923\n",
      "Epoch 3, Batch 183, Test Loss: 0.6284360885620117\n",
      "Epoch 3, Batch 184, Test Loss: 0.5688687562942505\n",
      "Epoch 3, Batch 185, Test Loss: 0.8809781670570374\n",
      "Epoch 3, Batch 186, Test Loss: 0.6220083236694336\n",
      "Epoch 3, Batch 187, Test Loss: 0.46667784452438354\n",
      "Epoch 3, Batch 188, Test Loss: 0.5593532919883728\n",
      "Epoch 3, Batch 189, Test Loss: 0.5909073948860168\n",
      "Epoch 3, Batch 190, Test Loss: 0.5892398357391357\n",
      "Epoch 3, Batch 191, Test Loss: 0.7410681247711182\n",
      "Epoch 3, Batch 192, Test Loss: 0.6342105269432068\n",
      "Epoch 3, Batch 193, Test Loss: 0.8994523882865906\n",
      "Epoch 3, Batch 194, Test Loss: 0.6729207634925842\n",
      "Epoch 3, Batch 195, Test Loss: 0.6417715549468994\n",
      "Epoch 3, Batch 196, Test Loss: 0.7498987317085266\n",
      "Epoch 3, Batch 197, Test Loss: 0.6174893379211426\n",
      "Epoch 3, Batch 198, Test Loss: 0.802575945854187\n",
      "Epoch 3, Batch 199, Test Loss: 0.7683967351913452\n",
      "Epoch 3, Batch 200, Test Loss: 0.7375574707984924\n",
      "Epoch 3, Batch 201, Test Loss: 0.5758638381958008\n",
      "Epoch 3, Batch 202, Test Loss: 0.7044285535812378\n",
      "Epoch 3, Batch 203, Test Loss: 0.4918562173843384\n",
      "Epoch 3, Batch 204, Test Loss: 0.6717574000358582\n",
      "Epoch 3, Batch 205, Test Loss: 0.7601250410079956\n",
      "Epoch 3, Batch 206, Test Loss: 0.6313382387161255\n",
      "Epoch 3, Batch 207, Test Loss: 0.733082115650177\n",
      "Epoch 3, Batch 208, Test Loss: 0.7011345624923706\n",
      "Epoch 3, Batch 209, Test Loss: 0.6612458825111389\n",
      "Epoch 3, Batch 210, Test Loss: 0.8201059699058533\n",
      "Epoch 3, Batch 211, Test Loss: 0.8472726345062256\n",
      "Epoch 3, Batch 212, Test Loss: 0.8572867512702942\n",
      "Epoch 3, Batch 213, Test Loss: 0.6109968423843384\n",
      "Epoch 3, Batch 214, Test Loss: 0.7615209221839905\n",
      "Epoch 3, Batch 215, Test Loss: 0.9461841583251953\n",
      "Epoch 3, Batch 216, Test Loss: 0.8149779438972473\n",
      "Epoch 3, Batch 217, Test Loss: 0.8853648900985718\n",
      "Epoch 3, Batch 218, Test Loss: 0.8603398203849792\n",
      "Epoch 3, Batch 219, Test Loss: 0.7198207974433899\n",
      "Epoch 3, Batch 220, Test Loss: 0.7586451768875122\n",
      "Epoch 3, Batch 221, Test Loss: 0.8711054921150208\n",
      "Epoch 3, Batch 222, Test Loss: 0.6502450704574585\n",
      "Epoch 3, Batch 223, Test Loss: 0.6200406551361084\n",
      "Epoch 3, Batch 224, Test Loss: 0.7838425636291504\n",
      "Epoch 3, Batch 225, Test Loss: 0.47553494572639465\n",
      "Epoch 3, Batch 226, Test Loss: 0.7701348066329956\n",
      "Epoch 3, Batch 227, Test Loss: 0.6704418063163757\n",
      "Epoch 3, Batch 228, Test Loss: 0.5409093499183655\n",
      "Epoch 3, Batch 229, Test Loss: 0.6328748464584351\n",
      "Epoch 3, Batch 230, Test Loss: 0.6618061065673828\n",
      "Epoch 3, Batch 231, Test Loss: 0.5669821500778198\n",
      "Epoch 3, Batch 232, Test Loss: 0.6170527338981628\n",
      "Epoch 3, Batch 233, Test Loss: 0.9128904342651367\n",
      "Epoch 3, Batch 234, Test Loss: 0.7587890028953552\n",
      "Epoch 3, Batch 235, Test Loss: 0.5405862927436829\n",
      "Epoch 3, Batch 236, Test Loss: 0.6464901566505432\n",
      "Epoch 3, Batch 237, Test Loss: 0.6413878798484802\n",
      "Epoch 3, Batch 238, Test Loss: 0.8381448984146118\n",
      "Epoch 3, Batch 239, Test Loss: 0.46666404604911804\n",
      "Epoch 3, Batch 240, Test Loss: 0.7233014106750488\n",
      "Epoch 3, Batch 241, Test Loss: 0.7389162182807922\n",
      "Epoch 3, Batch 242, Test Loss: 0.7044313549995422\n",
      "Epoch 3, Batch 243, Test Loss: 0.6689544320106506\n",
      "Epoch 3, Batch 244, Test Loss: 0.8088552951812744\n",
      "Epoch 3, Batch 245, Test Loss: 0.6614665985107422\n",
      "Epoch 3, Batch 246, Test Loss: 0.7619755864143372\n",
      "Epoch 3, Batch 247, Test Loss: 0.9270254969596863\n",
      "Epoch 3, Batch 248, Test Loss: 0.7419764995574951\n",
      "Epoch 3, Batch 249, Test Loss: 0.7819567918777466\n",
      "Epoch 3, Batch 250, Test Loss: 0.5959740281105042\n",
      "Epoch 3, Batch 251, Test Loss: 0.5714172720909119\n",
      "Epoch 3, Batch 252, Test Loss: 0.8057148456573486\n",
      "Epoch 3, Batch 253, Test Loss: 0.5865199565887451\n",
      "Epoch 3, Batch 254, Test Loss: 0.571992039680481\n",
      "Epoch 3, Batch 255, Test Loss: 0.8271850347518921\n",
      "Epoch 3, Batch 256, Test Loss: 0.7792508006095886\n",
      "Epoch 3, Batch 257, Test Loss: 0.8825236558914185\n",
      "Epoch 3, Batch 258, Test Loss: 0.7105050683021545\n",
      "Epoch 3, Batch 259, Test Loss: 0.6298859715461731\n",
      "Epoch 3, Batch 260, Test Loss: 0.7880757451057434\n",
      "Epoch 3, Batch 261, Test Loss: 0.6811468005180359\n",
      "Epoch 3, Batch 262, Test Loss: 0.683696985244751\n",
      "Epoch 3, Batch 263, Test Loss: 0.7640308141708374\n",
      "Epoch 3, Batch 264, Test Loss: 0.7482272386550903\n",
      "Epoch 3, Batch 265, Test Loss: 0.6005414128303528\n",
      "Epoch 3, Batch 266, Test Loss: 0.7132607698440552\n",
      "Epoch 3, Batch 267, Test Loss: 0.7032693028450012\n",
      "Epoch 3, Batch 268, Test Loss: 0.7144283652305603\n",
      "Epoch 3, Batch 269, Test Loss: 0.876873791217804\n",
      "Epoch 3, Batch 270, Test Loss: 0.6991035342216492\n",
      "Epoch 3, Batch 271, Test Loss: 0.6918707489967346\n",
      "Epoch 3, Batch 272, Test Loss: 0.6142306923866272\n",
      "Epoch 3, Batch 273, Test Loss: 0.7525621056556702\n",
      "Epoch 3, Batch 274, Test Loss: 0.541422426700592\n",
      "Epoch 3, Batch 275, Test Loss: 0.9235037565231323\n",
      "Epoch 3, Batch 276, Test Loss: 0.852291464805603\n",
      "Epoch 3, Batch 277, Test Loss: 0.9186055660247803\n",
      "Epoch 3, Batch 278, Test Loss: 0.6785662174224854\n",
      "Epoch 3, Batch 279, Test Loss: 0.9084304571151733\n",
      "Epoch 3, Batch 280, Test Loss: 0.7350121736526489\n",
      "Epoch 3, Batch 281, Test Loss: 0.7350658178329468\n",
      "Epoch 3, Batch 282, Test Loss: 0.6738605499267578\n",
      "Epoch 3, Batch 283, Test Loss: 0.9827022552490234\n",
      "Epoch 3, Batch 284, Test Loss: 0.6504126787185669\n",
      "Epoch 3, Batch 285, Test Loss: 0.6460340619087219\n",
      "Epoch 3, Batch 286, Test Loss: 0.6239409446716309\n",
      "Epoch 3, Batch 287, Test Loss: 0.8461791276931763\n",
      "Epoch 3, Batch 288, Test Loss: 0.7334654927253723\n",
      "Epoch 3, Batch 289, Test Loss: 0.8601133227348328\n",
      "Epoch 3, Batch 290, Test Loss: 0.8385254144668579\n",
      "Epoch 3, Batch 291, Test Loss: 0.7019303441047668\n",
      "Epoch 3, Batch 292, Test Loss: 0.8532499670982361\n",
      "Epoch 3, Batch 293, Test Loss: 0.7652329206466675\n",
      "Epoch 3, Batch 294, Test Loss: 0.7405425906181335\n",
      "Epoch 3, Batch 295, Test Loss: 0.8095734119415283\n",
      "Epoch 3, Batch 296, Test Loss: 0.6056086421012878\n",
      "Epoch 3, Batch 297, Test Loss: 0.9154419898986816\n",
      "Epoch 3, Batch 298, Test Loss: 0.6806255578994751\n",
      "Epoch 3, Batch 299, Test Loss: 0.6517540812492371\n",
      "Epoch 3, Batch 300, Test Loss: 0.7667919397354126\n",
      "Epoch 3, Batch 301, Test Loss: 0.5674960613250732\n",
      "Epoch 3, Batch 302, Test Loss: 0.8634690046310425\n",
      "Epoch 3, Batch 303, Test Loss: 0.5722225904464722\n",
      "Epoch 3, Batch 304, Test Loss: 0.802026093006134\n",
      "Epoch 3, Batch 305, Test Loss: 0.5897214412689209\n",
      "Epoch 3, Batch 306, Test Loss: 0.5869103670120239\n",
      "Epoch 3, Batch 307, Test Loss: 0.7201849818229675\n",
      "Epoch 3, Batch 308, Test Loss: 0.709595799446106\n",
      "Epoch 3, Batch 309, Test Loss: 0.7383102178573608\n",
      "Epoch 3, Batch 310, Test Loss: 0.8671066761016846\n",
      "Epoch 3, Batch 311, Test Loss: 0.7359782457351685\n",
      "Epoch 3, Batch 312, Test Loss: 0.600000262260437\n",
      "Epoch 3, Batch 313, Test Loss: 0.698427677154541\n",
      "Epoch 3, Batch 314, Test Loss: 0.7914194464683533\n",
      "Epoch 3, Batch 315, Test Loss: 0.7680402398109436\n",
      "Epoch 3, Batch 316, Test Loss: 0.676190197467804\n",
      "Epoch 3, Batch 317, Test Loss: 0.7143224477767944\n",
      "Epoch 3, Batch 318, Test Loss: 0.6783217787742615\n",
      "Epoch 3, Batch 319, Test Loss: 1.009507417678833\n",
      "Epoch 3, Batch 320, Test Loss: 0.566058874130249\n",
      "Epoch 3, Batch 321, Test Loss: 0.6945821046829224\n",
      "Epoch 3, Batch 322, Test Loss: 0.8270924091339111\n",
      "Epoch 3, Batch 323, Test Loss: 0.6504712104797363\n",
      "Epoch 3, Batch 324, Test Loss: 0.6226498484611511\n",
      "Epoch 3, Batch 325, Test Loss: 0.6899430751800537\n",
      "Epoch 3, Batch 326, Test Loss: 0.6557732224464417\n",
      "Epoch 3, Batch 327, Test Loss: 0.8363605737686157\n",
      "Epoch 3, Batch 328, Test Loss: 0.5471703410148621\n",
      "Epoch 3, Batch 329, Test Loss: 0.7729678750038147\n",
      "Epoch 3, Batch 330, Test Loss: 0.6495543122291565\n",
      "Epoch 3, Batch 331, Test Loss: 0.8153046369552612\n",
      "Epoch 3, Batch 332, Test Loss: 0.781984269618988\n",
      "Epoch 3, Batch 333, Test Loss: 0.7384598851203918\n",
      "Epoch 3, Batch 334, Test Loss: 0.6394649744033813\n",
      "Epoch 3, Batch 335, Test Loss: 0.6884849071502686\n",
      "Epoch 3, Batch 336, Test Loss: 0.6164466738700867\n",
      "Epoch 3, Batch 337, Test Loss: 0.7230451107025146\n",
      "Epoch 3, Batch 338, Test Loss: 0.5470348000526428\n",
      "Epoch 3, Batch 339, Test Loss: 0.6492601633071899\n",
      "Epoch 3, Batch 340, Test Loss: 0.669682502746582\n",
      "Epoch 3, Batch 341, Test Loss: 0.6400347948074341\n",
      "Epoch 3, Batch 342, Test Loss: 0.602649450302124\n",
      "Epoch 3, Batch 343, Test Loss: 0.8160430192947388\n",
      "Epoch 3, Batch 344, Test Loss: 0.5347489714622498\n",
      "Epoch 3, Batch 345, Test Loss: 0.8463382124900818\n",
      "Epoch 3, Batch 346, Test Loss: 0.8233339190483093\n",
      "Epoch 3, Batch 347, Test Loss: 0.6527355313301086\n",
      "Epoch 3, Batch 348, Test Loss: 0.667187511920929\n",
      "Epoch 3, Batch 349, Test Loss: 0.8189949989318848\n",
      "Epoch 3, Batch 350, Test Loss: 0.6348156332969666\n",
      "Epoch 3, Batch 351, Test Loss: 0.7242271900177002\n",
      "Epoch 3, Batch 352, Test Loss: 0.6468429565429688\n",
      "Epoch 3, Batch 353, Test Loss: 0.7281427979469299\n",
      "Epoch 3, Batch 354, Test Loss: 1.047662377357483\n",
      "Epoch 3, Batch 355, Test Loss: 0.8126475214958191\n",
      "Epoch 3, Batch 356, Test Loss: 0.8272342681884766\n",
      "Epoch 3, Batch 357, Test Loss: 0.6298408508300781\n",
      "Epoch 3, Batch 358, Test Loss: 0.7358102798461914\n",
      "Epoch 3, Batch 359, Test Loss: 0.5347384214401245\n",
      "Epoch 3, Batch 360, Test Loss: 0.754392147064209\n",
      "Epoch 3, Batch 361, Test Loss: 0.7066420912742615\n",
      "Epoch 3, Batch 362, Test Loss: 0.7557979226112366\n",
      "Epoch 3, Batch 363, Test Loss: 0.7908304929733276\n",
      "Epoch 3, Batch 364, Test Loss: 0.5942608118057251\n",
      "Epoch 3, Batch 365, Test Loss: 0.8453555107116699\n",
      "Epoch 3, Batch 366, Test Loss: 0.8353801965713501\n",
      "Epoch 3, Batch 367, Test Loss: 0.6889035105705261\n",
      "Epoch 3, Batch 368, Test Loss: 0.7483122944831848\n",
      "Epoch 3, Batch 369, Test Loss: 0.7458786964416504\n",
      "Epoch 3, Batch 370, Test Loss: 0.6173621416091919\n",
      "Epoch 3, Batch 371, Test Loss: 0.5750252604484558\n",
      "Epoch 3, Batch 372, Test Loss: 0.7112756371498108\n",
      "Epoch 3, Batch 373, Test Loss: 0.8333091735839844\n",
      "Epoch 3, Batch 374, Test Loss: 0.6715167760848999\n",
      "Epoch 3, Batch 375, Test Loss: 0.6665371656417847\n",
      "Epoch 3, Batch 376, Test Loss: 0.6715692281723022\n",
      "Epoch 3, Batch 377, Test Loss: 0.8041974306106567\n",
      "Epoch 3, Batch 378, Test Loss: 0.6278734803199768\n",
      "Epoch 3, Batch 379, Test Loss: 0.6108022928237915\n",
      "Epoch 3, Batch 380, Test Loss: 0.7060022950172424\n",
      "Epoch 3, Batch 381, Test Loss: 0.6490738391876221\n",
      "Epoch 3, Batch 382, Test Loss: 0.6285189986228943\n",
      "Epoch 3, Batch 383, Test Loss: 0.7030177712440491\n",
      "Epoch 3, Batch 384, Test Loss: 0.5237264633178711\n",
      "Epoch 3, Batch 385, Test Loss: 0.7732381224632263\n",
      "Epoch 3, Batch 386, Test Loss: 0.8890677690505981\n",
      "Epoch 3, Batch 387, Test Loss: 0.7625560164451599\n",
      "Epoch 3, Batch 388, Test Loss: 0.7317848801612854\n",
      "Epoch 3, Batch 389, Test Loss: 0.7083413600921631\n",
      "Epoch 3, Batch 390, Test Loss: 0.6584895849227905\n",
      "Epoch 3, Batch 391, Test Loss: 0.7069704532623291\n",
      "Epoch 3, Batch 392, Test Loss: 0.7736263871192932\n",
      "Epoch 3, Batch 393, Test Loss: 0.7360353469848633\n",
      "Epoch 3, Batch 394, Test Loss: 0.5062409043312073\n",
      "Epoch 3, Batch 395, Test Loss: 0.5554015040397644\n",
      "Epoch 3, Batch 396, Test Loss: 0.6071066856384277\n",
      "Epoch 3, Batch 397, Test Loss: 0.7337303161621094\n",
      "Epoch 3, Batch 398, Test Loss: 0.7171096205711365\n",
      "Epoch 3, Batch 399, Test Loss: 0.7498081922531128\n",
      "Epoch 3, Batch 400, Test Loss: 0.860784113407135\n",
      "Epoch 3, Batch 401, Test Loss: 0.4803658127784729\n",
      "Epoch 3, Batch 402, Test Loss: 0.9858591556549072\n",
      "Epoch 3, Batch 403, Test Loss: 0.7416772246360779\n",
      "Epoch 3, Batch 404, Test Loss: 0.8817340731620789\n",
      "Epoch 3, Batch 405, Test Loss: 0.6470562815666199\n",
      "Epoch 3, Batch 406, Test Loss: 0.6583688259124756\n",
      "Epoch 3, Batch 407, Test Loss: 0.7069971561431885\n",
      "Epoch 3, Batch 408, Test Loss: 0.7245504260063171\n",
      "Epoch 3, Batch 409, Test Loss: 0.6118278503417969\n",
      "Epoch 3, Batch 410, Test Loss: 0.6331817507743835\n",
      "Epoch 3, Batch 411, Test Loss: 0.6478495597839355\n",
      "Epoch 3, Batch 412, Test Loss: 0.5678075551986694\n",
      "Epoch 3, Batch 413, Test Loss: 0.6778953671455383\n",
      "Epoch 3, Batch 414, Test Loss: 0.7887188196182251\n",
      "Epoch 3, Batch 415, Test Loss: 0.6682864427566528\n",
      "Epoch 3, Batch 416, Test Loss: 0.5378599166870117\n",
      "Epoch 3, Batch 417, Test Loss: 0.7934101223945618\n",
      "Epoch 3, Batch 418, Test Loss: 0.8237071633338928\n",
      "Epoch 3, Batch 419, Test Loss: 0.6034037470817566\n",
      "Epoch 3, Batch 420, Test Loss: 0.7079561948776245\n",
      "Epoch 3, Batch 421, Test Loss: 0.6107813119888306\n",
      "Epoch 3, Batch 422, Test Loss: 0.7697432041168213\n",
      "Epoch 3, Batch 423, Test Loss: 0.5314552187919617\n",
      "Epoch 3, Batch 424, Test Loss: 0.5616550445556641\n",
      "Epoch 3, Batch 425, Test Loss: 0.7661316394805908\n",
      "Epoch 3, Batch 426, Test Loss: 0.8270269632339478\n",
      "Epoch 3, Batch 427, Test Loss: 0.8494093418121338\n",
      "Epoch 3, Batch 428, Test Loss: 0.6590147018432617\n",
      "Epoch 3, Batch 429, Test Loss: 0.7051069736480713\n",
      "Epoch 3, Batch 430, Test Loss: 0.7902970910072327\n",
      "Epoch 3, Batch 431, Test Loss: 0.7113376259803772\n",
      "Epoch 3, Batch 432, Test Loss: 0.616736888885498\n",
      "Epoch 3, Batch 433, Test Loss: 0.7793511748313904\n",
      "Epoch 3, Batch 434, Test Loss: 0.5954092741012573\n",
      "Epoch 3, Batch 435, Test Loss: 0.6949669122695923\n",
      "Epoch 3, Batch 436, Test Loss: 0.7190991640090942\n",
      "Epoch 3, Batch 437, Test Loss: 0.6304916143417358\n",
      "Epoch 3, Batch 438, Test Loss: 0.6167209148406982\n",
      "Epoch 3, Batch 439, Test Loss: 0.7163318991661072\n",
      "Epoch 3, Batch 440, Test Loss: 0.720215380191803\n",
      "Epoch 3, Batch 441, Test Loss: 0.5718337297439575\n",
      "Epoch 3, Batch 442, Test Loss: 0.7426216006278992\n",
      "Epoch 3, Batch 443, Test Loss: 0.7726089954376221\n",
      "Epoch 3, Batch 444, Test Loss: 0.6199246644973755\n",
      "Epoch 3, Batch 445, Test Loss: 0.7504540681838989\n",
      "Epoch 3, Batch 446, Test Loss: 0.5766453742980957\n",
      "Epoch 3, Batch 447, Test Loss: 0.5726086497306824\n",
      "Epoch 3, Batch 448, Test Loss: 0.9064154624938965\n",
      "Epoch 3, Batch 449, Test Loss: 0.6624854207038879\n",
      "Epoch 3, Batch 450, Test Loss: 0.6595274209976196\n",
      "Epoch 3, Batch 451, Test Loss: 0.5805402994155884\n",
      "Epoch 3, Batch 452, Test Loss: 0.9153436422348022\n",
      "Epoch 3, Batch 453, Test Loss: 0.7169776558876038\n",
      "Epoch 3, Batch 454, Test Loss: 0.5146937966346741\n",
      "Epoch 3, Batch 455, Test Loss: 0.5681943893432617\n",
      "Epoch 3, Batch 456, Test Loss: 0.7399169206619263\n",
      "Epoch 3, Batch 457, Test Loss: 0.7212350964546204\n",
      "Epoch 3, Batch 458, Test Loss: 0.6720125675201416\n",
      "Epoch 3, Batch 459, Test Loss: 0.8445003032684326\n",
      "Epoch 3, Batch 460, Test Loss: 0.5123188495635986\n",
      "Epoch 3, Batch 461, Test Loss: 0.629141628742218\n",
      "Epoch 3, Batch 462, Test Loss: 0.8690314888954163\n",
      "Epoch 3, Batch 463, Test Loss: 0.6106656193733215\n",
      "Epoch 3, Batch 464, Test Loss: 0.6531952023506165\n",
      "Epoch 3, Batch 465, Test Loss: 0.5892118811607361\n",
      "Epoch 3, Batch 466, Test Loss: 0.7151121497154236\n",
      "Epoch 3, Batch 467, Test Loss: 0.7023143768310547\n",
      "Epoch 3, Batch 468, Test Loss: 0.7974949479103088\n",
      "Epoch 3, Batch 469, Test Loss: 0.7113404273986816\n",
      "Epoch 3, Batch 470, Test Loss: 0.7722368240356445\n",
      "Epoch 3, Batch 471, Test Loss: 0.6279978156089783\n",
      "Epoch 3, Batch 472, Test Loss: 0.7904699444770813\n",
      "Epoch 3, Batch 473, Test Loss: 0.731410026550293\n",
      "Epoch 3, Batch 474, Test Loss: 0.5857900977134705\n",
      "Epoch 3, Batch 475, Test Loss: 0.641124427318573\n",
      "Epoch 3, Batch 476, Test Loss: 0.7826420664787292\n",
      "Epoch 3, Batch 477, Test Loss: 0.6754504442214966\n",
      "Epoch 3, Batch 478, Test Loss: 0.6748908162117004\n",
      "Epoch 3, Batch 479, Test Loss: 0.7593211531639099\n",
      "Epoch 3, Batch 480, Test Loss: 0.5323569178581238\n",
      "Epoch 3, Batch 481, Test Loss: 0.6674817800521851\n",
      "Epoch 3, Batch 482, Test Loss: 0.7982988357543945\n",
      "Epoch 3, Batch 483, Test Loss: 0.6205103993415833\n",
      "Epoch 3, Batch 484, Test Loss: 0.5657600164413452\n",
      "Epoch 3, Batch 485, Test Loss: 0.7241396903991699\n",
      "Epoch 3, Batch 486, Test Loss: 0.6982510089874268\n",
      "Epoch 3, Batch 487, Test Loss: 0.7265021204948425\n",
      "Epoch 3, Batch 488, Test Loss: 0.781833827495575\n",
      "Epoch 3, Batch 489, Test Loss: 0.7456013560295105\n",
      "Epoch 3, Batch 490, Test Loss: 0.6072952747344971\n",
      "Epoch 3, Batch 491, Test Loss: 0.6884490847587585\n",
      "Epoch 3, Batch 492, Test Loss: 0.7126248478889465\n",
      "Epoch 3, Batch 493, Test Loss: 0.7171766757965088\n",
      "Epoch 3, Batch 494, Test Loss: 0.7075967788696289\n",
      "Epoch 3, Batch 495, Test Loss: 0.7273237109184265\n",
      "Epoch 3, Batch 496, Test Loss: 0.8424921035766602\n",
      "Epoch 3, Batch 497, Test Loss: 0.7393301725387573\n",
      "Epoch 3, Batch 498, Test Loss: 0.706455647945404\n",
      "Epoch 3, Batch 499, Test Loss: 0.5813328623771667\n",
      "Epoch 3, Batch 500, Test Loss: 0.5861518383026123\n",
      "Epoch 3, Batch 501, Test Loss: 0.8777891993522644\n",
      "Epoch 3, Batch 502, Test Loss: 0.7562832832336426\n",
      "Epoch 3, Batch 503, Test Loss: 0.5944808721542358\n",
      "Epoch 3, Batch 504, Test Loss: 0.72928786277771\n",
      "Epoch 3, Batch 505, Test Loss: 0.6372290253639221\n",
      "Epoch 3, Batch 506, Test Loss: 0.6883887052536011\n",
      "Epoch 3, Batch 507, Test Loss: 0.7402933239936829\n",
      "Epoch 3, Batch 508, Test Loss: 0.698451042175293\n",
      "Epoch 3, Batch 509, Test Loss: 0.5882699489593506\n",
      "Epoch 3, Batch 510, Test Loss: 0.6915236711502075\n",
      "Epoch 3, Batch 511, Test Loss: 0.7284669280052185\n",
      "Epoch 3, Batch 512, Test Loss: 0.8373209238052368\n",
      "Epoch 3, Batch 513, Test Loss: 0.5396600365638733\n",
      "Epoch 3, Batch 514, Test Loss: 0.6220704317092896\n",
      "Epoch 3, Batch 515, Test Loss: 0.643629252910614\n",
      "Epoch 3, Batch 516, Test Loss: 0.8727138638496399\n",
      "Epoch 3, Batch 517, Test Loss: 0.7181718945503235\n",
      "Epoch 3, Batch 518, Test Loss: 0.9815441370010376\n",
      "Epoch 3, Batch 519, Test Loss: 0.7467178702354431\n",
      "Epoch 3, Batch 520, Test Loss: 0.6028097867965698\n",
      "Epoch 3, Batch 521, Test Loss: 0.5509979724884033\n",
      "Epoch 3, Batch 522, Test Loss: 0.6706733703613281\n",
      "Epoch 3, Batch 523, Test Loss: 0.6658682823181152\n",
      "Epoch 3, Batch 524, Test Loss: 0.5091318488121033\n",
      "Epoch 3, Batch 525, Test Loss: 0.8348627686500549\n",
      "Epoch 3, Batch 526, Test Loss: 0.6081324219703674\n",
      "Epoch 3, Batch 527, Test Loss: 0.7390704154968262\n",
      "Epoch 3, Batch 528, Test Loss: 0.6730477809906006\n",
      "Epoch 3, Batch 529, Test Loss: 0.7971410751342773\n",
      "Epoch 3, Batch 530, Test Loss: 0.5989978909492493\n",
      "Epoch 3, Batch 531, Test Loss: 0.5325675010681152\n",
      "Epoch 3, Batch 532, Test Loss: 0.7233426570892334\n",
      "Epoch 3, Batch 533, Test Loss: 0.7763677835464478\n",
      "Epoch 3, Batch 534, Test Loss: 0.9871111512184143\n",
      "Epoch 3, Batch 535, Test Loss: 0.7875943183898926\n",
      "Epoch 3, Batch 536, Test Loss: 0.609279215335846\n",
      "Epoch 3, Batch 537, Test Loss: 0.5592606067657471\n",
      "Epoch 3, Batch 538, Test Loss: 0.7052151560783386\n",
      "Epoch 3, Batch 539, Test Loss: 0.7074180841445923\n",
      "Epoch 3, Batch 540, Test Loss: 0.6472988724708557\n",
      "Epoch 3, Batch 541, Test Loss: 0.6576408743858337\n",
      "Epoch 3, Batch 542, Test Loss: 0.8550352454185486\n",
      "Epoch 3, Batch 543, Test Loss: 0.5866035223007202\n",
      "Epoch 3, Batch 544, Test Loss: 0.7052972912788391\n",
      "Epoch 3, Batch 545, Test Loss: 0.6808679699897766\n",
      "Epoch 3, Batch 546, Test Loss: 0.6065084338188171\n",
      "Epoch 3, Batch 547, Test Loss: 0.585922122001648\n",
      "Epoch 3, Batch 548, Test Loss: 0.6612530946731567\n",
      "Epoch 3, Batch 549, Test Loss: 0.6939778923988342\n",
      "Epoch 3, Batch 550, Test Loss: 0.8808745741844177\n",
      "Epoch 3, Batch 551, Test Loss: 0.6718166470527649\n",
      "Epoch 3, Batch 552, Test Loss: 0.7909340858459473\n",
      "Epoch 3, Batch 553, Test Loss: 0.7705649137496948\n",
      "Epoch 3, Batch 554, Test Loss: 0.7725319862365723\n",
      "Epoch 3, Batch 555, Test Loss: 0.6119154691696167\n",
      "Epoch 3, Batch 556, Test Loss: 0.6294163465499878\n",
      "Epoch 3, Batch 557, Test Loss: 0.6024515628814697\n",
      "Epoch 3, Batch 558, Test Loss: 0.7382642030715942\n",
      "Epoch 3, Batch 559, Test Loss: 0.5886615514755249\n",
      "Epoch 3, Batch 560, Test Loss: 0.539624810218811\n",
      "Epoch 3, Batch 561, Test Loss: 0.6447838544845581\n",
      "Epoch 3, Batch 562, Test Loss: 0.4939807653427124\n",
      "Epoch 3, Batch 563, Test Loss: 0.6348823308944702\n",
      "Epoch 3, Batch 564, Test Loss: 0.7134928703308105\n",
      "Epoch 3, Batch 565, Test Loss: 0.8587499856948853\n",
      "Epoch 3, Batch 566, Test Loss: 0.7907612919807434\n",
      "Epoch 3, Batch 567, Test Loss: 0.6177234649658203\n",
      "Epoch 3, Batch 568, Test Loss: 0.6381338238716125\n",
      "Epoch 3, Batch 569, Test Loss: 0.7524895071983337\n",
      "Epoch 3, Batch 570, Test Loss: 0.6118334531784058\n",
      "Epoch 3, Batch 571, Test Loss: 0.8058143258094788\n",
      "Epoch 3, Batch 572, Test Loss: 0.8063811659812927\n",
      "Epoch 3, Batch 573, Test Loss: 0.7237043380737305\n",
      "Epoch 3, Batch 574, Test Loss: 0.6641368865966797\n",
      "Epoch 3, Batch 575, Test Loss: 0.6734592318534851\n",
      "Epoch 3, Batch 576, Test Loss: 0.7130934000015259\n",
      "Epoch 3, Batch 577, Test Loss: 0.7668750286102295\n",
      "Epoch 3, Batch 578, Test Loss: 0.810272753238678\n",
      "Epoch 3, Batch 579, Test Loss: 0.6311039924621582\n",
      "Epoch 3, Batch 580, Test Loss: 0.6524502038955688\n",
      "Epoch 3, Batch 581, Test Loss: 0.7034028768539429\n",
      "Epoch 3, Batch 582, Test Loss: 0.6302890777587891\n",
      "Epoch 3, Batch 583, Test Loss: 0.6871575117111206\n",
      "Epoch 3, Batch 584, Test Loss: 0.5825669765472412\n",
      "Epoch 3, Batch 585, Test Loss: 0.7683382034301758\n",
      "Epoch 3, Batch 586, Test Loss: 1.0527119636535645\n",
      "Epoch 3, Batch 587, Test Loss: 0.703871488571167\n",
      "Epoch 3, Batch 588, Test Loss: 0.8955716490745544\n",
      "Epoch 3, Batch 589, Test Loss: 0.546802818775177\n",
      "Epoch 3, Batch 590, Test Loss: 0.6890401840209961\n",
      "Epoch 3, Batch 591, Test Loss: 0.75114905834198\n",
      "Epoch 3, Batch 592, Test Loss: 0.7649358510971069\n",
      "Epoch 3, Batch 593, Test Loss: 0.6803011298179626\n",
      "Epoch 3, Batch 594, Test Loss: 0.6834607720375061\n",
      "Epoch 3, Batch 595, Test Loss: 0.7035685777664185\n",
      "Epoch 3, Batch 596, Test Loss: 0.624370813369751\n",
      "Epoch 3, Batch 597, Test Loss: 0.5719738006591797\n",
      "Epoch 3, Batch 598, Test Loss: 0.7108588814735413\n",
      "Epoch 3, Batch 599, Test Loss: 0.6275336742401123\n",
      "Epoch 3, Batch 600, Test Loss: 0.5904302597045898\n",
      "Epoch 3, Batch 601, Test Loss: 0.8085894584655762\n",
      "Epoch 3, Batch 602, Test Loss: 0.7187108397483826\n",
      "Epoch 3, Batch 603, Test Loss: 0.762025773525238\n",
      "Epoch 3, Batch 604, Test Loss: 0.6301772594451904\n",
      "Epoch 3, Batch 605, Test Loss: 0.6013692617416382\n",
      "Epoch 3, Batch 606, Test Loss: 0.794543981552124\n",
      "Epoch 3, Batch 607, Test Loss: 0.6680683493614197\n",
      "Epoch 3, Batch 608, Test Loss: 0.6207150220870972\n",
      "Epoch 3, Batch 609, Test Loss: 0.6968992948532104\n",
      "Epoch 3, Batch 610, Test Loss: 0.6123896837234497\n",
      "Epoch 3, Batch 611, Test Loss: 0.633414089679718\n",
      "Epoch 3, Batch 612, Test Loss: 0.6093775629997253\n",
      "Epoch 3, Batch 613, Test Loss: 0.8112933039665222\n",
      "Epoch 3, Batch 614, Test Loss: 0.7093422412872314\n",
      "Epoch 3, Batch 615, Test Loss: 0.8040222525596619\n",
      "Epoch 3, Batch 616, Test Loss: 0.6567203998565674\n",
      "Epoch 3, Batch 617, Test Loss: 0.6200305223464966\n",
      "Epoch 3, Batch 618, Test Loss: 1.0113641023635864\n",
      "Epoch 3, Batch 619, Test Loss: 0.6871737241744995\n",
      "Epoch 3, Batch 620, Test Loss: 0.6733750104904175\n",
      "Epoch 3, Batch 621, Test Loss: 0.7276663184165955\n",
      "Epoch 3, Batch 622, Test Loss: 0.6036421060562134\n",
      "Epoch 3, Batch 623, Test Loss: 0.6008604168891907\n",
      "Epoch 3, Batch 624, Test Loss: 0.4848777949810028\n",
      "Epoch 3, Batch 625, Test Loss: 0.6547737717628479\n",
      "Epoch 3, Batch 626, Test Loss: 0.6012393236160278\n",
      "Epoch 3, Batch 627, Test Loss: 0.560623288154602\n",
      "Epoch 3, Batch 628, Test Loss: 0.707099437713623\n",
      "Epoch 3, Batch 629, Test Loss: 0.7335895895957947\n",
      "Epoch 3, Batch 630, Test Loss: 0.5660362839698792\n",
      "Epoch 3, Batch 631, Test Loss: 0.7209460139274597\n",
      "Epoch 3, Batch 632, Test Loss: 0.8725233674049377\n",
      "Epoch 3, Batch 633, Test Loss: 0.61789470911026\n",
      "Epoch 3, Batch 634, Test Loss: 0.6153781414031982\n",
      "Epoch 3, Batch 635, Test Loss: 0.7565562129020691\n",
      "Epoch 3, Batch 636, Test Loss: 0.5905971527099609\n",
      "Epoch 3, Batch 637, Test Loss: 0.6997876167297363\n",
      "Epoch 3, Batch 638, Test Loss: 0.7277256846427917\n",
      "Epoch 3, Batch 639, Test Loss: 0.8072309494018555\n",
      "Epoch 3, Batch 640, Test Loss: 0.9787065386772156\n",
      "Epoch 3, Batch 641, Test Loss: 0.7874094843864441\n",
      "Epoch 3, Batch 642, Test Loss: 0.7898585796356201\n",
      "Epoch 3, Batch 643, Test Loss: 0.7869586944580078\n",
      "Epoch 3, Batch 644, Test Loss: 0.7913070917129517\n",
      "Epoch 3, Batch 645, Test Loss: 0.6665960550308228\n",
      "Epoch 3, Batch 646, Test Loss: 0.6695190072059631\n",
      "Epoch 3, Batch 647, Test Loss: 0.7042708396911621\n",
      "Epoch 3, Batch 648, Test Loss: 0.6924388408660889\n",
      "Epoch 3, Batch 649, Test Loss: 0.7516647577285767\n",
      "Epoch 3, Batch 650, Test Loss: 0.8055456280708313\n",
      "Epoch 3, Batch 651, Test Loss: 0.7379216551780701\n",
      "Epoch 3, Batch 652, Test Loss: 0.6571891903877258\n",
      "Epoch 3, Batch 653, Test Loss: 0.5680769681930542\n",
      "Epoch 3, Batch 654, Test Loss: 0.8621733784675598\n",
      "Epoch 3, Batch 655, Test Loss: 0.6546441316604614\n",
      "Epoch 3, Batch 656, Test Loss: 0.7654944062232971\n",
      "Epoch 3, Batch 657, Test Loss: 0.5553090572357178\n",
      "Epoch 3, Batch 658, Test Loss: 0.5794728994369507\n",
      "Epoch 3, Batch 659, Test Loss: 0.6791305541992188\n",
      "Epoch 3, Batch 660, Test Loss: 0.7885202169418335\n",
      "Epoch 3, Batch 661, Test Loss: 0.746622622013092\n",
      "Epoch 3, Batch 662, Test Loss: 0.8092629313468933\n",
      "Epoch 3, Batch 663, Test Loss: 0.7860568761825562\n",
      "Epoch 3, Batch 664, Test Loss: 0.6412439346313477\n",
      "Epoch 3, Batch 665, Test Loss: 0.7375538349151611\n",
      "Epoch 3, Batch 666, Test Loss: 0.4831857979297638\n",
      "Epoch 3, Batch 667, Test Loss: 0.6108987331390381\n",
      "Epoch 3, Batch 668, Test Loss: 0.8568265438079834\n",
      "Epoch 3, Batch 669, Test Loss: 0.8703111410140991\n",
      "Epoch 3, Batch 670, Test Loss: 0.8287879824638367\n",
      "Epoch 3, Batch 671, Test Loss: 0.816442608833313\n",
      "Epoch 3, Batch 672, Test Loss: 0.7185835242271423\n",
      "Epoch 3, Batch 673, Test Loss: 0.7492519021034241\n",
      "Epoch 3, Batch 674, Test Loss: 0.7718747854232788\n",
      "Epoch 3, Batch 675, Test Loss: 0.8182806372642517\n",
      "Epoch 3, Batch 676, Test Loss: 0.67337965965271\n",
      "Epoch 3, Batch 677, Test Loss: 0.6112052798271179\n",
      "Epoch 3, Batch 678, Test Loss: 0.6135501265525818\n",
      "Epoch 3, Batch 679, Test Loss: 1.0987918376922607\n",
      "Epoch 3, Batch 680, Test Loss: 0.5692861080169678\n",
      "Epoch 3, Batch 681, Test Loss: 0.6344019174575806\n",
      "Epoch 3, Batch 682, Test Loss: 0.8831879496574402\n",
      "Epoch 3, Batch 683, Test Loss: 0.7086242437362671\n",
      "Epoch 3, Batch 684, Test Loss: 0.7112354636192322\n",
      "Epoch 3, Batch 685, Test Loss: 0.7604735493659973\n",
      "Epoch 3, Batch 686, Test Loss: 0.6630492806434631\n",
      "Epoch 3, Batch 687, Test Loss: 0.7705861330032349\n",
      "Epoch 3, Batch 688, Test Loss: 0.9135282039642334\n",
      "Epoch 3, Batch 689, Test Loss: 0.710580587387085\n",
      "Epoch 3, Batch 690, Test Loss: 0.6568011045455933\n",
      "Epoch 3, Batch 691, Test Loss: 0.6361285448074341\n",
      "Epoch 3, Batch 692, Test Loss: 0.692075252532959\n",
      "Epoch 3, Batch 693, Test Loss: 0.6652950644493103\n",
      "Epoch 3, Batch 694, Test Loss: 0.9715403914451599\n",
      "Epoch 3, Batch 695, Test Loss: 0.6332216262817383\n",
      "Epoch 3, Batch 696, Test Loss: 0.6840618848800659\n",
      "Epoch 3, Batch 697, Test Loss: 0.5869882702827454\n",
      "Epoch 3, Batch 698, Test Loss: 0.8138333559036255\n",
      "Epoch 3, Batch 699, Test Loss: 0.5628165006637573\n",
      "Epoch 3, Batch 700, Test Loss: 0.6904032230377197\n",
      "Epoch 3, Batch 701, Test Loss: 0.6476297974586487\n",
      "Epoch 3, Batch 702, Test Loss: 0.6779619455337524\n",
      "Epoch 3, Batch 703, Test Loss: 0.7051352262496948\n",
      "Epoch 3, Batch 704, Test Loss: 0.581246554851532\n",
      "Epoch 3, Batch 705, Test Loss: 0.9600701928138733\n",
      "Epoch 3, Batch 706, Test Loss: 0.4912874698638916\n",
      "Epoch 3, Batch 707, Test Loss: 0.654102087020874\n",
      "Epoch 3, Batch 708, Test Loss: 0.8383622169494629\n",
      "Epoch 3, Batch 709, Test Loss: 0.6351482272148132\n",
      "Epoch 3, Batch 710, Test Loss: 0.6405025720596313\n",
      "Epoch 3, Batch 711, Test Loss: 0.7927078008651733\n",
      "Epoch 3, Batch 712, Test Loss: 0.6292657852172852\n",
      "Epoch 3, Batch 713, Test Loss: 0.7232909202575684\n",
      "Epoch 3, Batch 714, Test Loss: 0.614108681678772\n",
      "Epoch 3, Batch 715, Test Loss: 0.8878343105316162\n",
      "Epoch 3, Batch 716, Test Loss: 0.8203972578048706\n",
      "Epoch 3, Batch 717, Test Loss: 0.6417108774185181\n",
      "Epoch 3, Batch 718, Test Loss: 0.7253149747848511\n",
      "Epoch 3, Batch 719, Test Loss: 0.7760210037231445\n",
      "Epoch 3, Batch 720, Test Loss: 0.8956044316291809\n",
      "Epoch 3, Batch 721, Test Loss: 0.6915357708930969\n",
      "Epoch 3, Batch 722, Test Loss: 0.7087883949279785\n",
      "Epoch 3, Batch 723, Test Loss: 0.550386369228363\n",
      "Epoch 3, Batch 724, Test Loss: 0.759517252445221\n",
      "Epoch 3, Batch 725, Test Loss: 0.6258162260055542\n",
      "Epoch 3, Batch 726, Test Loss: 0.5812761187553406\n",
      "Epoch 3, Batch 727, Test Loss: 0.6557150483131409\n",
      "Epoch 3, Batch 728, Test Loss: 0.5284019112586975\n",
      "Epoch 3, Batch 729, Test Loss: 0.6082155704498291\n",
      "Epoch 3, Batch 730, Test Loss: 0.7263268828392029\n",
      "Epoch 3, Batch 731, Test Loss: 0.6729632616043091\n",
      "Epoch 3, Batch 732, Test Loss: 0.8036938905715942\n",
      "Epoch 3, Batch 733, Test Loss: 0.6933354735374451\n",
      "Epoch 3, Batch 734, Test Loss: 0.5259534120559692\n",
      "Epoch 3, Batch 735, Test Loss: 0.786293625831604\n",
      "Epoch 3, Batch 736, Test Loss: 0.7248032689094543\n",
      "Epoch 3, Batch 737, Test Loss: 0.7909520268440247\n",
      "Epoch 3, Batch 738, Test Loss: 0.5643222332000732\n",
      "Epoch 3, Batch 739, Test Loss: 0.6188071966171265\n",
      "Epoch 3, Batch 740, Test Loss: 0.48968932032585144\n",
      "Epoch 3, Batch 741, Test Loss: 0.5859485864639282\n",
      "Epoch 3, Batch 742, Test Loss: 0.7574467062950134\n",
      "Epoch 3, Batch 743, Test Loss: 0.7096877098083496\n",
      "Epoch 3, Batch 744, Test Loss: 0.6095116138458252\n",
      "Epoch 3, Batch 745, Test Loss: 0.8784841299057007\n",
      "Epoch 3, Batch 746, Test Loss: 0.8029335737228394\n",
      "Epoch 3, Batch 747, Test Loss: 0.7781421542167664\n",
      "Epoch 3, Batch 748, Test Loss: 0.7111769318580627\n",
      "Epoch 3, Batch 749, Test Loss: 0.7047605514526367\n",
      "Epoch 3, Batch 750, Test Loss: 0.9566115140914917\n",
      "Epoch 3, Batch 751, Test Loss: 0.6874889135360718\n",
      "Epoch 3, Batch 752, Test Loss: 0.7825247645378113\n",
      "Epoch 3, Batch 753, Test Loss: 0.8299953937530518\n",
      "Epoch 3, Batch 754, Test Loss: 0.7232009172439575\n",
      "Epoch 3, Batch 755, Test Loss: 0.7439109683036804\n",
      "Epoch 3, Batch 756, Test Loss: 0.7239201068878174\n",
      "Epoch 3, Batch 757, Test Loss: 0.7424429059028625\n",
      "Epoch 3, Batch 758, Test Loss: 0.7684396505355835\n",
      "Epoch 3, Batch 759, Test Loss: 0.5845723748207092\n",
      "Epoch 3, Batch 760, Test Loss: 0.6014179587364197\n",
      "Epoch 3, Batch 761, Test Loss: 0.8410360217094421\n",
      "Epoch 3, Batch 762, Test Loss: 0.6986237168312073\n",
      "Epoch 3, Batch 763, Test Loss: 0.8445579409599304\n",
      "Epoch 3, Batch 764, Test Loss: 0.5823919177055359\n",
      "Epoch 3, Batch 765, Test Loss: 0.4929877519607544\n",
      "Epoch 3, Batch 766, Test Loss: 0.8041298389434814\n",
      "Epoch 3, Batch 767, Test Loss: 0.6469668745994568\n",
      "Epoch 3, Batch 768, Test Loss: 0.6516106724739075\n",
      "Epoch 3, Batch 769, Test Loss: 0.7289838194847107\n",
      "Epoch 3, Batch 770, Test Loss: 0.9133554100990295\n",
      "Epoch 3, Batch 771, Test Loss: 0.520605742931366\n",
      "Epoch 3, Batch 772, Test Loss: 0.671043872833252\n",
      "Epoch 3, Batch 773, Test Loss: 0.7788567543029785\n",
      "Epoch 3, Batch 774, Test Loss: 0.6103801727294922\n",
      "Epoch 3, Batch 775, Test Loss: 0.6708244681358337\n",
      "Epoch 3, Batch 776, Test Loss: 0.7648966312408447\n",
      "Epoch 3, Batch 777, Test Loss: 0.817423939704895\n",
      "Epoch 3, Batch 778, Test Loss: 0.7289904356002808\n",
      "Epoch 3, Batch 779, Test Loss: 0.7098664045333862\n",
      "Epoch 3, Batch 780, Test Loss: 0.716554582118988\n",
      "Epoch 3, Batch 781, Test Loss: 0.6838870048522949\n",
      "Epoch 3, Batch 782, Test Loss: 0.5332378149032593\n",
      "Epoch 3, Batch 783, Test Loss: 0.5194337368011475\n",
      "Epoch 3, Batch 784, Test Loss: 0.573448121547699\n",
      "Epoch 3, Batch 785, Test Loss: 0.6754570603370667\n",
      "Epoch 3, Batch 786, Test Loss: 0.8373204469680786\n",
      "Epoch 3, Batch 787, Test Loss: 0.457422137260437\n",
      "Epoch 3, Batch 788, Test Loss: 0.7652024626731873\n",
      "Epoch 3, Batch 789, Test Loss: 0.5300396680831909\n",
      "Epoch 3, Batch 790, Test Loss: 0.7603264451026917\n",
      "Epoch 3, Batch 791, Test Loss: 0.6841489672660828\n",
      "Epoch 3, Batch 792, Test Loss: 0.7441803216934204\n",
      "Epoch 3, Batch 793, Test Loss: 0.7517268657684326\n",
      "Epoch 3, Batch 794, Test Loss: 0.6473809480667114\n",
      "Epoch 3, Batch 795, Test Loss: 0.6331785321235657\n",
      "Epoch 3, Batch 796, Test Loss: 0.760431170463562\n",
      "Epoch 3, Batch 797, Test Loss: 0.7282606363296509\n",
      "Epoch 3, Batch 798, Test Loss: 0.6924567222595215\n",
      "Epoch 3, Batch 799, Test Loss: 0.5459873080253601\n",
      "Epoch 3, Batch 800, Test Loss: 0.73747318983078\n",
      "Epoch 3, Batch 801, Test Loss: 0.4822862148284912\n",
      "Epoch 3, Batch 802, Test Loss: 0.7529475092887878\n",
      "Epoch 3, Batch 803, Test Loss: 0.6791583895683289\n",
      "Epoch 3, Batch 804, Test Loss: 0.7312667369842529\n",
      "Epoch 3, Batch 805, Test Loss: 0.6584818959236145\n",
      "Epoch 3, Batch 806, Test Loss: 0.8053691983222961\n",
      "Epoch 3, Batch 807, Test Loss: 0.7293188571929932\n",
      "Epoch 3, Batch 808, Test Loss: 0.9335989952087402\n",
      "Epoch 3, Batch 809, Test Loss: 0.6979011297225952\n",
      "Epoch 3, Batch 810, Test Loss: 0.8169792294502258\n",
      "Epoch 3, Batch 811, Test Loss: 0.6712933778762817\n",
      "Epoch 3, Batch 812, Test Loss: 0.7169269323348999\n",
      "Epoch 3, Batch 813, Test Loss: 0.7784229516983032\n",
      "Epoch 3, Batch 814, Test Loss: 0.6157729029655457\n",
      "Epoch 3, Batch 815, Test Loss: 0.595457911491394\n",
      "Epoch 3, Batch 816, Test Loss: 1.287830114364624\n",
      "Epoch 3, Batch 817, Test Loss: 0.4367525279521942\n",
      "Epoch 3, Batch 818, Test Loss: 0.550916850566864\n",
      "Epoch 3, Batch 819, Test Loss: 0.709883987903595\n",
      "Epoch 3, Batch 820, Test Loss: 0.9101316332817078\n",
      "Epoch 3, Batch 821, Test Loss: 0.6768165230751038\n",
      "Epoch 3, Batch 822, Test Loss: 0.6446678042411804\n",
      "Epoch 3, Batch 823, Test Loss: 0.6790391802787781\n",
      "Epoch 3, Batch 824, Test Loss: 0.7012282013893127\n",
      "Epoch 3, Batch 825, Test Loss: 0.6015047430992126\n",
      "Epoch 3, Batch 826, Test Loss: 0.7753992080688477\n",
      "Epoch 3, Batch 827, Test Loss: 0.7262834906578064\n",
      "Epoch 3, Batch 828, Test Loss: 0.6101769208908081\n",
      "Epoch 3, Batch 829, Test Loss: 0.6174528002738953\n",
      "Epoch 3, Batch 830, Test Loss: 0.7982377409934998\n",
      "Epoch 3, Batch 831, Test Loss: 0.7694371342658997\n",
      "Epoch 3, Batch 832, Test Loss: 0.6954454779624939\n",
      "Epoch 3, Batch 833, Test Loss: 0.6826220750808716\n",
      "Epoch 3, Batch 834, Test Loss: 0.7017527222633362\n",
      "Epoch 3, Batch 835, Test Loss: 0.6203248500823975\n",
      "Epoch 3, Batch 836, Test Loss: 0.850161075592041\n",
      "Epoch 3, Batch 837, Test Loss: 0.7720474004745483\n",
      "Epoch 3, Batch 838, Test Loss: 0.7197251319885254\n",
      "Epoch 3, Batch 839, Test Loss: 0.8396291136741638\n",
      "Epoch 3, Batch 840, Test Loss: 0.7029168605804443\n",
      "Epoch 3, Batch 841, Test Loss: 0.7507739663124084\n",
      "Epoch 3, Batch 842, Test Loss: 0.5432687997817993\n",
      "Epoch 3, Batch 843, Test Loss: 0.5726915597915649\n",
      "Epoch 3, Batch 844, Test Loss: 0.7973735332489014\n",
      "Epoch 3, Batch 845, Test Loss: 0.7694283723831177\n",
      "Epoch 3, Batch 846, Test Loss: 0.9288655519485474\n",
      "Epoch 3, Batch 847, Test Loss: 0.5652615427970886\n",
      "Epoch 3, Batch 848, Test Loss: 0.6571762561798096\n",
      "Epoch 3, Batch 849, Test Loss: 0.5887566804885864\n",
      "Epoch 3, Batch 850, Test Loss: 0.7076385021209717\n",
      "Epoch 3, Batch 851, Test Loss: 0.6742897033691406\n",
      "Epoch 3, Batch 852, Test Loss: 0.6963093876838684\n",
      "Epoch 3, Batch 853, Test Loss: 0.41497915983200073\n",
      "Epoch 3, Batch 854, Test Loss: 0.7072240710258484\n",
      "Epoch 3, Batch 855, Test Loss: 0.8113152980804443\n",
      "Epoch 3, Batch 856, Test Loss: 0.599168062210083\n",
      "Epoch 3, Batch 857, Test Loss: 0.6088384985923767\n",
      "Epoch 3, Batch 858, Test Loss: 0.9797314405441284\n",
      "Epoch 3, Batch 859, Test Loss: 0.7450096011161804\n",
      "Epoch 3, Batch 860, Test Loss: 0.8131291270256042\n",
      "Epoch 3, Batch 861, Test Loss: 0.6269530057907104\n",
      "Epoch 3, Batch 862, Test Loss: 0.5467072129249573\n",
      "Epoch 3, Batch 863, Test Loss: 0.6529843211174011\n",
      "Epoch 3, Batch 864, Test Loss: 0.718148946762085\n",
      "Epoch 3, Batch 865, Test Loss: 0.6895143389701843\n",
      "Epoch 3, Batch 866, Test Loss: 0.8524461388587952\n",
      "Epoch 3, Batch 867, Test Loss: 0.9042456746101379\n",
      "Epoch 3, Batch 868, Test Loss: 0.6757625341415405\n",
      "Epoch 3, Batch 869, Test Loss: 0.6726925373077393\n",
      "Epoch 3, Batch 870, Test Loss: 0.7350397109985352\n",
      "Epoch 3, Batch 871, Test Loss: 0.8287861347198486\n",
      "Epoch 3, Batch 872, Test Loss: 0.9962702989578247\n",
      "Epoch 3, Batch 873, Test Loss: 0.7459558844566345\n",
      "Epoch 3, Batch 874, Test Loss: 0.7487072944641113\n",
      "Epoch 3, Batch 875, Test Loss: 0.8819383382797241\n",
      "Epoch 3, Batch 876, Test Loss: 0.6529055833816528\n",
      "Epoch 3, Batch 877, Test Loss: 0.8187423944473267\n",
      "Epoch 3, Batch 878, Test Loss: 0.7043764591217041\n",
      "Epoch 3, Batch 879, Test Loss: 0.662115216255188\n",
      "Epoch 3, Batch 880, Test Loss: 0.7794021964073181\n",
      "Epoch 3, Batch 881, Test Loss: 0.6230937242507935\n",
      "Epoch 3, Batch 882, Test Loss: 0.9231645464897156\n",
      "Epoch 3, Batch 883, Test Loss: 0.7506967782974243\n",
      "Epoch 3, Batch 884, Test Loss: 0.7205438613891602\n",
      "Epoch 3, Batch 885, Test Loss: 0.8588705062866211\n",
      "Epoch 3, Batch 886, Test Loss: 0.7382466793060303\n",
      "Epoch 3, Batch 887, Test Loss: 0.6615789532661438\n",
      "Epoch 3, Batch 888, Test Loss: 0.666132390499115\n",
      "Epoch 3, Batch 889, Test Loss: 0.7705683708190918\n",
      "Epoch 3, Batch 890, Test Loss: 0.6123380064964294\n",
      "Epoch 3, Batch 891, Test Loss: 0.7492378354072571\n",
      "Epoch 3, Batch 892, Test Loss: 0.6495463252067566\n",
      "Epoch 3, Batch 893, Test Loss: 0.7202864289283752\n",
      "Epoch 3, Batch 894, Test Loss: 0.8935112357139587\n",
      "Epoch 3, Batch 895, Test Loss: 0.5955901145935059\n",
      "Epoch 3, Batch 896, Test Loss: 0.7226094007492065\n",
      "Epoch 3, Batch 897, Test Loss: 0.6749765276908875\n",
      "Epoch 3, Batch 898, Test Loss: 0.6451916098594666\n",
      "Epoch 3, Batch 899, Test Loss: 0.8041508197784424\n",
      "Epoch 3, Batch 900, Test Loss: 0.5232835412025452\n",
      "Epoch 3, Batch 901, Test Loss: 0.6475449800491333\n",
      "Epoch 3, Batch 902, Test Loss: 0.6396124958992004\n",
      "Epoch 3, Batch 903, Test Loss: 0.653176486492157\n",
      "Epoch 3, Batch 904, Test Loss: 0.5871838331222534\n",
      "Epoch 3, Batch 905, Test Loss: 0.6894198060035706\n",
      "Epoch 3, Batch 906, Test Loss: 0.8861894607543945\n",
      "Epoch 3, Batch 907, Test Loss: 0.5827788710594177\n",
      "Epoch 3, Batch 908, Test Loss: 0.9157191514968872\n",
      "Epoch 3, Batch 909, Test Loss: 0.5323492884635925\n",
      "Epoch 3, Batch 910, Test Loss: 0.6519158482551575\n",
      "Epoch 3, Batch 911, Test Loss: 0.573564887046814\n",
      "Epoch 3, Batch 912, Test Loss: 0.5524860620498657\n",
      "Epoch 3, Batch 913, Test Loss: 0.7403373718261719\n",
      "Epoch 3, Batch 914, Test Loss: 0.7636321187019348\n",
      "Epoch 3, Batch 915, Test Loss: 0.7553945779800415\n",
      "Epoch 3, Batch 916, Test Loss: 0.7433143854141235\n",
      "Epoch 3, Batch 917, Test Loss: 0.7061118483543396\n",
      "Epoch 3, Batch 918, Test Loss: 0.8184866905212402\n",
      "Epoch 3, Batch 919, Test Loss: 0.7151325345039368\n",
      "Epoch 3, Batch 920, Test Loss: 0.8290143609046936\n",
      "Epoch 3, Batch 921, Test Loss: 0.8184787631034851\n",
      "Epoch 3, Batch 922, Test Loss: 0.6996439099311829\n",
      "Epoch 3, Batch 923, Test Loss: 0.6367833614349365\n",
      "Epoch 3, Batch 924, Test Loss: 0.7691726684570312\n",
      "Epoch 3, Batch 925, Test Loss: 0.8794870972633362\n",
      "Epoch 3, Batch 926, Test Loss: 0.5756157636642456\n",
      "Epoch 3, Batch 927, Test Loss: 0.7658587694168091\n",
      "Epoch 3, Batch 928, Test Loss: 0.6730379462242126\n",
      "Epoch 3, Batch 929, Test Loss: 0.5944850444793701\n",
      "Epoch 3, Batch 930, Test Loss: 0.7327760457992554\n",
      "Epoch 3, Batch 931, Test Loss: 0.6808790564537048\n",
      "Epoch 3, Batch 932, Test Loss: 0.6993807554244995\n",
      "Epoch 3, Batch 933, Test Loss: 0.812458872795105\n",
      "Epoch 3, Batch 934, Test Loss: 0.6917362213134766\n",
      "Epoch 3, Batch 935, Test Loss: 0.7599323391914368\n",
      "Epoch 3, Batch 936, Test Loss: 0.674436628818512\n",
      "Epoch 3, Batch 937, Test Loss: 0.5727007389068604\n",
      "Epoch 3, Batch 938, Test Loss: 0.5733575820922852\n",
      "Accuracy of Test set: 0.74095\n",
      "Epoch 4, Batch 1, Loss: 1.0314382314682007\n",
      "Epoch 4, Batch 2, Loss: 0.5388574004173279\n",
      "Epoch 4, Batch 3, Loss: 0.7837115526199341\n",
      "Epoch 4, Batch 4, Loss: 0.9271355271339417\n",
      "Epoch 4, Batch 5, Loss: 0.598152220249176\n",
      "Epoch 4, Batch 6, Loss: 0.5811161994934082\n",
      "Epoch 4, Batch 7, Loss: 0.8893437385559082\n",
      "Epoch 4, Batch 8, Loss: 0.6283791661262512\n",
      "Epoch 4, Batch 9, Loss: 0.7519466876983643\n",
      "Epoch 4, Batch 10, Loss: 0.5528173446655273\n",
      "Epoch 4, Batch 11, Loss: 0.6106584072113037\n",
      "Epoch 4, Batch 12, Loss: 0.7269565463066101\n",
      "Epoch 4, Batch 13, Loss: 0.6788583397865295\n",
      "Epoch 4, Batch 14, Loss: 0.7745294570922852\n",
      "Epoch 4, Batch 15, Loss: 0.6170534491539001\n",
      "Epoch 4, Batch 16, Loss: 0.6258848905563354\n",
      "Epoch 4, Batch 17, Loss: 0.7178668975830078\n",
      "Epoch 4, Batch 18, Loss: 0.8315562009811401\n",
      "Epoch 4, Batch 19, Loss: 0.6042452454566956\n",
      "Epoch 4, Batch 20, Loss: 0.7212095856666565\n",
      "Epoch 4, Batch 21, Loss: 0.9010675549507141\n",
      "Epoch 4, Batch 22, Loss: 0.7125672101974487\n",
      "Epoch 4, Batch 23, Loss: 0.587369441986084\n",
      "Epoch 4, Batch 24, Loss: 0.7213698625564575\n",
      "Epoch 4, Batch 25, Loss: 0.6825973987579346\n",
      "Epoch 4, Batch 26, Loss: 0.8541021347045898\n",
      "Epoch 4, Batch 27, Loss: 0.8359706997871399\n",
      "Epoch 4, Batch 28, Loss: 0.7380364537239075\n",
      "Epoch 4, Batch 29, Loss: 0.7728002667427063\n",
      "Epoch 4, Batch 30, Loss: 0.7260496020317078\n",
      "Epoch 4, Batch 31, Loss: 0.612438976764679\n",
      "Epoch 4, Batch 32, Loss: 0.8652535676956177\n",
      "Epoch 4, Batch 33, Loss: 0.6977574825286865\n",
      "Epoch 4, Batch 34, Loss: 0.6285367012023926\n",
      "Epoch 4, Batch 35, Loss: 0.8494632840156555\n",
      "Epoch 4, Batch 36, Loss: 0.6706832647323608\n",
      "Epoch 4, Batch 37, Loss: 0.6630547642707825\n",
      "Epoch 4, Batch 38, Loss: 0.63848876953125\n",
      "Epoch 4, Batch 39, Loss: 0.7425145506858826\n",
      "Epoch 4, Batch 40, Loss: 0.7193465828895569\n",
      "Epoch 4, Batch 41, Loss: 0.659166157245636\n",
      "Epoch 4, Batch 42, Loss: 0.6648386120796204\n",
      "Epoch 4, Batch 43, Loss: 0.6072447299957275\n",
      "Epoch 4, Batch 44, Loss: 0.7530577182769775\n",
      "Epoch 4, Batch 45, Loss: 0.5246474742889404\n",
      "Epoch 4, Batch 46, Loss: 0.7048059701919556\n",
      "Epoch 4, Batch 47, Loss: 0.6240649223327637\n",
      "Epoch 4, Batch 48, Loss: 0.8777281641960144\n",
      "Epoch 4, Batch 49, Loss: 0.9249243140220642\n",
      "Epoch 4, Batch 50, Loss: 0.7817211151123047\n",
      "Epoch 4, Batch 51, Loss: 0.6756845712661743\n",
      "Epoch 4, Batch 52, Loss: 0.5723050236701965\n",
      "Epoch 4, Batch 53, Loss: 0.5525360703468323\n",
      "Epoch 4, Batch 54, Loss: 0.644930899143219\n",
      "Epoch 4, Batch 55, Loss: 0.5454668402671814\n",
      "Epoch 4, Batch 56, Loss: 0.6889177560806274\n",
      "Epoch 4, Batch 57, Loss: 0.8330406546592712\n",
      "Epoch 4, Batch 58, Loss: 0.6607663631439209\n",
      "Epoch 4, Batch 59, Loss: 0.5982387065887451\n",
      "Epoch 4, Batch 60, Loss: 0.7088877558708191\n",
      "Epoch 4, Batch 61, Loss: 0.8814123868942261\n",
      "Epoch 4, Batch 62, Loss: 0.711972713470459\n",
      "Epoch 4, Batch 63, Loss: 0.6498233079910278\n",
      "Epoch 4, Batch 64, Loss: 0.7296923398971558\n",
      "Epoch 4, Batch 65, Loss: 0.5086941123008728\n",
      "Epoch 4, Batch 66, Loss: 1.061672568321228\n",
      "Epoch 4, Batch 67, Loss: 0.5929926037788391\n",
      "Epoch 4, Batch 68, Loss: 0.7606620192527771\n",
      "Epoch 4, Batch 69, Loss: 0.736290693283081\n",
      "Epoch 4, Batch 70, Loss: 0.8868095874786377\n",
      "Epoch 4, Batch 71, Loss: 0.7016793489456177\n",
      "Epoch 4, Batch 72, Loss: 0.664126992225647\n",
      "Epoch 4, Batch 73, Loss: 0.5282166600227356\n",
      "Epoch 4, Batch 74, Loss: 0.6281387805938721\n",
      "Epoch 4, Batch 75, Loss: 0.7952262163162231\n",
      "Epoch 4, Batch 76, Loss: 0.6146272420883179\n",
      "Epoch 4, Batch 77, Loss: 0.7178868651390076\n",
      "Epoch 4, Batch 78, Loss: 0.5622642636299133\n",
      "Epoch 4, Batch 79, Loss: 0.707880437374115\n",
      "Epoch 4, Batch 80, Loss: 0.7215003371238708\n",
      "Epoch 4, Batch 81, Loss: 0.6457691788673401\n",
      "Epoch 4, Batch 82, Loss: 0.7598942518234253\n",
      "Epoch 4, Batch 83, Loss: 0.6764194369316101\n",
      "Epoch 4, Batch 84, Loss: 0.684668242931366\n",
      "Epoch 4, Batch 85, Loss: 0.7962394952774048\n",
      "Epoch 4, Batch 86, Loss: 0.6609436273574829\n",
      "Epoch 4, Batch 87, Loss: 0.6756972670555115\n",
      "Epoch 4, Batch 88, Loss: 0.7786075472831726\n",
      "Epoch 4, Batch 89, Loss: 0.753007173538208\n",
      "Epoch 4, Batch 90, Loss: 0.7982138395309448\n",
      "Epoch 4, Batch 91, Loss: 0.9429632425308228\n",
      "Epoch 4, Batch 92, Loss: 0.5986438989639282\n",
      "Epoch 4, Batch 93, Loss: 0.5859941840171814\n",
      "Epoch 4, Batch 94, Loss: 0.7859678268432617\n",
      "Epoch 4, Batch 95, Loss: 0.7073012590408325\n",
      "Epoch 4, Batch 96, Loss: 0.5713513493537903\n",
      "Epoch 4, Batch 97, Loss: 0.6310445666313171\n",
      "Epoch 4, Batch 98, Loss: 0.6651737093925476\n",
      "Epoch 4, Batch 99, Loss: 0.7406706809997559\n",
      "Epoch 4, Batch 100, Loss: 0.751358151435852\n",
      "Epoch 4, Batch 101, Loss: 0.6745786666870117\n",
      "Epoch 4, Batch 102, Loss: 0.6963279843330383\n",
      "Epoch 4, Batch 103, Loss: 0.8088172674179077\n",
      "Epoch 4, Batch 104, Loss: 0.7848674654960632\n",
      "Epoch 4, Batch 105, Loss: 0.6547004580497742\n",
      "Epoch 4, Batch 106, Loss: 0.601163387298584\n",
      "Epoch 4, Batch 107, Loss: 0.866743803024292\n",
      "Epoch 4, Batch 108, Loss: 0.6392602920532227\n",
      "Epoch 4, Batch 109, Loss: 0.5318730473518372\n",
      "Epoch 4, Batch 110, Loss: 0.5928622484207153\n",
      "Epoch 4, Batch 111, Loss: 0.7638758420944214\n",
      "Epoch 4, Batch 112, Loss: 0.8678472638130188\n",
      "Epoch 4, Batch 113, Loss: 0.5790978670120239\n",
      "Epoch 4, Batch 114, Loss: 0.7928539514541626\n",
      "Epoch 4, Batch 115, Loss: 0.7521508932113647\n",
      "Epoch 4, Batch 116, Loss: 0.7176008224487305\n",
      "Epoch 4, Batch 117, Loss: 0.6922627091407776\n",
      "Epoch 4, Batch 118, Loss: 0.7399572134017944\n",
      "Epoch 4, Batch 119, Loss: 0.8030163645744324\n",
      "Epoch 4, Batch 120, Loss: 0.6915977597236633\n",
      "Epoch 4, Batch 121, Loss: 0.8347679376602173\n",
      "Epoch 4, Batch 122, Loss: 0.8118602633476257\n",
      "Epoch 4, Batch 123, Loss: 0.8320797681808472\n",
      "Epoch 4, Batch 124, Loss: 0.7499582767486572\n",
      "Epoch 4, Batch 125, Loss: 0.7641776204109192\n",
      "Epoch 4, Batch 126, Loss: 0.8491681814193726\n",
      "Epoch 4, Batch 127, Loss: 0.830885648727417\n",
      "Epoch 4, Batch 128, Loss: 0.6641842722892761\n",
      "Epoch 4, Batch 129, Loss: 0.6924471855163574\n",
      "Epoch 4, Batch 130, Loss: 0.8721213936805725\n",
      "Epoch 4, Batch 131, Loss: 0.5468376874923706\n",
      "Epoch 4, Batch 132, Loss: 0.8195156455039978\n",
      "Epoch 4, Batch 133, Loss: 0.578668475151062\n",
      "Epoch 4, Batch 134, Loss: 0.7001855373382568\n",
      "Epoch 4, Batch 135, Loss: 0.7339054346084595\n",
      "Epoch 4, Batch 136, Loss: 0.6724614500999451\n",
      "Epoch 4, Batch 137, Loss: 0.7614976763725281\n",
      "Epoch 4, Batch 138, Loss: 0.8975184559822083\n",
      "Epoch 4, Batch 139, Loss: 0.6825785040855408\n",
      "Epoch 4, Batch 140, Loss: 0.5344839692115784\n",
      "Epoch 4, Batch 141, Loss: 0.8464429378509521\n",
      "Epoch 4, Batch 142, Loss: 0.733701229095459\n",
      "Epoch 4, Batch 143, Loss: 0.7051324844360352\n",
      "Epoch 4, Batch 144, Loss: 0.6425926685333252\n",
      "Epoch 4, Batch 145, Loss: 0.6847667098045349\n",
      "Epoch 4, Batch 146, Loss: 0.7016633152961731\n",
      "Epoch 4, Batch 147, Loss: 0.8589205741882324\n",
      "Epoch 4, Batch 148, Loss: 0.5712912678718567\n",
      "Epoch 4, Batch 149, Loss: 0.6213926672935486\n",
      "Epoch 4, Batch 150, Loss: 0.784989058971405\n",
      "Epoch 4, Batch 151, Loss: 0.7656570076942444\n",
      "Epoch 4, Batch 152, Loss: 0.6915274858474731\n",
      "Epoch 4, Batch 153, Loss: 0.7401138544082642\n",
      "Epoch 4, Batch 154, Loss: 0.7274069786071777\n",
      "Epoch 4, Batch 155, Loss: 0.5863196849822998\n",
      "Epoch 4, Batch 156, Loss: 0.718564510345459\n",
      "Epoch 4, Batch 157, Loss: 1.0734132528305054\n",
      "Epoch 4, Batch 158, Loss: 0.534434974193573\n",
      "Epoch 4, Batch 159, Loss: 0.7332978248596191\n",
      "Epoch 4, Batch 160, Loss: 0.5573743581771851\n",
      "Epoch 4, Batch 161, Loss: 0.6038491725921631\n",
      "Epoch 4, Batch 162, Loss: 0.7234567403793335\n",
      "Epoch 4, Batch 163, Loss: 0.5426417589187622\n",
      "Epoch 4, Batch 164, Loss: 0.6987259387969971\n",
      "Epoch 4, Batch 165, Loss: 0.6903401017189026\n",
      "Epoch 4, Batch 166, Loss: 0.7365655899047852\n",
      "Epoch 4, Batch 167, Loss: 0.760014533996582\n",
      "Epoch 4, Batch 168, Loss: 0.7080907821655273\n",
      "Epoch 4, Batch 169, Loss: 0.5421230792999268\n",
      "Epoch 4, Batch 170, Loss: 0.4445004165172577\n",
      "Epoch 4, Batch 171, Loss: 0.6508377194404602\n",
      "Epoch 4, Batch 172, Loss: 0.6944963932037354\n",
      "Epoch 4, Batch 173, Loss: 0.8835587501525879\n",
      "Epoch 4, Batch 174, Loss: 0.6331225633621216\n",
      "Epoch 4, Batch 175, Loss: 0.6333203315734863\n",
      "Epoch 4, Batch 176, Loss: 0.7394546270370483\n",
      "Epoch 4, Batch 177, Loss: 0.6506249308586121\n",
      "Epoch 4, Batch 178, Loss: 0.8313782215118408\n",
      "Epoch 4, Batch 179, Loss: 0.5370465517044067\n",
      "Epoch 4, Batch 180, Loss: 0.7473839521408081\n",
      "Epoch 4, Batch 181, Loss: 0.7190935015678406\n",
      "Epoch 4, Batch 182, Loss: 0.6961874961853027\n",
      "Epoch 4, Batch 183, Loss: 0.7322295904159546\n",
      "Epoch 4, Batch 184, Loss: 0.7120720744132996\n",
      "Epoch 4, Batch 185, Loss: 0.550941526889801\n",
      "Epoch 4, Batch 186, Loss: 0.873420238494873\n",
      "Epoch 4, Batch 187, Loss: 0.6471387147903442\n",
      "Epoch 4, Batch 188, Loss: 0.6123905181884766\n",
      "Epoch 4, Batch 189, Loss: 0.5438714027404785\n",
      "Epoch 4, Batch 190, Loss: 0.6472970843315125\n",
      "Epoch 4, Batch 191, Loss: 0.8245072364807129\n",
      "Epoch 4, Batch 192, Loss: 0.6302441358566284\n",
      "Epoch 4, Batch 193, Loss: 0.9761551022529602\n",
      "Epoch 4, Batch 194, Loss: 0.5144291520118713\n",
      "Epoch 4, Batch 195, Loss: 0.6388171911239624\n",
      "Epoch 4, Batch 196, Loss: 0.830495297908783\n",
      "Epoch 4, Batch 197, Loss: 0.567901611328125\n",
      "Epoch 4, Batch 198, Loss: 0.5646803975105286\n",
      "Epoch 4, Batch 199, Loss: 0.5242964625358582\n",
      "Epoch 4, Batch 200, Loss: 0.6491537094116211\n",
      "Epoch 4, Batch 201, Loss: 0.7960058450698853\n",
      "Epoch 4, Batch 202, Loss: 0.6241962909698486\n",
      "Epoch 4, Batch 203, Loss: 0.7153506875038147\n",
      "Epoch 4, Batch 204, Loss: 0.6622792482376099\n",
      "Epoch 4, Batch 205, Loss: 0.668709933757782\n",
      "Epoch 4, Batch 206, Loss: 0.6191865801811218\n",
      "Epoch 4, Batch 207, Loss: 0.6972482800483704\n",
      "Epoch 4, Batch 208, Loss: 0.6824870705604553\n",
      "Epoch 4, Batch 209, Loss: 0.6124762892723083\n",
      "Epoch 4, Batch 210, Loss: 0.7984664440155029\n",
      "Epoch 4, Batch 211, Loss: 0.6935959458351135\n",
      "Epoch 4, Batch 212, Loss: 0.5783024430274963\n",
      "Epoch 4, Batch 213, Loss: 0.738612949848175\n",
      "Epoch 4, Batch 214, Loss: 0.5560266375541687\n",
      "Epoch 4, Batch 215, Loss: 0.6109355092048645\n",
      "Epoch 4, Batch 216, Loss: 0.7976246476173401\n",
      "Epoch 4, Batch 217, Loss: 0.5279877185821533\n",
      "Epoch 4, Batch 218, Loss: 0.7974741458892822\n",
      "Epoch 4, Batch 219, Loss: 0.5425605177879333\n",
      "Epoch 4, Batch 220, Loss: 0.7277774214744568\n",
      "Epoch 4, Batch 221, Loss: 0.5830318331718445\n",
      "Epoch 4, Batch 222, Loss: 0.6063529253005981\n",
      "Epoch 4, Batch 223, Loss: 0.6279350519180298\n",
      "Epoch 4, Batch 224, Loss: 0.5544847249984741\n",
      "Epoch 4, Batch 225, Loss: 0.7519629597663879\n",
      "Epoch 4, Batch 226, Loss: 0.6005021929740906\n",
      "Epoch 4, Batch 227, Loss: 0.7446577548980713\n",
      "Epoch 4, Batch 228, Loss: 0.8872758746147156\n",
      "Epoch 4, Batch 229, Loss: 0.5700660943984985\n",
      "Epoch 4, Batch 230, Loss: 0.4648059606552124\n",
      "Epoch 4, Batch 231, Loss: 0.7721909284591675\n",
      "Epoch 4, Batch 232, Loss: 0.7694838047027588\n",
      "Epoch 4, Batch 233, Loss: 0.6981488466262817\n",
      "Epoch 4, Batch 234, Loss: 0.9756258726119995\n",
      "Epoch 4, Batch 235, Loss: 0.7316046953201294\n",
      "Epoch 4, Batch 236, Loss: 0.6067314743995667\n",
      "Epoch 4, Batch 237, Loss: 0.4025384783744812\n",
      "Epoch 4, Batch 238, Loss: 0.6381928324699402\n",
      "Epoch 4, Batch 239, Loss: 0.658967137336731\n",
      "Epoch 4, Batch 240, Loss: 0.6286857724189758\n",
      "Epoch 4, Batch 241, Loss: 0.5048776865005493\n",
      "Epoch 4, Batch 242, Loss: 0.8076316118240356\n",
      "Epoch 4, Batch 243, Loss: 0.783483624458313\n",
      "Epoch 4, Batch 244, Loss: 0.5364984273910522\n",
      "Epoch 4, Batch 245, Loss: 0.6697967052459717\n",
      "Epoch 4, Batch 246, Loss: 0.6125063300132751\n",
      "Epoch 4, Batch 247, Loss: 0.6307919025421143\n",
      "Epoch 4, Batch 248, Loss: 0.6523647308349609\n",
      "Epoch 4, Batch 249, Loss: 0.5925031900405884\n",
      "Epoch 4, Batch 250, Loss: 0.7877960205078125\n",
      "Epoch 4, Batch 251, Loss: 0.6622042655944824\n",
      "Epoch 4, Batch 252, Loss: 0.8633472323417664\n",
      "Epoch 4, Batch 253, Loss: 0.8575066924095154\n",
      "Epoch 4, Batch 254, Loss: 0.7736061215400696\n",
      "Epoch 4, Batch 255, Loss: 0.7246741056442261\n",
      "Epoch 4, Batch 256, Loss: 0.876390814781189\n",
      "Epoch 4, Batch 257, Loss: 0.8115695714950562\n",
      "Epoch 4, Batch 258, Loss: 0.7766737937927246\n",
      "Epoch 4, Batch 259, Loss: 0.7184407711029053\n",
      "Epoch 4, Batch 260, Loss: 0.9521499872207642\n",
      "Epoch 4, Batch 261, Loss: 0.5978926420211792\n",
      "Epoch 4, Batch 262, Loss: 0.8527001142501831\n",
      "Epoch 4, Batch 263, Loss: 0.5633431077003479\n",
      "Epoch 4, Batch 264, Loss: 0.7718988656997681\n",
      "Epoch 4, Batch 265, Loss: 0.6888352036476135\n",
      "Epoch 4, Batch 266, Loss: 0.8726058006286621\n",
      "Epoch 4, Batch 267, Loss: 0.6696233153343201\n",
      "Epoch 4, Batch 268, Loss: 0.6238217353820801\n",
      "Epoch 4, Batch 269, Loss: 0.6740763783454895\n",
      "Epoch 4, Batch 270, Loss: 0.6430500745773315\n",
      "Epoch 4, Batch 271, Loss: 0.7926718592643738\n",
      "Epoch 4, Batch 272, Loss: 0.6898068785667419\n",
      "Epoch 4, Batch 273, Loss: 0.7422931790351868\n",
      "Epoch 4, Batch 274, Loss: 0.5795005559921265\n",
      "Epoch 4, Batch 275, Loss: 0.66722571849823\n",
      "Epoch 4, Batch 276, Loss: 0.969811737537384\n",
      "Epoch 4, Batch 277, Loss: 0.8316177725791931\n",
      "Epoch 4, Batch 278, Loss: 0.8476592898368835\n",
      "Epoch 4, Batch 279, Loss: 0.615234911441803\n",
      "Epoch 4, Batch 280, Loss: 0.4962014853954315\n",
      "Epoch 4, Batch 281, Loss: 0.6505574584007263\n",
      "Epoch 4, Batch 282, Loss: 0.7154253125190735\n",
      "Epoch 4, Batch 283, Loss: 0.912272572517395\n",
      "Epoch 4, Batch 284, Loss: 0.5468713045120239\n",
      "Epoch 4, Batch 285, Loss: 0.47446098923683167\n",
      "Epoch 4, Batch 286, Loss: 0.8610460162162781\n",
      "Epoch 4, Batch 287, Loss: 0.881212592124939\n",
      "Epoch 4, Batch 288, Loss: 0.7878656387329102\n",
      "Epoch 4, Batch 289, Loss: 0.4777519404888153\n",
      "Epoch 4, Batch 290, Loss: 0.35732075572013855\n",
      "Epoch 4, Batch 291, Loss: 0.794439435005188\n",
      "Epoch 4, Batch 292, Loss: 0.7268504500389099\n",
      "Epoch 4, Batch 293, Loss: 0.7108347415924072\n",
      "Epoch 4, Batch 294, Loss: 0.6021526455879211\n",
      "Epoch 4, Batch 295, Loss: 0.7667445540428162\n",
      "Epoch 4, Batch 296, Loss: 0.7044109106063843\n",
      "Epoch 4, Batch 297, Loss: 0.7040007710456848\n",
      "Epoch 4, Batch 298, Loss: 0.555311918258667\n",
      "Epoch 4, Batch 299, Loss: 0.671528160572052\n",
      "Epoch 4, Batch 300, Loss: 0.603983998298645\n",
      "Epoch 4, Batch 301, Loss: 0.6161532998085022\n",
      "Epoch 4, Batch 302, Loss: 0.6578829288482666\n",
      "Epoch 4, Batch 303, Loss: 0.5382066369056702\n",
      "Epoch 4, Batch 304, Loss: 0.6511549353599548\n",
      "Epoch 4, Batch 305, Loss: 0.6195144057273865\n",
      "Epoch 4, Batch 306, Loss: 0.8053932785987854\n",
      "Epoch 4, Batch 307, Loss: 0.7172709107398987\n",
      "Epoch 4, Batch 308, Loss: 0.6943994760513306\n",
      "Epoch 4, Batch 309, Loss: 0.5929000377655029\n",
      "Epoch 4, Batch 310, Loss: 0.7804015874862671\n",
      "Epoch 4, Batch 311, Loss: 0.5547538995742798\n",
      "Epoch 4, Batch 312, Loss: 0.8789451122283936\n",
      "Epoch 4, Batch 313, Loss: 0.7535864114761353\n",
      "Epoch 4, Batch 314, Loss: 0.6257065534591675\n",
      "Epoch 4, Batch 315, Loss: 0.6101061105728149\n",
      "Epoch 4, Batch 316, Loss: 0.771744430065155\n",
      "Epoch 4, Batch 317, Loss: 0.5783904790878296\n",
      "Epoch 4, Batch 318, Loss: 0.5938783884048462\n",
      "Epoch 4, Batch 319, Loss: 0.6661083102226257\n",
      "Epoch 4, Batch 320, Loss: 0.6601820588111877\n",
      "Epoch 4, Batch 321, Loss: 0.7640987634658813\n",
      "Epoch 4, Batch 322, Loss: 0.9551844000816345\n",
      "Epoch 4, Batch 323, Loss: 0.8111833930015564\n",
      "Epoch 4, Batch 324, Loss: 0.618281364440918\n",
      "Epoch 4, Batch 325, Loss: 0.5591790080070496\n",
      "Epoch 4, Batch 326, Loss: 0.6845555305480957\n",
      "Epoch 4, Batch 327, Loss: 0.698688805103302\n",
      "Epoch 4, Batch 328, Loss: 0.5753250122070312\n",
      "Epoch 4, Batch 329, Loss: 0.5523163080215454\n",
      "Epoch 4, Batch 330, Loss: 0.639970600605011\n",
      "Epoch 4, Batch 331, Loss: 0.7434247732162476\n",
      "Epoch 4, Batch 332, Loss: 0.6800470352172852\n",
      "Epoch 4, Batch 333, Loss: 0.6775438189506531\n",
      "Epoch 4, Batch 334, Loss: 0.6600309610366821\n",
      "Epoch 4, Batch 335, Loss: 1.0607857704162598\n",
      "Epoch 4, Batch 336, Loss: 0.7631823420524597\n",
      "Epoch 4, Batch 337, Loss: 0.8387829661369324\n",
      "Epoch 4, Batch 338, Loss: 0.6696475148200989\n",
      "Epoch 4, Batch 339, Loss: 0.753052294254303\n",
      "Epoch 4, Batch 340, Loss: 0.5297461152076721\n",
      "Epoch 4, Batch 341, Loss: 0.6160274744033813\n",
      "Epoch 4, Batch 342, Loss: 0.7028088569641113\n",
      "Epoch 4, Batch 343, Loss: 0.6416114568710327\n",
      "Epoch 4, Batch 344, Loss: 0.6350106000900269\n",
      "Epoch 4, Batch 345, Loss: 0.6619181632995605\n",
      "Epoch 4, Batch 346, Loss: 0.7879517674446106\n",
      "Epoch 4, Batch 347, Loss: 0.7201549410820007\n",
      "Epoch 4, Batch 348, Loss: 0.9216774106025696\n",
      "Epoch 4, Batch 349, Loss: 0.5497951507568359\n",
      "Epoch 4, Batch 350, Loss: 0.6714346408843994\n",
      "Epoch 4, Batch 351, Loss: 0.8712437152862549\n",
      "Epoch 4, Batch 352, Loss: 0.7752034664154053\n",
      "Epoch 4, Batch 353, Loss: 0.6926091909408569\n",
      "Epoch 4, Batch 354, Loss: 0.5895159244537354\n",
      "Epoch 4, Batch 355, Loss: 0.8904669284820557\n",
      "Epoch 4, Batch 356, Loss: 0.7262007594108582\n",
      "Epoch 4, Batch 357, Loss: 0.5106684565544128\n",
      "Epoch 4, Batch 358, Loss: 0.893557071685791\n",
      "Epoch 4, Batch 359, Loss: 0.7615338563919067\n",
      "Epoch 4, Batch 360, Loss: 0.5748872756958008\n",
      "Epoch 4, Batch 361, Loss: 0.5688216686248779\n",
      "Epoch 4, Batch 362, Loss: 0.7184600234031677\n",
      "Epoch 4, Batch 363, Loss: 0.7409709095954895\n",
      "Epoch 4, Batch 364, Loss: 0.7296076416969299\n",
      "Epoch 4, Batch 365, Loss: 0.5936201810836792\n",
      "Epoch 4, Batch 366, Loss: 0.8170790076255798\n",
      "Epoch 4, Batch 367, Loss: 0.6935582160949707\n",
      "Epoch 4, Batch 368, Loss: 0.5511268973350525\n",
      "Epoch 4, Batch 369, Loss: 0.5536292195320129\n",
      "Epoch 4, Batch 370, Loss: 0.6554338335990906\n",
      "Epoch 4, Batch 371, Loss: 0.840303897857666\n",
      "Epoch 4, Batch 372, Loss: 0.7494351267814636\n",
      "Epoch 4, Batch 373, Loss: 0.7825596928596497\n",
      "Epoch 4, Batch 374, Loss: 0.5957956314086914\n",
      "Epoch 4, Batch 375, Loss: 0.5153477787971497\n",
      "Epoch 4, Batch 376, Loss: 0.7113862633705139\n",
      "Epoch 4, Batch 377, Loss: 0.6076194643974304\n",
      "Epoch 4, Batch 378, Loss: 0.6940434575080872\n",
      "Epoch 4, Batch 379, Loss: 0.657698392868042\n",
      "Epoch 4, Batch 380, Loss: 0.8555907607078552\n",
      "Epoch 4, Batch 381, Loss: 0.7504334449768066\n",
      "Epoch 4, Batch 382, Loss: 0.793096661567688\n",
      "Epoch 4, Batch 383, Loss: 0.5296487808227539\n",
      "Epoch 4, Batch 384, Loss: 0.9871692657470703\n",
      "Epoch 4, Batch 385, Loss: 0.8435659408569336\n",
      "Epoch 4, Batch 386, Loss: 0.6509287357330322\n",
      "Epoch 4, Batch 387, Loss: 0.6865906715393066\n",
      "Epoch 4, Batch 388, Loss: 0.8549768328666687\n",
      "Epoch 4, Batch 389, Loss: 0.6310691833496094\n",
      "Epoch 4, Batch 390, Loss: 0.6969102621078491\n",
      "Epoch 4, Batch 391, Loss: 0.580899715423584\n",
      "Epoch 4, Batch 392, Loss: 0.5903776288032532\n",
      "Epoch 4, Batch 393, Loss: 0.7180724143981934\n",
      "Epoch 4, Batch 394, Loss: 0.6313461661338806\n",
      "Epoch 4, Batch 395, Loss: 0.6416449546813965\n",
      "Epoch 4, Batch 396, Loss: 0.5213342308998108\n",
      "Epoch 4, Batch 397, Loss: 0.6883432865142822\n",
      "Epoch 4, Batch 398, Loss: 0.4595580995082855\n",
      "Epoch 4, Batch 399, Loss: 0.8037779331207275\n",
      "Epoch 4, Batch 400, Loss: 0.5819929838180542\n",
      "Epoch 4, Batch 401, Loss: 0.7607616782188416\n",
      "Epoch 4, Batch 402, Loss: 0.5240740776062012\n",
      "Epoch 4, Batch 403, Loss: 0.7821133732795715\n",
      "Epoch 4, Batch 404, Loss: 0.5882663726806641\n",
      "Epoch 4, Batch 405, Loss: 0.6591895222663879\n",
      "Epoch 4, Batch 406, Loss: 0.7627661228179932\n",
      "Epoch 4, Batch 407, Loss: 0.7891767024993896\n",
      "Epoch 4, Batch 408, Loss: 0.644813060760498\n",
      "Epoch 4, Batch 409, Loss: 0.8120276927947998\n",
      "Epoch 4, Batch 410, Loss: 1.0892552137374878\n",
      "Epoch 4, Batch 411, Loss: 0.5709097385406494\n",
      "Epoch 4, Batch 412, Loss: 0.6278393864631653\n",
      "Epoch 4, Batch 413, Loss: 0.7871742844581604\n",
      "Epoch 4, Batch 414, Loss: 0.6522627472877502\n",
      "Epoch 4, Batch 415, Loss: 0.8455842733383179\n",
      "Epoch 4, Batch 416, Loss: 0.5978266596794128\n",
      "Epoch 4, Batch 417, Loss: 0.6464285254478455\n",
      "Epoch 4, Batch 418, Loss: 0.8324889540672302\n",
      "Epoch 4, Batch 419, Loss: 0.7857718467712402\n",
      "Epoch 4, Batch 420, Loss: 0.7330758571624756\n",
      "Epoch 4, Batch 421, Loss: 0.713912844657898\n",
      "Epoch 4, Batch 422, Loss: 0.5954488515853882\n",
      "Epoch 4, Batch 423, Loss: 0.7556938529014587\n",
      "Epoch 4, Batch 424, Loss: 0.6127740740776062\n",
      "Epoch 4, Batch 425, Loss: 0.6042996644973755\n",
      "Epoch 4, Batch 426, Loss: 0.5008466243743896\n",
      "Epoch 4, Batch 427, Loss: 0.9400148987770081\n",
      "Epoch 4, Batch 428, Loss: 0.6850671172142029\n",
      "Epoch 4, Batch 429, Loss: 0.5541811585426331\n",
      "Epoch 4, Batch 430, Loss: 0.5915806293487549\n",
      "Epoch 4, Batch 431, Loss: 0.6280901432037354\n",
      "Epoch 4, Batch 432, Loss: 0.5342851877212524\n",
      "Epoch 4, Batch 433, Loss: 0.7000372409820557\n",
      "Epoch 4, Batch 434, Loss: 0.8850001096725464\n",
      "Epoch 4, Batch 435, Loss: 1.0997374057769775\n",
      "Epoch 4, Batch 436, Loss: 0.6888154149055481\n",
      "Epoch 4, Batch 437, Loss: 0.5872580409049988\n",
      "Epoch 4, Batch 438, Loss: 0.8292408585548401\n",
      "Epoch 4, Batch 439, Loss: 0.6346314549446106\n",
      "Epoch 4, Batch 440, Loss: 0.7056465744972229\n",
      "Epoch 4, Batch 441, Loss: 0.5944689512252808\n",
      "Epoch 4, Batch 442, Loss: 0.8942130208015442\n",
      "Epoch 4, Batch 443, Loss: 0.6643935441970825\n",
      "Epoch 4, Batch 444, Loss: 0.7086541652679443\n",
      "Epoch 4, Batch 445, Loss: 0.6211646795272827\n",
      "Epoch 4, Batch 446, Loss: 0.822900652885437\n",
      "Epoch 4, Batch 447, Loss: 0.5630871653556824\n",
      "Epoch 4, Batch 448, Loss: 0.6839853525161743\n",
      "Epoch 4, Batch 449, Loss: 0.5403666496276855\n",
      "Epoch 4, Batch 450, Loss: 0.6083647012710571\n",
      "Epoch 4, Batch 451, Loss: 0.4257129430770874\n",
      "Epoch 4, Batch 452, Loss: 0.8826210498809814\n",
      "Epoch 4, Batch 453, Loss: 0.6681569814682007\n",
      "Epoch 4, Batch 454, Loss: 0.7371836304664612\n",
      "Epoch 4, Batch 455, Loss: 0.8330879211425781\n",
      "Epoch 4, Batch 456, Loss: 0.6232483983039856\n",
      "Epoch 4, Batch 457, Loss: 0.72406005859375\n",
      "Epoch 4, Batch 458, Loss: 0.752169668674469\n",
      "Epoch 4, Batch 459, Loss: 0.718491792678833\n",
      "Epoch 4, Batch 460, Loss: 0.7052471041679382\n",
      "Epoch 4, Batch 461, Loss: 0.5096288919448853\n",
      "Epoch 4, Batch 462, Loss: 0.703687310218811\n",
      "Epoch 4, Batch 463, Loss: 0.7616022229194641\n",
      "Epoch 4, Batch 464, Loss: 0.6319801807403564\n",
      "Epoch 4, Batch 465, Loss: 0.6461517214775085\n",
      "Epoch 4, Batch 466, Loss: 0.7445717453956604\n",
      "Epoch 4, Batch 467, Loss: 0.7682414650917053\n",
      "Epoch 4, Batch 468, Loss: 0.627426028251648\n",
      "Epoch 4, Batch 469, Loss: 0.7298409342765808\n",
      "Epoch 4, Batch 470, Loss: 0.7555341720581055\n",
      "Epoch 4, Batch 471, Loss: 0.6959549188613892\n",
      "Epoch 4, Batch 472, Loss: 0.8290006518363953\n",
      "Epoch 4, Batch 473, Loss: 0.7137401700019836\n",
      "Epoch 4, Batch 474, Loss: 0.6346870064735413\n",
      "Epoch 4, Batch 475, Loss: 0.6040420532226562\n",
      "Epoch 4, Batch 476, Loss: 0.7123112082481384\n",
      "Epoch 4, Batch 477, Loss: 0.6531663537025452\n",
      "Epoch 4, Batch 478, Loss: 0.6235890984535217\n",
      "Epoch 4, Batch 479, Loss: 0.6504234075546265\n",
      "Epoch 4, Batch 480, Loss: 0.595633327960968\n",
      "Epoch 4, Batch 481, Loss: 0.5547083616256714\n",
      "Epoch 4, Batch 482, Loss: 0.8215193748474121\n",
      "Epoch 4, Batch 483, Loss: 0.6180869936943054\n",
      "Epoch 4, Batch 484, Loss: 0.5674727559089661\n",
      "Epoch 4, Batch 485, Loss: 0.8210187554359436\n",
      "Epoch 4, Batch 486, Loss: 0.583522379398346\n",
      "Epoch 4, Batch 487, Loss: 0.5518414378166199\n",
      "Epoch 4, Batch 488, Loss: 0.5345919132232666\n",
      "Epoch 4, Batch 489, Loss: 0.913145124912262\n",
      "Epoch 4, Batch 490, Loss: 0.6167782545089722\n",
      "Epoch 4, Batch 491, Loss: 0.7891923189163208\n",
      "Epoch 4, Batch 492, Loss: 0.6653666496276855\n",
      "Epoch 4, Batch 493, Loss: 0.8401522636413574\n",
      "Epoch 4, Batch 494, Loss: 0.7720085382461548\n",
      "Epoch 4, Batch 495, Loss: 0.4917924106121063\n",
      "Epoch 4, Batch 496, Loss: 0.7522039413452148\n",
      "Epoch 4, Batch 497, Loss: 0.7185813188552856\n",
      "Epoch 4, Batch 498, Loss: 0.6253401041030884\n",
      "Epoch 4, Batch 499, Loss: 0.7442194223403931\n",
      "Epoch 4, Batch 500, Loss: 0.7741692066192627\n",
      "Epoch 4, Batch 501, Loss: 0.5425757765769958\n",
      "Epoch 4, Batch 502, Loss: 0.6876657605171204\n",
      "Epoch 4, Batch 503, Loss: 0.62984699010849\n",
      "Epoch 4, Batch 504, Loss: 0.689556360244751\n",
      "Epoch 4, Batch 505, Loss: 0.7000189423561096\n",
      "Epoch 4, Batch 506, Loss: 0.7931533455848694\n",
      "Epoch 4, Batch 507, Loss: 0.5881229639053345\n",
      "Epoch 4, Batch 508, Loss: 0.7593868970870972\n",
      "Epoch 4, Batch 509, Loss: 0.614862859249115\n",
      "Epoch 4, Batch 510, Loss: 0.6020497679710388\n",
      "Epoch 4, Batch 511, Loss: 0.7116736173629761\n",
      "Epoch 4, Batch 512, Loss: 0.585713267326355\n",
      "Epoch 4, Batch 513, Loss: 0.7521916627883911\n",
      "Epoch 4, Batch 514, Loss: 0.5365216732025146\n",
      "Epoch 4, Batch 515, Loss: 0.8080633282661438\n",
      "Epoch 4, Batch 516, Loss: 0.8652849793434143\n",
      "Epoch 4, Batch 517, Loss: 0.7040383815765381\n",
      "Epoch 4, Batch 518, Loss: 0.7684106230735779\n",
      "Epoch 4, Batch 519, Loss: 0.5619299411773682\n",
      "Epoch 4, Batch 520, Loss: 0.6964905858039856\n",
      "Epoch 4, Batch 521, Loss: 0.6552812457084656\n",
      "Epoch 4, Batch 522, Loss: 0.7009345889091492\n",
      "Epoch 4, Batch 523, Loss: 0.6728477478027344\n",
      "Epoch 4, Batch 524, Loss: 0.5784235000610352\n",
      "Epoch 4, Batch 525, Loss: 0.7450860738754272\n",
      "Epoch 4, Batch 526, Loss: 0.6493865251541138\n",
      "Epoch 4, Batch 527, Loss: 0.842069685459137\n",
      "Epoch 4, Batch 528, Loss: 0.9131961464881897\n",
      "Epoch 4, Batch 529, Loss: 0.5866379737854004\n",
      "Epoch 4, Batch 530, Loss: 0.6752240657806396\n",
      "Epoch 4, Batch 531, Loss: 0.6795167326927185\n",
      "Epoch 4, Batch 532, Loss: 0.845969557762146\n",
      "Epoch 4, Batch 533, Loss: 0.7753549218177795\n",
      "Epoch 4, Batch 534, Loss: 0.4960815906524658\n",
      "Epoch 4, Batch 535, Loss: 0.6519494652748108\n",
      "Epoch 4, Batch 536, Loss: 0.5946758389472961\n",
      "Epoch 4, Batch 537, Loss: 0.5175696611404419\n",
      "Epoch 4, Batch 538, Loss: 0.6177590489387512\n",
      "Epoch 4, Batch 539, Loss: 0.6743510961532593\n",
      "Epoch 4, Batch 540, Loss: 0.77778160572052\n",
      "Epoch 4, Batch 541, Loss: 0.4462258815765381\n",
      "Epoch 4, Batch 542, Loss: 0.7056781053543091\n",
      "Epoch 4, Batch 543, Loss: 0.7678645253181458\n",
      "Epoch 4, Batch 544, Loss: 0.5361753702163696\n",
      "Epoch 4, Batch 545, Loss: 0.8398628234863281\n",
      "Epoch 4, Batch 546, Loss: 0.7072091102600098\n",
      "Epoch 4, Batch 547, Loss: 0.7030954360961914\n",
      "Epoch 4, Batch 548, Loss: 0.5788208842277527\n",
      "Epoch 4, Batch 549, Loss: 0.611961841583252\n",
      "Epoch 4, Batch 550, Loss: 0.8239139914512634\n",
      "Epoch 4, Batch 551, Loss: 0.8940592408180237\n",
      "Epoch 4, Batch 552, Loss: 0.6809141635894775\n",
      "Epoch 4, Batch 553, Loss: 0.5679086446762085\n",
      "Epoch 4, Batch 554, Loss: 0.7482300996780396\n",
      "Epoch 4, Batch 555, Loss: 0.9023125171661377\n",
      "Epoch 4, Batch 556, Loss: 0.5976285934448242\n",
      "Epoch 4, Batch 557, Loss: 0.5963890552520752\n",
      "Epoch 4, Batch 558, Loss: 0.7244881987571716\n",
      "Epoch 4, Batch 559, Loss: 0.8057979941368103\n",
      "Epoch 4, Batch 560, Loss: 0.5601034760475159\n",
      "Epoch 4, Batch 561, Loss: 0.6367784142494202\n",
      "Epoch 4, Batch 562, Loss: 0.6035760641098022\n",
      "Epoch 4, Batch 563, Loss: 0.7450030446052551\n",
      "Epoch 4, Batch 564, Loss: 0.7775878310203552\n",
      "Epoch 4, Batch 565, Loss: 0.9371739625930786\n",
      "Epoch 4, Batch 566, Loss: 0.585096001625061\n",
      "Epoch 4, Batch 567, Loss: 0.6615631580352783\n",
      "Epoch 4, Batch 568, Loss: 0.7001908421516418\n",
      "Epoch 4, Batch 569, Loss: 0.8338257074356079\n",
      "Epoch 4, Batch 570, Loss: 0.630113959312439\n",
      "Epoch 4, Batch 571, Loss: 0.6078823804855347\n",
      "Epoch 4, Batch 572, Loss: 0.5150030255317688\n",
      "Epoch 4, Batch 573, Loss: 0.7116317749023438\n",
      "Epoch 4, Batch 574, Loss: 0.7472441792488098\n",
      "Epoch 4, Batch 575, Loss: 0.45770788192749023\n",
      "Epoch 4, Batch 576, Loss: 0.6943045258522034\n",
      "Epoch 4, Batch 577, Loss: 0.7517856955528259\n",
      "Epoch 4, Batch 578, Loss: 0.6065205335617065\n",
      "Epoch 4, Batch 579, Loss: 0.6806420683860779\n",
      "Epoch 4, Batch 580, Loss: 0.47415050864219666\n",
      "Epoch 4, Batch 581, Loss: 0.6497139930725098\n",
      "Epoch 4, Batch 582, Loss: 0.5121405720710754\n",
      "Epoch 4, Batch 583, Loss: 0.7337883114814758\n",
      "Epoch 4, Batch 584, Loss: 0.6159918308258057\n",
      "Epoch 4, Batch 585, Loss: 0.6448120474815369\n",
      "Epoch 4, Batch 586, Loss: 0.5873414874076843\n",
      "Epoch 4, Batch 587, Loss: 0.9093322157859802\n",
      "Epoch 4, Batch 588, Loss: 0.5526007413864136\n",
      "Epoch 4, Batch 589, Loss: 0.7667783498764038\n",
      "Epoch 4, Batch 590, Loss: 0.5353198051452637\n",
      "Epoch 4, Batch 591, Loss: 0.5854207277297974\n",
      "Epoch 4, Batch 592, Loss: 0.7124866843223572\n",
      "Epoch 4, Batch 593, Loss: 0.6062305569648743\n",
      "Epoch 4, Batch 594, Loss: 0.8063156604766846\n",
      "Epoch 4, Batch 595, Loss: 0.6445280909538269\n",
      "Epoch 4, Batch 596, Loss: 0.6342197060585022\n",
      "Epoch 4, Batch 597, Loss: 0.6730881929397583\n",
      "Epoch 4, Batch 598, Loss: 0.6890907883644104\n",
      "Epoch 4, Batch 599, Loss: 0.7968958020210266\n",
      "Epoch 4, Batch 600, Loss: 0.6993914246559143\n",
      "Epoch 4, Batch 601, Loss: 0.545380711555481\n",
      "Epoch 4, Batch 602, Loss: 1.083376407623291\n",
      "Epoch 4, Batch 603, Loss: 0.8361772298812866\n",
      "Epoch 4, Batch 604, Loss: 0.6732698678970337\n",
      "Epoch 4, Batch 605, Loss: 0.4636152684688568\n",
      "Epoch 4, Batch 606, Loss: 0.5513265132904053\n",
      "Epoch 4, Batch 607, Loss: 0.8648470640182495\n",
      "Epoch 4, Batch 608, Loss: 0.8502815961837769\n",
      "Epoch 4, Batch 609, Loss: 0.6183737516403198\n",
      "Epoch 4, Batch 610, Loss: 0.6084766387939453\n",
      "Epoch 4, Batch 611, Loss: 0.6218421459197998\n",
      "Epoch 4, Batch 612, Loss: 0.6837955713272095\n",
      "Epoch 4, Batch 613, Loss: 0.8724300265312195\n",
      "Epoch 4, Batch 614, Loss: 0.740517258644104\n",
      "Epoch 4, Batch 615, Loss: 0.6828888058662415\n",
      "Epoch 4, Batch 616, Loss: 0.5896269083023071\n",
      "Epoch 4, Batch 617, Loss: 0.6067874431610107\n",
      "Epoch 4, Batch 618, Loss: 0.7757009267807007\n",
      "Epoch 4, Batch 619, Loss: 0.5248262882232666\n",
      "Epoch 4, Batch 620, Loss: 0.6135908365249634\n",
      "Epoch 4, Batch 621, Loss: 0.7293018698692322\n",
      "Epoch 4, Batch 622, Loss: 0.6706278324127197\n",
      "Epoch 4, Batch 623, Loss: 0.8271659016609192\n",
      "Epoch 4, Batch 624, Loss: 0.7483767867088318\n",
      "Epoch 4, Batch 625, Loss: 0.6612198948860168\n",
      "Epoch 4, Batch 626, Loss: 0.714148223400116\n",
      "Epoch 4, Batch 627, Loss: 0.8307161331176758\n",
      "Epoch 4, Batch 628, Loss: 0.6991801261901855\n",
      "Epoch 4, Batch 629, Loss: 0.7171436548233032\n",
      "Epoch 4, Batch 630, Loss: 0.7154309153556824\n",
      "Epoch 4, Batch 631, Loss: 0.6764252185821533\n",
      "Epoch 4, Batch 632, Loss: 0.6762511134147644\n",
      "Epoch 4, Batch 633, Loss: 0.8743768334388733\n",
      "Epoch 4, Batch 634, Loss: 0.6386557817459106\n",
      "Epoch 4, Batch 635, Loss: 0.776584267616272\n",
      "Epoch 4, Batch 636, Loss: 0.6420035362243652\n",
      "Epoch 4, Batch 637, Loss: 0.5781505703926086\n",
      "Epoch 4, Batch 638, Loss: 0.6148549318313599\n",
      "Epoch 4, Batch 639, Loss: 0.6118794679641724\n",
      "Epoch 4, Batch 640, Loss: 0.7852333188056946\n",
      "Epoch 4, Batch 641, Loss: 0.6636137962341309\n",
      "Epoch 4, Batch 642, Loss: 0.5185626149177551\n",
      "Epoch 4, Batch 643, Loss: 0.5240969657897949\n",
      "Epoch 4, Batch 644, Loss: 0.6651465892791748\n",
      "Epoch 4, Batch 645, Loss: 0.5309542417526245\n",
      "Epoch 4, Batch 646, Loss: 0.6306442618370056\n",
      "Epoch 4, Batch 647, Loss: 0.6767845749855042\n",
      "Epoch 4, Batch 648, Loss: 0.5062384605407715\n",
      "Epoch 4, Batch 649, Loss: 0.6155796051025391\n",
      "Epoch 4, Batch 650, Loss: 0.7287490963935852\n",
      "Epoch 4, Batch 651, Loss: 0.46137377619743347\n",
      "Epoch 4, Batch 652, Loss: 0.6141409873962402\n",
      "Epoch 4, Batch 653, Loss: 0.8104704022407532\n",
      "Epoch 4, Batch 654, Loss: 0.6180608868598938\n",
      "Epoch 4, Batch 655, Loss: 0.7018690705299377\n",
      "Epoch 4, Batch 656, Loss: 0.4726141095161438\n",
      "Epoch 4, Batch 657, Loss: 0.6474130153656006\n",
      "Epoch 4, Batch 658, Loss: 0.6215359568595886\n",
      "Epoch 4, Batch 659, Loss: 0.6987332701683044\n",
      "Epoch 4, Batch 660, Loss: 0.6311356425285339\n",
      "Epoch 4, Batch 661, Loss: 0.6833000183105469\n",
      "Epoch 4, Batch 662, Loss: 0.6855687499046326\n",
      "Epoch 4, Batch 663, Loss: 0.6297622323036194\n",
      "Epoch 4, Batch 664, Loss: 0.5606930255889893\n",
      "Epoch 4, Batch 665, Loss: 0.5593051910400391\n",
      "Epoch 4, Batch 666, Loss: 0.6135067343711853\n",
      "Epoch 4, Batch 667, Loss: 1.2140376567840576\n",
      "Epoch 4, Batch 668, Loss: 0.5832244157791138\n",
      "Epoch 4, Batch 669, Loss: 0.5458827614784241\n",
      "Epoch 4, Batch 670, Loss: 0.6933754086494446\n",
      "Epoch 4, Batch 671, Loss: 0.5697121024131775\n",
      "Epoch 4, Batch 672, Loss: 0.5915563702583313\n",
      "Epoch 4, Batch 673, Loss: 0.6283020973205566\n",
      "Epoch 4, Batch 674, Loss: 0.5308132171630859\n",
      "Epoch 4, Batch 675, Loss: 0.6053174138069153\n",
      "Epoch 4, Batch 676, Loss: 0.6452149748802185\n",
      "Epoch 4, Batch 677, Loss: 0.6786680817604065\n",
      "Epoch 4, Batch 678, Loss: 0.5966786742210388\n",
      "Epoch 4, Batch 679, Loss: 0.5356084704399109\n",
      "Epoch 4, Batch 680, Loss: 0.5487104654312134\n",
      "Epoch 4, Batch 681, Loss: 0.7082920074462891\n",
      "Epoch 4, Batch 682, Loss: 0.6248409152030945\n",
      "Epoch 4, Batch 683, Loss: 0.6167268753051758\n",
      "Epoch 4, Batch 684, Loss: 0.6263783574104309\n",
      "Epoch 4, Batch 685, Loss: 0.6689556837081909\n",
      "Epoch 4, Batch 686, Loss: 0.6705052852630615\n",
      "Epoch 4, Batch 687, Loss: 0.5880984663963318\n",
      "Epoch 4, Batch 688, Loss: 0.719371497631073\n",
      "Epoch 4, Batch 689, Loss: 0.5468814373016357\n",
      "Epoch 4, Batch 690, Loss: 0.6851212978363037\n",
      "Epoch 4, Batch 691, Loss: 0.6447185277938843\n",
      "Epoch 4, Batch 692, Loss: 0.5530472993850708\n",
      "Epoch 4, Batch 693, Loss: 0.6135975122451782\n",
      "Epoch 4, Batch 694, Loss: 0.6426191329956055\n",
      "Epoch 4, Batch 695, Loss: 0.5608477592468262\n",
      "Epoch 4, Batch 696, Loss: 0.6686392426490784\n",
      "Epoch 4, Batch 697, Loss: 0.7488999366760254\n",
      "Epoch 4, Batch 698, Loss: 0.6961766481399536\n",
      "Epoch 4, Batch 699, Loss: 0.772743284702301\n",
      "Epoch 4, Batch 700, Loss: 0.9338191151618958\n",
      "Epoch 4, Batch 701, Loss: 0.9489938020706177\n",
      "Epoch 4, Batch 702, Loss: 0.534180223941803\n",
      "Epoch 4, Batch 703, Loss: 0.5210435390472412\n",
      "Epoch 4, Batch 704, Loss: 0.7883775234222412\n",
      "Epoch 4, Batch 705, Loss: 0.7211743593215942\n",
      "Epoch 4, Batch 706, Loss: 0.6848394274711609\n",
      "Epoch 4, Batch 707, Loss: 0.6998111605644226\n",
      "Epoch 4, Batch 708, Loss: 0.5117433071136475\n",
      "Epoch 4, Batch 709, Loss: 0.6814151406288147\n",
      "Epoch 4, Batch 710, Loss: 0.5267421007156372\n",
      "Epoch 4, Batch 711, Loss: 0.8476545214653015\n",
      "Epoch 4, Batch 712, Loss: 0.6196600198745728\n",
      "Epoch 4, Batch 713, Loss: 0.4404940903186798\n",
      "Epoch 4, Batch 714, Loss: 0.37366899847984314\n",
      "Epoch 4, Batch 715, Loss: 0.46763622760772705\n",
      "Epoch 4, Batch 716, Loss: 0.5086164474487305\n",
      "Epoch 4, Batch 717, Loss: 0.5716504454612732\n",
      "Epoch 4, Batch 718, Loss: 0.5487157702445984\n",
      "Epoch 4, Batch 719, Loss: 0.8684110641479492\n",
      "Epoch 4, Batch 720, Loss: 0.5623605251312256\n",
      "Epoch 4, Batch 721, Loss: 0.5753352046012878\n",
      "Epoch 4, Batch 722, Loss: 0.7302308082580566\n",
      "Epoch 4, Batch 723, Loss: 0.653416633605957\n",
      "Epoch 4, Batch 724, Loss: 0.8525365591049194\n",
      "Epoch 4, Batch 725, Loss: 0.9350377321243286\n",
      "Epoch 4, Batch 726, Loss: 0.6776078343391418\n",
      "Epoch 4, Batch 727, Loss: 0.6125777363777161\n",
      "Epoch 4, Batch 728, Loss: 0.6242364645004272\n",
      "Epoch 4, Batch 729, Loss: 0.6508946418762207\n",
      "Epoch 4, Batch 730, Loss: 0.5892239809036255\n",
      "Epoch 4, Batch 731, Loss: 0.6813908815383911\n",
      "Epoch 4, Batch 732, Loss: 0.7160698771476746\n",
      "Epoch 4, Batch 733, Loss: 0.6959616541862488\n",
      "Epoch 4, Batch 734, Loss: 0.6287608742713928\n",
      "Epoch 4, Batch 735, Loss: 0.9695940613746643\n",
      "Epoch 4, Batch 736, Loss: 0.5496208667755127\n",
      "Epoch 4, Batch 737, Loss: 0.7191026210784912\n",
      "Epoch 4, Batch 738, Loss: 0.5346759557723999\n",
      "Epoch 4, Batch 739, Loss: 0.6530425548553467\n",
      "Epoch 4, Batch 740, Loss: 0.58372563123703\n",
      "Epoch 4, Batch 741, Loss: 0.5239704847335815\n",
      "Epoch 4, Batch 742, Loss: 0.6055145263671875\n",
      "Epoch 4, Batch 743, Loss: 0.6816033720970154\n",
      "Epoch 4, Batch 744, Loss: 0.802905797958374\n",
      "Epoch 4, Batch 745, Loss: 0.7751821875572205\n",
      "Epoch 4, Batch 746, Loss: 0.7072934508323669\n",
      "Epoch 4, Batch 747, Loss: 0.50188148021698\n",
      "Epoch 4, Batch 748, Loss: 0.581599771976471\n",
      "Epoch 4, Batch 749, Loss: 0.7112666368484497\n",
      "Epoch 4, Batch 750, Loss: 0.4910593628883362\n",
      "Epoch 4, Batch 751, Loss: 0.5797064304351807\n",
      "Epoch 4, Batch 752, Loss: 0.7150664329528809\n",
      "Epoch 4, Batch 753, Loss: 0.7128915190696716\n",
      "Epoch 4, Batch 754, Loss: 0.9715611934661865\n",
      "Epoch 4, Batch 755, Loss: 0.7774555087089539\n",
      "Epoch 4, Batch 756, Loss: 0.6391087770462036\n",
      "Epoch 4, Batch 757, Loss: 0.5283909440040588\n",
      "Epoch 4, Batch 758, Loss: 0.6379122138023376\n",
      "Epoch 4, Batch 759, Loss: 0.6963407397270203\n",
      "Epoch 4, Batch 760, Loss: 0.8139299154281616\n",
      "Epoch 4, Batch 761, Loss: 0.7847328186035156\n",
      "Epoch 4, Batch 762, Loss: 0.8350520730018616\n",
      "Epoch 4, Batch 763, Loss: 0.48057594895362854\n",
      "Epoch 4, Batch 764, Loss: 0.7649592161178589\n",
      "Epoch 4, Batch 765, Loss: 0.582054615020752\n",
      "Epoch 4, Batch 766, Loss: 0.640032947063446\n",
      "Epoch 4, Batch 767, Loss: 0.3930281400680542\n",
      "Epoch 4, Batch 768, Loss: 0.5380793809890747\n",
      "Epoch 4, Batch 769, Loss: 0.5831719636917114\n",
      "Epoch 4, Batch 770, Loss: 0.4774768650531769\n",
      "Epoch 4, Batch 771, Loss: 0.5369731783866882\n",
      "Epoch 4, Batch 772, Loss: 0.6603258848190308\n",
      "Epoch 4, Batch 773, Loss: 0.7076912522315979\n",
      "Epoch 4, Batch 774, Loss: 0.5812094211578369\n",
      "Epoch 4, Batch 775, Loss: 0.6941808462142944\n",
      "Epoch 4, Batch 776, Loss: 0.6743239164352417\n",
      "Epoch 4, Batch 777, Loss: 0.6038907766342163\n",
      "Epoch 4, Batch 778, Loss: 0.6726729869842529\n",
      "Epoch 4, Batch 779, Loss: 0.7577546834945679\n",
      "Epoch 4, Batch 780, Loss: 0.6156710386276245\n",
      "Epoch 4, Batch 781, Loss: 0.6017383337020874\n",
      "Epoch 4, Batch 782, Loss: 0.6710507869720459\n",
      "Epoch 4, Batch 783, Loss: 0.5044246315956116\n",
      "Epoch 4, Batch 784, Loss: 0.6947163939476013\n",
      "Epoch 4, Batch 785, Loss: 0.6254573464393616\n",
      "Epoch 4, Batch 786, Loss: 0.6946675777435303\n",
      "Epoch 4, Batch 787, Loss: 0.6428284049034119\n",
      "Epoch 4, Batch 788, Loss: 0.8583554029464722\n",
      "Epoch 4, Batch 789, Loss: 0.8652342557907104\n",
      "Epoch 4, Batch 790, Loss: 0.5617805123329163\n",
      "Epoch 4, Batch 791, Loss: 0.5450021028518677\n",
      "Epoch 4, Batch 792, Loss: 0.7055712342262268\n",
      "Epoch 4, Batch 793, Loss: 0.9024966955184937\n",
      "Epoch 4, Batch 794, Loss: 0.8073570728302002\n",
      "Epoch 4, Batch 795, Loss: 0.6465628147125244\n",
      "Epoch 4, Batch 796, Loss: 0.5577068328857422\n",
      "Epoch 4, Batch 797, Loss: 0.7597267031669617\n",
      "Epoch 4, Batch 798, Loss: 0.5324391722679138\n",
      "Epoch 4, Batch 799, Loss: 0.6408513188362122\n",
      "Epoch 4, Batch 800, Loss: 0.5868577361106873\n",
      "Epoch 4, Batch 801, Loss: 0.8582333922386169\n",
      "Epoch 4, Batch 802, Loss: 0.9074217081069946\n",
      "Epoch 4, Batch 803, Loss: 0.6939663887023926\n",
      "Epoch 4, Batch 804, Loss: 0.6461267471313477\n",
      "Epoch 4, Batch 805, Loss: 0.5475672483444214\n",
      "Epoch 4, Batch 806, Loss: 0.6620082259178162\n",
      "Epoch 4, Batch 807, Loss: 0.6019791960716248\n",
      "Epoch 4, Batch 808, Loss: 0.5922578573226929\n",
      "Epoch 4, Batch 809, Loss: 0.47922322154045105\n",
      "Epoch 4, Batch 810, Loss: 0.6293225884437561\n",
      "Epoch 4, Batch 811, Loss: 0.6331234574317932\n",
      "Epoch 4, Batch 812, Loss: 0.6348158717155457\n",
      "Epoch 4, Batch 813, Loss: 0.857255220413208\n",
      "Epoch 4, Batch 814, Loss: 0.5429751873016357\n",
      "Epoch 4, Batch 815, Loss: 0.7268954515457153\n",
      "Epoch 4, Batch 816, Loss: 0.7486671805381775\n",
      "Epoch 4, Batch 817, Loss: 0.6709078550338745\n",
      "Epoch 4, Batch 818, Loss: 0.5133411884307861\n",
      "Epoch 4, Batch 819, Loss: 0.8360450863838196\n",
      "Epoch 4, Batch 820, Loss: 1.0882915258407593\n",
      "Epoch 4, Batch 821, Loss: 0.7198578715324402\n",
      "Epoch 4, Batch 822, Loss: 0.7542448043823242\n",
      "Epoch 4, Batch 823, Loss: 0.6586751341819763\n",
      "Epoch 4, Batch 824, Loss: 0.7930429577827454\n",
      "Epoch 4, Batch 825, Loss: 0.8128259181976318\n",
      "Epoch 4, Batch 826, Loss: 0.6893050074577332\n",
      "Epoch 4, Batch 827, Loss: 0.8545680046081543\n",
      "Epoch 4, Batch 828, Loss: 0.775399923324585\n",
      "Epoch 4, Batch 829, Loss: 0.6391985416412354\n",
      "Epoch 4, Batch 830, Loss: 0.5704259276390076\n",
      "Epoch 4, Batch 831, Loss: 0.5949113368988037\n",
      "Epoch 4, Batch 832, Loss: 0.6027562618255615\n",
      "Epoch 4, Batch 833, Loss: 0.6875844597816467\n",
      "Epoch 4, Batch 834, Loss: 0.692808985710144\n",
      "Epoch 4, Batch 835, Loss: 0.5316334366798401\n",
      "Epoch 4, Batch 836, Loss: 0.5472488403320312\n",
      "Epoch 4, Batch 837, Loss: 0.6168134212493896\n",
      "Epoch 4, Batch 838, Loss: 0.5619197487831116\n",
      "Epoch 4, Batch 839, Loss: 0.7570551037788391\n",
      "Epoch 4, Batch 840, Loss: 0.6564444303512573\n",
      "Epoch 4, Batch 841, Loss: 0.796258807182312\n",
      "Epoch 4, Batch 842, Loss: 0.6774516701698303\n",
      "Epoch 4, Batch 843, Loss: 0.566881537437439\n",
      "Epoch 4, Batch 844, Loss: 0.5773174166679382\n",
      "Epoch 4, Batch 845, Loss: 0.6279394030570984\n",
      "Epoch 4, Batch 846, Loss: 0.5281183123588562\n",
      "Epoch 4, Batch 847, Loss: 0.7616888880729675\n",
      "Epoch 4, Batch 848, Loss: 0.5881457328796387\n",
      "Epoch 4, Batch 849, Loss: 0.8184511661529541\n",
      "Epoch 4, Batch 850, Loss: 0.8044208288192749\n",
      "Epoch 4, Batch 851, Loss: 0.5475237965583801\n",
      "Epoch 4, Batch 852, Loss: 0.5888252854347229\n",
      "Epoch 4, Batch 853, Loss: 0.4871275722980499\n",
      "Epoch 4, Batch 854, Loss: 0.5649884939193726\n",
      "Epoch 4, Batch 855, Loss: 0.8458505272865295\n",
      "Epoch 4, Batch 856, Loss: 0.5699024796485901\n",
      "Epoch 4, Batch 857, Loss: 0.46944659948349\n",
      "Epoch 4, Batch 858, Loss: 0.8667645454406738\n",
      "Epoch 4, Batch 859, Loss: 0.6327060461044312\n",
      "Epoch 4, Batch 860, Loss: 0.9749100804328918\n",
      "Epoch 4, Batch 861, Loss: 0.8058689832687378\n",
      "Epoch 4, Batch 862, Loss: 0.8033509850502014\n",
      "Epoch 4, Batch 863, Loss: 0.6595553755760193\n",
      "Epoch 4, Batch 864, Loss: 0.43473491072654724\n",
      "Epoch 4, Batch 865, Loss: 0.5812847018241882\n",
      "Epoch 4, Batch 866, Loss: 0.6676713824272156\n",
      "Epoch 4, Batch 867, Loss: 0.7857500314712524\n",
      "Epoch 4, Batch 868, Loss: 0.5588833689689636\n",
      "Epoch 4, Batch 869, Loss: 0.516903281211853\n",
      "Epoch 4, Batch 870, Loss: 0.6199595928192139\n",
      "Epoch 4, Batch 871, Loss: 0.7338263988494873\n",
      "Epoch 4, Batch 872, Loss: 0.6036124229431152\n",
      "Epoch 4, Batch 873, Loss: 0.58095782995224\n",
      "Epoch 4, Batch 874, Loss: 0.7382211685180664\n",
      "Epoch 4, Batch 875, Loss: 0.6056899428367615\n",
      "Epoch 4, Batch 876, Loss: 0.7086789608001709\n",
      "Epoch 4, Batch 877, Loss: 0.7964366674423218\n",
      "Epoch 4, Batch 878, Loss: 0.5501010417938232\n",
      "Epoch 4, Batch 879, Loss: 0.8038733601570129\n",
      "Epoch 4, Batch 880, Loss: 0.7276362180709839\n",
      "Epoch 4, Batch 881, Loss: 0.7987375855445862\n",
      "Epoch 4, Batch 882, Loss: 0.6045852303504944\n",
      "Epoch 4, Batch 883, Loss: 0.6222318410873413\n",
      "Epoch 4, Batch 884, Loss: 0.650429368019104\n",
      "Epoch 4, Batch 885, Loss: 0.5879064202308655\n",
      "Epoch 4, Batch 886, Loss: 0.75571608543396\n",
      "Epoch 4, Batch 887, Loss: 0.6593102216720581\n",
      "Epoch 4, Batch 888, Loss: 0.8640455007553101\n",
      "Epoch 4, Batch 889, Loss: 0.6154072880744934\n",
      "Epoch 4, Batch 890, Loss: 0.7041903734207153\n",
      "Epoch 4, Batch 891, Loss: 0.6319869756698608\n",
      "Epoch 4, Batch 892, Loss: 0.5387446284294128\n",
      "Epoch 4, Batch 893, Loss: 0.41719964146614075\n",
      "Epoch 4, Batch 894, Loss: 0.5795371532440186\n",
      "Epoch 4, Batch 895, Loss: 0.5264954566955566\n",
      "Epoch 4, Batch 896, Loss: 0.7145471572875977\n",
      "Epoch 4, Batch 897, Loss: 0.5512060523033142\n",
      "Epoch 4, Batch 898, Loss: 0.7569617033004761\n",
      "Epoch 4, Batch 899, Loss: 0.4253014624118805\n",
      "Epoch 4, Batch 900, Loss: 0.6008422374725342\n",
      "Epoch 4, Batch 901, Loss: 0.7390509843826294\n",
      "Epoch 4, Batch 902, Loss: 0.5960908532142639\n",
      "Epoch 4, Batch 903, Loss: 0.7087712287902832\n",
      "Epoch 4, Batch 904, Loss: 0.6717836856842041\n",
      "Epoch 4, Batch 905, Loss: 0.671619713306427\n",
      "Epoch 4, Batch 906, Loss: 0.7447377443313599\n",
      "Epoch 4, Batch 907, Loss: 0.561889111995697\n",
      "Epoch 4, Batch 908, Loss: 0.6112854480743408\n",
      "Epoch 4, Batch 909, Loss: 0.6717941761016846\n",
      "Epoch 4, Batch 910, Loss: 0.6716915369033813\n",
      "Epoch 4, Batch 911, Loss: 0.5951076745986938\n",
      "Epoch 4, Batch 912, Loss: 0.7296023964881897\n",
      "Epoch 4, Batch 913, Loss: 0.496743381023407\n",
      "Epoch 4, Batch 914, Loss: 0.685824990272522\n",
      "Epoch 4, Batch 915, Loss: 0.6717919707298279\n",
      "Epoch 4, Batch 916, Loss: 0.6358693242073059\n",
      "Epoch 4, Batch 917, Loss: 0.9042259454727173\n",
      "Epoch 4, Batch 918, Loss: 0.5705387592315674\n",
      "Epoch 4, Batch 919, Loss: 0.5519483089447021\n",
      "Epoch 4, Batch 920, Loss: 0.8838356137275696\n",
      "Epoch 4, Batch 921, Loss: 0.7321563959121704\n",
      "Epoch 4, Batch 922, Loss: 0.8259485960006714\n",
      "Epoch 4, Batch 923, Loss: 0.7368447780609131\n",
      "Epoch 4, Batch 924, Loss: 0.6951226592063904\n",
      "Epoch 4, Batch 925, Loss: 0.689416766166687\n",
      "Epoch 4, Batch 926, Loss: 0.6283390522003174\n",
      "Epoch 4, Batch 927, Loss: 0.5569694638252258\n",
      "Epoch 4, Batch 928, Loss: 0.8217310905456543\n",
      "Epoch 4, Batch 929, Loss: 0.5917335152626038\n",
      "Epoch 4, Batch 930, Loss: 0.6221711039543152\n",
      "Epoch 4, Batch 931, Loss: 0.6878156661987305\n",
      "Epoch 4, Batch 932, Loss: 0.6143942475318909\n",
      "Epoch 4, Batch 933, Loss: 0.5650941729545593\n",
      "Epoch 4, Batch 934, Loss: 0.7233662605285645\n",
      "Epoch 4, Batch 935, Loss: 0.6287007331848145\n",
      "Epoch 4, Batch 936, Loss: 0.7145649790763855\n",
      "Epoch 4, Batch 937, Loss: 0.6693518161773682\n",
      "Epoch 4, Batch 938, Loss: 0.5234267115592957\n",
      "Accuracy of train set: 0.74855\n",
      "Epoch 4, Batch 1, Test Loss: 0.6863453388214111\n",
      "Epoch 4, Batch 2, Test Loss: 0.6846033930778503\n",
      "Epoch 4, Batch 3, Test Loss: 0.7821481227874756\n",
      "Epoch 4, Batch 4, Test Loss: 0.566292405128479\n",
      "Epoch 4, Batch 5, Test Loss: 0.7045527696609497\n",
      "Epoch 4, Batch 6, Test Loss: 0.5871575474739075\n",
      "Epoch 4, Batch 7, Test Loss: 0.5393060445785522\n",
      "Epoch 4, Batch 8, Test Loss: 0.8210201263427734\n",
      "Epoch 4, Batch 9, Test Loss: 0.7102763652801514\n",
      "Epoch 4, Batch 10, Test Loss: 0.4525451362133026\n",
      "Epoch 4, Batch 11, Test Loss: 0.7238410711288452\n",
      "Epoch 4, Batch 12, Test Loss: 0.5743095874786377\n",
      "Epoch 4, Batch 13, Test Loss: 0.96495121717453\n",
      "Epoch 4, Batch 14, Test Loss: 0.6449064016342163\n",
      "Epoch 4, Batch 15, Test Loss: 0.6561325788497925\n",
      "Epoch 4, Batch 16, Test Loss: 0.6400159001350403\n",
      "Epoch 4, Batch 17, Test Loss: 0.6405542492866516\n",
      "Epoch 4, Batch 18, Test Loss: 0.5170522332191467\n",
      "Epoch 4, Batch 19, Test Loss: 0.47966623306274414\n",
      "Epoch 4, Batch 20, Test Loss: 0.6257691383361816\n",
      "Epoch 4, Batch 21, Test Loss: 0.686858594417572\n",
      "Epoch 4, Batch 22, Test Loss: 0.6547033190727234\n",
      "Epoch 4, Batch 23, Test Loss: 0.5808577537536621\n",
      "Epoch 4, Batch 24, Test Loss: 0.9680426716804504\n",
      "Epoch 4, Batch 25, Test Loss: 0.6769176721572876\n",
      "Epoch 4, Batch 26, Test Loss: 0.8419359922409058\n",
      "Epoch 4, Batch 27, Test Loss: 0.4780854880809784\n",
      "Epoch 4, Batch 28, Test Loss: 0.7424290776252747\n",
      "Epoch 4, Batch 29, Test Loss: 0.5272218585014343\n",
      "Epoch 4, Batch 30, Test Loss: 0.7858834862709045\n",
      "Epoch 4, Batch 31, Test Loss: 0.7373328804969788\n",
      "Epoch 4, Batch 32, Test Loss: 0.7477323412895203\n",
      "Epoch 4, Batch 33, Test Loss: 0.6992740631103516\n",
      "Epoch 4, Batch 34, Test Loss: 0.6431650519371033\n",
      "Epoch 4, Batch 35, Test Loss: 0.6517219543457031\n",
      "Epoch 4, Batch 36, Test Loss: 0.5855506062507629\n",
      "Epoch 4, Batch 37, Test Loss: 0.7678149342536926\n",
      "Epoch 4, Batch 38, Test Loss: 0.8526486158370972\n",
      "Epoch 4, Batch 39, Test Loss: 0.6466382741928101\n",
      "Epoch 4, Batch 40, Test Loss: 0.540069580078125\n",
      "Epoch 4, Batch 41, Test Loss: 0.5794731974601746\n",
      "Epoch 4, Batch 42, Test Loss: 0.5754592418670654\n",
      "Epoch 4, Batch 43, Test Loss: 0.5860978364944458\n",
      "Epoch 4, Batch 44, Test Loss: 0.7458726167678833\n",
      "Epoch 4, Batch 45, Test Loss: 0.7289413213729858\n",
      "Epoch 4, Batch 46, Test Loss: 0.6036456823348999\n",
      "Epoch 4, Batch 47, Test Loss: 0.5819598436355591\n",
      "Epoch 4, Batch 48, Test Loss: 0.4555199444293976\n",
      "Epoch 4, Batch 49, Test Loss: 0.7730100750923157\n",
      "Epoch 4, Batch 50, Test Loss: 0.8036082983016968\n",
      "Epoch 4, Batch 51, Test Loss: 0.7045787572860718\n",
      "Epoch 4, Batch 52, Test Loss: 0.6862896680831909\n",
      "Epoch 4, Batch 53, Test Loss: 0.5035159587860107\n",
      "Epoch 4, Batch 54, Test Loss: 0.7619596719741821\n",
      "Epoch 4, Batch 55, Test Loss: 0.8873154520988464\n",
      "Epoch 4, Batch 56, Test Loss: 0.6944918632507324\n",
      "Epoch 4, Batch 57, Test Loss: 0.6015531420707703\n",
      "Epoch 4, Batch 58, Test Loss: 0.7873654961585999\n",
      "Epoch 4, Batch 59, Test Loss: 0.7170894742012024\n",
      "Epoch 4, Batch 60, Test Loss: 0.8663576245307922\n",
      "Epoch 4, Batch 61, Test Loss: 0.7049682140350342\n",
      "Epoch 4, Batch 62, Test Loss: 0.6605759859085083\n",
      "Epoch 4, Batch 63, Test Loss: 0.6744910478591919\n",
      "Epoch 4, Batch 64, Test Loss: 0.6612507700920105\n",
      "Epoch 4, Batch 65, Test Loss: 0.5275965332984924\n",
      "Epoch 4, Batch 66, Test Loss: 0.6285526156425476\n",
      "Epoch 4, Batch 67, Test Loss: 0.6205092072486877\n",
      "Epoch 4, Batch 68, Test Loss: 0.6667258143424988\n",
      "Epoch 4, Batch 69, Test Loss: 0.7258813977241516\n",
      "Epoch 4, Batch 70, Test Loss: 0.7378983497619629\n",
      "Epoch 4, Batch 71, Test Loss: 0.7664745450019836\n",
      "Epoch 4, Batch 72, Test Loss: 0.84343022108078\n",
      "Epoch 4, Batch 73, Test Loss: 0.37627750635147095\n",
      "Epoch 4, Batch 74, Test Loss: 0.7260837554931641\n",
      "Epoch 4, Batch 75, Test Loss: 0.8374121189117432\n",
      "Epoch 4, Batch 76, Test Loss: 0.6277393698692322\n",
      "Epoch 4, Batch 77, Test Loss: 0.8475425839424133\n",
      "Epoch 4, Batch 78, Test Loss: 0.5748663544654846\n",
      "Epoch 4, Batch 79, Test Loss: 0.6921073794364929\n",
      "Epoch 4, Batch 80, Test Loss: 0.5381177663803101\n",
      "Epoch 4, Batch 81, Test Loss: 0.5520998239517212\n",
      "Epoch 4, Batch 82, Test Loss: 0.6895214915275574\n",
      "Epoch 4, Batch 83, Test Loss: 0.5590895414352417\n",
      "Epoch 4, Batch 84, Test Loss: 0.6029691100120544\n",
      "Epoch 4, Batch 85, Test Loss: 0.6357578039169312\n",
      "Epoch 4, Batch 86, Test Loss: 0.7476195693016052\n",
      "Epoch 4, Batch 87, Test Loss: 0.5053404569625854\n",
      "Epoch 4, Batch 88, Test Loss: 0.6785975694656372\n",
      "Epoch 4, Batch 89, Test Loss: 0.8851290941238403\n",
      "Epoch 4, Batch 90, Test Loss: 0.7806848883628845\n",
      "Epoch 4, Batch 91, Test Loss: 0.476967990398407\n",
      "Epoch 4, Batch 92, Test Loss: 0.8841477632522583\n",
      "Epoch 4, Batch 93, Test Loss: 0.5765007138252258\n",
      "Epoch 4, Batch 94, Test Loss: 0.4913282096385956\n",
      "Epoch 4, Batch 95, Test Loss: 0.7263820171356201\n",
      "Epoch 4, Batch 96, Test Loss: 0.6516863703727722\n",
      "Epoch 4, Batch 97, Test Loss: 0.737328290939331\n",
      "Epoch 4, Batch 98, Test Loss: 0.5319676399230957\n",
      "Epoch 4, Batch 99, Test Loss: 0.561993420124054\n",
      "Epoch 4, Batch 100, Test Loss: 0.5033259987831116\n",
      "Epoch 4, Batch 101, Test Loss: 0.652666449546814\n",
      "Epoch 4, Batch 102, Test Loss: 0.4090859293937683\n",
      "Epoch 4, Batch 103, Test Loss: 0.8209089040756226\n",
      "Epoch 4, Batch 104, Test Loss: 0.5198377370834351\n",
      "Epoch 4, Batch 105, Test Loss: 0.5972867608070374\n",
      "Epoch 4, Batch 106, Test Loss: 0.7601268887519836\n",
      "Epoch 4, Batch 107, Test Loss: 0.5419603586196899\n",
      "Epoch 4, Batch 108, Test Loss: 0.8556903004646301\n",
      "Epoch 4, Batch 109, Test Loss: 0.7265633940696716\n",
      "Epoch 4, Batch 110, Test Loss: 0.8031280636787415\n",
      "Epoch 4, Batch 111, Test Loss: 0.6216704845428467\n",
      "Epoch 4, Batch 112, Test Loss: 0.8303196430206299\n",
      "Epoch 4, Batch 113, Test Loss: 0.8645105361938477\n",
      "Epoch 4, Batch 114, Test Loss: 0.7736133933067322\n",
      "Epoch 4, Batch 115, Test Loss: 0.6938233971595764\n",
      "Epoch 4, Batch 116, Test Loss: 0.7176048159599304\n",
      "Epoch 4, Batch 117, Test Loss: 0.6373566389083862\n",
      "Epoch 4, Batch 118, Test Loss: 0.6055578589439392\n",
      "Epoch 4, Batch 119, Test Loss: 0.6262266635894775\n",
      "Epoch 4, Batch 120, Test Loss: 0.7241048812866211\n",
      "Epoch 4, Batch 121, Test Loss: 0.5590598583221436\n",
      "Epoch 4, Batch 122, Test Loss: 0.7244520783424377\n",
      "Epoch 4, Batch 123, Test Loss: 0.5566254258155823\n",
      "Epoch 4, Batch 124, Test Loss: 0.7087340354919434\n",
      "Epoch 4, Batch 125, Test Loss: 0.5782384872436523\n",
      "Epoch 4, Batch 126, Test Loss: 0.6189195513725281\n",
      "Epoch 4, Batch 127, Test Loss: 0.7061113119125366\n",
      "Epoch 4, Batch 128, Test Loss: 0.6785646677017212\n",
      "Epoch 4, Batch 129, Test Loss: 0.6288334131240845\n",
      "Epoch 4, Batch 130, Test Loss: 0.5357539057731628\n",
      "Epoch 4, Batch 131, Test Loss: 0.649187445640564\n",
      "Epoch 4, Batch 132, Test Loss: 0.713371753692627\n",
      "Epoch 4, Batch 133, Test Loss: 0.7546752691268921\n",
      "Epoch 4, Batch 134, Test Loss: 0.8442891836166382\n",
      "Epoch 4, Batch 135, Test Loss: 0.5959644317626953\n",
      "Epoch 4, Batch 136, Test Loss: 0.6925430297851562\n",
      "Epoch 4, Batch 137, Test Loss: 0.6140040159225464\n",
      "Epoch 4, Batch 138, Test Loss: 0.9582264423370361\n",
      "Epoch 4, Batch 139, Test Loss: 0.4953772723674774\n",
      "Epoch 4, Batch 140, Test Loss: 0.5310763716697693\n",
      "Epoch 4, Batch 141, Test Loss: 0.6636964082717896\n",
      "Epoch 4, Batch 142, Test Loss: 0.6492816805839539\n",
      "Epoch 4, Batch 143, Test Loss: 0.7943744659423828\n",
      "Epoch 4, Batch 144, Test Loss: 0.6928403377532959\n",
      "Epoch 4, Batch 145, Test Loss: 0.7865790128707886\n",
      "Epoch 4, Batch 146, Test Loss: 0.7574058771133423\n",
      "Epoch 4, Batch 147, Test Loss: 0.7693299055099487\n",
      "Epoch 4, Batch 148, Test Loss: 0.5836816430091858\n",
      "Epoch 4, Batch 149, Test Loss: 0.5020516514778137\n",
      "Epoch 4, Batch 150, Test Loss: 0.7758051753044128\n",
      "Epoch 4, Batch 151, Test Loss: 0.5505601167678833\n",
      "Epoch 4, Batch 152, Test Loss: 0.8128823041915894\n",
      "Epoch 4, Batch 153, Test Loss: 0.7094782590866089\n",
      "Epoch 4, Batch 154, Test Loss: 0.7289540767669678\n",
      "Epoch 4, Batch 155, Test Loss: 0.6226416826248169\n",
      "Epoch 4, Batch 156, Test Loss: 0.7466773986816406\n",
      "Epoch 4, Batch 157, Test Loss: 0.9588196277618408\n",
      "Epoch 4, Batch 158, Test Loss: 0.7708576321601868\n",
      "Epoch 4, Batch 159, Test Loss: 0.6690255999565125\n",
      "Epoch 4, Batch 160, Test Loss: 0.7083582282066345\n",
      "Epoch 4, Batch 161, Test Loss: 0.7985620498657227\n",
      "Epoch 4, Batch 162, Test Loss: 0.7748047709465027\n",
      "Epoch 4, Batch 163, Test Loss: 0.6064481139183044\n",
      "Epoch 4, Batch 164, Test Loss: 0.5002415180206299\n",
      "Epoch 4, Batch 165, Test Loss: 0.6843056082725525\n",
      "Epoch 4, Batch 166, Test Loss: 0.8618139624595642\n",
      "Epoch 4, Batch 167, Test Loss: 0.7844216227531433\n",
      "Epoch 4, Batch 168, Test Loss: 0.6778206825256348\n",
      "Epoch 4, Batch 169, Test Loss: 0.5538423657417297\n",
      "Epoch 4, Batch 170, Test Loss: 0.5836790204048157\n",
      "Epoch 4, Batch 171, Test Loss: 0.7214771509170532\n",
      "Epoch 4, Batch 172, Test Loss: 0.840648889541626\n",
      "Epoch 4, Batch 173, Test Loss: 0.7543940544128418\n",
      "Epoch 4, Batch 174, Test Loss: 0.6793031692504883\n",
      "Epoch 4, Batch 175, Test Loss: 0.6955806016921997\n",
      "Epoch 4, Batch 176, Test Loss: 0.6285575032234192\n",
      "Epoch 4, Batch 177, Test Loss: 0.6957995295524597\n",
      "Epoch 4, Batch 178, Test Loss: 0.5307210683822632\n",
      "Epoch 4, Batch 179, Test Loss: 0.530913233757019\n",
      "Epoch 4, Batch 180, Test Loss: 0.4766277074813843\n",
      "Epoch 4, Batch 181, Test Loss: 0.5195736885070801\n",
      "Epoch 4, Batch 182, Test Loss: 0.6080575585365295\n",
      "Epoch 4, Batch 183, Test Loss: 0.6810511350631714\n",
      "Epoch 4, Batch 184, Test Loss: 0.5921093821525574\n",
      "Epoch 4, Batch 185, Test Loss: 0.5151292681694031\n",
      "Epoch 4, Batch 186, Test Loss: 0.8075721859931946\n",
      "Epoch 4, Batch 187, Test Loss: 0.5780093669891357\n",
      "Epoch 4, Batch 188, Test Loss: 0.5927460193634033\n",
      "Epoch 4, Batch 189, Test Loss: 0.5740958452224731\n",
      "Epoch 4, Batch 190, Test Loss: 0.6749526262283325\n",
      "Epoch 4, Batch 191, Test Loss: 0.6513329148292542\n",
      "Epoch 4, Batch 192, Test Loss: 0.891996443271637\n",
      "Epoch 4, Batch 193, Test Loss: 0.6480275988578796\n",
      "Epoch 4, Batch 194, Test Loss: 0.6706724762916565\n",
      "Epoch 4, Batch 195, Test Loss: 0.831716775894165\n",
      "Epoch 4, Batch 196, Test Loss: 0.6316118240356445\n",
      "Epoch 4, Batch 197, Test Loss: 0.7199655771255493\n",
      "Epoch 4, Batch 198, Test Loss: 0.6167036890983582\n",
      "Epoch 4, Batch 199, Test Loss: 0.6831114292144775\n",
      "Epoch 4, Batch 200, Test Loss: 0.6294747591018677\n",
      "Epoch 4, Batch 201, Test Loss: 0.9013971090316772\n",
      "Epoch 4, Batch 202, Test Loss: 0.7019597291946411\n",
      "Epoch 4, Batch 203, Test Loss: 0.7083204388618469\n",
      "Epoch 4, Batch 204, Test Loss: 0.8228418231010437\n",
      "Epoch 4, Batch 205, Test Loss: 0.5721102952957153\n",
      "Epoch 4, Batch 206, Test Loss: 0.584323525428772\n",
      "Epoch 4, Batch 207, Test Loss: 0.5717912912368774\n",
      "Epoch 4, Batch 208, Test Loss: 0.6696084141731262\n",
      "Epoch 4, Batch 209, Test Loss: 0.7105475664138794\n",
      "Epoch 4, Batch 210, Test Loss: 0.7653863430023193\n",
      "Epoch 4, Batch 211, Test Loss: 0.7028980255126953\n",
      "Epoch 4, Batch 212, Test Loss: 0.6076211929321289\n",
      "Epoch 4, Batch 213, Test Loss: 0.6553549766540527\n",
      "Epoch 4, Batch 214, Test Loss: 0.7630516886711121\n",
      "Epoch 4, Batch 215, Test Loss: 0.7006208896636963\n",
      "Epoch 4, Batch 216, Test Loss: 0.6611526608467102\n",
      "Epoch 4, Batch 217, Test Loss: 0.7371214628219604\n",
      "Epoch 4, Batch 218, Test Loss: 0.5059412717819214\n",
      "Epoch 4, Batch 219, Test Loss: 0.7956408262252808\n",
      "Epoch 4, Batch 220, Test Loss: 0.5249461531639099\n",
      "Epoch 4, Batch 221, Test Loss: 0.5960605144500732\n",
      "Epoch 4, Batch 222, Test Loss: 0.6044373512268066\n",
      "Epoch 4, Batch 223, Test Loss: 0.6301082968711853\n",
      "Epoch 4, Batch 224, Test Loss: 0.6954121589660645\n",
      "Epoch 4, Batch 225, Test Loss: 0.520882785320282\n",
      "Epoch 4, Batch 226, Test Loss: 0.7647367119789124\n",
      "Epoch 4, Batch 227, Test Loss: 0.42631176114082336\n",
      "Epoch 4, Batch 228, Test Loss: 0.8322750926017761\n",
      "Epoch 4, Batch 229, Test Loss: 0.7127361297607422\n",
      "Epoch 4, Batch 230, Test Loss: 0.5620929598808289\n",
      "Epoch 4, Batch 231, Test Loss: 0.7332248091697693\n",
      "Epoch 4, Batch 232, Test Loss: 0.64369797706604\n",
      "Epoch 4, Batch 233, Test Loss: 0.8260092735290527\n",
      "Epoch 4, Batch 234, Test Loss: 0.6198621392250061\n",
      "Epoch 4, Batch 235, Test Loss: 0.6600381731987\n",
      "Epoch 4, Batch 236, Test Loss: 0.5662050843238831\n",
      "Epoch 4, Batch 237, Test Loss: 0.6106754541397095\n",
      "Epoch 4, Batch 238, Test Loss: 0.5631208419799805\n",
      "Epoch 4, Batch 239, Test Loss: 0.5962051749229431\n",
      "Epoch 4, Batch 240, Test Loss: 0.795522928237915\n",
      "Epoch 4, Batch 241, Test Loss: 0.5492571592330933\n",
      "Epoch 4, Batch 242, Test Loss: 0.8775354623794556\n",
      "Epoch 4, Batch 243, Test Loss: 0.665702223777771\n",
      "Epoch 4, Batch 244, Test Loss: 0.5758472681045532\n",
      "Epoch 4, Batch 245, Test Loss: 0.501695990562439\n",
      "Epoch 4, Batch 246, Test Loss: 0.6922944188117981\n",
      "Epoch 4, Batch 247, Test Loss: 0.7013188004493713\n",
      "Epoch 4, Batch 248, Test Loss: 0.9364375472068787\n",
      "Epoch 4, Batch 249, Test Loss: 0.593720555305481\n",
      "Epoch 4, Batch 250, Test Loss: 0.516319990158081\n",
      "Epoch 4, Batch 251, Test Loss: 0.6964455842971802\n",
      "Epoch 4, Batch 252, Test Loss: 0.845374345779419\n",
      "Epoch 4, Batch 253, Test Loss: 0.6124298572540283\n",
      "Epoch 4, Batch 254, Test Loss: 0.6579011678695679\n",
      "Epoch 4, Batch 255, Test Loss: 0.8185669779777527\n",
      "Epoch 4, Batch 256, Test Loss: 0.7956821322441101\n",
      "Epoch 4, Batch 257, Test Loss: 0.5507601499557495\n",
      "Epoch 4, Batch 258, Test Loss: 0.6372181177139282\n",
      "Epoch 4, Batch 259, Test Loss: 0.837774932384491\n",
      "Epoch 4, Batch 260, Test Loss: 0.6059596538543701\n",
      "Epoch 4, Batch 261, Test Loss: 0.7083772420883179\n",
      "Epoch 4, Batch 262, Test Loss: 0.5955936908721924\n",
      "Epoch 4, Batch 263, Test Loss: 0.7388361692428589\n",
      "Epoch 4, Batch 264, Test Loss: 0.5030695796012878\n",
      "Epoch 4, Batch 265, Test Loss: 0.8958953022956848\n",
      "Epoch 4, Batch 266, Test Loss: 0.7107003331184387\n",
      "Epoch 4, Batch 267, Test Loss: 0.6654484272003174\n",
      "Epoch 4, Batch 268, Test Loss: 0.8100245594978333\n",
      "Epoch 4, Batch 269, Test Loss: 0.8113641142845154\n",
      "Epoch 4, Batch 270, Test Loss: 0.7101182341575623\n",
      "Epoch 4, Batch 271, Test Loss: 0.6806759238243103\n",
      "Epoch 4, Batch 272, Test Loss: 0.6664906144142151\n",
      "Epoch 4, Batch 273, Test Loss: 0.49027159810066223\n",
      "Epoch 4, Batch 274, Test Loss: 0.5229651927947998\n",
      "Epoch 4, Batch 275, Test Loss: 0.7362411022186279\n",
      "Epoch 4, Batch 276, Test Loss: 0.43483537435531616\n",
      "Epoch 4, Batch 277, Test Loss: 0.7344302535057068\n",
      "Epoch 4, Batch 278, Test Loss: 0.7002890706062317\n",
      "Epoch 4, Batch 279, Test Loss: 0.8344531655311584\n",
      "Epoch 4, Batch 280, Test Loss: 0.5758949518203735\n",
      "Epoch 4, Batch 281, Test Loss: 0.6671285033226013\n",
      "Epoch 4, Batch 282, Test Loss: 0.7503993511199951\n",
      "Epoch 4, Batch 283, Test Loss: 0.7687162756919861\n",
      "Epoch 4, Batch 284, Test Loss: 0.709436297416687\n",
      "Epoch 4, Batch 285, Test Loss: 0.665503740310669\n",
      "Epoch 4, Batch 286, Test Loss: 0.5654095411300659\n",
      "Epoch 4, Batch 287, Test Loss: 0.7847722172737122\n",
      "Epoch 4, Batch 288, Test Loss: 0.7700154781341553\n",
      "Epoch 4, Batch 289, Test Loss: 0.5230777859687805\n",
      "Epoch 4, Batch 290, Test Loss: 0.4589354991912842\n",
      "Epoch 4, Batch 291, Test Loss: 0.7258265614509583\n",
      "Epoch 4, Batch 292, Test Loss: 0.6220835447311401\n",
      "Epoch 4, Batch 293, Test Loss: 0.7418699860572815\n",
      "Epoch 4, Batch 294, Test Loss: 0.7340404987335205\n",
      "Epoch 4, Batch 295, Test Loss: 0.5710448026657104\n",
      "Epoch 4, Batch 296, Test Loss: 0.6242605447769165\n",
      "Epoch 4, Batch 297, Test Loss: 0.8342455625534058\n",
      "Epoch 4, Batch 298, Test Loss: 0.6433603763580322\n",
      "Epoch 4, Batch 299, Test Loss: 0.6845319867134094\n",
      "Epoch 4, Batch 300, Test Loss: 0.5428482294082642\n",
      "Epoch 4, Batch 301, Test Loss: 0.4652053117752075\n",
      "Epoch 4, Batch 302, Test Loss: 0.6731892824172974\n",
      "Epoch 4, Batch 303, Test Loss: 0.6496636867523193\n",
      "Epoch 4, Batch 304, Test Loss: 0.542090892791748\n",
      "Epoch 4, Batch 305, Test Loss: 0.8754852414131165\n",
      "Epoch 4, Batch 306, Test Loss: 0.729163646697998\n",
      "Epoch 4, Batch 307, Test Loss: 0.7398308515548706\n",
      "Epoch 4, Batch 308, Test Loss: 0.7401134967803955\n",
      "Epoch 4, Batch 309, Test Loss: 0.6390073895454407\n",
      "Epoch 4, Batch 310, Test Loss: 0.5836440920829773\n",
      "Epoch 4, Batch 311, Test Loss: 0.6783993244171143\n",
      "Epoch 4, Batch 312, Test Loss: 0.6046919226646423\n",
      "Epoch 4, Batch 313, Test Loss: 0.6442099213600159\n",
      "Epoch 4, Batch 314, Test Loss: 0.6498347520828247\n",
      "Epoch 4, Batch 315, Test Loss: 0.8879600167274475\n",
      "Epoch 4, Batch 316, Test Loss: 0.5976254343986511\n",
      "Epoch 4, Batch 317, Test Loss: 0.5738779306411743\n",
      "Epoch 4, Batch 318, Test Loss: 0.7279714345932007\n",
      "Epoch 4, Batch 319, Test Loss: 0.6313422322273254\n",
      "Epoch 4, Batch 320, Test Loss: 0.579383134841919\n",
      "Epoch 4, Batch 321, Test Loss: 0.6255624294281006\n",
      "Epoch 4, Batch 322, Test Loss: 0.6293877363204956\n",
      "Epoch 4, Batch 323, Test Loss: 0.689896821975708\n",
      "Epoch 4, Batch 324, Test Loss: 0.6899046897888184\n",
      "Epoch 4, Batch 325, Test Loss: 0.7908876538276672\n",
      "Epoch 4, Batch 326, Test Loss: 0.6116731762886047\n",
      "Epoch 4, Batch 327, Test Loss: 0.725640594959259\n",
      "Epoch 4, Batch 328, Test Loss: 0.7121341824531555\n",
      "Epoch 4, Batch 329, Test Loss: 0.5435730814933777\n",
      "Epoch 4, Batch 330, Test Loss: 0.5592806339263916\n",
      "Epoch 4, Batch 331, Test Loss: 0.9315343499183655\n",
      "Epoch 4, Batch 332, Test Loss: 0.7555118799209595\n",
      "Epoch 4, Batch 333, Test Loss: 0.7340566515922546\n",
      "Epoch 4, Batch 334, Test Loss: 0.8047598600387573\n",
      "Epoch 4, Batch 335, Test Loss: 0.7666500210762024\n",
      "Epoch 4, Batch 336, Test Loss: 0.5347601771354675\n",
      "Epoch 4, Batch 337, Test Loss: 0.5537240505218506\n",
      "Epoch 4, Batch 338, Test Loss: 0.8278149366378784\n",
      "Epoch 4, Batch 339, Test Loss: 0.8392015695571899\n",
      "Epoch 4, Batch 340, Test Loss: 0.6555644273757935\n",
      "Epoch 4, Batch 341, Test Loss: 0.6549685001373291\n",
      "Epoch 4, Batch 342, Test Loss: 0.6595599055290222\n",
      "Epoch 4, Batch 343, Test Loss: 0.7366805076599121\n",
      "Epoch 4, Batch 344, Test Loss: 0.6461942791938782\n",
      "Epoch 4, Batch 345, Test Loss: 0.5096195936203003\n",
      "Epoch 4, Batch 346, Test Loss: 0.6132261157035828\n",
      "Epoch 4, Batch 347, Test Loss: 1.1009756326675415\n",
      "Epoch 4, Batch 348, Test Loss: 0.9243834018707275\n",
      "Epoch 4, Batch 349, Test Loss: 0.5567427277565002\n",
      "Epoch 4, Batch 350, Test Loss: 0.6470122337341309\n",
      "Epoch 4, Batch 351, Test Loss: 0.5592456459999084\n",
      "Epoch 4, Batch 352, Test Loss: 0.6216078400611877\n",
      "Epoch 4, Batch 353, Test Loss: 0.658420205116272\n",
      "Epoch 4, Batch 354, Test Loss: 0.8125172853469849\n",
      "Epoch 4, Batch 355, Test Loss: 0.5496517419815063\n",
      "Epoch 4, Batch 356, Test Loss: 0.6413168907165527\n",
      "Epoch 4, Batch 357, Test Loss: 1.0081666707992554\n",
      "Epoch 4, Batch 358, Test Loss: 0.6030430197715759\n",
      "Epoch 4, Batch 359, Test Loss: 0.7330256104469299\n",
      "Epoch 4, Batch 360, Test Loss: 0.5947456359863281\n",
      "Epoch 4, Batch 361, Test Loss: 0.6710366606712341\n",
      "Epoch 4, Batch 362, Test Loss: 0.727431058883667\n",
      "Epoch 4, Batch 363, Test Loss: 0.9659930467605591\n",
      "Epoch 4, Batch 364, Test Loss: 0.6709911823272705\n",
      "Epoch 4, Batch 365, Test Loss: 0.763279139995575\n",
      "Epoch 4, Batch 366, Test Loss: 0.6269301176071167\n",
      "Epoch 4, Batch 367, Test Loss: 0.851177453994751\n",
      "Epoch 4, Batch 368, Test Loss: 0.6064831018447876\n",
      "Epoch 4, Batch 369, Test Loss: 0.7475988268852234\n",
      "Epoch 4, Batch 370, Test Loss: 0.6311794519424438\n",
      "Epoch 4, Batch 371, Test Loss: 0.6680458784103394\n",
      "Epoch 4, Batch 372, Test Loss: 0.8845025300979614\n",
      "Epoch 4, Batch 373, Test Loss: 0.8152810335159302\n",
      "Epoch 4, Batch 374, Test Loss: 0.6533520817756653\n",
      "Epoch 4, Batch 375, Test Loss: 0.6534510850906372\n",
      "Epoch 4, Batch 376, Test Loss: 0.8351256251335144\n",
      "Epoch 4, Batch 377, Test Loss: 0.6593160033226013\n",
      "Epoch 4, Batch 378, Test Loss: 0.6667260527610779\n",
      "Epoch 4, Batch 379, Test Loss: 0.6684276461601257\n",
      "Epoch 4, Batch 380, Test Loss: 0.6533399224281311\n",
      "Epoch 4, Batch 381, Test Loss: 0.8525505065917969\n",
      "Epoch 4, Batch 382, Test Loss: 0.8390618562698364\n",
      "Epoch 4, Batch 383, Test Loss: 0.6842188835144043\n",
      "Epoch 4, Batch 384, Test Loss: 0.574944257736206\n",
      "Epoch 4, Batch 385, Test Loss: 0.5066279768943787\n",
      "Epoch 4, Batch 386, Test Loss: 0.4105473756790161\n",
      "Epoch 4, Batch 387, Test Loss: 0.6174461245536804\n",
      "Epoch 4, Batch 388, Test Loss: 0.6788400411605835\n",
      "Epoch 4, Batch 389, Test Loss: 0.8734273314476013\n",
      "Epoch 4, Batch 390, Test Loss: 0.3960094153881073\n",
      "Epoch 4, Batch 391, Test Loss: 0.8672783374786377\n",
      "Epoch 4, Batch 392, Test Loss: 0.7095622420310974\n",
      "Epoch 4, Batch 393, Test Loss: 0.7702605128288269\n",
      "Epoch 4, Batch 394, Test Loss: 0.6820919513702393\n",
      "Epoch 4, Batch 395, Test Loss: 0.5201855897903442\n",
      "Epoch 4, Batch 396, Test Loss: 0.7746498584747314\n",
      "Epoch 4, Batch 397, Test Loss: 0.7056837677955627\n",
      "Epoch 4, Batch 398, Test Loss: 0.9327072501182556\n",
      "Epoch 4, Batch 399, Test Loss: 0.6518101692199707\n",
      "Epoch 4, Batch 400, Test Loss: 0.754719614982605\n",
      "Epoch 4, Batch 401, Test Loss: 0.7149131298065186\n",
      "Epoch 4, Batch 402, Test Loss: 0.8134444355964661\n",
      "Epoch 4, Batch 403, Test Loss: 0.7425305843353271\n",
      "Epoch 4, Batch 404, Test Loss: 0.47591233253479004\n",
      "Epoch 4, Batch 405, Test Loss: 0.8002501130104065\n",
      "Epoch 4, Batch 406, Test Loss: 0.9619240760803223\n",
      "Epoch 4, Batch 407, Test Loss: 0.6996675729751587\n",
      "Epoch 4, Batch 408, Test Loss: 0.8817464113235474\n",
      "Epoch 4, Batch 409, Test Loss: 0.7444271445274353\n",
      "Epoch 4, Batch 410, Test Loss: 0.8039520978927612\n",
      "Epoch 4, Batch 411, Test Loss: 0.547692060470581\n",
      "Epoch 4, Batch 412, Test Loss: 0.7857018709182739\n",
      "Epoch 4, Batch 413, Test Loss: 0.534293532371521\n",
      "Epoch 4, Batch 414, Test Loss: 0.8991098403930664\n",
      "Epoch 4, Batch 415, Test Loss: 0.7390902638435364\n",
      "Epoch 4, Batch 416, Test Loss: 0.6432672739028931\n",
      "Epoch 4, Batch 417, Test Loss: 0.6874127388000488\n",
      "Epoch 4, Batch 418, Test Loss: 0.7334175109863281\n",
      "Epoch 4, Batch 419, Test Loss: 0.7663094997406006\n",
      "Epoch 4, Batch 420, Test Loss: 0.5862632393836975\n",
      "Epoch 4, Batch 421, Test Loss: 0.9490010738372803\n",
      "Epoch 4, Batch 422, Test Loss: 0.5982141494750977\n",
      "Epoch 4, Batch 423, Test Loss: 0.8120504021644592\n",
      "Epoch 4, Batch 424, Test Loss: 0.8289744853973389\n",
      "Epoch 4, Batch 425, Test Loss: 0.669714093208313\n",
      "Epoch 4, Batch 426, Test Loss: 0.9098114967346191\n",
      "Epoch 4, Batch 427, Test Loss: 0.6673387885093689\n",
      "Epoch 4, Batch 428, Test Loss: 0.780859112739563\n",
      "Epoch 4, Batch 429, Test Loss: 0.6871979236602783\n",
      "Epoch 4, Batch 430, Test Loss: 0.7839303612709045\n",
      "Epoch 4, Batch 431, Test Loss: 0.694827675819397\n",
      "Epoch 4, Batch 432, Test Loss: 0.7259730696678162\n",
      "Epoch 4, Batch 433, Test Loss: 0.5589816570281982\n",
      "Epoch 4, Batch 434, Test Loss: 0.613531768321991\n",
      "Epoch 4, Batch 435, Test Loss: 0.6286270618438721\n",
      "Epoch 4, Batch 436, Test Loss: 0.7432128190994263\n",
      "Epoch 4, Batch 437, Test Loss: 0.8458855748176575\n",
      "Epoch 4, Batch 438, Test Loss: 0.6379427909851074\n",
      "Epoch 4, Batch 439, Test Loss: 0.855617105960846\n",
      "Epoch 4, Batch 440, Test Loss: 0.6888188123703003\n",
      "Epoch 4, Batch 441, Test Loss: 0.6223593950271606\n",
      "Epoch 4, Batch 442, Test Loss: 0.9618980288505554\n",
      "Epoch 4, Batch 443, Test Loss: 0.8594551086425781\n",
      "Epoch 4, Batch 444, Test Loss: 0.6941924691200256\n",
      "Epoch 4, Batch 445, Test Loss: 0.6826043128967285\n",
      "Epoch 4, Batch 446, Test Loss: 0.6382765173912048\n",
      "Epoch 4, Batch 447, Test Loss: 0.4500563442707062\n",
      "Epoch 4, Batch 448, Test Loss: 0.6724107265472412\n",
      "Epoch 4, Batch 449, Test Loss: 0.6614779233932495\n",
      "Epoch 4, Batch 450, Test Loss: 0.8454321622848511\n",
      "Epoch 4, Batch 451, Test Loss: 0.7910295724868774\n",
      "Epoch 4, Batch 452, Test Loss: 0.8130517601966858\n",
      "Epoch 4, Batch 453, Test Loss: 0.879934549331665\n",
      "Epoch 4, Batch 454, Test Loss: 0.8258918523788452\n",
      "Epoch 4, Batch 455, Test Loss: 0.6287977695465088\n",
      "Epoch 4, Batch 456, Test Loss: 0.6887010931968689\n",
      "Epoch 4, Batch 457, Test Loss: 0.6697486042976379\n",
      "Epoch 4, Batch 458, Test Loss: 0.5698875784873962\n",
      "Epoch 4, Batch 459, Test Loss: 0.5730932950973511\n",
      "Epoch 4, Batch 460, Test Loss: 0.8443195819854736\n",
      "Epoch 4, Batch 461, Test Loss: 0.6407301425933838\n",
      "Epoch 4, Batch 462, Test Loss: 0.8011302947998047\n",
      "Epoch 4, Batch 463, Test Loss: 0.7932576537132263\n",
      "Epoch 4, Batch 464, Test Loss: 0.6903866529464722\n",
      "Epoch 4, Batch 465, Test Loss: 0.8368543386459351\n",
      "Epoch 4, Batch 466, Test Loss: 0.5768294334411621\n",
      "Epoch 4, Batch 467, Test Loss: 0.6445668935775757\n",
      "Epoch 4, Batch 468, Test Loss: 0.6651333570480347\n",
      "Epoch 4, Batch 469, Test Loss: 1.0267393589019775\n",
      "Epoch 4, Batch 470, Test Loss: 0.44854462146759033\n",
      "Epoch 4, Batch 471, Test Loss: 0.7257253527641296\n",
      "Epoch 4, Batch 472, Test Loss: 0.6091490387916565\n",
      "Epoch 4, Batch 473, Test Loss: 0.869562029838562\n",
      "Epoch 4, Batch 474, Test Loss: 0.4903237521648407\n",
      "Epoch 4, Batch 475, Test Loss: 0.8568798303604126\n",
      "Epoch 4, Batch 476, Test Loss: 0.7118983268737793\n",
      "Epoch 4, Batch 477, Test Loss: 0.6440059542655945\n",
      "Epoch 4, Batch 478, Test Loss: 0.5128563642501831\n",
      "Epoch 4, Batch 479, Test Loss: 0.7139387130737305\n",
      "Epoch 4, Batch 480, Test Loss: 0.47388237714767456\n",
      "Epoch 4, Batch 481, Test Loss: 0.8316690921783447\n",
      "Epoch 4, Batch 482, Test Loss: 0.9035813808441162\n",
      "Epoch 4, Batch 483, Test Loss: 0.728305995464325\n",
      "Epoch 4, Batch 484, Test Loss: 0.7228571176528931\n",
      "Epoch 4, Batch 485, Test Loss: 0.6671129465103149\n",
      "Epoch 4, Batch 486, Test Loss: 0.6188051104545593\n",
      "Epoch 4, Batch 487, Test Loss: 0.6995230317115784\n",
      "Epoch 4, Batch 488, Test Loss: 0.7484220862388611\n",
      "Epoch 4, Batch 489, Test Loss: 0.8195401430130005\n",
      "Epoch 4, Batch 490, Test Loss: 0.5611811876296997\n",
      "Epoch 4, Batch 491, Test Loss: 0.7664357423782349\n",
      "Epoch 4, Batch 492, Test Loss: 1.0128912925720215\n",
      "Epoch 4, Batch 493, Test Loss: 0.6855260729789734\n",
      "Epoch 4, Batch 494, Test Loss: 0.771685779094696\n",
      "Epoch 4, Batch 495, Test Loss: 0.6444628834724426\n",
      "Epoch 4, Batch 496, Test Loss: 0.6331153512001038\n",
      "Epoch 4, Batch 497, Test Loss: 0.7166469097137451\n",
      "Epoch 4, Batch 498, Test Loss: 0.7111942768096924\n",
      "Epoch 4, Batch 499, Test Loss: 0.5774530172348022\n",
      "Epoch 4, Batch 500, Test Loss: 0.5790159702301025\n",
      "Epoch 4, Batch 501, Test Loss: 0.8096370100975037\n",
      "Epoch 4, Batch 502, Test Loss: 0.5309038758277893\n",
      "Epoch 4, Batch 503, Test Loss: 0.8233195543289185\n",
      "Epoch 4, Batch 504, Test Loss: 0.6207926273345947\n",
      "Epoch 4, Batch 505, Test Loss: 0.7397149205207825\n",
      "Epoch 4, Batch 506, Test Loss: 0.7278783917427063\n",
      "Epoch 4, Batch 507, Test Loss: 0.8330605030059814\n",
      "Epoch 4, Batch 508, Test Loss: 0.8762940168380737\n",
      "Epoch 4, Batch 509, Test Loss: 0.8413403034210205\n",
      "Epoch 4, Batch 510, Test Loss: 0.6051088571548462\n",
      "Epoch 4, Batch 511, Test Loss: 0.8604249954223633\n",
      "Epoch 4, Batch 512, Test Loss: 0.6959457397460938\n",
      "Epoch 4, Batch 513, Test Loss: 0.5403931140899658\n",
      "Epoch 4, Batch 514, Test Loss: 0.5162268877029419\n",
      "Epoch 4, Batch 515, Test Loss: 0.6217976212501526\n",
      "Epoch 4, Batch 516, Test Loss: 0.7302980422973633\n",
      "Epoch 4, Batch 517, Test Loss: 0.4695981740951538\n",
      "Epoch 4, Batch 518, Test Loss: 0.6652894020080566\n",
      "Epoch 4, Batch 519, Test Loss: 0.7476959228515625\n",
      "Epoch 4, Batch 520, Test Loss: 0.6601563096046448\n",
      "Epoch 4, Batch 521, Test Loss: 0.6054078340530396\n",
      "Epoch 4, Batch 522, Test Loss: 0.929821252822876\n",
      "Epoch 4, Batch 523, Test Loss: 0.5149109363555908\n",
      "Epoch 4, Batch 524, Test Loss: 0.5848808288574219\n",
      "Epoch 4, Batch 525, Test Loss: 0.740153431892395\n",
      "Epoch 4, Batch 526, Test Loss: 0.8922452926635742\n",
      "Epoch 4, Batch 527, Test Loss: 0.7117487192153931\n",
      "Epoch 4, Batch 528, Test Loss: 0.7469537854194641\n",
      "Epoch 4, Batch 529, Test Loss: 0.6786888837814331\n",
      "Epoch 4, Batch 530, Test Loss: 0.6392689943313599\n",
      "Epoch 4, Batch 531, Test Loss: 0.837852954864502\n",
      "Epoch 4, Batch 532, Test Loss: 0.8482257723808289\n",
      "Epoch 4, Batch 533, Test Loss: 0.6328906416893005\n",
      "Epoch 4, Batch 534, Test Loss: 0.8241232633590698\n",
      "Epoch 4, Batch 535, Test Loss: 0.611473798751831\n",
      "Epoch 4, Batch 536, Test Loss: 0.6707379817962646\n",
      "Epoch 4, Batch 537, Test Loss: 0.6163735389709473\n",
      "Epoch 4, Batch 538, Test Loss: 0.5389484763145447\n",
      "Epoch 4, Batch 539, Test Loss: 0.6105244159698486\n",
      "Epoch 4, Batch 540, Test Loss: 0.8940649628639221\n",
      "Epoch 4, Batch 541, Test Loss: 0.6790457963943481\n",
      "Epoch 4, Batch 542, Test Loss: 0.6079915761947632\n",
      "Epoch 4, Batch 543, Test Loss: 0.9389652609825134\n",
      "Epoch 4, Batch 544, Test Loss: 0.9019036889076233\n",
      "Epoch 4, Batch 545, Test Loss: 0.4992319345474243\n",
      "Epoch 4, Batch 546, Test Loss: 0.7500828504562378\n",
      "Epoch 4, Batch 547, Test Loss: 0.48193928599357605\n",
      "Epoch 4, Batch 548, Test Loss: 0.8272156715393066\n",
      "Epoch 4, Batch 549, Test Loss: 0.5895128846168518\n",
      "Epoch 4, Batch 550, Test Loss: 0.6589670777320862\n",
      "Epoch 4, Batch 551, Test Loss: 0.8257945775985718\n",
      "Epoch 4, Batch 552, Test Loss: 0.5305043458938599\n",
      "Epoch 4, Batch 553, Test Loss: 0.555979311466217\n",
      "Epoch 4, Batch 554, Test Loss: 0.7416355609893799\n",
      "Epoch 4, Batch 555, Test Loss: 0.8925831317901611\n",
      "Epoch 4, Batch 556, Test Loss: 0.6432294845581055\n",
      "Epoch 4, Batch 557, Test Loss: 0.6043890714645386\n",
      "Epoch 4, Batch 558, Test Loss: 0.7766591906547546\n",
      "Epoch 4, Batch 559, Test Loss: 0.6232443451881409\n",
      "Epoch 4, Batch 560, Test Loss: 0.7038122415542603\n",
      "Epoch 4, Batch 561, Test Loss: 0.43772321939468384\n",
      "Epoch 4, Batch 562, Test Loss: 0.6659935116767883\n",
      "Epoch 4, Batch 563, Test Loss: 0.5811441540718079\n",
      "Epoch 4, Batch 564, Test Loss: 0.7535625696182251\n",
      "Epoch 4, Batch 565, Test Loss: 0.7277270555496216\n",
      "Epoch 4, Batch 566, Test Loss: 0.7134838104248047\n",
      "Epoch 4, Batch 567, Test Loss: 0.5786367654800415\n",
      "Epoch 4, Batch 568, Test Loss: 1.011240839958191\n",
      "Epoch 4, Batch 569, Test Loss: 0.5609222054481506\n",
      "Epoch 4, Batch 570, Test Loss: 0.7536917328834534\n",
      "Epoch 4, Batch 571, Test Loss: 0.6438742876052856\n",
      "Epoch 4, Batch 572, Test Loss: 0.6077840328216553\n",
      "Epoch 4, Batch 573, Test Loss: 0.6450493931770325\n",
      "Epoch 4, Batch 574, Test Loss: 0.7811452746391296\n",
      "Epoch 4, Batch 575, Test Loss: 0.5872772932052612\n",
      "Epoch 4, Batch 576, Test Loss: 0.6996137499809265\n",
      "Epoch 4, Batch 577, Test Loss: 0.5701162219047546\n",
      "Epoch 4, Batch 578, Test Loss: 0.6278092861175537\n",
      "Epoch 4, Batch 579, Test Loss: 0.5383722186088562\n",
      "Epoch 4, Batch 580, Test Loss: 0.5617091059684753\n",
      "Epoch 4, Batch 581, Test Loss: 0.6199207901954651\n",
      "Epoch 4, Batch 582, Test Loss: 0.6122081279754639\n",
      "Epoch 4, Batch 583, Test Loss: 0.6759161949157715\n",
      "Epoch 4, Batch 584, Test Loss: 0.6127057671546936\n",
      "Epoch 4, Batch 585, Test Loss: 0.9473023414611816\n",
      "Epoch 4, Batch 586, Test Loss: 0.8269091248512268\n",
      "Epoch 4, Batch 587, Test Loss: 0.8859695792198181\n",
      "Epoch 4, Batch 588, Test Loss: 0.68147873878479\n",
      "Epoch 4, Batch 589, Test Loss: 0.724128246307373\n",
      "Epoch 4, Batch 590, Test Loss: 0.5176222324371338\n",
      "Epoch 4, Batch 591, Test Loss: 0.49712997674942017\n",
      "Epoch 4, Batch 592, Test Loss: 0.4865179657936096\n",
      "Epoch 4, Batch 593, Test Loss: 0.7972613573074341\n",
      "Epoch 4, Batch 594, Test Loss: 0.6094889640808105\n",
      "Epoch 4, Batch 595, Test Loss: 0.6192391514778137\n",
      "Epoch 4, Batch 596, Test Loss: 0.8535678386688232\n",
      "Epoch 4, Batch 597, Test Loss: 0.5967825651168823\n",
      "Epoch 4, Batch 598, Test Loss: 0.47444573044776917\n",
      "Epoch 4, Batch 599, Test Loss: 0.7053347229957581\n",
      "Epoch 4, Batch 600, Test Loss: 0.7517448663711548\n",
      "Epoch 4, Batch 601, Test Loss: 0.5586944818496704\n",
      "Epoch 4, Batch 602, Test Loss: 0.6244266629219055\n",
      "Epoch 4, Batch 603, Test Loss: 0.5593487024307251\n",
      "Epoch 4, Batch 604, Test Loss: 0.7260034680366516\n",
      "Epoch 4, Batch 605, Test Loss: 0.6704002618789673\n",
      "Epoch 4, Batch 606, Test Loss: 0.6941618919372559\n",
      "Epoch 4, Batch 607, Test Loss: 0.3866821825504303\n",
      "Epoch 4, Batch 608, Test Loss: 0.663714587688446\n",
      "Epoch 4, Batch 609, Test Loss: 0.676901638507843\n",
      "Epoch 4, Batch 610, Test Loss: 0.49588945508003235\n",
      "Epoch 4, Batch 611, Test Loss: 0.8819484710693359\n",
      "Epoch 4, Batch 612, Test Loss: 0.7665005326271057\n",
      "Epoch 4, Batch 613, Test Loss: 0.729651689529419\n",
      "Epoch 4, Batch 614, Test Loss: 0.7934828996658325\n",
      "Epoch 4, Batch 615, Test Loss: 0.6485442519187927\n",
      "Epoch 4, Batch 616, Test Loss: 0.6503092050552368\n",
      "Epoch 4, Batch 617, Test Loss: 0.6996166110038757\n",
      "Epoch 4, Batch 618, Test Loss: 0.7776505351066589\n",
      "Epoch 4, Batch 619, Test Loss: 0.5490195155143738\n",
      "Epoch 4, Batch 620, Test Loss: 1.0074732303619385\n",
      "Epoch 4, Batch 621, Test Loss: 0.9769117832183838\n",
      "Epoch 4, Batch 622, Test Loss: 0.7375050187110901\n",
      "Epoch 4, Batch 623, Test Loss: 0.5842279195785522\n",
      "Epoch 4, Batch 624, Test Loss: 0.4970179498195648\n",
      "Epoch 4, Batch 625, Test Loss: 0.8290412425994873\n",
      "Epoch 4, Batch 626, Test Loss: 0.6472322940826416\n",
      "Epoch 4, Batch 627, Test Loss: 0.5946775078773499\n",
      "Epoch 4, Batch 628, Test Loss: 0.5075369477272034\n",
      "Epoch 4, Batch 629, Test Loss: 0.5518811941146851\n",
      "Epoch 4, Batch 630, Test Loss: 0.6500975489616394\n",
      "Epoch 4, Batch 631, Test Loss: 0.7533780336380005\n",
      "Epoch 4, Batch 632, Test Loss: 0.7056496143341064\n",
      "Epoch 4, Batch 633, Test Loss: 0.8164888620376587\n",
      "Epoch 4, Batch 634, Test Loss: 0.7700163722038269\n",
      "Epoch 4, Batch 635, Test Loss: 0.5823392271995544\n",
      "Epoch 4, Batch 636, Test Loss: 0.7021967172622681\n",
      "Epoch 4, Batch 637, Test Loss: 0.8300602436065674\n",
      "Epoch 4, Batch 638, Test Loss: 0.5930973887443542\n",
      "Epoch 4, Batch 639, Test Loss: 0.5807257294654846\n",
      "Epoch 4, Batch 640, Test Loss: 0.7933962345123291\n",
      "Epoch 4, Batch 641, Test Loss: 0.5946164727210999\n",
      "Epoch 4, Batch 642, Test Loss: 0.7400556206703186\n",
      "Epoch 4, Batch 643, Test Loss: 0.679554283618927\n",
      "Epoch 4, Batch 644, Test Loss: 0.4611469805240631\n",
      "Epoch 4, Batch 645, Test Loss: 0.879787027835846\n",
      "Epoch 4, Batch 646, Test Loss: 0.8366140127182007\n",
      "Epoch 4, Batch 647, Test Loss: 0.6568010449409485\n",
      "Epoch 4, Batch 648, Test Loss: 0.5582188963890076\n",
      "Epoch 4, Batch 649, Test Loss: 0.8249553442001343\n",
      "Epoch 4, Batch 650, Test Loss: 0.6491303443908691\n",
      "Epoch 4, Batch 651, Test Loss: 0.7451315522193909\n",
      "Epoch 4, Batch 652, Test Loss: 0.8568283319473267\n",
      "Epoch 4, Batch 653, Test Loss: 0.7626532316207886\n",
      "Epoch 4, Batch 654, Test Loss: 0.7018899917602539\n",
      "Epoch 4, Batch 655, Test Loss: 0.6058266758918762\n",
      "Epoch 4, Batch 656, Test Loss: 0.87652587890625\n",
      "Epoch 4, Batch 657, Test Loss: 0.9285072684288025\n",
      "Epoch 4, Batch 658, Test Loss: 0.49761709570884705\n",
      "Epoch 4, Batch 659, Test Loss: 0.6499321460723877\n",
      "Epoch 4, Batch 660, Test Loss: 0.7785168290138245\n",
      "Epoch 4, Batch 661, Test Loss: 0.795074999332428\n",
      "Epoch 4, Batch 662, Test Loss: 0.9730054140090942\n",
      "Epoch 4, Batch 663, Test Loss: 0.7149642109870911\n",
      "Epoch 4, Batch 664, Test Loss: 0.6297038197517395\n",
      "Epoch 4, Batch 665, Test Loss: 0.5265316963195801\n",
      "Epoch 4, Batch 666, Test Loss: 0.7846295237541199\n",
      "Epoch 4, Batch 667, Test Loss: 0.6405499577522278\n",
      "Epoch 4, Batch 668, Test Loss: 0.7944590449333191\n",
      "Epoch 4, Batch 669, Test Loss: 0.7274023294448853\n",
      "Epoch 4, Batch 670, Test Loss: 0.4861814081668854\n",
      "Epoch 4, Batch 671, Test Loss: 0.852875828742981\n",
      "Epoch 4, Batch 672, Test Loss: 0.8444190621376038\n",
      "Epoch 4, Batch 673, Test Loss: 0.626323938369751\n",
      "Epoch 4, Batch 674, Test Loss: 0.8731598258018494\n",
      "Epoch 4, Batch 675, Test Loss: 0.68743497133255\n",
      "Epoch 4, Batch 676, Test Loss: 0.7448544502258301\n",
      "Epoch 4, Batch 677, Test Loss: 0.5661910772323608\n",
      "Epoch 4, Batch 678, Test Loss: 0.7688511610031128\n",
      "Epoch 4, Batch 679, Test Loss: 0.6208894848823547\n",
      "Epoch 4, Batch 680, Test Loss: 0.5725713968276978\n",
      "Epoch 4, Batch 681, Test Loss: 0.7757725119590759\n",
      "Epoch 4, Batch 682, Test Loss: 0.5509507656097412\n",
      "Epoch 4, Batch 683, Test Loss: 0.890088677406311\n",
      "Epoch 4, Batch 684, Test Loss: 0.6697136759757996\n",
      "Epoch 4, Batch 685, Test Loss: 0.5341471433639526\n",
      "Epoch 4, Batch 686, Test Loss: 0.6145172715187073\n",
      "Epoch 4, Batch 687, Test Loss: 0.5704907774925232\n",
      "Epoch 4, Batch 688, Test Loss: 0.6534696817398071\n",
      "Epoch 4, Batch 689, Test Loss: 0.6330575942993164\n",
      "Epoch 4, Batch 690, Test Loss: 0.7418679594993591\n",
      "Epoch 4, Batch 691, Test Loss: 0.6617340445518494\n",
      "Epoch 4, Batch 692, Test Loss: 0.6427611112594604\n",
      "Epoch 4, Batch 693, Test Loss: 0.654914140701294\n",
      "Epoch 4, Batch 694, Test Loss: 0.6400234699249268\n",
      "Epoch 4, Batch 695, Test Loss: 0.6675921082496643\n",
      "Epoch 4, Batch 696, Test Loss: 0.6156996488571167\n",
      "Epoch 4, Batch 697, Test Loss: 0.8699550628662109\n",
      "Epoch 4, Batch 698, Test Loss: 0.5899742245674133\n",
      "Epoch 4, Batch 699, Test Loss: 0.6087578535079956\n",
      "Epoch 4, Batch 700, Test Loss: 0.6551381945610046\n",
      "Epoch 4, Batch 701, Test Loss: 0.6913120746612549\n",
      "Epoch 4, Batch 702, Test Loss: 0.7834798693656921\n",
      "Epoch 4, Batch 703, Test Loss: 0.6615300178527832\n",
      "Epoch 4, Batch 704, Test Loss: 0.5574128031730652\n",
      "Epoch 4, Batch 705, Test Loss: 0.7046469449996948\n",
      "Epoch 4, Batch 706, Test Loss: 0.6644309163093567\n",
      "Epoch 4, Batch 707, Test Loss: 0.5953395962715149\n",
      "Epoch 4, Batch 708, Test Loss: 0.7902438640594482\n",
      "Epoch 4, Batch 709, Test Loss: 0.6495645046234131\n",
      "Epoch 4, Batch 710, Test Loss: 0.5973570942878723\n",
      "Epoch 4, Batch 711, Test Loss: 0.9293400645256042\n",
      "Epoch 4, Batch 712, Test Loss: 0.6098527908325195\n",
      "Epoch 4, Batch 713, Test Loss: 0.48503023386001587\n",
      "Epoch 4, Batch 714, Test Loss: 0.48581719398498535\n",
      "Epoch 4, Batch 715, Test Loss: 0.7912991642951965\n",
      "Epoch 4, Batch 716, Test Loss: 0.772068440914154\n",
      "Epoch 4, Batch 717, Test Loss: 0.6552433967590332\n",
      "Epoch 4, Batch 718, Test Loss: 0.4801449477672577\n",
      "Epoch 4, Batch 719, Test Loss: 0.687440037727356\n",
      "Epoch 4, Batch 720, Test Loss: 0.494789183139801\n",
      "Epoch 4, Batch 721, Test Loss: 0.653766393661499\n",
      "Epoch 4, Batch 722, Test Loss: 0.43605414032936096\n",
      "Epoch 4, Batch 723, Test Loss: 0.6144893765449524\n",
      "Epoch 4, Batch 724, Test Loss: 0.9032329320907593\n",
      "Epoch 4, Batch 725, Test Loss: 0.47014573216438293\n",
      "Epoch 4, Batch 726, Test Loss: 0.7280794978141785\n",
      "Epoch 4, Batch 727, Test Loss: 0.5919169187545776\n",
      "Epoch 4, Batch 728, Test Loss: 0.7613804936408997\n",
      "Epoch 4, Batch 729, Test Loss: 0.6744161248207092\n",
      "Epoch 4, Batch 730, Test Loss: 0.7166491746902466\n",
      "Epoch 4, Batch 731, Test Loss: 0.7373172044754028\n",
      "Epoch 4, Batch 732, Test Loss: 0.6515280604362488\n",
      "Epoch 4, Batch 733, Test Loss: 0.7627931833267212\n",
      "Epoch 4, Batch 734, Test Loss: 0.7741063237190247\n",
      "Epoch 4, Batch 735, Test Loss: 0.8677462935447693\n",
      "Epoch 4, Batch 736, Test Loss: 0.7414019107818604\n",
      "Epoch 4, Batch 737, Test Loss: 0.7826398015022278\n",
      "Epoch 4, Batch 738, Test Loss: 0.687122642993927\n",
      "Epoch 4, Batch 739, Test Loss: 0.5432381629943848\n",
      "Epoch 4, Batch 740, Test Loss: 0.8527930378913879\n",
      "Epoch 4, Batch 741, Test Loss: 0.815933346748352\n",
      "Epoch 4, Batch 742, Test Loss: 0.7655162215232849\n",
      "Epoch 4, Batch 743, Test Loss: 0.8120227456092834\n",
      "Epoch 4, Batch 744, Test Loss: 0.5798547863960266\n",
      "Epoch 4, Batch 745, Test Loss: 0.6637910604476929\n",
      "Epoch 4, Batch 746, Test Loss: 0.8698455691337585\n",
      "Epoch 4, Batch 747, Test Loss: 0.8196862936019897\n",
      "Epoch 4, Batch 748, Test Loss: 0.7923595905303955\n",
      "Epoch 4, Batch 749, Test Loss: 0.6722294092178345\n",
      "Epoch 4, Batch 750, Test Loss: 0.8891094326972961\n",
      "Epoch 4, Batch 751, Test Loss: 0.7262791991233826\n",
      "Epoch 4, Batch 752, Test Loss: 1.0283703804016113\n",
      "Epoch 4, Batch 753, Test Loss: 0.793049156665802\n",
      "Epoch 4, Batch 754, Test Loss: 0.6739343404769897\n",
      "Epoch 4, Batch 755, Test Loss: 0.8763242959976196\n",
      "Epoch 4, Batch 756, Test Loss: 0.5493090748786926\n",
      "Epoch 4, Batch 757, Test Loss: 0.7944067120552063\n",
      "Epoch 4, Batch 758, Test Loss: 0.6987578868865967\n",
      "Epoch 4, Batch 759, Test Loss: 0.5997603535652161\n",
      "Epoch 4, Batch 760, Test Loss: 0.7603133916854858\n",
      "Epoch 4, Batch 761, Test Loss: 0.6496710777282715\n",
      "Epoch 4, Batch 762, Test Loss: 0.735343337059021\n",
      "Epoch 4, Batch 763, Test Loss: 0.6943562626838684\n",
      "Epoch 4, Batch 764, Test Loss: 1.085222601890564\n",
      "Epoch 4, Batch 765, Test Loss: 0.7536308765411377\n",
      "Epoch 4, Batch 766, Test Loss: 0.5758581161499023\n",
      "Epoch 4, Batch 767, Test Loss: 0.7848390340805054\n",
      "Epoch 4, Batch 768, Test Loss: 0.6298046708106995\n",
      "Epoch 4, Batch 769, Test Loss: 0.48361337184906006\n",
      "Epoch 4, Batch 770, Test Loss: 0.8391889333724976\n",
      "Epoch 4, Batch 771, Test Loss: 0.6317814588546753\n",
      "Epoch 4, Batch 772, Test Loss: 0.6073945760726929\n",
      "Epoch 4, Batch 773, Test Loss: 0.628054678440094\n",
      "Epoch 4, Batch 774, Test Loss: 0.8114293813705444\n",
      "Epoch 4, Batch 775, Test Loss: 0.6104535460472107\n",
      "Epoch 4, Batch 776, Test Loss: 0.6042644381523132\n",
      "Epoch 4, Batch 777, Test Loss: 0.9525214433670044\n",
      "Epoch 4, Batch 778, Test Loss: 0.7059809565544128\n",
      "Epoch 4, Batch 779, Test Loss: 0.6539206504821777\n",
      "Epoch 4, Batch 780, Test Loss: 0.5622905492782593\n",
      "Epoch 4, Batch 781, Test Loss: 0.6955952644348145\n",
      "Epoch 4, Batch 782, Test Loss: 0.7927970886230469\n",
      "Epoch 4, Batch 783, Test Loss: 0.8154586553573608\n",
      "Epoch 4, Batch 784, Test Loss: 0.5271217226982117\n",
      "Epoch 4, Batch 785, Test Loss: 0.6452267169952393\n",
      "Epoch 4, Batch 786, Test Loss: 0.5635896921157837\n",
      "Epoch 4, Batch 787, Test Loss: 0.7018852829933167\n",
      "Epoch 4, Batch 788, Test Loss: 0.7715719938278198\n",
      "Epoch 4, Batch 789, Test Loss: 0.7244418263435364\n",
      "Epoch 4, Batch 790, Test Loss: 0.7769975662231445\n",
      "Epoch 4, Batch 791, Test Loss: 0.7649944424629211\n",
      "Epoch 4, Batch 792, Test Loss: 0.5168417692184448\n",
      "Epoch 4, Batch 793, Test Loss: 0.556210994720459\n",
      "Epoch 4, Batch 794, Test Loss: 0.6888166666030884\n",
      "Epoch 4, Batch 795, Test Loss: 0.6430001258850098\n",
      "Epoch 4, Batch 796, Test Loss: 0.6753515601158142\n",
      "Epoch 4, Batch 797, Test Loss: 0.8595359325408936\n",
      "Epoch 4, Batch 798, Test Loss: 0.7691981792449951\n",
      "Epoch 4, Batch 799, Test Loss: 0.7002520561218262\n",
      "Epoch 4, Batch 800, Test Loss: 0.6148418188095093\n",
      "Epoch 4, Batch 801, Test Loss: 0.7426577806472778\n",
      "Epoch 4, Batch 802, Test Loss: 0.9466089010238647\n",
      "Epoch 4, Batch 803, Test Loss: 0.6924513578414917\n",
      "Epoch 4, Batch 804, Test Loss: 0.8384742736816406\n",
      "Epoch 4, Batch 805, Test Loss: 0.7032777070999146\n",
      "Epoch 4, Batch 806, Test Loss: 0.6545224189758301\n",
      "Epoch 4, Batch 807, Test Loss: 0.8882062435150146\n",
      "Epoch 4, Batch 808, Test Loss: 0.7130028009414673\n",
      "Epoch 4, Batch 809, Test Loss: 0.7595936059951782\n",
      "Epoch 4, Batch 810, Test Loss: 0.888545036315918\n",
      "Epoch 4, Batch 811, Test Loss: 0.4333314597606659\n",
      "Epoch 4, Batch 812, Test Loss: 0.8564612865447998\n",
      "Epoch 4, Batch 813, Test Loss: 0.6290342807769775\n",
      "Epoch 4, Batch 814, Test Loss: 0.7030535340309143\n",
      "Epoch 4, Batch 815, Test Loss: 0.9476643204689026\n",
      "Epoch 4, Batch 816, Test Loss: 0.5384635925292969\n",
      "Epoch 4, Batch 817, Test Loss: 0.7855696082115173\n",
      "Epoch 4, Batch 818, Test Loss: 0.533518373966217\n",
      "Epoch 4, Batch 819, Test Loss: 0.5603649616241455\n",
      "Epoch 4, Batch 820, Test Loss: 0.7744023203849792\n",
      "Epoch 4, Batch 821, Test Loss: 0.6453728079795837\n",
      "Epoch 4, Batch 822, Test Loss: 0.8253412842750549\n",
      "Epoch 4, Batch 823, Test Loss: 0.5756428241729736\n",
      "Epoch 4, Batch 824, Test Loss: 0.7774502635002136\n",
      "Epoch 4, Batch 825, Test Loss: 0.7281255125999451\n",
      "Epoch 4, Batch 826, Test Loss: 0.5754650235176086\n",
      "Epoch 4, Batch 827, Test Loss: 0.6726716160774231\n",
      "Epoch 4, Batch 828, Test Loss: 0.7537201642990112\n",
      "Epoch 4, Batch 829, Test Loss: 0.659191906452179\n",
      "Epoch 4, Batch 830, Test Loss: 0.6530359983444214\n",
      "Epoch 4, Batch 831, Test Loss: 0.8296525478363037\n",
      "Epoch 4, Batch 832, Test Loss: 0.9445448517799377\n",
      "Epoch 4, Batch 833, Test Loss: 0.7420365214347839\n",
      "Epoch 4, Batch 834, Test Loss: 0.7158851623535156\n",
      "Epoch 4, Batch 835, Test Loss: 0.61403489112854\n",
      "Epoch 4, Batch 836, Test Loss: 0.6335480809211731\n",
      "Epoch 4, Batch 837, Test Loss: 0.9010388851165771\n",
      "Epoch 4, Batch 838, Test Loss: 0.664003312587738\n",
      "Epoch 4, Batch 839, Test Loss: 0.5601715445518494\n",
      "Epoch 4, Batch 840, Test Loss: 0.5472380518913269\n",
      "Epoch 4, Batch 841, Test Loss: 0.7711590528488159\n",
      "Epoch 4, Batch 842, Test Loss: 0.5900547504425049\n",
      "Epoch 4, Batch 843, Test Loss: 0.734947681427002\n",
      "Epoch 4, Batch 844, Test Loss: 0.6726335287094116\n",
      "Epoch 4, Batch 845, Test Loss: 0.7121136784553528\n",
      "Epoch 4, Batch 846, Test Loss: 0.6058042049407959\n",
      "Epoch 4, Batch 847, Test Loss: 0.6834614276885986\n",
      "Epoch 4, Batch 848, Test Loss: 0.9126289486885071\n",
      "Epoch 4, Batch 849, Test Loss: 0.6996097564697266\n",
      "Epoch 4, Batch 850, Test Loss: 0.832965612411499\n",
      "Epoch 4, Batch 851, Test Loss: 0.7633398771286011\n",
      "Epoch 4, Batch 852, Test Loss: 0.7195034027099609\n",
      "Epoch 4, Batch 853, Test Loss: 0.4399604797363281\n",
      "Epoch 4, Batch 854, Test Loss: 0.7334895133972168\n",
      "Epoch 4, Batch 855, Test Loss: 0.8330634832382202\n",
      "Epoch 4, Batch 856, Test Loss: 0.759901762008667\n",
      "Epoch 4, Batch 857, Test Loss: 0.49720343947410583\n",
      "Epoch 4, Batch 858, Test Loss: 0.6310155987739563\n",
      "Epoch 4, Batch 859, Test Loss: 0.6107673048973083\n",
      "Epoch 4, Batch 860, Test Loss: 0.6734869480133057\n",
      "Epoch 4, Batch 861, Test Loss: 0.6574043035507202\n",
      "Epoch 4, Batch 862, Test Loss: 0.702113151550293\n",
      "Epoch 4, Batch 863, Test Loss: 0.6906324028968811\n",
      "Epoch 4, Batch 864, Test Loss: 0.8294031620025635\n",
      "Epoch 4, Batch 865, Test Loss: 0.978131890296936\n",
      "Epoch 4, Batch 866, Test Loss: 0.5836942791938782\n",
      "Epoch 4, Batch 867, Test Loss: 0.8454660773277283\n",
      "Epoch 4, Batch 868, Test Loss: 0.6114429831504822\n",
      "Epoch 4, Batch 869, Test Loss: 0.807672381401062\n",
      "Epoch 4, Batch 870, Test Loss: 0.6505023837089539\n",
      "Epoch 4, Batch 871, Test Loss: 0.714565098285675\n",
      "Epoch 4, Batch 872, Test Loss: 0.6621853709220886\n",
      "Epoch 4, Batch 873, Test Loss: 0.6048094630241394\n",
      "Epoch 4, Batch 874, Test Loss: 0.6772294640541077\n",
      "Epoch 4, Batch 875, Test Loss: 0.549568772315979\n",
      "Epoch 4, Batch 876, Test Loss: 0.6041358113288879\n",
      "Epoch 4, Batch 877, Test Loss: 0.8894782066345215\n",
      "Epoch 4, Batch 878, Test Loss: 0.8098092079162598\n",
      "Epoch 4, Batch 879, Test Loss: 0.522830069065094\n",
      "Epoch 4, Batch 880, Test Loss: 0.6925538182258606\n",
      "Epoch 4, Batch 881, Test Loss: 0.8454558849334717\n",
      "Epoch 4, Batch 882, Test Loss: 0.7957779169082642\n",
      "Epoch 4, Batch 883, Test Loss: 0.537882387638092\n",
      "Epoch 4, Batch 884, Test Loss: 0.5165024399757385\n",
      "Epoch 4, Batch 885, Test Loss: 0.512885332107544\n",
      "Epoch 4, Batch 886, Test Loss: 0.49609488248825073\n",
      "Epoch 4, Batch 887, Test Loss: 0.6987587213516235\n",
      "Epoch 4, Batch 888, Test Loss: 0.5356475114822388\n",
      "Epoch 4, Batch 889, Test Loss: 0.6274356245994568\n",
      "Epoch 4, Batch 890, Test Loss: 0.8295137882232666\n",
      "Epoch 4, Batch 891, Test Loss: 0.7153680324554443\n",
      "Epoch 4, Batch 892, Test Loss: 0.6251605749130249\n",
      "Epoch 4, Batch 893, Test Loss: 0.5794024467468262\n",
      "Epoch 4, Batch 894, Test Loss: 0.7807794809341431\n",
      "Epoch 4, Batch 895, Test Loss: 0.7605185508728027\n",
      "Epoch 4, Batch 896, Test Loss: 0.5923243761062622\n",
      "Epoch 4, Batch 897, Test Loss: 0.6102463603019714\n",
      "Epoch 4, Batch 898, Test Loss: 0.8157312870025635\n",
      "Epoch 4, Batch 899, Test Loss: 0.9218307733535767\n",
      "Epoch 4, Batch 900, Test Loss: 0.5754125714302063\n",
      "Epoch 4, Batch 901, Test Loss: 0.5821923017501831\n",
      "Epoch 4, Batch 902, Test Loss: 0.6634091138839722\n",
      "Epoch 4, Batch 903, Test Loss: 0.7210975885391235\n",
      "Epoch 4, Batch 904, Test Loss: 0.5831378102302551\n",
      "Epoch 4, Batch 905, Test Loss: 0.8431192636489868\n",
      "Epoch 4, Batch 906, Test Loss: 0.8085939884185791\n",
      "Epoch 4, Batch 907, Test Loss: 0.6806942820549011\n",
      "Epoch 4, Batch 908, Test Loss: 0.7795863151550293\n",
      "Epoch 4, Batch 909, Test Loss: 0.5423938632011414\n",
      "Epoch 4, Batch 910, Test Loss: 0.7600113153457642\n",
      "Epoch 4, Batch 911, Test Loss: 0.6949469447135925\n",
      "Epoch 4, Batch 912, Test Loss: 0.740702211856842\n",
      "Epoch 4, Batch 913, Test Loss: 0.6215125322341919\n",
      "Epoch 4, Batch 914, Test Loss: 0.8914881348609924\n",
      "Epoch 4, Batch 915, Test Loss: 0.8154498338699341\n",
      "Epoch 4, Batch 916, Test Loss: 0.9658124446868896\n",
      "Epoch 4, Batch 917, Test Loss: 0.6961526870727539\n",
      "Epoch 4, Batch 918, Test Loss: 0.6776612997055054\n",
      "Epoch 4, Batch 919, Test Loss: 0.7936092615127563\n",
      "Epoch 4, Batch 920, Test Loss: 0.9821622371673584\n",
      "Epoch 4, Batch 921, Test Loss: 0.8276380300521851\n",
      "Epoch 4, Batch 922, Test Loss: 0.43482527136802673\n",
      "Epoch 4, Batch 923, Test Loss: 0.458313912153244\n",
      "Epoch 4, Batch 924, Test Loss: 0.5953484177589417\n",
      "Epoch 4, Batch 925, Test Loss: 0.5643491148948669\n",
      "Epoch 4, Batch 926, Test Loss: 0.8556250333786011\n",
      "Epoch 4, Batch 927, Test Loss: 0.9254546165466309\n",
      "Epoch 4, Batch 928, Test Loss: 0.6184825301170349\n",
      "Epoch 4, Batch 929, Test Loss: 0.83388352394104\n",
      "Epoch 4, Batch 930, Test Loss: 0.5213832259178162\n",
      "Epoch 4, Batch 931, Test Loss: 0.855266273021698\n",
      "Epoch 4, Batch 932, Test Loss: 0.6726111173629761\n",
      "Epoch 4, Batch 933, Test Loss: 0.49176084995269775\n",
      "Epoch 4, Batch 934, Test Loss: 0.7539070844650269\n",
      "Epoch 4, Batch 935, Test Loss: 0.7026397585868835\n",
      "Epoch 4, Batch 936, Test Loss: 0.8940638303756714\n",
      "Epoch 4, Batch 937, Test Loss: 0.5639630556106567\n",
      "Epoch 4, Batch 938, Test Loss: 0.7464012503623962\n",
      "Accuracy of Test set: 0.7326333333333334\n",
      "Epoch 5, Batch 1, Loss: 0.8201385736465454\n",
      "Epoch 5, Batch 2, Loss: 0.5345163941383362\n",
      "Epoch 5, Batch 3, Loss: 0.8015046119689941\n",
      "Epoch 5, Batch 4, Loss: 0.6641115546226501\n",
      "Epoch 5, Batch 5, Loss: 0.8047163486480713\n",
      "Epoch 5, Batch 6, Loss: 0.6086404323577881\n",
      "Epoch 5, Batch 7, Loss: 0.6575934886932373\n",
      "Epoch 5, Batch 8, Loss: 0.6794254183769226\n",
      "Epoch 5, Batch 9, Loss: 0.7211270332336426\n",
      "Epoch 5, Batch 10, Loss: 0.6200554966926575\n",
      "Epoch 5, Batch 11, Loss: 0.6471313238143921\n",
      "Epoch 5, Batch 12, Loss: 0.5012081265449524\n",
      "Epoch 5, Batch 13, Loss: 0.383813738822937\n",
      "Epoch 5, Batch 14, Loss: 0.8393586874008179\n",
      "Epoch 5, Batch 15, Loss: 0.7222623825073242\n",
      "Epoch 5, Batch 16, Loss: 0.8279213905334473\n",
      "Epoch 5, Batch 17, Loss: 0.6799601316452026\n",
      "Epoch 5, Batch 18, Loss: 0.49827706813812256\n",
      "Epoch 5, Batch 19, Loss: 0.5179295539855957\n",
      "Epoch 5, Batch 20, Loss: 0.514347493648529\n",
      "Epoch 5, Batch 21, Loss: 0.583215057849884\n",
      "Epoch 5, Batch 22, Loss: 0.6374326348304749\n",
      "Epoch 5, Batch 23, Loss: 0.6663378477096558\n",
      "Epoch 5, Batch 24, Loss: 0.8065676689147949\n",
      "Epoch 5, Batch 25, Loss: 0.7659948468208313\n",
      "Epoch 5, Batch 26, Loss: 0.5084534287452698\n",
      "Epoch 5, Batch 27, Loss: 0.615490734577179\n",
      "Epoch 5, Batch 28, Loss: 0.5658798217773438\n",
      "Epoch 5, Batch 29, Loss: 0.5121541619300842\n",
      "Epoch 5, Batch 30, Loss: 0.7142695188522339\n",
      "Epoch 5, Batch 31, Loss: 0.6909086108207703\n",
      "Epoch 5, Batch 32, Loss: 0.5386577844619751\n",
      "Epoch 5, Batch 33, Loss: 0.5536718964576721\n",
      "Epoch 5, Batch 34, Loss: 0.871890127658844\n",
      "Epoch 5, Batch 35, Loss: 0.6458218097686768\n",
      "Epoch 5, Batch 36, Loss: 0.9652543067932129\n",
      "Epoch 5, Batch 37, Loss: 0.726790189743042\n",
      "Epoch 5, Batch 38, Loss: 0.563681423664093\n",
      "Epoch 5, Batch 39, Loss: 0.5088562369346619\n",
      "Epoch 5, Batch 40, Loss: 0.6378307938575745\n",
      "Epoch 5, Batch 41, Loss: 0.6353720426559448\n",
      "Epoch 5, Batch 42, Loss: 0.5722392797470093\n",
      "Epoch 5, Batch 43, Loss: 0.6225057244300842\n",
      "Epoch 5, Batch 44, Loss: 0.5027573108673096\n",
      "Epoch 5, Batch 45, Loss: 0.34583964943885803\n",
      "Epoch 5, Batch 46, Loss: 0.6179378628730774\n",
      "Epoch 5, Batch 47, Loss: 0.7214855551719666\n",
      "Epoch 5, Batch 48, Loss: 0.8070253133773804\n",
      "Epoch 5, Batch 49, Loss: 0.6933516263961792\n",
      "Epoch 5, Batch 50, Loss: 0.4651503264904022\n",
      "Epoch 5, Batch 51, Loss: 0.5795101523399353\n",
      "Epoch 5, Batch 52, Loss: 0.7289496660232544\n",
      "Epoch 5, Batch 53, Loss: 0.5030707716941833\n",
      "Epoch 5, Batch 54, Loss: 0.5765559673309326\n",
      "Epoch 5, Batch 55, Loss: 0.6952232122421265\n",
      "Epoch 5, Batch 56, Loss: 0.6955087184906006\n",
      "Epoch 5, Batch 57, Loss: 0.6562203168869019\n",
      "Epoch 5, Batch 58, Loss: 0.6791907548904419\n",
      "Epoch 5, Batch 59, Loss: 0.5946763157844543\n",
      "Epoch 5, Batch 60, Loss: 0.705192506313324\n",
      "Epoch 5, Batch 61, Loss: 0.50493323802948\n",
      "Epoch 5, Batch 62, Loss: 0.6015220880508423\n",
      "Epoch 5, Batch 63, Loss: 0.7188358306884766\n",
      "Epoch 5, Batch 64, Loss: 0.6537075638771057\n",
      "Epoch 5, Batch 65, Loss: 0.7847258448600769\n",
      "Epoch 5, Batch 66, Loss: 0.6453083753585815\n",
      "Epoch 5, Batch 67, Loss: 0.6556515693664551\n",
      "Epoch 5, Batch 68, Loss: 0.7069661021232605\n",
      "Epoch 5, Batch 69, Loss: 0.5407784581184387\n",
      "Epoch 5, Batch 70, Loss: 0.7410815954208374\n",
      "Epoch 5, Batch 71, Loss: 0.5821080207824707\n",
      "Epoch 5, Batch 72, Loss: 0.49817174673080444\n",
      "Epoch 5, Batch 73, Loss: 0.6176719665527344\n",
      "Epoch 5, Batch 74, Loss: 0.7497521638870239\n",
      "Epoch 5, Batch 75, Loss: 0.7340410947799683\n",
      "Epoch 5, Batch 76, Loss: 0.4839058518409729\n",
      "Epoch 5, Batch 77, Loss: 0.6945382952690125\n",
      "Epoch 5, Batch 78, Loss: 0.7580980062484741\n",
      "Epoch 5, Batch 79, Loss: 0.6885581016540527\n",
      "Epoch 5, Batch 80, Loss: 0.5749740600585938\n",
      "Epoch 5, Batch 81, Loss: 0.6247912645339966\n",
      "Epoch 5, Batch 82, Loss: 0.6526674032211304\n",
      "Epoch 5, Batch 83, Loss: 0.7126317024230957\n",
      "Epoch 5, Batch 84, Loss: 0.5917633175849915\n",
      "Epoch 5, Batch 85, Loss: 0.8946539163589478\n",
      "Epoch 5, Batch 86, Loss: 0.5370728373527527\n",
      "Epoch 5, Batch 87, Loss: 0.6610567569732666\n",
      "Epoch 5, Batch 88, Loss: 0.6738236546516418\n",
      "Epoch 5, Batch 89, Loss: 0.9414817094802856\n",
      "Epoch 5, Batch 90, Loss: 0.623881995677948\n",
      "Epoch 5, Batch 91, Loss: 0.4715579152107239\n",
      "Epoch 5, Batch 92, Loss: 0.6537380218505859\n",
      "Epoch 5, Batch 93, Loss: 0.7350945472717285\n",
      "Epoch 5, Batch 94, Loss: 0.4427555501461029\n",
      "Epoch 5, Batch 95, Loss: 0.5277670621871948\n",
      "Epoch 5, Batch 96, Loss: 1.1305427551269531\n",
      "Epoch 5, Batch 97, Loss: 0.7385238409042358\n",
      "Epoch 5, Batch 98, Loss: 0.5607037544250488\n",
      "Epoch 5, Batch 99, Loss: 0.5919331908226013\n",
      "Epoch 5, Batch 100, Loss: 0.5946402549743652\n",
      "Epoch 5, Batch 101, Loss: 0.7119964361190796\n",
      "Epoch 5, Batch 102, Loss: 0.6508610844612122\n",
      "Epoch 5, Batch 103, Loss: 0.4827415347099304\n",
      "Epoch 5, Batch 104, Loss: 0.6407526135444641\n",
      "Epoch 5, Batch 105, Loss: 0.5979188680648804\n",
      "Epoch 5, Batch 106, Loss: 0.6425179243087769\n",
      "Epoch 5, Batch 107, Loss: 0.6364138126373291\n",
      "Epoch 5, Batch 108, Loss: 0.6503354907035828\n",
      "Epoch 5, Batch 109, Loss: 0.8083329200744629\n",
      "Epoch 5, Batch 110, Loss: 0.4465353786945343\n",
      "Epoch 5, Batch 111, Loss: 0.5891849994659424\n",
      "Epoch 5, Batch 112, Loss: 0.52901291847229\n",
      "Epoch 5, Batch 113, Loss: 0.498247891664505\n",
      "Epoch 5, Batch 114, Loss: 0.6715983748435974\n",
      "Epoch 5, Batch 115, Loss: 0.7783833742141724\n",
      "Epoch 5, Batch 116, Loss: 0.6038352251052856\n",
      "Epoch 5, Batch 117, Loss: 0.6833301782608032\n",
      "Epoch 5, Batch 118, Loss: 0.5047091245651245\n",
      "Epoch 5, Batch 119, Loss: 0.690792441368103\n",
      "Epoch 5, Batch 120, Loss: 0.5703573226928711\n",
      "Epoch 5, Batch 121, Loss: 0.5981006026268005\n",
      "Epoch 5, Batch 122, Loss: 0.6878373026847839\n",
      "Epoch 5, Batch 123, Loss: 0.584263026714325\n",
      "Epoch 5, Batch 124, Loss: 0.838826060295105\n",
      "Epoch 5, Batch 125, Loss: 0.7469842433929443\n",
      "Epoch 5, Batch 126, Loss: 0.5263975858688354\n",
      "Epoch 5, Batch 127, Loss: 0.5449199080467224\n",
      "Epoch 5, Batch 128, Loss: 0.7403321862220764\n",
      "Epoch 5, Batch 129, Loss: 0.6678000688552856\n",
      "Epoch 5, Batch 130, Loss: 0.5423751473426819\n",
      "Epoch 5, Batch 131, Loss: 0.7950232625007629\n",
      "Epoch 5, Batch 132, Loss: 0.6340082287788391\n",
      "Epoch 5, Batch 133, Loss: 0.7749781608581543\n",
      "Epoch 5, Batch 134, Loss: 0.735281765460968\n",
      "Epoch 5, Batch 135, Loss: 0.8844384551048279\n",
      "Epoch 5, Batch 136, Loss: 0.7794378399848938\n",
      "Epoch 5, Batch 137, Loss: 0.6241601705551147\n",
      "Epoch 5, Batch 138, Loss: 0.5361020565032959\n",
      "Epoch 5, Batch 139, Loss: 0.5407543778419495\n",
      "Epoch 5, Batch 140, Loss: 0.6057632565498352\n",
      "Epoch 5, Batch 141, Loss: 0.655802309513092\n",
      "Epoch 5, Batch 142, Loss: 0.752872884273529\n",
      "Epoch 5, Batch 143, Loss: 0.5884795188903809\n",
      "Epoch 5, Batch 144, Loss: 0.6879116296768188\n",
      "Epoch 5, Batch 145, Loss: 0.6181933879852295\n",
      "Epoch 5, Batch 146, Loss: 0.8416188955307007\n",
      "Epoch 5, Batch 147, Loss: 0.6781643629074097\n",
      "Epoch 5, Batch 148, Loss: 0.6838788986206055\n",
      "Epoch 5, Batch 149, Loss: 0.4755188226699829\n",
      "Epoch 5, Batch 150, Loss: 0.44166451692581177\n",
      "Epoch 5, Batch 151, Loss: 0.6194686889648438\n",
      "Epoch 5, Batch 152, Loss: 0.5810689926147461\n",
      "Epoch 5, Batch 153, Loss: 0.6786503791809082\n",
      "Epoch 5, Batch 154, Loss: 1.00080406665802\n",
      "Epoch 5, Batch 155, Loss: 0.741349458694458\n",
      "Epoch 5, Batch 156, Loss: 0.6046710014343262\n",
      "Epoch 5, Batch 157, Loss: 0.559779942035675\n",
      "Epoch 5, Batch 158, Loss: 0.7659401297569275\n",
      "Epoch 5, Batch 159, Loss: 0.7685853838920593\n",
      "Epoch 5, Batch 160, Loss: 0.6501438617706299\n",
      "Epoch 5, Batch 161, Loss: 0.6291080117225647\n",
      "Epoch 5, Batch 162, Loss: 0.6505734324455261\n",
      "Epoch 5, Batch 163, Loss: 0.44630318880081177\n",
      "Epoch 5, Batch 164, Loss: 0.5520966649055481\n",
      "Epoch 5, Batch 165, Loss: 0.5743570923805237\n",
      "Epoch 5, Batch 166, Loss: 0.8857443332672119\n",
      "Epoch 5, Batch 167, Loss: 0.7614662647247314\n",
      "Epoch 5, Batch 168, Loss: 0.7182903289794922\n",
      "Epoch 5, Batch 169, Loss: 0.5156131386756897\n",
      "Epoch 5, Batch 170, Loss: 0.6553722023963928\n",
      "Epoch 5, Batch 171, Loss: 0.7395922541618347\n",
      "Epoch 5, Batch 172, Loss: 0.6417636871337891\n",
      "Epoch 5, Batch 173, Loss: 0.688502311706543\n",
      "Epoch 5, Batch 174, Loss: 0.5368006825447083\n",
      "Epoch 5, Batch 175, Loss: 0.5831234455108643\n",
      "Epoch 5, Batch 176, Loss: 0.4611648917198181\n",
      "Epoch 5, Batch 177, Loss: 0.5751025676727295\n",
      "Epoch 5, Batch 178, Loss: 0.7016042470932007\n",
      "Epoch 5, Batch 179, Loss: 0.6333122849464417\n",
      "Epoch 5, Batch 180, Loss: 0.7224733233451843\n",
      "Epoch 5, Batch 181, Loss: 0.7614684104919434\n",
      "Epoch 5, Batch 182, Loss: 0.6601837277412415\n",
      "Epoch 5, Batch 183, Loss: 0.5862377285957336\n",
      "Epoch 5, Batch 184, Loss: 0.7037725448608398\n",
      "Epoch 5, Batch 185, Loss: 0.539527952671051\n",
      "Epoch 5, Batch 186, Loss: 0.6475974321365356\n",
      "Epoch 5, Batch 187, Loss: 0.6201357841491699\n",
      "Epoch 5, Batch 188, Loss: 0.7211437821388245\n",
      "Epoch 5, Batch 189, Loss: 0.7385756373405457\n",
      "Epoch 5, Batch 190, Loss: 0.5455787777900696\n",
      "Epoch 5, Batch 191, Loss: 0.6869364976882935\n",
      "Epoch 5, Batch 192, Loss: 0.5691513419151306\n",
      "Epoch 5, Batch 193, Loss: 0.677229106426239\n",
      "Epoch 5, Batch 194, Loss: 0.5412771701812744\n",
      "Epoch 5, Batch 195, Loss: 0.6963095664978027\n",
      "Epoch 5, Batch 196, Loss: 0.765178918838501\n",
      "Epoch 5, Batch 197, Loss: 0.6537956595420837\n",
      "Epoch 5, Batch 198, Loss: 0.5293357372283936\n",
      "Epoch 5, Batch 199, Loss: 0.6339881420135498\n",
      "Epoch 5, Batch 200, Loss: 0.5928986668586731\n",
      "Epoch 5, Batch 201, Loss: 0.7894150018692017\n",
      "Epoch 5, Batch 202, Loss: 0.6213598847389221\n",
      "Epoch 5, Batch 203, Loss: 0.5021998286247253\n",
      "Epoch 5, Batch 204, Loss: 0.9261255264282227\n",
      "Epoch 5, Batch 205, Loss: 0.5416999459266663\n",
      "Epoch 5, Batch 206, Loss: 0.705988347530365\n",
      "Epoch 5, Batch 207, Loss: 0.5338633060455322\n",
      "Epoch 5, Batch 208, Loss: 0.621816873550415\n",
      "Epoch 5, Batch 209, Loss: 0.8363624811172485\n",
      "Epoch 5, Batch 210, Loss: 0.5680526494979858\n",
      "Epoch 5, Batch 211, Loss: 0.6099597811698914\n",
      "Epoch 5, Batch 212, Loss: 0.5960959196090698\n",
      "Epoch 5, Batch 213, Loss: 0.5781279802322388\n",
      "Epoch 5, Batch 214, Loss: 0.7366924285888672\n",
      "Epoch 5, Batch 215, Loss: 0.5898432731628418\n",
      "Epoch 5, Batch 216, Loss: 0.6003333330154419\n",
      "Epoch 5, Batch 217, Loss: 0.6839015483856201\n",
      "Epoch 5, Batch 218, Loss: 0.6774461269378662\n",
      "Epoch 5, Batch 219, Loss: 0.7648459672927856\n",
      "Epoch 5, Batch 220, Loss: 0.6541545391082764\n",
      "Epoch 5, Batch 221, Loss: 0.7611196041107178\n",
      "Epoch 5, Batch 222, Loss: 0.8312190771102905\n",
      "Epoch 5, Batch 223, Loss: 0.6252376437187195\n",
      "Epoch 5, Batch 224, Loss: 0.7461218237876892\n",
      "Epoch 5, Batch 225, Loss: 0.8030063509941101\n",
      "Epoch 5, Batch 226, Loss: 0.6227026581764221\n",
      "Epoch 5, Batch 227, Loss: 0.6428877115249634\n",
      "Epoch 5, Batch 228, Loss: 0.5701931715011597\n",
      "Epoch 5, Batch 229, Loss: 0.6396211385726929\n",
      "Epoch 5, Batch 230, Loss: 0.5852347612380981\n",
      "Epoch 5, Batch 231, Loss: 0.9173963069915771\n",
      "Epoch 5, Batch 232, Loss: 0.6996992826461792\n",
      "Epoch 5, Batch 233, Loss: 0.7242980003356934\n",
      "Epoch 5, Batch 234, Loss: 0.6166674494743347\n",
      "Epoch 5, Batch 235, Loss: 0.8642720580101013\n",
      "Epoch 5, Batch 236, Loss: 0.67522794008255\n",
      "Epoch 5, Batch 237, Loss: 0.6106799244880676\n",
      "Epoch 5, Batch 238, Loss: 0.6246328949928284\n",
      "Epoch 5, Batch 239, Loss: 0.6715341806411743\n",
      "Epoch 5, Batch 240, Loss: 0.607975959777832\n",
      "Epoch 5, Batch 241, Loss: 0.5137144327163696\n",
      "Epoch 5, Batch 242, Loss: 0.795896053314209\n",
      "Epoch 5, Batch 243, Loss: 0.5064125061035156\n",
      "Epoch 5, Batch 244, Loss: 0.8986434936523438\n",
      "Epoch 5, Batch 245, Loss: 0.6488263607025146\n",
      "Epoch 5, Batch 246, Loss: 0.6610792875289917\n",
      "Epoch 5, Batch 247, Loss: 0.6015833020210266\n",
      "Epoch 5, Batch 248, Loss: 0.5522816181182861\n",
      "Epoch 5, Batch 249, Loss: 0.512383759021759\n",
      "Epoch 5, Batch 250, Loss: 0.7090640664100647\n",
      "Epoch 5, Batch 251, Loss: 0.6893545985221863\n",
      "Epoch 5, Batch 252, Loss: 0.527205765247345\n",
      "Epoch 5, Batch 253, Loss: 0.543418824672699\n",
      "Epoch 5, Batch 254, Loss: 0.5616076588630676\n",
      "Epoch 5, Batch 255, Loss: 0.6429144740104675\n",
      "Epoch 5, Batch 256, Loss: 0.6676238775253296\n",
      "Epoch 5, Batch 257, Loss: 0.6155291199684143\n",
      "Epoch 5, Batch 258, Loss: 0.5190606713294983\n",
      "Epoch 5, Batch 259, Loss: 0.6588186025619507\n",
      "Epoch 5, Batch 260, Loss: 0.609923243522644\n",
      "Epoch 5, Batch 261, Loss: 0.5549272298812866\n",
      "Epoch 5, Batch 262, Loss: 0.5581780076026917\n",
      "Epoch 5, Batch 263, Loss: 0.667523980140686\n",
      "Epoch 5, Batch 264, Loss: 0.5609402060508728\n",
      "Epoch 5, Batch 265, Loss: 0.7928615808486938\n",
      "Epoch 5, Batch 266, Loss: 0.6155756711959839\n",
      "Epoch 5, Batch 267, Loss: 0.7809025645256042\n",
      "Epoch 5, Batch 268, Loss: 0.790526270866394\n",
      "Epoch 5, Batch 269, Loss: 0.5820735692977905\n",
      "Epoch 5, Batch 270, Loss: 0.6438989639282227\n",
      "Epoch 5, Batch 271, Loss: 0.7054976224899292\n",
      "Epoch 5, Batch 272, Loss: 0.6498785018920898\n",
      "Epoch 5, Batch 273, Loss: 0.7009042501449585\n",
      "Epoch 5, Batch 274, Loss: 0.5974209904670715\n",
      "Epoch 5, Batch 275, Loss: 0.46015259623527527\n",
      "Epoch 5, Batch 276, Loss: 0.6227669715881348\n",
      "Epoch 5, Batch 277, Loss: 0.7754592299461365\n",
      "Epoch 5, Batch 278, Loss: 0.6629305481910706\n",
      "Epoch 5, Batch 279, Loss: 0.6919184327125549\n",
      "Epoch 5, Batch 280, Loss: 0.4350283145904541\n",
      "Epoch 5, Batch 281, Loss: 0.6691361665725708\n",
      "Epoch 5, Batch 282, Loss: 0.9542014002799988\n",
      "Epoch 5, Batch 283, Loss: 0.6057794094085693\n",
      "Epoch 5, Batch 284, Loss: 0.6674177646636963\n",
      "Epoch 5, Batch 285, Loss: 0.6231437921524048\n",
      "Epoch 5, Batch 286, Loss: 0.717243492603302\n",
      "Epoch 5, Batch 287, Loss: 0.49478980898857117\n",
      "Epoch 5, Batch 288, Loss: 0.6506352424621582\n",
      "Epoch 5, Batch 289, Loss: 0.3713523745536804\n",
      "Epoch 5, Batch 290, Loss: 0.6590758562088013\n",
      "Epoch 5, Batch 291, Loss: 0.6033774614334106\n",
      "Epoch 5, Batch 292, Loss: 0.5993819832801819\n",
      "Epoch 5, Batch 293, Loss: 0.6565241813659668\n",
      "Epoch 5, Batch 294, Loss: 0.7821323871612549\n",
      "Epoch 5, Batch 295, Loss: 0.5059289336204529\n",
      "Epoch 5, Batch 296, Loss: 0.8557416796684265\n",
      "Epoch 5, Batch 297, Loss: 0.6043879985809326\n",
      "Epoch 5, Batch 298, Loss: 0.658661425113678\n",
      "Epoch 5, Batch 299, Loss: 0.5575040578842163\n",
      "Epoch 5, Batch 300, Loss: 0.6655330061912537\n",
      "Epoch 5, Batch 301, Loss: 0.5373019576072693\n",
      "Epoch 5, Batch 302, Loss: 0.4789113402366638\n",
      "Epoch 5, Batch 303, Loss: 0.3962060809135437\n",
      "Epoch 5, Batch 304, Loss: 0.6229585409164429\n",
      "Epoch 5, Batch 305, Loss: 0.6320770382881165\n",
      "Epoch 5, Batch 306, Loss: 0.7028393745422363\n",
      "Epoch 5, Batch 307, Loss: 0.7194507718086243\n",
      "Epoch 5, Batch 308, Loss: 0.6071767210960388\n",
      "Epoch 5, Batch 309, Loss: 0.9677451848983765\n",
      "Epoch 5, Batch 310, Loss: 0.7055877447128296\n",
      "Epoch 5, Batch 311, Loss: 0.7171021103858948\n",
      "Epoch 5, Batch 312, Loss: 0.4819789230823517\n",
      "Epoch 5, Batch 313, Loss: 0.6596143841743469\n",
      "Epoch 5, Batch 314, Loss: 0.5417001247406006\n",
      "Epoch 5, Batch 315, Loss: 0.7651246786117554\n",
      "Epoch 5, Batch 316, Loss: 0.558089554309845\n",
      "Epoch 5, Batch 317, Loss: 0.4241938292980194\n",
      "Epoch 5, Batch 318, Loss: 0.6347509622573853\n",
      "Epoch 5, Batch 319, Loss: 0.39487800002098083\n",
      "Epoch 5, Batch 320, Loss: 0.6680029630661011\n",
      "Epoch 5, Batch 321, Loss: 0.7103930115699768\n",
      "Epoch 5, Batch 322, Loss: 0.5338333249092102\n",
      "Epoch 5, Batch 323, Loss: 0.5361595749855042\n",
      "Epoch 5, Batch 324, Loss: 0.5362453460693359\n",
      "Epoch 5, Batch 325, Loss: 0.5130763053894043\n",
      "Epoch 5, Batch 326, Loss: 0.7774448990821838\n",
      "Epoch 5, Batch 327, Loss: 0.5925872325897217\n",
      "Epoch 5, Batch 328, Loss: 0.5598018169403076\n",
      "Epoch 5, Batch 329, Loss: 0.7116323113441467\n",
      "Epoch 5, Batch 330, Loss: 0.7596808075904846\n",
      "Epoch 5, Batch 331, Loss: 0.6102499961853027\n",
      "Epoch 5, Batch 332, Loss: 0.48756295442581177\n",
      "Epoch 5, Batch 333, Loss: 0.8604053854942322\n",
      "Epoch 5, Batch 334, Loss: 0.6070061326026917\n",
      "Epoch 5, Batch 335, Loss: 0.48796600103378296\n",
      "Epoch 5, Batch 336, Loss: 0.653643012046814\n",
      "Epoch 5, Batch 337, Loss: 0.7944775819778442\n",
      "Epoch 5, Batch 338, Loss: 0.7559114694595337\n",
      "Epoch 5, Batch 339, Loss: 0.7156208753585815\n",
      "Epoch 5, Batch 340, Loss: 0.6651305556297302\n",
      "Epoch 5, Batch 341, Loss: 0.4531378448009491\n",
      "Epoch 5, Batch 342, Loss: 0.9099701046943665\n",
      "Epoch 5, Batch 343, Loss: 0.4138737618923187\n",
      "Epoch 5, Batch 344, Loss: 0.7354698181152344\n",
      "Epoch 5, Batch 345, Loss: 0.6928709149360657\n",
      "Epoch 5, Batch 346, Loss: 0.6747510433197021\n",
      "Epoch 5, Batch 347, Loss: 0.6861141920089722\n",
      "Epoch 5, Batch 348, Loss: 0.5822649598121643\n",
      "Epoch 5, Batch 349, Loss: 0.7122313976287842\n",
      "Epoch 5, Batch 350, Loss: 0.9348495006561279\n",
      "Epoch 5, Batch 351, Loss: 0.6079897284507751\n",
      "Epoch 5, Batch 352, Loss: 0.5797025561332703\n",
      "Epoch 5, Batch 353, Loss: 0.7804571986198425\n",
      "Epoch 5, Batch 354, Loss: 0.6365811228752136\n",
      "Epoch 5, Batch 355, Loss: 0.6762312650680542\n",
      "Epoch 5, Batch 356, Loss: 0.8281465768814087\n",
      "Epoch 5, Batch 357, Loss: 0.7150099873542786\n",
      "Epoch 5, Batch 358, Loss: 0.7219301462173462\n",
      "Epoch 5, Batch 359, Loss: 0.5038403272628784\n",
      "Epoch 5, Batch 360, Loss: 0.8091179132461548\n",
      "Epoch 5, Batch 361, Loss: 0.5958505272865295\n",
      "Epoch 5, Batch 362, Loss: 0.5756944417953491\n",
      "Epoch 5, Batch 363, Loss: 0.5772331953048706\n",
      "Epoch 5, Batch 364, Loss: 0.7434059381484985\n",
      "Epoch 5, Batch 365, Loss: 0.5853328108787537\n",
      "Epoch 5, Batch 366, Loss: 0.5765472054481506\n",
      "Epoch 5, Batch 367, Loss: 0.5573774576187134\n",
      "Epoch 5, Batch 368, Loss: 0.8204576373100281\n",
      "Epoch 5, Batch 369, Loss: 0.6260846257209778\n",
      "Epoch 5, Batch 370, Loss: 0.524212658405304\n",
      "Epoch 5, Batch 371, Loss: 0.7665439248085022\n",
      "Epoch 5, Batch 372, Loss: 0.5581153035163879\n",
      "Epoch 5, Batch 373, Loss: 0.5999060869216919\n",
      "Epoch 5, Batch 374, Loss: 0.858617901802063\n",
      "Epoch 5, Batch 375, Loss: 0.6744024157524109\n",
      "Epoch 5, Batch 376, Loss: 0.661419153213501\n",
      "Epoch 5, Batch 377, Loss: 0.4810282289981842\n",
      "Epoch 5, Batch 378, Loss: 0.7849432229995728\n",
      "Epoch 5, Batch 379, Loss: 0.549930989742279\n",
      "Epoch 5, Batch 380, Loss: 0.65281081199646\n",
      "Epoch 5, Batch 381, Loss: 0.663362443447113\n",
      "Epoch 5, Batch 382, Loss: 0.9105445146560669\n",
      "Epoch 5, Batch 383, Loss: 0.6602513790130615\n",
      "Epoch 5, Batch 384, Loss: 0.6567673683166504\n",
      "Epoch 5, Batch 385, Loss: 0.6452203392982483\n",
      "Epoch 5, Batch 386, Loss: 0.5577216744422913\n",
      "Epoch 5, Batch 387, Loss: 0.5354354381561279\n",
      "Epoch 5, Batch 388, Loss: 0.7022602558135986\n",
      "Epoch 5, Batch 389, Loss: 0.5194125771522522\n",
      "Epoch 5, Batch 390, Loss: 0.718976616859436\n",
      "Epoch 5, Batch 391, Loss: 0.6731325387954712\n",
      "Epoch 5, Batch 392, Loss: 0.8380304574966431\n",
      "Epoch 5, Batch 393, Loss: 0.6025426983833313\n",
      "Epoch 5, Batch 394, Loss: 0.5563802719116211\n",
      "Epoch 5, Batch 395, Loss: 0.704353928565979\n",
      "Epoch 5, Batch 396, Loss: 0.5770705938339233\n",
      "Epoch 5, Batch 397, Loss: 0.7598841190338135\n",
      "Epoch 5, Batch 398, Loss: 0.774556577205658\n",
      "Epoch 5, Batch 399, Loss: 0.5058299899101257\n",
      "Epoch 5, Batch 400, Loss: 0.6358134746551514\n",
      "Epoch 5, Batch 401, Loss: 0.6287544965744019\n",
      "Epoch 5, Batch 402, Loss: 0.5264614820480347\n",
      "Epoch 5, Batch 403, Loss: 0.7077800035476685\n",
      "Epoch 5, Batch 404, Loss: 0.812880277633667\n",
      "Epoch 5, Batch 405, Loss: 0.66559898853302\n",
      "Epoch 5, Batch 406, Loss: 0.9492969512939453\n",
      "Epoch 5, Batch 407, Loss: 0.5171886086463928\n",
      "Epoch 5, Batch 408, Loss: 0.716768205165863\n",
      "Epoch 5, Batch 409, Loss: 0.8077138066291809\n",
      "Epoch 5, Batch 410, Loss: 0.8592767715454102\n",
      "Epoch 5, Batch 411, Loss: 0.5230342149734497\n",
      "Epoch 5, Batch 412, Loss: 0.5866494178771973\n",
      "Epoch 5, Batch 413, Loss: 0.5766632556915283\n",
      "Epoch 5, Batch 414, Loss: 0.5936528444290161\n",
      "Epoch 5, Batch 415, Loss: 0.5050502419471741\n",
      "Epoch 5, Batch 416, Loss: 0.7472764849662781\n",
      "Epoch 5, Batch 417, Loss: 0.6631788611412048\n",
      "Epoch 5, Batch 418, Loss: 0.5119757652282715\n",
      "Epoch 5, Batch 419, Loss: 0.5936563611030579\n",
      "Epoch 5, Batch 420, Loss: 0.5920274257659912\n",
      "Epoch 5, Batch 421, Loss: 0.6724845170974731\n",
      "Epoch 5, Batch 422, Loss: 0.5406579375267029\n",
      "Epoch 5, Batch 423, Loss: 0.45167306065559387\n",
      "Epoch 5, Batch 424, Loss: 0.4390604794025421\n",
      "Epoch 5, Batch 425, Loss: 0.6096489429473877\n",
      "Epoch 5, Batch 426, Loss: 0.5616532564163208\n",
      "Epoch 5, Batch 427, Loss: 0.7462774515151978\n",
      "Epoch 5, Batch 428, Loss: 0.5179169178009033\n",
      "Epoch 5, Batch 429, Loss: 0.4997211694717407\n",
      "Epoch 5, Batch 430, Loss: 0.6562024354934692\n",
      "Epoch 5, Batch 431, Loss: 0.5326132774353027\n",
      "Epoch 5, Batch 432, Loss: 0.6432388424873352\n",
      "Epoch 5, Batch 433, Loss: 0.6155240535736084\n",
      "Epoch 5, Batch 434, Loss: 0.5142191052436829\n",
      "Epoch 5, Batch 435, Loss: 0.6450337171554565\n",
      "Epoch 5, Batch 436, Loss: 0.6377378702163696\n",
      "Epoch 5, Batch 437, Loss: 0.6050704121589661\n",
      "Epoch 5, Batch 438, Loss: 0.5895159244537354\n",
      "Epoch 5, Batch 439, Loss: 0.5745452642440796\n",
      "Epoch 5, Batch 440, Loss: 0.7326616048812866\n",
      "Epoch 5, Batch 441, Loss: 0.5615361928939819\n",
      "Epoch 5, Batch 442, Loss: 0.6350013017654419\n",
      "Epoch 5, Batch 443, Loss: 0.8635570406913757\n",
      "Epoch 5, Batch 444, Loss: 0.6776678562164307\n",
      "Epoch 5, Batch 445, Loss: 0.5766103863716125\n",
      "Epoch 5, Batch 446, Loss: 0.6501091122627258\n",
      "Epoch 5, Batch 447, Loss: 0.6859373450279236\n",
      "Epoch 5, Batch 448, Loss: 0.6211860179901123\n",
      "Epoch 5, Batch 449, Loss: 0.5607817769050598\n",
      "Epoch 5, Batch 450, Loss: 0.49151700735092163\n",
      "Epoch 5, Batch 451, Loss: 0.5973538160324097\n",
      "Epoch 5, Batch 452, Loss: 0.5220495462417603\n",
      "Epoch 5, Batch 453, Loss: 0.9474959969520569\n",
      "Epoch 5, Batch 454, Loss: 0.5972965955734253\n",
      "Epoch 5, Batch 455, Loss: 0.6680289506912231\n",
      "Epoch 5, Batch 456, Loss: 0.8266309499740601\n",
      "Epoch 5, Batch 457, Loss: 0.6013813614845276\n",
      "Epoch 5, Batch 458, Loss: 0.6522166728973389\n",
      "Epoch 5, Batch 459, Loss: 0.5651533603668213\n",
      "Epoch 5, Batch 460, Loss: 0.7012102603912354\n",
      "Epoch 5, Batch 461, Loss: 0.6053894758224487\n",
      "Epoch 5, Batch 462, Loss: 0.45925214886665344\n",
      "Epoch 5, Batch 463, Loss: 0.5579339265823364\n",
      "Epoch 5, Batch 464, Loss: 0.7783569097518921\n",
      "Epoch 5, Batch 465, Loss: 0.5855855941772461\n",
      "Epoch 5, Batch 466, Loss: 0.5872666239738464\n",
      "Epoch 5, Batch 467, Loss: 0.5337921380996704\n",
      "Epoch 5, Batch 468, Loss: 0.6496005654335022\n",
      "Epoch 5, Batch 469, Loss: 0.635726809501648\n",
      "Epoch 5, Batch 470, Loss: 0.5523697733879089\n",
      "Epoch 5, Batch 471, Loss: 0.6391743421554565\n",
      "Epoch 5, Batch 472, Loss: 0.5421971082687378\n",
      "Epoch 5, Batch 473, Loss: 0.5443542003631592\n",
      "Epoch 5, Batch 474, Loss: 0.708043098449707\n",
      "Epoch 5, Batch 475, Loss: 0.5616684556007385\n",
      "Epoch 5, Batch 476, Loss: 0.6470800638198853\n",
      "Epoch 5, Batch 477, Loss: 0.6177785992622375\n",
      "Epoch 5, Batch 478, Loss: 0.7790735363960266\n",
      "Epoch 5, Batch 479, Loss: 0.5050020813941956\n",
      "Epoch 5, Batch 480, Loss: 0.7078511714935303\n",
      "Epoch 5, Batch 481, Loss: 0.7409940361976624\n",
      "Epoch 5, Batch 482, Loss: 0.7223125696182251\n",
      "Epoch 5, Batch 483, Loss: 0.6057416200637817\n",
      "Epoch 5, Batch 484, Loss: 0.6947669982910156\n",
      "Epoch 5, Batch 485, Loss: 0.9525483250617981\n",
      "Epoch 5, Batch 486, Loss: 0.5323208570480347\n",
      "Epoch 5, Batch 487, Loss: 0.5285050272941589\n",
      "Epoch 5, Batch 488, Loss: 0.5403551459312439\n",
      "Epoch 5, Batch 489, Loss: 0.640313982963562\n",
      "Epoch 5, Batch 490, Loss: 0.5923292636871338\n",
      "Epoch 5, Batch 491, Loss: 0.6070171594619751\n",
      "Epoch 5, Batch 492, Loss: 0.8910940289497375\n",
      "Epoch 5, Batch 493, Loss: 0.547601044178009\n",
      "Epoch 5, Batch 494, Loss: 0.6623695492744446\n",
      "Epoch 5, Batch 495, Loss: 0.6492366790771484\n",
      "Epoch 5, Batch 496, Loss: 0.46476489305496216\n",
      "Epoch 5, Batch 497, Loss: 0.7010005116462708\n",
      "Epoch 5, Batch 498, Loss: 0.7093173861503601\n",
      "Epoch 5, Batch 499, Loss: 0.5786465406417847\n",
      "Epoch 5, Batch 500, Loss: 0.7020511031150818\n",
      "Epoch 5, Batch 501, Loss: 0.6608527302742004\n",
      "Epoch 5, Batch 502, Loss: 0.8532659411430359\n",
      "Epoch 5, Batch 503, Loss: 0.5165042877197266\n",
      "Epoch 5, Batch 504, Loss: 0.625867486000061\n",
      "Epoch 5, Batch 505, Loss: 0.7553215026855469\n",
      "Epoch 5, Batch 506, Loss: 0.6262516975402832\n",
      "Epoch 5, Batch 507, Loss: 0.5818403959274292\n",
      "Epoch 5, Batch 508, Loss: 0.7287059426307678\n",
      "Epoch 5, Batch 509, Loss: 0.8968770503997803\n",
      "Epoch 5, Batch 510, Loss: 0.6042495369911194\n",
      "Epoch 5, Batch 511, Loss: 0.9491729736328125\n",
      "Epoch 5, Batch 512, Loss: 0.42577797174453735\n",
      "Epoch 5, Batch 513, Loss: 0.7042991518974304\n",
      "Epoch 5, Batch 514, Loss: 0.5742889642715454\n",
      "Epoch 5, Batch 515, Loss: 0.6077297329902649\n",
      "Epoch 5, Batch 516, Loss: 0.6301659345626831\n",
      "Epoch 5, Batch 517, Loss: 0.6312767267227173\n",
      "Epoch 5, Batch 518, Loss: 0.479465126991272\n",
      "Epoch 5, Batch 519, Loss: 0.5132037401199341\n",
      "Epoch 5, Batch 520, Loss: 0.39006203413009644\n",
      "Epoch 5, Batch 521, Loss: 0.5721843242645264\n",
      "Epoch 5, Batch 522, Loss: 0.6386306285858154\n",
      "Epoch 5, Batch 523, Loss: 0.6125415563583374\n",
      "Epoch 5, Batch 524, Loss: 0.6399925947189331\n",
      "Epoch 5, Batch 525, Loss: 0.48762404918670654\n",
      "Epoch 5, Batch 526, Loss: 0.8177236318588257\n",
      "Epoch 5, Batch 527, Loss: 0.584457278251648\n",
      "Epoch 5, Batch 528, Loss: 0.7361140251159668\n",
      "Epoch 5, Batch 529, Loss: 0.5857974886894226\n",
      "Epoch 5, Batch 530, Loss: 0.445395827293396\n",
      "Epoch 5, Batch 531, Loss: 1.0268871784210205\n",
      "Epoch 5, Batch 532, Loss: 0.5017086863517761\n",
      "Epoch 5, Batch 533, Loss: 0.6353658437728882\n",
      "Epoch 5, Batch 534, Loss: 0.5520356893539429\n",
      "Epoch 5, Batch 535, Loss: 0.4625665545463562\n",
      "Epoch 5, Batch 536, Loss: 0.6739332675933838\n",
      "Epoch 5, Batch 537, Loss: 0.7496994137763977\n",
      "Epoch 5, Batch 538, Loss: 0.7402403354644775\n",
      "Epoch 5, Batch 539, Loss: 0.6828317642211914\n",
      "Epoch 5, Batch 540, Loss: 0.4551399350166321\n",
      "Epoch 5, Batch 541, Loss: 0.828245222568512\n",
      "Epoch 5, Batch 542, Loss: 0.5401310920715332\n",
      "Epoch 5, Batch 543, Loss: 0.6515332460403442\n",
      "Epoch 5, Batch 544, Loss: 0.7248842716217041\n",
      "Epoch 5, Batch 545, Loss: 0.45672324299812317\n",
      "Epoch 5, Batch 546, Loss: 0.4474151134490967\n",
      "Epoch 5, Batch 547, Loss: 0.7725497484207153\n",
      "Epoch 5, Batch 548, Loss: 0.8024490475654602\n",
      "Epoch 5, Batch 549, Loss: 0.48842519521713257\n",
      "Epoch 5, Batch 550, Loss: 0.8532779216766357\n",
      "Epoch 5, Batch 551, Loss: 0.6069672107696533\n",
      "Epoch 5, Batch 552, Loss: 0.6954617500305176\n",
      "Epoch 5, Batch 553, Loss: 0.7599527835845947\n",
      "Epoch 5, Batch 554, Loss: 0.7342471480369568\n",
      "Epoch 5, Batch 555, Loss: 0.7193213701248169\n",
      "Epoch 5, Batch 556, Loss: 0.6530869603157043\n",
      "Epoch 5, Batch 557, Loss: 0.660728931427002\n",
      "Epoch 5, Batch 558, Loss: 0.5119697451591492\n",
      "Epoch 5, Batch 559, Loss: 0.8466806411743164\n",
      "Epoch 5, Batch 560, Loss: 0.6683677434921265\n",
      "Epoch 5, Batch 561, Loss: 0.6729587912559509\n",
      "Epoch 5, Batch 562, Loss: 0.6145123839378357\n",
      "Epoch 5, Batch 563, Loss: 0.6344850659370422\n",
      "Epoch 5, Batch 564, Loss: 0.6967007517814636\n",
      "Epoch 5, Batch 565, Loss: 0.6219407916069031\n",
      "Epoch 5, Batch 566, Loss: 0.6350429058074951\n",
      "Epoch 5, Batch 567, Loss: 0.6476103067398071\n",
      "Epoch 5, Batch 568, Loss: 0.5744897127151489\n",
      "Epoch 5, Batch 569, Loss: 0.599541425704956\n",
      "Epoch 5, Batch 570, Loss: 0.7874232530593872\n",
      "Epoch 5, Batch 571, Loss: 0.7701008915901184\n",
      "Epoch 5, Batch 572, Loss: 0.6184989213943481\n",
      "Epoch 5, Batch 573, Loss: 0.5864124894142151\n",
      "Epoch 5, Batch 574, Loss: 0.5488113164901733\n",
      "Epoch 5, Batch 575, Loss: 0.5486208200454712\n",
      "Epoch 5, Batch 576, Loss: 0.684980034828186\n",
      "Epoch 5, Batch 577, Loss: 0.603247880935669\n",
      "Epoch 5, Batch 578, Loss: 0.4881136119365692\n",
      "Epoch 5, Batch 579, Loss: 0.4917221665382385\n",
      "Epoch 5, Batch 580, Loss: 0.7140765190124512\n",
      "Epoch 5, Batch 581, Loss: 0.5726518034934998\n",
      "Epoch 5, Batch 582, Loss: 0.6065987348556519\n",
      "Epoch 5, Batch 583, Loss: 0.6694015860557556\n",
      "Epoch 5, Batch 584, Loss: 0.7300055027008057\n",
      "Epoch 5, Batch 585, Loss: 0.6889631748199463\n",
      "Epoch 5, Batch 586, Loss: 0.4786975085735321\n",
      "Epoch 5, Batch 587, Loss: 0.5920966863632202\n",
      "Epoch 5, Batch 588, Loss: 0.8521064519882202\n",
      "Epoch 5, Batch 589, Loss: 0.5297560691833496\n",
      "Epoch 5, Batch 590, Loss: 0.8338930010795593\n",
      "Epoch 5, Batch 591, Loss: 0.6378239989280701\n",
      "Epoch 5, Batch 592, Loss: 0.4660917818546295\n",
      "Epoch 5, Batch 593, Loss: 0.5389071702957153\n",
      "Epoch 5, Batch 594, Loss: 0.6444082856178284\n",
      "Epoch 5, Batch 595, Loss: 0.6172997951507568\n",
      "Epoch 5, Batch 596, Loss: 0.7068427801132202\n",
      "Epoch 5, Batch 597, Loss: 0.3934747874736786\n",
      "Epoch 5, Batch 598, Loss: 0.5443904399871826\n",
      "Epoch 5, Batch 599, Loss: 0.6255244016647339\n",
      "Epoch 5, Batch 600, Loss: 0.7060533761978149\n",
      "Epoch 5, Batch 601, Loss: 0.45000386238098145\n",
      "Epoch 5, Batch 602, Loss: 0.7274236679077148\n",
      "Epoch 5, Batch 603, Loss: 0.5337831377983093\n",
      "Epoch 5, Batch 604, Loss: 0.5345377922058105\n",
      "Epoch 5, Batch 605, Loss: 0.42774519324302673\n",
      "Epoch 5, Batch 606, Loss: 0.6242285966873169\n",
      "Epoch 5, Batch 607, Loss: 0.5658589601516724\n",
      "Epoch 5, Batch 608, Loss: 0.7529538869857788\n",
      "Epoch 5, Batch 609, Loss: 0.789871096611023\n",
      "Epoch 5, Batch 610, Loss: 0.7039104700088501\n",
      "Epoch 5, Batch 611, Loss: 0.4851447343826294\n",
      "Epoch 5, Batch 612, Loss: 0.48952558636665344\n",
      "Epoch 5, Batch 613, Loss: 0.6233152151107788\n",
      "Epoch 5, Batch 614, Loss: 0.6080104112625122\n",
      "Epoch 5, Batch 615, Loss: 0.7446064949035645\n",
      "Epoch 5, Batch 616, Loss: 0.6562310457229614\n",
      "Epoch 5, Batch 617, Loss: 0.5311229228973389\n",
      "Epoch 5, Batch 618, Loss: 0.5694429874420166\n",
      "Epoch 5, Batch 619, Loss: 0.907904326915741\n",
      "Epoch 5, Batch 620, Loss: 0.6900284290313721\n",
      "Epoch 5, Batch 621, Loss: 0.6278896331787109\n",
      "Epoch 5, Batch 622, Loss: 0.5898540019989014\n",
      "Epoch 5, Batch 623, Loss: 0.6193587779998779\n",
      "Epoch 5, Batch 624, Loss: 0.5688976049423218\n",
      "Epoch 5, Batch 625, Loss: 0.852759599685669\n",
      "Epoch 5, Batch 626, Loss: 0.6918657422065735\n",
      "Epoch 5, Batch 627, Loss: 0.693303644657135\n",
      "Epoch 5, Batch 628, Loss: 0.5963932275772095\n",
      "Epoch 5, Batch 629, Loss: 0.7670333981513977\n",
      "Epoch 5, Batch 630, Loss: 0.6421192288398743\n",
      "Epoch 5, Batch 631, Loss: 0.7576755285263062\n",
      "Epoch 5, Batch 632, Loss: 0.6190120577812195\n",
      "Epoch 5, Batch 633, Loss: 0.7274434566497803\n",
      "Epoch 5, Batch 634, Loss: 0.7176923751831055\n",
      "Epoch 5, Batch 635, Loss: 0.5286315083503723\n",
      "Epoch 5, Batch 636, Loss: 0.5654059052467346\n",
      "Epoch 5, Batch 637, Loss: 0.6681322455406189\n",
      "Epoch 5, Batch 638, Loss: 0.7651619911193848\n",
      "Epoch 5, Batch 639, Loss: 0.5104896426200867\n",
      "Epoch 5, Batch 640, Loss: 0.5818604230880737\n",
      "Epoch 5, Batch 641, Loss: 0.820380449295044\n",
      "Epoch 5, Batch 642, Loss: 0.46343711018562317\n",
      "Epoch 5, Batch 643, Loss: 0.5650824308395386\n",
      "Epoch 5, Batch 644, Loss: 0.47903066873550415\n",
      "Epoch 5, Batch 645, Loss: 0.6347225308418274\n",
      "Epoch 5, Batch 646, Loss: 0.6019377708435059\n",
      "Epoch 5, Batch 647, Loss: 0.6148020625114441\n",
      "Epoch 5, Batch 648, Loss: 0.4886699914932251\n",
      "Epoch 5, Batch 649, Loss: 0.5134404301643372\n",
      "Epoch 5, Batch 650, Loss: 0.5198529362678528\n",
      "Epoch 5, Batch 651, Loss: 0.789505124092102\n",
      "Epoch 5, Batch 652, Loss: 0.6954977512359619\n",
      "Epoch 5, Batch 653, Loss: 0.5072462558746338\n",
      "Epoch 5, Batch 654, Loss: 0.4497659504413605\n",
      "Epoch 5, Batch 655, Loss: 0.6753138899803162\n",
      "Epoch 5, Batch 656, Loss: 0.6493959426879883\n",
      "Epoch 5, Batch 657, Loss: 0.7696499228477478\n",
      "Epoch 5, Batch 658, Loss: 0.8239202499389648\n",
      "Epoch 5, Batch 659, Loss: 0.6914271116256714\n",
      "Epoch 5, Batch 660, Loss: 0.6197536587715149\n",
      "Epoch 5, Batch 661, Loss: 0.5646440386772156\n",
      "Epoch 5, Batch 662, Loss: 0.6654318571090698\n",
      "Epoch 5, Batch 663, Loss: 0.631669819355011\n",
      "Epoch 5, Batch 664, Loss: 0.78121018409729\n",
      "Epoch 5, Batch 665, Loss: 0.6702497005462646\n",
      "Epoch 5, Batch 666, Loss: 0.661909282207489\n",
      "Epoch 5, Batch 667, Loss: 0.5358248949050903\n",
      "Epoch 5, Batch 668, Loss: 0.6293381452560425\n",
      "Epoch 5, Batch 669, Loss: 0.5155806541442871\n",
      "Epoch 5, Batch 670, Loss: 0.7058753967285156\n",
      "Epoch 5, Batch 671, Loss: 0.8009924292564392\n",
      "Epoch 5, Batch 672, Loss: 0.5642401576042175\n",
      "Epoch 5, Batch 673, Loss: 0.4567788243293762\n",
      "Epoch 5, Batch 674, Loss: 0.8121623396873474\n",
      "Epoch 5, Batch 675, Loss: 0.4336419403553009\n",
      "Epoch 5, Batch 676, Loss: 0.6903313994407654\n",
      "Epoch 5, Batch 677, Loss: 0.5568928718566895\n",
      "Epoch 5, Batch 678, Loss: 0.5578191876411438\n",
      "Epoch 5, Batch 679, Loss: 0.5881905555725098\n",
      "Epoch 5, Batch 680, Loss: 0.7554394006729126\n",
      "Epoch 5, Batch 681, Loss: 0.6696732044219971\n",
      "Epoch 5, Batch 682, Loss: 0.5372937917709351\n",
      "Epoch 5, Batch 683, Loss: 0.49854576587677\n",
      "Epoch 5, Batch 684, Loss: 0.6228216886520386\n",
      "Epoch 5, Batch 685, Loss: 0.5839880108833313\n",
      "Epoch 5, Batch 686, Loss: 0.4927132725715637\n",
      "Epoch 5, Batch 687, Loss: 0.5833438634872437\n",
      "Epoch 5, Batch 688, Loss: 0.6667749881744385\n",
      "Epoch 5, Batch 689, Loss: 0.7031162977218628\n",
      "Epoch 5, Batch 690, Loss: 0.5365738868713379\n",
      "Epoch 5, Batch 691, Loss: 0.5260949730873108\n",
      "Epoch 5, Batch 692, Loss: 0.8553385734558105\n",
      "Epoch 5, Batch 693, Loss: 0.7072359323501587\n",
      "Epoch 5, Batch 694, Loss: 0.6417544484138489\n",
      "Epoch 5, Batch 695, Loss: 0.74729984998703\n",
      "Epoch 5, Batch 696, Loss: 0.7484556436538696\n",
      "Epoch 5, Batch 697, Loss: 0.891999363899231\n",
      "Epoch 5, Batch 698, Loss: 0.5069988965988159\n",
      "Epoch 5, Batch 699, Loss: 0.7480887770652771\n",
      "Epoch 5, Batch 700, Loss: 0.6116903424263\n",
      "Epoch 5, Batch 701, Loss: 0.6191191077232361\n",
      "Epoch 5, Batch 702, Loss: 0.7186381816864014\n",
      "Epoch 5, Batch 703, Loss: 0.5551780462265015\n",
      "Epoch 5, Batch 704, Loss: 0.7380325198173523\n",
      "Epoch 5, Batch 705, Loss: 0.5429105758666992\n",
      "Epoch 5, Batch 706, Loss: 0.46948060393333435\n",
      "Epoch 5, Batch 707, Loss: 0.6748294234275818\n",
      "Epoch 5, Batch 708, Loss: 0.6191326379776001\n",
      "Epoch 5, Batch 709, Loss: 0.42858603596687317\n",
      "Epoch 5, Batch 710, Loss: 0.527381181716919\n",
      "Epoch 5, Batch 711, Loss: 0.7711912393569946\n",
      "Epoch 5, Batch 712, Loss: 0.41687899827957153\n",
      "Epoch 5, Batch 713, Loss: 0.6618636250495911\n",
      "Epoch 5, Batch 714, Loss: 0.5008187294006348\n",
      "Epoch 5, Batch 715, Loss: 0.597739577293396\n",
      "Epoch 5, Batch 716, Loss: 0.6002040505409241\n",
      "Epoch 5, Batch 717, Loss: 0.6087057590484619\n",
      "Epoch 5, Batch 718, Loss: 0.4301632046699524\n",
      "Epoch 5, Batch 719, Loss: 0.9364979863166809\n",
      "Epoch 5, Batch 720, Loss: 0.6942309141159058\n",
      "Epoch 5, Batch 721, Loss: 0.7214906811714172\n",
      "Epoch 5, Batch 722, Loss: 0.6970303654670715\n",
      "Epoch 5, Batch 723, Loss: 0.5243743658065796\n",
      "Epoch 5, Batch 724, Loss: 0.5834024548530579\n",
      "Epoch 5, Batch 725, Loss: 0.6455544233322144\n",
      "Epoch 5, Batch 726, Loss: 0.47775018215179443\n",
      "Epoch 5, Batch 727, Loss: 0.4887228012084961\n",
      "Epoch 5, Batch 728, Loss: 0.6842820048332214\n",
      "Epoch 5, Batch 729, Loss: 0.892106294631958\n",
      "Epoch 5, Batch 730, Loss: 0.5212159156799316\n",
      "Epoch 5, Batch 731, Loss: 0.5031090974807739\n",
      "Epoch 5, Batch 732, Loss: 0.8851351737976074\n",
      "Epoch 5, Batch 733, Loss: 0.6697899699211121\n",
      "Epoch 5, Batch 734, Loss: 0.5121435523033142\n",
      "Epoch 5, Batch 735, Loss: 0.6310915946960449\n",
      "Epoch 5, Batch 736, Loss: 0.7614845037460327\n",
      "Epoch 5, Batch 737, Loss: 0.6765131950378418\n",
      "Epoch 5, Batch 738, Loss: 0.6467433571815491\n",
      "Epoch 5, Batch 739, Loss: 0.5264075398445129\n",
      "Epoch 5, Batch 740, Loss: 0.5697757005691528\n",
      "Epoch 5, Batch 741, Loss: 0.6483815312385559\n",
      "Epoch 5, Batch 742, Loss: 0.5779721736907959\n",
      "Epoch 5, Batch 743, Loss: 0.5037912726402283\n",
      "Epoch 5, Batch 744, Loss: 0.4830653667449951\n",
      "Epoch 5, Batch 745, Loss: 0.684666633605957\n",
      "Epoch 5, Batch 746, Loss: 0.6563672423362732\n",
      "Epoch 5, Batch 747, Loss: 0.6693806648254395\n",
      "Epoch 5, Batch 748, Loss: 0.6634112596511841\n",
      "Epoch 5, Batch 749, Loss: 0.4747195541858673\n",
      "Epoch 5, Batch 750, Loss: 0.5854381322860718\n",
      "Epoch 5, Batch 751, Loss: 0.5272616744041443\n",
      "Epoch 5, Batch 752, Loss: 0.5487266182899475\n",
      "Epoch 5, Batch 753, Loss: 0.7151817083358765\n",
      "Epoch 5, Batch 754, Loss: 0.6306386590003967\n",
      "Epoch 5, Batch 755, Loss: 0.8092722296714783\n",
      "Epoch 5, Batch 756, Loss: 0.5440272092819214\n",
      "Epoch 5, Batch 757, Loss: 0.5214579701423645\n",
      "Epoch 5, Batch 758, Loss: 0.4232797622680664\n",
      "Epoch 5, Batch 759, Loss: 0.80378258228302\n",
      "Epoch 5, Batch 760, Loss: 0.5508332848548889\n",
      "Epoch 5, Batch 761, Loss: 0.7062863707542419\n",
      "Epoch 5, Batch 762, Loss: 0.5010991096496582\n",
      "Epoch 5, Batch 763, Loss: 0.8334767818450928\n",
      "Epoch 5, Batch 764, Loss: 0.5017400979995728\n",
      "Epoch 5, Batch 765, Loss: 0.7419662475585938\n",
      "Epoch 5, Batch 766, Loss: 0.6287578344345093\n",
      "Epoch 5, Batch 767, Loss: 0.788235068321228\n",
      "Epoch 5, Batch 768, Loss: 0.5279629230499268\n",
      "Epoch 5, Batch 769, Loss: 0.5859448909759521\n",
      "Epoch 5, Batch 770, Loss: 0.6096882820129395\n",
      "Epoch 5, Batch 771, Loss: 0.5715742707252502\n",
      "Epoch 5, Batch 772, Loss: 0.8973532915115356\n",
      "Epoch 5, Batch 773, Loss: 0.6827980279922485\n",
      "Epoch 5, Batch 774, Loss: 0.5746595859527588\n",
      "Epoch 5, Batch 775, Loss: 0.5921757817268372\n",
      "Epoch 5, Batch 776, Loss: 0.5703029632568359\n",
      "Epoch 5, Batch 777, Loss: 0.7978256344795227\n",
      "Epoch 5, Batch 778, Loss: 0.6653754711151123\n",
      "Epoch 5, Batch 779, Loss: 0.7211639285087585\n",
      "Epoch 5, Batch 780, Loss: 0.4790189862251282\n",
      "Epoch 5, Batch 781, Loss: 0.5179076790809631\n",
      "Epoch 5, Batch 782, Loss: 0.5938404202461243\n",
      "Epoch 5, Batch 783, Loss: 0.6012094020843506\n",
      "Epoch 5, Batch 784, Loss: 0.6341692209243774\n",
      "Epoch 5, Batch 785, Loss: 0.5340583324432373\n",
      "Epoch 5, Batch 786, Loss: 0.617625892162323\n",
      "Epoch 5, Batch 787, Loss: 0.48256269097328186\n",
      "Epoch 5, Batch 788, Loss: 0.46253281831741333\n",
      "Epoch 5, Batch 789, Loss: 0.5065422654151917\n",
      "Epoch 5, Batch 790, Loss: 0.45114850997924805\n",
      "Epoch 5, Batch 791, Loss: 0.39546290040016174\n",
      "Epoch 5, Batch 792, Loss: 0.8044309020042419\n",
      "Epoch 5, Batch 793, Loss: 0.6155608296394348\n",
      "Epoch 5, Batch 794, Loss: 0.8698666095733643\n",
      "Epoch 5, Batch 795, Loss: 0.48457449674606323\n",
      "Epoch 5, Batch 796, Loss: 0.7326374053955078\n",
      "Epoch 5, Batch 797, Loss: 0.5677076578140259\n",
      "Epoch 5, Batch 798, Loss: 0.730017900466919\n",
      "Epoch 5, Batch 799, Loss: 0.4910137355327606\n",
      "Epoch 5, Batch 800, Loss: 0.6708810329437256\n",
      "Epoch 5, Batch 801, Loss: 0.7546254396438599\n",
      "Epoch 5, Batch 802, Loss: 0.5284886360168457\n",
      "Epoch 5, Batch 803, Loss: 0.6059635877609253\n",
      "Epoch 5, Batch 804, Loss: 0.44238805770874023\n",
      "Epoch 5, Batch 805, Loss: 0.4704962372779846\n",
      "Epoch 5, Batch 806, Loss: 0.5535608530044556\n",
      "Epoch 5, Batch 807, Loss: 0.6452119946479797\n",
      "Epoch 5, Batch 808, Loss: 0.709530770778656\n",
      "Epoch 5, Batch 809, Loss: 0.6957777738571167\n",
      "Epoch 5, Batch 810, Loss: 0.7290751934051514\n",
      "Epoch 5, Batch 811, Loss: 0.6750414371490479\n",
      "Epoch 5, Batch 812, Loss: 0.5172266960144043\n",
      "Epoch 5, Batch 813, Loss: 0.5556337237358093\n",
      "Epoch 5, Batch 814, Loss: 0.5024893283843994\n",
      "Epoch 5, Batch 815, Loss: 0.4588105082511902\n",
      "Epoch 5, Batch 816, Loss: 0.730634331703186\n",
      "Epoch 5, Batch 817, Loss: 0.765303909778595\n",
      "Epoch 5, Batch 818, Loss: 0.48601478338241577\n",
      "Epoch 5, Batch 819, Loss: 0.6353758573532104\n",
      "Epoch 5, Batch 820, Loss: 0.5792450904846191\n",
      "Epoch 5, Batch 821, Loss: 0.4995537996292114\n",
      "Epoch 5, Batch 822, Loss: 0.43692028522491455\n",
      "Epoch 5, Batch 823, Loss: 0.5854102969169617\n",
      "Epoch 5, Batch 824, Loss: 0.7677860856056213\n",
      "Epoch 5, Batch 825, Loss: 0.6049238443374634\n",
      "Epoch 5, Batch 826, Loss: 0.567532479763031\n",
      "Epoch 5, Batch 827, Loss: 0.7763341069221497\n",
      "Epoch 5, Batch 828, Loss: 0.6179152727127075\n",
      "Epoch 5, Batch 829, Loss: 0.6589012145996094\n",
      "Epoch 5, Batch 830, Loss: 0.5971055626869202\n",
      "Epoch 5, Batch 831, Loss: 0.6253017783164978\n",
      "Epoch 5, Batch 832, Loss: 0.6600004434585571\n",
      "Epoch 5, Batch 833, Loss: 0.635718047618866\n",
      "Epoch 5, Batch 834, Loss: 0.5877330303192139\n",
      "Epoch 5, Batch 835, Loss: 0.6573877930641174\n",
      "Epoch 5, Batch 836, Loss: 0.5432721972465515\n",
      "Epoch 5, Batch 837, Loss: 0.5250198841094971\n",
      "Epoch 5, Batch 838, Loss: 0.5670367479324341\n",
      "Epoch 5, Batch 839, Loss: 0.5971089601516724\n",
      "Epoch 5, Batch 840, Loss: 0.7037532329559326\n",
      "Epoch 5, Batch 841, Loss: 0.6287981867790222\n",
      "Epoch 5, Batch 842, Loss: 0.6469393372535706\n",
      "Epoch 5, Batch 843, Loss: 0.6324654221534729\n",
      "Epoch 5, Batch 844, Loss: 0.5224414467811584\n",
      "Epoch 5, Batch 845, Loss: 0.6122861504554749\n",
      "Epoch 5, Batch 846, Loss: 0.5080997943878174\n",
      "Epoch 5, Batch 847, Loss: 0.48166245222091675\n",
      "Epoch 5, Batch 848, Loss: 0.5594984292984009\n",
      "Epoch 5, Batch 849, Loss: 0.4966013729572296\n",
      "Epoch 5, Batch 850, Loss: 0.5008501410484314\n",
      "Epoch 5, Batch 851, Loss: 0.7258952856063843\n",
      "Epoch 5, Batch 852, Loss: 0.4766974449157715\n",
      "Epoch 5, Batch 853, Loss: 0.601072371006012\n",
      "Epoch 5, Batch 854, Loss: 0.5567013025283813\n",
      "Epoch 5, Batch 855, Loss: 0.5566613674163818\n",
      "Epoch 5, Batch 856, Loss: 0.5917752981185913\n",
      "Epoch 5, Batch 857, Loss: 0.7142029404640198\n",
      "Epoch 5, Batch 858, Loss: 0.682835042476654\n",
      "Epoch 5, Batch 859, Loss: 0.754054605960846\n",
      "Epoch 5, Batch 860, Loss: 0.6800535917282104\n",
      "Epoch 5, Batch 861, Loss: 0.5924335718154907\n",
      "Epoch 5, Batch 862, Loss: 0.49624067544937134\n",
      "Epoch 5, Batch 863, Loss: 0.4957992136478424\n",
      "Epoch 5, Batch 864, Loss: 0.9750045537948608\n",
      "Epoch 5, Batch 865, Loss: 0.780214786529541\n",
      "Epoch 5, Batch 866, Loss: 0.580897867679596\n",
      "Epoch 5, Batch 867, Loss: 0.5641229152679443\n",
      "Epoch 5, Batch 868, Loss: 0.5268609523773193\n",
      "Epoch 5, Batch 869, Loss: 0.692365288734436\n",
      "Epoch 5, Batch 870, Loss: 0.6148043870925903\n",
      "Epoch 5, Batch 871, Loss: 0.5525215268135071\n",
      "Epoch 5, Batch 872, Loss: 0.7239624261856079\n",
      "Epoch 5, Batch 873, Loss: 0.6093500256538391\n",
      "Epoch 5, Batch 874, Loss: 0.5806835889816284\n",
      "Epoch 5, Batch 875, Loss: 0.5802605748176575\n",
      "Epoch 5, Batch 876, Loss: 0.5160871744155884\n",
      "Epoch 5, Batch 877, Loss: 0.6159777641296387\n",
      "Epoch 5, Batch 878, Loss: 0.5161140561103821\n",
      "Epoch 5, Batch 879, Loss: 0.45999377965927124\n",
      "Epoch 5, Batch 880, Loss: 0.5912602543830872\n",
      "Epoch 5, Batch 881, Loss: 0.7128660082817078\n",
      "Epoch 5, Batch 882, Loss: 0.6313098669052124\n",
      "Epoch 5, Batch 883, Loss: 0.6821039319038391\n",
      "Epoch 5, Batch 884, Loss: 0.46177953481674194\n",
      "Epoch 5, Batch 885, Loss: 0.5566349029541016\n",
      "Epoch 5, Batch 886, Loss: 0.6678162813186646\n",
      "Epoch 5, Batch 887, Loss: 0.7726919054985046\n",
      "Epoch 5, Batch 888, Loss: 0.6520930528640747\n",
      "Epoch 5, Batch 889, Loss: 0.6311063766479492\n",
      "Epoch 5, Batch 890, Loss: 0.6667006611824036\n",
      "Epoch 5, Batch 891, Loss: 0.6863143444061279\n",
      "Epoch 5, Batch 892, Loss: 0.6628740429878235\n",
      "Epoch 5, Batch 893, Loss: 0.7250608801841736\n",
      "Epoch 5, Batch 894, Loss: 0.36947599053382874\n",
      "Epoch 5, Batch 895, Loss: 0.5672349333763123\n",
      "Epoch 5, Batch 896, Loss: 0.9114890098571777\n",
      "Epoch 5, Batch 897, Loss: 0.5055097341537476\n",
      "Epoch 5, Batch 898, Loss: 0.5286255478858948\n",
      "Epoch 5, Batch 899, Loss: 0.39484211802482605\n",
      "Epoch 5, Batch 900, Loss: 0.5425101518630981\n",
      "Epoch 5, Batch 901, Loss: 0.4841589629650116\n",
      "Epoch 5, Batch 902, Loss: 0.796257734298706\n",
      "Epoch 5, Batch 903, Loss: 0.6912018060684204\n",
      "Epoch 5, Batch 904, Loss: 0.6271092295646667\n",
      "Epoch 5, Batch 905, Loss: 0.5866175889968872\n",
      "Epoch 5, Batch 906, Loss: 0.7050379514694214\n",
      "Epoch 5, Batch 907, Loss: 0.9590655565261841\n",
      "Epoch 5, Batch 908, Loss: 0.6458206176757812\n",
      "Epoch 5, Batch 909, Loss: 0.4885671138763428\n",
      "Epoch 5, Batch 910, Loss: 0.4292118549346924\n",
      "Epoch 5, Batch 911, Loss: 0.7066655158996582\n",
      "Epoch 5, Batch 912, Loss: 0.9039561152458191\n",
      "Epoch 5, Batch 913, Loss: 0.5659777522087097\n",
      "Epoch 5, Batch 914, Loss: 0.5505367517471313\n",
      "Epoch 5, Batch 915, Loss: 0.46868300437927246\n",
      "Epoch 5, Batch 916, Loss: 0.5689055323600769\n",
      "Epoch 5, Batch 917, Loss: 0.7810794711112976\n",
      "Epoch 5, Batch 918, Loss: 0.5870885848999023\n",
      "Epoch 5, Batch 919, Loss: 0.6481654047966003\n",
      "Epoch 5, Batch 920, Loss: 0.5932085514068604\n",
      "Epoch 5, Batch 921, Loss: 0.6639583706855774\n",
      "Epoch 5, Batch 922, Loss: 0.41804248094558716\n",
      "Epoch 5, Batch 923, Loss: 0.5896898508071899\n",
      "Epoch 5, Batch 924, Loss: 0.6942844390869141\n",
      "Epoch 5, Batch 925, Loss: 0.8169654607772827\n",
      "Epoch 5, Batch 926, Loss: 0.5948380827903748\n",
      "Epoch 5, Batch 927, Loss: 0.6793398857116699\n",
      "Epoch 5, Batch 928, Loss: 0.6852824091911316\n",
      "Epoch 5, Batch 929, Loss: 0.6926677823066711\n",
      "Epoch 5, Batch 930, Loss: 0.4526565670967102\n",
      "Epoch 5, Batch 931, Loss: 0.7284524440765381\n",
      "Epoch 5, Batch 932, Loss: 0.6936150193214417\n",
      "Epoch 5, Batch 933, Loss: 0.5440050959587097\n",
      "Epoch 5, Batch 934, Loss: 0.49217963218688965\n",
      "Epoch 5, Batch 935, Loss: 0.7714172601699829\n",
      "Epoch 5, Batch 936, Loss: 0.483125239610672\n",
      "Epoch 5, Batch 937, Loss: 0.725088894367218\n",
      "Epoch 5, Batch 938, Loss: 0.46413207054138184\n",
      "Accuracy of train set: 0.7700833333333333\n",
      "Epoch 5, Batch 1, Test Loss: 0.5724207758903503\n",
      "Epoch 5, Batch 2, Test Loss: 0.627909779548645\n",
      "Epoch 5, Batch 3, Test Loss: 0.5367625951766968\n",
      "Epoch 5, Batch 4, Test Loss: 0.6034806370735168\n",
      "Epoch 5, Batch 5, Test Loss: 0.6303155422210693\n",
      "Epoch 5, Batch 6, Test Loss: 0.675990641117096\n",
      "Epoch 5, Batch 7, Test Loss: 0.5303590893745422\n",
      "Epoch 5, Batch 8, Test Loss: 0.6130095720291138\n",
      "Epoch 5, Batch 9, Test Loss: 0.6270140409469604\n",
      "Epoch 5, Batch 10, Test Loss: 0.5313587188720703\n",
      "Epoch 5, Batch 11, Test Loss: 0.4535358250141144\n",
      "Epoch 5, Batch 12, Test Loss: 0.45252037048339844\n",
      "Epoch 5, Batch 13, Test Loss: 0.5365079641342163\n",
      "Epoch 5, Batch 14, Test Loss: 0.39123618602752686\n",
      "Epoch 5, Batch 15, Test Loss: 0.45554691553115845\n",
      "Epoch 5, Batch 16, Test Loss: 0.8311251997947693\n",
      "Epoch 5, Batch 17, Test Loss: 0.6139388680458069\n",
      "Epoch 5, Batch 18, Test Loss: 0.8076927065849304\n",
      "Epoch 5, Batch 19, Test Loss: 0.7622599601745605\n",
      "Epoch 5, Batch 20, Test Loss: 0.8289765119552612\n",
      "Epoch 5, Batch 21, Test Loss: 0.60554039478302\n",
      "Epoch 5, Batch 22, Test Loss: 0.46829694509506226\n",
      "Epoch 5, Batch 23, Test Loss: 0.5086212158203125\n",
      "Epoch 5, Batch 24, Test Loss: 0.6955314874649048\n",
      "Epoch 5, Batch 25, Test Loss: 0.5786656141281128\n",
      "Epoch 5, Batch 26, Test Loss: 0.6307097673416138\n",
      "Epoch 5, Batch 27, Test Loss: 0.41337719559669495\n",
      "Epoch 5, Batch 28, Test Loss: 0.571103572845459\n",
      "Epoch 5, Batch 29, Test Loss: 0.6636785268783569\n",
      "Epoch 5, Batch 30, Test Loss: 0.47071537375450134\n",
      "Epoch 5, Batch 31, Test Loss: 0.623816728591919\n",
      "Epoch 5, Batch 32, Test Loss: 0.564826250076294\n",
      "Epoch 5, Batch 33, Test Loss: 0.7112970948219299\n",
      "Epoch 5, Batch 34, Test Loss: 0.5361860990524292\n",
      "Epoch 5, Batch 35, Test Loss: 0.6785101890563965\n",
      "Epoch 5, Batch 36, Test Loss: 0.5412440896034241\n",
      "Epoch 5, Batch 37, Test Loss: 0.5913490056991577\n",
      "Epoch 5, Batch 38, Test Loss: 0.5444558262825012\n",
      "Epoch 5, Batch 39, Test Loss: 0.5410752296447754\n",
      "Epoch 5, Batch 40, Test Loss: 0.612419843673706\n",
      "Epoch 5, Batch 41, Test Loss: 0.6279971599578857\n",
      "Epoch 5, Batch 42, Test Loss: 0.6444242000579834\n",
      "Epoch 5, Batch 43, Test Loss: 0.5949875712394714\n",
      "Epoch 5, Batch 44, Test Loss: 0.6191108822822571\n",
      "Epoch 5, Batch 45, Test Loss: 0.7672377824783325\n",
      "Epoch 5, Batch 46, Test Loss: 0.6737343668937683\n",
      "Epoch 5, Batch 47, Test Loss: 0.4917129874229431\n",
      "Epoch 5, Batch 48, Test Loss: 0.6142794489860535\n",
      "Epoch 5, Batch 49, Test Loss: 0.5909762382507324\n",
      "Epoch 5, Batch 50, Test Loss: 0.5420613288879395\n",
      "Epoch 5, Batch 51, Test Loss: 0.7078526616096497\n",
      "Epoch 5, Batch 52, Test Loss: 0.5289855599403381\n",
      "Epoch 5, Batch 53, Test Loss: 0.7019158601760864\n",
      "Epoch 5, Batch 54, Test Loss: 0.6448744535446167\n",
      "Epoch 5, Batch 55, Test Loss: 0.6632953882217407\n",
      "Epoch 5, Batch 56, Test Loss: 0.576160192489624\n",
      "Epoch 5, Batch 57, Test Loss: 0.46824944019317627\n",
      "Epoch 5, Batch 58, Test Loss: 0.47797513008117676\n",
      "Epoch 5, Batch 59, Test Loss: 0.4928973913192749\n",
      "Epoch 5, Batch 60, Test Loss: 0.5431354641914368\n",
      "Epoch 5, Batch 61, Test Loss: 0.5926440954208374\n",
      "Epoch 5, Batch 62, Test Loss: 0.6412997245788574\n",
      "Epoch 5, Batch 63, Test Loss: 0.6273403763771057\n",
      "Epoch 5, Batch 64, Test Loss: 0.5309305191040039\n",
      "Epoch 5, Batch 65, Test Loss: 0.4747666120529175\n",
      "Epoch 5, Batch 66, Test Loss: 0.6996625661849976\n",
      "Epoch 5, Batch 67, Test Loss: 0.7199753522872925\n",
      "Epoch 5, Batch 68, Test Loss: 1.0738688707351685\n",
      "Epoch 5, Batch 69, Test Loss: 0.5568994879722595\n",
      "Epoch 5, Batch 70, Test Loss: 0.553343653678894\n",
      "Epoch 5, Batch 71, Test Loss: 0.7433388233184814\n",
      "Epoch 5, Batch 72, Test Loss: 0.6470618844032288\n",
      "Epoch 5, Batch 73, Test Loss: 0.5174577236175537\n",
      "Epoch 5, Batch 74, Test Loss: 0.7158641815185547\n",
      "Epoch 5, Batch 75, Test Loss: 0.7049158811569214\n",
      "Epoch 5, Batch 76, Test Loss: 0.6126335263252258\n",
      "Epoch 5, Batch 77, Test Loss: 0.7545049786567688\n",
      "Epoch 5, Batch 78, Test Loss: 0.71775883436203\n",
      "Epoch 5, Batch 79, Test Loss: 0.6832283735275269\n",
      "Epoch 5, Batch 80, Test Loss: 0.7079119682312012\n",
      "Epoch 5, Batch 81, Test Loss: 0.6808868050575256\n",
      "Epoch 5, Batch 82, Test Loss: 0.5898670554161072\n",
      "Epoch 5, Batch 83, Test Loss: 0.5639829039573669\n",
      "Epoch 5, Batch 84, Test Loss: 0.4008272886276245\n",
      "Epoch 5, Batch 85, Test Loss: 0.7257868051528931\n",
      "Epoch 5, Batch 86, Test Loss: 0.49043935537338257\n",
      "Epoch 5, Batch 87, Test Loss: 0.44353121519088745\n",
      "Epoch 5, Batch 88, Test Loss: 0.5369915962219238\n",
      "Epoch 5, Batch 89, Test Loss: 0.5854011178016663\n",
      "Epoch 5, Batch 90, Test Loss: 0.5485992431640625\n",
      "Epoch 5, Batch 91, Test Loss: 0.6634879112243652\n",
      "Epoch 5, Batch 92, Test Loss: 0.7500829696655273\n",
      "Epoch 5, Batch 93, Test Loss: 0.7851458191871643\n",
      "Epoch 5, Batch 94, Test Loss: 0.7592977285385132\n",
      "Epoch 5, Batch 95, Test Loss: 0.775246262550354\n",
      "Epoch 5, Batch 96, Test Loss: 0.7312866449356079\n",
      "Epoch 5, Batch 97, Test Loss: 0.6237399578094482\n",
      "Epoch 5, Batch 98, Test Loss: 0.5441741943359375\n",
      "Epoch 5, Batch 99, Test Loss: 0.6346540451049805\n",
      "Epoch 5, Batch 100, Test Loss: 0.8399699926376343\n",
      "Epoch 5, Batch 101, Test Loss: 0.5842009782791138\n",
      "Epoch 5, Batch 102, Test Loss: 0.6155551671981812\n",
      "Epoch 5, Batch 103, Test Loss: 0.546323299407959\n",
      "Epoch 5, Batch 104, Test Loss: 0.6815242171287537\n",
      "Epoch 5, Batch 105, Test Loss: 0.6602524518966675\n",
      "Epoch 5, Batch 106, Test Loss: 0.5372449159622192\n",
      "Epoch 5, Batch 107, Test Loss: 0.44336384534835815\n",
      "Epoch 5, Batch 108, Test Loss: 0.5502756834030151\n",
      "Epoch 5, Batch 109, Test Loss: 0.5293081998825073\n",
      "Epoch 5, Batch 110, Test Loss: 0.7801201343536377\n",
      "Epoch 5, Batch 111, Test Loss: 0.6496112942695618\n",
      "Epoch 5, Batch 112, Test Loss: 0.6157518625259399\n",
      "Epoch 5, Batch 113, Test Loss: 0.8761134147644043\n",
      "Epoch 5, Batch 114, Test Loss: 0.4913209080696106\n",
      "Epoch 5, Batch 115, Test Loss: 0.6326711773872375\n",
      "Epoch 5, Batch 116, Test Loss: 0.6347798109054565\n",
      "Epoch 5, Batch 117, Test Loss: 0.5009969472885132\n",
      "Epoch 5, Batch 118, Test Loss: 0.7480970621109009\n",
      "Epoch 5, Batch 119, Test Loss: 0.6606443524360657\n",
      "Epoch 5, Batch 120, Test Loss: 0.6654380559921265\n",
      "Epoch 5, Batch 121, Test Loss: 0.7995031476020813\n",
      "Epoch 5, Batch 122, Test Loss: 0.5682504177093506\n",
      "Epoch 5, Batch 123, Test Loss: 0.5448086261749268\n",
      "Epoch 5, Batch 124, Test Loss: 0.7397489547729492\n",
      "Epoch 5, Batch 125, Test Loss: 0.520182192325592\n",
      "Epoch 5, Batch 126, Test Loss: 0.603513240814209\n",
      "Epoch 5, Batch 127, Test Loss: 0.6435672640800476\n",
      "Epoch 5, Batch 128, Test Loss: 0.6066668033599854\n",
      "Epoch 5, Batch 129, Test Loss: 0.6772317886352539\n",
      "Epoch 5, Batch 130, Test Loss: 0.5714697241783142\n",
      "Epoch 5, Batch 131, Test Loss: 0.5744578242301941\n",
      "Epoch 5, Batch 132, Test Loss: 0.5896417498588562\n",
      "Epoch 5, Batch 133, Test Loss: 0.4394013285636902\n",
      "Epoch 5, Batch 134, Test Loss: 0.7312580347061157\n",
      "Epoch 5, Batch 135, Test Loss: 0.4856148064136505\n",
      "Epoch 5, Batch 136, Test Loss: 0.7847259640693665\n",
      "Epoch 5, Batch 137, Test Loss: 0.5802066326141357\n",
      "Epoch 5, Batch 138, Test Loss: 0.6418386101722717\n",
      "Epoch 5, Batch 139, Test Loss: 0.7061723470687866\n",
      "Epoch 5, Batch 140, Test Loss: 0.7103381752967834\n",
      "Epoch 5, Batch 141, Test Loss: 0.6034221053123474\n",
      "Epoch 5, Batch 142, Test Loss: 0.5611998438835144\n",
      "Epoch 5, Batch 143, Test Loss: 0.6630956530570984\n",
      "Epoch 5, Batch 144, Test Loss: 0.5863134860992432\n",
      "Epoch 5, Batch 145, Test Loss: 0.5262905359268188\n",
      "Epoch 5, Batch 146, Test Loss: 0.5135045647621155\n",
      "Epoch 5, Batch 147, Test Loss: 0.5623419284820557\n",
      "Epoch 5, Batch 148, Test Loss: 0.6820403933525085\n",
      "Epoch 5, Batch 149, Test Loss: 0.7620337605476379\n",
      "Epoch 5, Batch 150, Test Loss: 0.6297747492790222\n",
      "Epoch 5, Batch 151, Test Loss: 0.6411314010620117\n",
      "Epoch 5, Batch 152, Test Loss: 0.6243282556533813\n",
      "Epoch 5, Batch 153, Test Loss: 0.44717490673065186\n",
      "Epoch 5, Batch 154, Test Loss: 0.5901455879211426\n",
      "Epoch 5, Batch 155, Test Loss: 0.496727854013443\n",
      "Epoch 5, Batch 156, Test Loss: 0.5021006464958191\n",
      "Epoch 5, Batch 157, Test Loss: 0.6396335363388062\n",
      "Epoch 5, Batch 158, Test Loss: 0.46539074182510376\n",
      "Epoch 5, Batch 159, Test Loss: 0.4902702569961548\n",
      "Epoch 5, Batch 160, Test Loss: 0.6821098923683167\n",
      "Epoch 5, Batch 161, Test Loss: 0.882368803024292\n",
      "Epoch 5, Batch 162, Test Loss: 0.6599089503288269\n",
      "Epoch 5, Batch 163, Test Loss: 0.7049201726913452\n",
      "Epoch 5, Batch 164, Test Loss: 0.5167428851127625\n",
      "Epoch 5, Batch 165, Test Loss: 0.5541674494743347\n",
      "Epoch 5, Batch 166, Test Loss: 0.7950742244720459\n",
      "Epoch 5, Batch 167, Test Loss: 0.5982702374458313\n",
      "Epoch 5, Batch 168, Test Loss: 0.6350395083427429\n",
      "Epoch 5, Batch 169, Test Loss: 0.45029914379119873\n",
      "Epoch 5, Batch 170, Test Loss: 0.5215060114860535\n",
      "Epoch 5, Batch 171, Test Loss: 0.5236477255821228\n",
      "Epoch 5, Batch 172, Test Loss: 0.6555910706520081\n",
      "Epoch 5, Batch 173, Test Loss: 0.5806690454483032\n",
      "Epoch 5, Batch 174, Test Loss: 0.6747468113899231\n",
      "Epoch 5, Batch 175, Test Loss: 0.5150793790817261\n",
      "Epoch 5, Batch 176, Test Loss: 0.5697464942932129\n",
      "Epoch 5, Batch 177, Test Loss: 0.801265299320221\n",
      "Epoch 5, Batch 178, Test Loss: 0.5294516682624817\n",
      "Epoch 5, Batch 179, Test Loss: 0.5270635485649109\n",
      "Epoch 5, Batch 180, Test Loss: 0.6323067545890808\n",
      "Epoch 5, Batch 181, Test Loss: 0.6968395113945007\n",
      "Epoch 5, Batch 182, Test Loss: 0.7638169527053833\n",
      "Epoch 5, Batch 183, Test Loss: 0.621552050113678\n",
      "Epoch 5, Batch 184, Test Loss: 0.5518970489501953\n",
      "Epoch 5, Batch 185, Test Loss: 0.49629122018814087\n",
      "Epoch 5, Batch 186, Test Loss: 0.6288703680038452\n",
      "Epoch 5, Batch 187, Test Loss: 0.8061618208885193\n",
      "Epoch 5, Batch 188, Test Loss: 0.606242299079895\n",
      "Epoch 5, Batch 189, Test Loss: 0.6362531781196594\n",
      "Epoch 5, Batch 190, Test Loss: 0.6165335178375244\n",
      "Epoch 5, Batch 191, Test Loss: 0.7720727324485779\n",
      "Epoch 5, Batch 192, Test Loss: 0.7975473999977112\n",
      "Epoch 5, Batch 193, Test Loss: 0.7606544494628906\n",
      "Epoch 5, Batch 194, Test Loss: 0.6110684871673584\n",
      "Epoch 5, Batch 195, Test Loss: 0.4695712924003601\n",
      "Epoch 5, Batch 196, Test Loss: 0.5321729183197021\n",
      "Epoch 5, Batch 197, Test Loss: 0.5663513541221619\n",
      "Epoch 5, Batch 198, Test Loss: 0.7213755249977112\n",
      "Epoch 5, Batch 199, Test Loss: 0.7829784154891968\n",
      "Epoch 5, Batch 200, Test Loss: 0.6248154640197754\n",
      "Epoch 5, Batch 201, Test Loss: 0.507625162601471\n",
      "Epoch 5, Batch 202, Test Loss: 0.5663375854492188\n",
      "Epoch 5, Batch 203, Test Loss: 0.5930965542793274\n",
      "Epoch 5, Batch 204, Test Loss: 0.7180873155593872\n",
      "Epoch 5, Batch 205, Test Loss: 0.6222672462463379\n",
      "Epoch 5, Batch 206, Test Loss: 0.5590937733650208\n",
      "Epoch 5, Batch 207, Test Loss: 0.5352193713188171\n",
      "Epoch 5, Batch 208, Test Loss: 0.6107193827629089\n",
      "Epoch 5, Batch 209, Test Loss: 0.49041077494621277\n",
      "Epoch 5, Batch 210, Test Loss: 0.693922758102417\n",
      "Epoch 5, Batch 211, Test Loss: 0.9889252185821533\n",
      "Epoch 5, Batch 212, Test Loss: 0.7130858898162842\n",
      "Epoch 5, Batch 213, Test Loss: 0.4621995985507965\n",
      "Epoch 5, Batch 214, Test Loss: 0.46708792448043823\n",
      "Epoch 5, Batch 215, Test Loss: 0.794433057308197\n",
      "Epoch 5, Batch 216, Test Loss: 0.761976420879364\n",
      "Epoch 5, Batch 217, Test Loss: 0.4652269184589386\n",
      "Epoch 5, Batch 218, Test Loss: 0.6160286664962769\n",
      "Epoch 5, Batch 219, Test Loss: 0.5573701858520508\n",
      "Epoch 5, Batch 220, Test Loss: 0.7815748453140259\n",
      "Epoch 5, Batch 221, Test Loss: 0.686968207359314\n",
      "Epoch 5, Batch 222, Test Loss: 0.4462243616580963\n",
      "Epoch 5, Batch 223, Test Loss: 0.7081519365310669\n",
      "Epoch 5, Batch 224, Test Loss: 0.7149767875671387\n",
      "Epoch 5, Batch 225, Test Loss: 0.5634110569953918\n",
      "Epoch 5, Batch 226, Test Loss: 0.7051619291305542\n",
      "Epoch 5, Batch 227, Test Loss: 0.5429893136024475\n",
      "Epoch 5, Batch 228, Test Loss: 0.8156954050064087\n",
      "Epoch 5, Batch 229, Test Loss: 0.6632059812545776\n",
      "Epoch 5, Batch 230, Test Loss: 0.6552082300186157\n",
      "Epoch 5, Batch 231, Test Loss: 0.6768155694007874\n",
      "Epoch 5, Batch 232, Test Loss: 0.6980757713317871\n",
      "Epoch 5, Batch 233, Test Loss: 0.5790965557098389\n",
      "Epoch 5, Batch 234, Test Loss: 0.6726746559143066\n",
      "Epoch 5, Batch 235, Test Loss: 0.6991804838180542\n",
      "Epoch 5, Batch 236, Test Loss: 0.5381555557250977\n",
      "Epoch 5, Batch 237, Test Loss: 0.7195960879325867\n",
      "Epoch 5, Batch 238, Test Loss: 0.6752174496650696\n",
      "Epoch 5, Batch 239, Test Loss: 0.741054356098175\n",
      "Epoch 5, Batch 240, Test Loss: 0.7771547436714172\n",
      "Epoch 5, Batch 241, Test Loss: 0.5945762395858765\n",
      "Epoch 5, Batch 242, Test Loss: 0.6679863929748535\n",
      "Epoch 5, Batch 243, Test Loss: 0.5192474722862244\n",
      "Epoch 5, Batch 244, Test Loss: 0.5823881030082703\n",
      "Epoch 5, Batch 245, Test Loss: 0.6490899920463562\n",
      "Epoch 5, Batch 246, Test Loss: 0.5725580453872681\n",
      "Epoch 5, Batch 247, Test Loss: 0.4857686460018158\n",
      "Epoch 5, Batch 248, Test Loss: 0.7577434182167053\n",
      "Epoch 5, Batch 249, Test Loss: 0.6541426777839661\n",
      "Epoch 5, Batch 250, Test Loss: 0.655802309513092\n",
      "Epoch 5, Batch 251, Test Loss: 0.5827036499977112\n",
      "Epoch 5, Batch 252, Test Loss: 0.7692553400993347\n",
      "Epoch 5, Batch 253, Test Loss: 0.5960209369659424\n",
      "Epoch 5, Batch 254, Test Loss: 0.6987242698669434\n",
      "Epoch 5, Batch 255, Test Loss: 0.709319531917572\n",
      "Epoch 5, Batch 256, Test Loss: 0.5747100114822388\n",
      "Epoch 5, Batch 257, Test Loss: 0.5371799468994141\n",
      "Epoch 5, Batch 258, Test Loss: 0.5163465142250061\n",
      "Epoch 5, Batch 259, Test Loss: 0.7098175287246704\n",
      "Epoch 5, Batch 260, Test Loss: 0.6593397259712219\n",
      "Epoch 5, Batch 261, Test Loss: 0.5149406790733337\n",
      "Epoch 5, Batch 262, Test Loss: 0.49360865354537964\n",
      "Epoch 5, Batch 263, Test Loss: 0.5729284286499023\n",
      "Epoch 5, Batch 264, Test Loss: 0.646784245967865\n",
      "Epoch 5, Batch 265, Test Loss: 0.5644198060035706\n",
      "Epoch 5, Batch 266, Test Loss: 0.5793072581291199\n",
      "Epoch 5, Batch 267, Test Loss: 0.5503045916557312\n",
      "Epoch 5, Batch 268, Test Loss: 0.5712182521820068\n",
      "Epoch 5, Batch 269, Test Loss: 0.5211928486824036\n",
      "Epoch 5, Batch 270, Test Loss: 0.6544839143753052\n",
      "Epoch 5, Batch 271, Test Loss: 0.7804793119430542\n",
      "Epoch 5, Batch 272, Test Loss: 0.6497200727462769\n",
      "Epoch 5, Batch 273, Test Loss: 0.6262848973274231\n",
      "Epoch 5, Batch 274, Test Loss: 0.5642575025558472\n",
      "Epoch 5, Batch 275, Test Loss: 0.5052269697189331\n",
      "Epoch 5, Batch 276, Test Loss: 0.691480278968811\n",
      "Epoch 5, Batch 277, Test Loss: 0.47666049003601074\n",
      "Epoch 5, Batch 278, Test Loss: 0.8299077153205872\n",
      "Epoch 5, Batch 279, Test Loss: 0.5356301665306091\n",
      "Epoch 5, Batch 280, Test Loss: 0.5351627469062805\n",
      "Epoch 5, Batch 281, Test Loss: 0.5473771095275879\n",
      "Epoch 5, Batch 282, Test Loss: 0.6453638672828674\n",
      "Epoch 5, Batch 283, Test Loss: 0.606022298336029\n",
      "Epoch 5, Batch 284, Test Loss: 0.5630037188529968\n",
      "Epoch 5, Batch 285, Test Loss: 0.6827832460403442\n",
      "Epoch 5, Batch 286, Test Loss: 0.6931608319282532\n",
      "Epoch 5, Batch 287, Test Loss: 0.5266615152359009\n",
      "Epoch 5, Batch 288, Test Loss: 0.525820255279541\n",
      "Epoch 5, Batch 289, Test Loss: 0.6288096904754639\n",
      "Epoch 5, Batch 290, Test Loss: 0.5908997058868408\n",
      "Epoch 5, Batch 291, Test Loss: 0.5619165897369385\n",
      "Epoch 5, Batch 292, Test Loss: 0.7665135860443115\n",
      "Epoch 5, Batch 293, Test Loss: 0.5880883932113647\n",
      "Epoch 5, Batch 294, Test Loss: 0.5816547274589539\n",
      "Epoch 5, Batch 295, Test Loss: 0.5121049880981445\n",
      "Epoch 5, Batch 296, Test Loss: 0.5681167840957642\n",
      "Epoch 5, Batch 297, Test Loss: 0.7678414583206177\n",
      "Epoch 5, Batch 298, Test Loss: 0.5134885907173157\n",
      "Epoch 5, Batch 299, Test Loss: 0.6095066070556641\n",
      "Epoch 5, Batch 300, Test Loss: 0.5350490212440491\n",
      "Epoch 5, Batch 301, Test Loss: 0.5115077495574951\n",
      "Epoch 5, Batch 302, Test Loss: 0.5256257057189941\n",
      "Epoch 5, Batch 303, Test Loss: 0.7464780807495117\n",
      "Epoch 5, Batch 304, Test Loss: 0.6715152263641357\n",
      "Epoch 5, Batch 305, Test Loss: 0.772100567817688\n",
      "Epoch 5, Batch 306, Test Loss: 0.7077041864395142\n",
      "Epoch 5, Batch 307, Test Loss: 0.6948089599609375\n",
      "Epoch 5, Batch 308, Test Loss: 0.6704153418540955\n",
      "Epoch 5, Batch 309, Test Loss: 0.7049382328987122\n",
      "Epoch 5, Batch 310, Test Loss: 0.5186071395874023\n",
      "Epoch 5, Batch 311, Test Loss: 0.6304423809051514\n",
      "Epoch 5, Batch 312, Test Loss: 0.5062271356582642\n",
      "Epoch 5, Batch 313, Test Loss: 0.6250149011611938\n",
      "Epoch 5, Batch 314, Test Loss: 0.6451923847198486\n",
      "Epoch 5, Batch 315, Test Loss: 0.5343245267868042\n",
      "Epoch 5, Batch 316, Test Loss: 0.9928235411643982\n",
      "Epoch 5, Batch 317, Test Loss: 0.7490002512931824\n",
      "Epoch 5, Batch 318, Test Loss: 0.6450597643852234\n",
      "Epoch 5, Batch 319, Test Loss: 0.5126391053199768\n",
      "Epoch 5, Batch 320, Test Loss: 0.6024293899536133\n",
      "Epoch 5, Batch 321, Test Loss: 0.6208952069282532\n",
      "Epoch 5, Batch 322, Test Loss: 0.6211678981781006\n",
      "Epoch 5, Batch 323, Test Loss: 0.6830064654350281\n",
      "Epoch 5, Batch 324, Test Loss: 0.6715255379676819\n",
      "Epoch 5, Batch 325, Test Loss: 0.6223330497741699\n",
      "Epoch 5, Batch 326, Test Loss: 0.49621111154556274\n",
      "Epoch 5, Batch 327, Test Loss: 0.5322527289390564\n",
      "Epoch 5, Batch 328, Test Loss: 0.44924023747444153\n",
      "Epoch 5, Batch 329, Test Loss: 0.649118959903717\n",
      "Epoch 5, Batch 330, Test Loss: 0.6910743713378906\n",
      "Epoch 5, Batch 331, Test Loss: 0.6056212782859802\n",
      "Epoch 5, Batch 332, Test Loss: 0.593523383140564\n",
      "Epoch 5, Batch 333, Test Loss: 0.7030022144317627\n",
      "Epoch 5, Batch 334, Test Loss: 0.4889848828315735\n",
      "Epoch 5, Batch 335, Test Loss: 0.5449765920639038\n",
      "Epoch 5, Batch 336, Test Loss: 0.5467577576637268\n",
      "Epoch 5, Batch 337, Test Loss: 0.5634305477142334\n",
      "Epoch 5, Batch 338, Test Loss: 0.7596786618232727\n",
      "Epoch 5, Batch 339, Test Loss: 0.6647789478302002\n",
      "Epoch 5, Batch 340, Test Loss: 0.715543270111084\n",
      "Epoch 5, Batch 341, Test Loss: 0.49602290987968445\n",
      "Epoch 5, Batch 342, Test Loss: 0.5083245038986206\n",
      "Epoch 5, Batch 343, Test Loss: 0.6037826538085938\n",
      "Epoch 5, Batch 344, Test Loss: 0.7121572494506836\n",
      "Epoch 5, Batch 345, Test Loss: 0.43926653265953064\n",
      "Epoch 5, Batch 346, Test Loss: 0.5051665902137756\n",
      "Epoch 5, Batch 347, Test Loss: 0.5483365058898926\n",
      "Epoch 5, Batch 348, Test Loss: 0.5517552495002747\n",
      "Epoch 5, Batch 349, Test Loss: 0.5259628891944885\n",
      "Epoch 5, Batch 350, Test Loss: 0.6572560667991638\n",
      "Epoch 5, Batch 351, Test Loss: 0.4468594789505005\n",
      "Epoch 5, Batch 352, Test Loss: 0.5875787138938904\n",
      "Epoch 5, Batch 353, Test Loss: 0.7341055274009705\n",
      "Epoch 5, Batch 354, Test Loss: 0.6502602100372314\n",
      "Epoch 5, Batch 355, Test Loss: 0.5349262952804565\n",
      "Epoch 5, Batch 356, Test Loss: 0.4282788932323456\n",
      "Epoch 5, Batch 357, Test Loss: 0.4412676990032196\n",
      "Epoch 5, Batch 358, Test Loss: 0.4630786180496216\n",
      "Epoch 5, Batch 359, Test Loss: 0.7524713277816772\n",
      "Epoch 5, Batch 360, Test Loss: 0.5399515628814697\n",
      "Epoch 5, Batch 361, Test Loss: 0.5561264753341675\n",
      "Epoch 5, Batch 362, Test Loss: 0.5242969989776611\n",
      "Epoch 5, Batch 363, Test Loss: 0.5766749978065491\n",
      "Epoch 5, Batch 364, Test Loss: 0.6757289171218872\n",
      "Epoch 5, Batch 365, Test Loss: 0.4795149862766266\n",
      "Epoch 5, Batch 366, Test Loss: 0.6537577509880066\n",
      "Epoch 5, Batch 367, Test Loss: 0.6116963028907776\n",
      "Epoch 5, Batch 368, Test Loss: 0.4285542964935303\n",
      "Epoch 5, Batch 369, Test Loss: 0.7222050428390503\n",
      "Epoch 5, Batch 370, Test Loss: 0.5948862433433533\n",
      "Epoch 5, Batch 371, Test Loss: 0.6101481914520264\n",
      "Epoch 5, Batch 372, Test Loss: 0.5381940007209778\n",
      "Epoch 5, Batch 373, Test Loss: 0.44997596740722656\n",
      "Epoch 5, Batch 374, Test Loss: 0.5593745708465576\n",
      "Epoch 5, Batch 375, Test Loss: 0.5749362707138062\n",
      "Epoch 5, Batch 376, Test Loss: 0.4365014135837555\n",
      "Epoch 5, Batch 377, Test Loss: 0.46653082966804504\n",
      "Epoch 5, Batch 378, Test Loss: 0.559100866317749\n",
      "Epoch 5, Batch 379, Test Loss: 0.5058180689811707\n",
      "Epoch 5, Batch 380, Test Loss: 0.6360143423080444\n",
      "Epoch 5, Batch 381, Test Loss: 0.5417333245277405\n",
      "Epoch 5, Batch 382, Test Loss: 0.407001256942749\n",
      "Epoch 5, Batch 383, Test Loss: 0.442934513092041\n",
      "Epoch 5, Batch 384, Test Loss: 0.5683178305625916\n",
      "Epoch 5, Batch 385, Test Loss: 0.4735681712627411\n",
      "Epoch 5, Batch 386, Test Loss: 0.5653632879257202\n",
      "Epoch 5, Batch 387, Test Loss: 0.7483482360839844\n",
      "Epoch 5, Batch 388, Test Loss: 0.6751108765602112\n",
      "Epoch 5, Batch 389, Test Loss: 0.8025200366973877\n",
      "Epoch 5, Batch 390, Test Loss: 0.7544403076171875\n",
      "Epoch 5, Batch 391, Test Loss: 0.6372604370117188\n",
      "Epoch 5, Batch 392, Test Loss: 0.8654739856719971\n",
      "Epoch 5, Batch 393, Test Loss: 0.5104724168777466\n",
      "Epoch 5, Batch 394, Test Loss: 0.7120684385299683\n",
      "Epoch 5, Batch 395, Test Loss: 0.6957554817199707\n",
      "Epoch 5, Batch 396, Test Loss: 0.7776275873184204\n",
      "Epoch 5, Batch 397, Test Loss: 0.7322615385055542\n",
      "Epoch 5, Batch 398, Test Loss: 0.600775420665741\n",
      "Epoch 5, Batch 399, Test Loss: 0.6562803387641907\n",
      "Epoch 5, Batch 400, Test Loss: 0.5261367559432983\n",
      "Epoch 5, Batch 401, Test Loss: 0.5335099697113037\n",
      "Epoch 5, Batch 402, Test Loss: 0.5627546310424805\n",
      "Epoch 5, Batch 403, Test Loss: 0.7329325079917908\n",
      "Epoch 5, Batch 404, Test Loss: 0.47465091943740845\n",
      "Epoch 5, Batch 405, Test Loss: 0.5045201778411865\n",
      "Epoch 5, Batch 406, Test Loss: 0.5149849653244019\n",
      "Epoch 5, Batch 407, Test Loss: 0.4977457821369171\n",
      "Epoch 5, Batch 408, Test Loss: 0.7922539114952087\n",
      "Epoch 5, Batch 409, Test Loss: 0.5072458386421204\n",
      "Epoch 5, Batch 410, Test Loss: 0.818947434425354\n",
      "Epoch 5, Batch 411, Test Loss: 0.5801042914390564\n",
      "Epoch 5, Batch 412, Test Loss: 0.5703829526901245\n",
      "Epoch 5, Batch 413, Test Loss: 0.6360806822776794\n",
      "Epoch 5, Batch 414, Test Loss: 0.3618803024291992\n",
      "Epoch 5, Batch 415, Test Loss: 0.47127479314804077\n",
      "Epoch 5, Batch 416, Test Loss: 0.5278290510177612\n",
      "Epoch 5, Batch 417, Test Loss: 0.5212854146957397\n",
      "Epoch 5, Batch 418, Test Loss: 0.6337025761604309\n",
      "Epoch 5, Batch 419, Test Loss: 0.3858271539211273\n",
      "Epoch 5, Batch 420, Test Loss: 0.5007269382476807\n",
      "Epoch 5, Batch 421, Test Loss: 0.7148517966270447\n",
      "Epoch 5, Batch 422, Test Loss: 0.6032814383506775\n",
      "Epoch 5, Batch 423, Test Loss: 0.6068301796913147\n",
      "Epoch 5, Batch 424, Test Loss: 1.032802939414978\n",
      "Epoch 5, Batch 425, Test Loss: 0.673681378364563\n",
      "Epoch 5, Batch 426, Test Loss: 0.7108333110809326\n",
      "Epoch 5, Batch 427, Test Loss: 0.6227602362632751\n",
      "Epoch 5, Batch 428, Test Loss: 0.5650745630264282\n",
      "Epoch 5, Batch 429, Test Loss: 0.565329909324646\n",
      "Epoch 5, Batch 430, Test Loss: 0.45332175493240356\n",
      "Epoch 5, Batch 431, Test Loss: 0.7020437121391296\n",
      "Epoch 5, Batch 432, Test Loss: 0.5948954224586487\n",
      "Epoch 5, Batch 433, Test Loss: 0.6071493625640869\n",
      "Epoch 5, Batch 434, Test Loss: 0.5971856117248535\n",
      "Epoch 5, Batch 435, Test Loss: 0.7887005805969238\n",
      "Epoch 5, Batch 436, Test Loss: 0.5900211930274963\n",
      "Epoch 5, Batch 437, Test Loss: 0.6449299454689026\n",
      "Epoch 5, Batch 438, Test Loss: 0.5520972609519958\n",
      "Epoch 5, Batch 439, Test Loss: 0.6394429802894592\n",
      "Epoch 5, Batch 440, Test Loss: 0.6956709027290344\n",
      "Epoch 5, Batch 441, Test Loss: 0.5657081007957458\n",
      "Epoch 5, Batch 442, Test Loss: 0.5069146156311035\n",
      "Epoch 5, Batch 443, Test Loss: 0.5263463854789734\n",
      "Epoch 5, Batch 444, Test Loss: 0.5067058205604553\n",
      "Epoch 5, Batch 445, Test Loss: 0.4074106216430664\n",
      "Epoch 5, Batch 446, Test Loss: 0.5179564952850342\n",
      "Epoch 5, Batch 447, Test Loss: 0.6812260150909424\n",
      "Epoch 5, Batch 448, Test Loss: 0.6302748322486877\n",
      "Epoch 5, Batch 449, Test Loss: 0.5522497892379761\n",
      "Epoch 5, Batch 450, Test Loss: 0.6508491039276123\n",
      "Epoch 5, Batch 451, Test Loss: 0.4936206340789795\n",
      "Epoch 5, Batch 452, Test Loss: 0.6948188543319702\n",
      "Epoch 5, Batch 453, Test Loss: 0.5544975996017456\n",
      "Epoch 5, Batch 454, Test Loss: 0.7100154161453247\n",
      "Epoch 5, Batch 455, Test Loss: 0.6240391135215759\n",
      "Epoch 5, Batch 456, Test Loss: 0.5687859654426575\n",
      "Epoch 5, Batch 457, Test Loss: 0.5453404188156128\n",
      "Epoch 5, Batch 458, Test Loss: 0.3267862796783447\n",
      "Epoch 5, Batch 459, Test Loss: 0.6388520002365112\n",
      "Epoch 5, Batch 460, Test Loss: 0.8855447173118591\n",
      "Epoch 5, Batch 461, Test Loss: 0.6558422446250916\n",
      "Epoch 5, Batch 462, Test Loss: 0.8535258769989014\n",
      "Epoch 5, Batch 463, Test Loss: 0.7116326689720154\n",
      "Epoch 5, Batch 464, Test Loss: 0.6476088166236877\n",
      "Epoch 5, Batch 465, Test Loss: 0.49375107884407043\n",
      "Epoch 5, Batch 466, Test Loss: 0.5542522668838501\n",
      "Epoch 5, Batch 467, Test Loss: 0.49988019466400146\n",
      "Epoch 5, Batch 468, Test Loss: 0.8664496541023254\n",
      "Epoch 5, Batch 469, Test Loss: 0.5570881962776184\n",
      "Epoch 5, Batch 470, Test Loss: 0.37638816237449646\n",
      "Epoch 5, Batch 471, Test Loss: 0.6067025661468506\n",
      "Epoch 5, Batch 472, Test Loss: 0.5544863343238831\n",
      "Epoch 5, Batch 473, Test Loss: 0.6432578563690186\n",
      "Epoch 5, Batch 474, Test Loss: 0.5720922350883484\n",
      "Epoch 5, Batch 475, Test Loss: 0.4278584122657776\n",
      "Epoch 5, Batch 476, Test Loss: 0.553004264831543\n",
      "Epoch 5, Batch 477, Test Loss: 0.49141252040863037\n",
      "Epoch 5, Batch 478, Test Loss: 0.6233566999435425\n",
      "Epoch 5, Batch 479, Test Loss: 0.6530107259750366\n",
      "Epoch 5, Batch 480, Test Loss: 0.4930521845817566\n",
      "Epoch 5, Batch 481, Test Loss: 0.49518516659736633\n",
      "Epoch 5, Batch 482, Test Loss: 0.48589855432510376\n",
      "Epoch 5, Batch 483, Test Loss: 0.6027664542198181\n",
      "Epoch 5, Batch 484, Test Loss: 0.580426812171936\n",
      "Epoch 5, Batch 485, Test Loss: 0.6175050139427185\n",
      "Epoch 5, Batch 486, Test Loss: 0.568200945854187\n",
      "Epoch 5, Batch 487, Test Loss: 0.6619821190834045\n",
      "Epoch 5, Batch 488, Test Loss: 0.6911184191703796\n",
      "Epoch 5, Batch 489, Test Loss: 0.5436520576477051\n",
      "Epoch 5, Batch 490, Test Loss: 0.5126689076423645\n",
      "Epoch 5, Batch 491, Test Loss: 0.5996422171592712\n",
      "Epoch 5, Batch 492, Test Loss: 0.6178200244903564\n",
      "Epoch 5, Batch 493, Test Loss: 0.6276460886001587\n",
      "Epoch 5, Batch 494, Test Loss: 0.8194610476493835\n",
      "Epoch 5, Batch 495, Test Loss: 0.6919504404067993\n",
      "Epoch 5, Batch 496, Test Loss: 0.7060720920562744\n",
      "Epoch 5, Batch 497, Test Loss: 0.5258780121803284\n",
      "Epoch 5, Batch 498, Test Loss: 0.5862513780593872\n",
      "Epoch 5, Batch 499, Test Loss: 0.6481719017028809\n",
      "Epoch 5, Batch 500, Test Loss: 0.5875871777534485\n",
      "Epoch 5, Batch 501, Test Loss: 0.46824678778648376\n",
      "Epoch 5, Batch 502, Test Loss: 0.6521218419075012\n",
      "Epoch 5, Batch 503, Test Loss: 0.6661373972892761\n",
      "Epoch 5, Batch 504, Test Loss: 0.48783913254737854\n",
      "Epoch 5, Batch 505, Test Loss: 0.5740160942077637\n",
      "Epoch 5, Batch 506, Test Loss: 0.6533926725387573\n",
      "Epoch 5, Batch 507, Test Loss: 0.4895830452442169\n",
      "Epoch 5, Batch 508, Test Loss: 0.5959429740905762\n",
      "Epoch 5, Batch 509, Test Loss: 0.5197229981422424\n",
      "Epoch 5, Batch 510, Test Loss: 0.6875662803649902\n",
      "Epoch 5, Batch 511, Test Loss: 0.5177568197250366\n",
      "Epoch 5, Batch 512, Test Loss: 0.6851292252540588\n",
      "Epoch 5, Batch 513, Test Loss: 0.6373821496963501\n",
      "Epoch 5, Batch 514, Test Loss: 0.6505283713340759\n",
      "Epoch 5, Batch 515, Test Loss: 0.7176651954650879\n",
      "Epoch 5, Batch 516, Test Loss: 0.6078797578811646\n",
      "Epoch 5, Batch 517, Test Loss: 0.5944677591323853\n",
      "Epoch 5, Batch 518, Test Loss: 0.6224599480628967\n",
      "Epoch 5, Batch 519, Test Loss: 0.7273289561271667\n",
      "Epoch 5, Batch 520, Test Loss: 0.7021063566207886\n",
      "Epoch 5, Batch 521, Test Loss: 0.7137186527252197\n",
      "Epoch 5, Batch 522, Test Loss: 0.7079453468322754\n",
      "Epoch 5, Batch 523, Test Loss: 0.6757980585098267\n",
      "Epoch 5, Batch 524, Test Loss: 0.46930378675460815\n",
      "Epoch 5, Batch 525, Test Loss: 0.7451061010360718\n",
      "Epoch 5, Batch 526, Test Loss: 0.5309933423995972\n",
      "Epoch 5, Batch 527, Test Loss: 0.7166458368301392\n",
      "Epoch 5, Batch 528, Test Loss: 0.5468953251838684\n",
      "Epoch 5, Batch 529, Test Loss: 0.9567599296569824\n",
      "Epoch 5, Batch 530, Test Loss: 0.6776739954948425\n",
      "Epoch 5, Batch 531, Test Loss: 0.6731451749801636\n",
      "Epoch 5, Batch 532, Test Loss: 0.732280969619751\n",
      "Epoch 5, Batch 533, Test Loss: 0.39315858483314514\n",
      "Epoch 5, Batch 534, Test Loss: 0.6005794405937195\n",
      "Epoch 5, Batch 535, Test Loss: 0.6636218428611755\n",
      "Epoch 5, Batch 536, Test Loss: 0.5359747409820557\n",
      "Epoch 5, Batch 537, Test Loss: 0.5758967995643616\n",
      "Epoch 5, Batch 538, Test Loss: 0.7557319402694702\n",
      "Epoch 5, Batch 539, Test Loss: 0.6757543683052063\n",
      "Epoch 5, Batch 540, Test Loss: 0.48530814051628113\n",
      "Epoch 5, Batch 541, Test Loss: 0.507916271686554\n",
      "Epoch 5, Batch 542, Test Loss: 0.6253268718719482\n",
      "Epoch 5, Batch 543, Test Loss: 0.6263217329978943\n",
      "Epoch 5, Batch 544, Test Loss: 0.4678599238395691\n",
      "Epoch 5, Batch 545, Test Loss: 0.6841151714324951\n",
      "Epoch 5, Batch 546, Test Loss: 0.7079172134399414\n",
      "Epoch 5, Batch 547, Test Loss: 0.5606428384780884\n",
      "Epoch 5, Batch 548, Test Loss: 0.6157163381576538\n",
      "Epoch 5, Batch 549, Test Loss: 0.5217304825782776\n",
      "Epoch 5, Batch 550, Test Loss: 0.45377612113952637\n",
      "Epoch 5, Batch 551, Test Loss: 0.5971901416778564\n",
      "Epoch 5, Batch 552, Test Loss: 0.6285020709037781\n",
      "Epoch 5, Batch 553, Test Loss: 0.871786892414093\n",
      "Epoch 5, Batch 554, Test Loss: 0.526104211807251\n",
      "Epoch 5, Batch 555, Test Loss: 0.6658372282981873\n",
      "Epoch 5, Batch 556, Test Loss: 0.6417266130447388\n",
      "Epoch 5, Batch 557, Test Loss: 0.5931594371795654\n",
      "Epoch 5, Batch 558, Test Loss: 0.612838625907898\n",
      "Epoch 5, Batch 559, Test Loss: 0.5163707733154297\n",
      "Epoch 5, Batch 560, Test Loss: 0.614656388759613\n",
      "Epoch 5, Batch 561, Test Loss: 0.6568478941917419\n",
      "Epoch 5, Batch 562, Test Loss: 0.5158274173736572\n",
      "Epoch 5, Batch 563, Test Loss: 0.5779479742050171\n",
      "Epoch 5, Batch 564, Test Loss: 0.6095905303955078\n",
      "Epoch 5, Batch 565, Test Loss: 0.5334562659263611\n",
      "Epoch 5, Batch 566, Test Loss: 0.6320342421531677\n",
      "Epoch 5, Batch 567, Test Loss: 0.513285756111145\n",
      "Epoch 5, Batch 568, Test Loss: 0.480304092168808\n",
      "Epoch 5, Batch 569, Test Loss: 0.6666551828384399\n",
      "Epoch 5, Batch 570, Test Loss: 0.5273337364196777\n",
      "Epoch 5, Batch 571, Test Loss: 0.8957739472389221\n",
      "Epoch 5, Batch 572, Test Loss: 0.6679635643959045\n",
      "Epoch 5, Batch 573, Test Loss: 0.679003894329071\n",
      "Epoch 5, Batch 574, Test Loss: 0.6115216612815857\n",
      "Epoch 5, Batch 575, Test Loss: 0.5519485473632812\n",
      "Epoch 5, Batch 576, Test Loss: 0.7009353637695312\n",
      "Epoch 5, Batch 577, Test Loss: 0.5793401598930359\n",
      "Epoch 5, Batch 578, Test Loss: 0.5585370659828186\n",
      "Epoch 5, Batch 579, Test Loss: 0.5890190601348877\n",
      "Epoch 5, Batch 580, Test Loss: 0.7094318270683289\n",
      "Epoch 5, Batch 581, Test Loss: 0.43353158235549927\n",
      "Epoch 5, Batch 582, Test Loss: 0.7156394720077515\n",
      "Epoch 5, Batch 583, Test Loss: 0.6559644937515259\n",
      "Epoch 5, Batch 584, Test Loss: 0.6767193078994751\n",
      "Epoch 5, Batch 585, Test Loss: 0.7646642327308655\n",
      "Epoch 5, Batch 586, Test Loss: 0.6844392418861389\n",
      "Epoch 5, Batch 587, Test Loss: 0.5600191950798035\n",
      "Epoch 5, Batch 588, Test Loss: 0.8504934310913086\n",
      "Epoch 5, Batch 589, Test Loss: 0.3408351540565491\n",
      "Epoch 5, Batch 590, Test Loss: 0.6330376267433167\n",
      "Epoch 5, Batch 591, Test Loss: 0.6827071309089661\n",
      "Epoch 5, Batch 592, Test Loss: 0.551581859588623\n",
      "Epoch 5, Batch 593, Test Loss: 0.6611860990524292\n",
      "Epoch 5, Batch 594, Test Loss: 0.38614213466644287\n",
      "Epoch 5, Batch 595, Test Loss: 0.5994280576705933\n",
      "Epoch 5, Batch 596, Test Loss: 0.5022810697555542\n",
      "Epoch 5, Batch 597, Test Loss: 0.47992849349975586\n",
      "Epoch 5, Batch 598, Test Loss: 0.6791524887084961\n",
      "Epoch 5, Batch 599, Test Loss: 0.6012299060821533\n",
      "Epoch 5, Batch 600, Test Loss: 0.651833176612854\n",
      "Epoch 5, Batch 601, Test Loss: 0.5530598759651184\n",
      "Epoch 5, Batch 602, Test Loss: 0.8726730346679688\n",
      "Epoch 5, Batch 603, Test Loss: 0.6127253770828247\n",
      "Epoch 5, Batch 604, Test Loss: 0.5828909277915955\n",
      "Epoch 5, Batch 605, Test Loss: 0.6561723947525024\n",
      "Epoch 5, Batch 606, Test Loss: 0.5779945850372314\n",
      "Epoch 5, Batch 607, Test Loss: 0.5780211091041565\n",
      "Epoch 5, Batch 608, Test Loss: 0.6058852076530457\n",
      "Epoch 5, Batch 609, Test Loss: 0.6119110584259033\n",
      "Epoch 5, Batch 610, Test Loss: 0.7148641347885132\n",
      "Epoch 5, Batch 611, Test Loss: 0.5412026643753052\n",
      "Epoch 5, Batch 612, Test Loss: 0.6365020275115967\n",
      "Epoch 5, Batch 613, Test Loss: 0.6225249767303467\n",
      "Epoch 5, Batch 614, Test Loss: 0.684325635433197\n",
      "Epoch 5, Batch 615, Test Loss: 0.7014394998550415\n",
      "Epoch 5, Batch 616, Test Loss: 0.7080310583114624\n",
      "Epoch 5, Batch 617, Test Loss: 0.5033203959465027\n",
      "Epoch 5, Batch 618, Test Loss: 0.6329813003540039\n",
      "Epoch 5, Batch 619, Test Loss: 0.5765514373779297\n",
      "Epoch 5, Batch 620, Test Loss: 0.441507488489151\n",
      "Epoch 5, Batch 621, Test Loss: 0.6755489110946655\n",
      "Epoch 5, Batch 622, Test Loss: 0.5771945714950562\n",
      "Epoch 5, Batch 623, Test Loss: 0.45859530568122864\n",
      "Epoch 5, Batch 624, Test Loss: 0.5191537141799927\n",
      "Epoch 5, Batch 625, Test Loss: 0.6267806887626648\n",
      "Epoch 5, Batch 626, Test Loss: 0.5538122057914734\n",
      "Epoch 5, Batch 627, Test Loss: 0.4264620840549469\n",
      "Epoch 5, Batch 628, Test Loss: 0.45593076944351196\n",
      "Epoch 5, Batch 629, Test Loss: 0.4892221689224243\n",
      "Epoch 5, Batch 630, Test Loss: 0.7756288051605225\n",
      "Epoch 5, Batch 631, Test Loss: 0.5415013432502747\n",
      "Epoch 5, Batch 632, Test Loss: 0.6467552185058594\n",
      "Epoch 5, Batch 633, Test Loss: 0.506622314453125\n",
      "Epoch 5, Batch 634, Test Loss: 0.36186331510543823\n",
      "Epoch 5, Batch 635, Test Loss: 0.6230972409248352\n",
      "Epoch 5, Batch 636, Test Loss: 0.44655004143714905\n",
      "Epoch 5, Batch 637, Test Loss: 0.5728576183319092\n",
      "Epoch 5, Batch 638, Test Loss: 0.5066683292388916\n",
      "Epoch 5, Batch 639, Test Loss: 0.45682772994041443\n",
      "Epoch 5, Batch 640, Test Loss: 0.7022481560707092\n",
      "Epoch 5, Batch 641, Test Loss: 0.5803134441375732\n",
      "Epoch 5, Batch 642, Test Loss: 0.42152532935142517\n",
      "Epoch 5, Batch 643, Test Loss: 0.5081562995910645\n",
      "Epoch 5, Batch 644, Test Loss: 0.6652786135673523\n",
      "Epoch 5, Batch 645, Test Loss: 0.5761017203330994\n",
      "Epoch 5, Batch 646, Test Loss: 0.43408361077308655\n",
      "Epoch 5, Batch 647, Test Loss: 0.6059184670448303\n",
      "Epoch 5, Batch 648, Test Loss: 0.5528217554092407\n",
      "Epoch 5, Batch 649, Test Loss: 0.6672667264938354\n",
      "Epoch 5, Batch 650, Test Loss: 0.4471854269504547\n",
      "Epoch 5, Batch 651, Test Loss: 0.6908165216445923\n",
      "Epoch 5, Batch 652, Test Loss: 0.6384895443916321\n",
      "Epoch 5, Batch 653, Test Loss: 0.6109949946403503\n",
      "Epoch 5, Batch 654, Test Loss: 0.7457967400550842\n",
      "Epoch 5, Batch 655, Test Loss: 1.0312575101852417\n",
      "Epoch 5, Batch 656, Test Loss: 0.6163750290870667\n",
      "Epoch 5, Batch 657, Test Loss: 0.6323441863059998\n",
      "Epoch 5, Batch 658, Test Loss: 0.6399765014648438\n",
      "Epoch 5, Batch 659, Test Loss: 0.4426916837692261\n",
      "Epoch 5, Batch 660, Test Loss: 0.5982168316841125\n",
      "Epoch 5, Batch 661, Test Loss: 0.49397915601730347\n",
      "Epoch 5, Batch 662, Test Loss: 0.7112910747528076\n",
      "Epoch 5, Batch 663, Test Loss: 0.5605081915855408\n",
      "Epoch 5, Batch 664, Test Loss: 0.5355383157730103\n",
      "Epoch 5, Batch 665, Test Loss: 0.7790007591247559\n",
      "Epoch 5, Batch 666, Test Loss: 0.4497826099395752\n",
      "Epoch 5, Batch 667, Test Loss: 0.5570956468582153\n",
      "Epoch 5, Batch 668, Test Loss: 0.7152257561683655\n",
      "Epoch 5, Batch 669, Test Loss: 0.7111522555351257\n",
      "Epoch 5, Batch 670, Test Loss: 0.6360622048377991\n",
      "Epoch 5, Batch 671, Test Loss: 0.6559537649154663\n",
      "Epoch 5, Batch 672, Test Loss: 0.565939724445343\n",
      "Epoch 5, Batch 673, Test Loss: 0.5664193630218506\n",
      "Epoch 5, Batch 674, Test Loss: 0.7433153390884399\n",
      "Epoch 5, Batch 675, Test Loss: 0.5235063433647156\n",
      "Epoch 5, Batch 676, Test Loss: 0.4737313687801361\n",
      "Epoch 5, Batch 677, Test Loss: 0.5375924706459045\n",
      "Epoch 5, Batch 678, Test Loss: 0.6558863520622253\n",
      "Epoch 5, Batch 679, Test Loss: 0.4272046983242035\n",
      "Epoch 5, Batch 680, Test Loss: 0.49902328848838806\n",
      "Epoch 5, Batch 681, Test Loss: 0.4796110987663269\n",
      "Epoch 5, Batch 682, Test Loss: 0.7090308666229248\n",
      "Epoch 5, Batch 683, Test Loss: 0.7327134013175964\n",
      "Epoch 5, Batch 684, Test Loss: 0.639724612236023\n",
      "Epoch 5, Batch 685, Test Loss: 0.5723907351493835\n",
      "Epoch 5, Batch 686, Test Loss: 0.637776255607605\n",
      "Epoch 5, Batch 687, Test Loss: 0.6300458312034607\n",
      "Epoch 5, Batch 688, Test Loss: 0.6038253307342529\n",
      "Epoch 5, Batch 689, Test Loss: 0.6679712533950806\n",
      "Epoch 5, Batch 690, Test Loss: 0.5559203624725342\n",
      "Epoch 5, Batch 691, Test Loss: 0.6205847859382629\n",
      "Epoch 5, Batch 692, Test Loss: 0.7654068470001221\n",
      "Epoch 5, Batch 693, Test Loss: 0.5283734798431396\n",
      "Epoch 5, Batch 694, Test Loss: 0.44143766164779663\n",
      "Epoch 5, Batch 695, Test Loss: 0.6930266618728638\n",
      "Epoch 5, Batch 696, Test Loss: 0.7555543184280396\n",
      "Epoch 5, Batch 697, Test Loss: 0.5257595181465149\n",
      "Epoch 5, Batch 698, Test Loss: 0.528054416179657\n",
      "Epoch 5, Batch 699, Test Loss: 0.5700804591178894\n",
      "Epoch 5, Batch 700, Test Loss: 0.6334535479545593\n",
      "Epoch 5, Batch 701, Test Loss: 0.6793206334114075\n",
      "Epoch 5, Batch 702, Test Loss: 0.44652870297431946\n",
      "Epoch 5, Batch 703, Test Loss: 0.5456764698028564\n",
      "Epoch 5, Batch 704, Test Loss: 0.538159191608429\n",
      "Epoch 5, Batch 705, Test Loss: 0.7092980742454529\n",
      "Epoch 5, Batch 706, Test Loss: 0.5061450600624084\n",
      "Epoch 5, Batch 707, Test Loss: 0.5055542588233948\n",
      "Epoch 5, Batch 708, Test Loss: 0.4753216505050659\n",
      "Epoch 5, Batch 709, Test Loss: 0.7142576575279236\n",
      "Epoch 5, Batch 710, Test Loss: 0.35325005650520325\n",
      "Epoch 5, Batch 711, Test Loss: 0.4278147220611572\n",
      "Epoch 5, Batch 712, Test Loss: 0.5383205413818359\n",
      "Epoch 5, Batch 713, Test Loss: 0.5187093615531921\n",
      "Epoch 5, Batch 714, Test Loss: 0.6994025111198425\n",
      "Epoch 5, Batch 715, Test Loss: 0.6049007773399353\n",
      "Epoch 5, Batch 716, Test Loss: 0.47928422689437866\n",
      "Epoch 5, Batch 717, Test Loss: 0.7253459095954895\n",
      "Epoch 5, Batch 718, Test Loss: 0.46508723497390747\n",
      "Epoch 5, Batch 719, Test Loss: 0.5748114585876465\n",
      "Epoch 5, Batch 720, Test Loss: 0.7271963357925415\n",
      "Epoch 5, Batch 721, Test Loss: 0.9566866159439087\n",
      "Epoch 5, Batch 722, Test Loss: 0.7986202239990234\n",
      "Epoch 5, Batch 723, Test Loss: 0.7087439298629761\n",
      "Epoch 5, Batch 724, Test Loss: 0.6122419238090515\n",
      "Epoch 5, Batch 725, Test Loss: 0.5522328615188599\n",
      "Epoch 5, Batch 726, Test Loss: 0.49694886803627014\n",
      "Epoch 5, Batch 727, Test Loss: 0.5685451626777649\n",
      "Epoch 5, Batch 728, Test Loss: 0.416880339384079\n",
      "Epoch 5, Batch 729, Test Loss: 0.6808343529701233\n",
      "Epoch 5, Batch 730, Test Loss: 0.596747636795044\n",
      "Epoch 5, Batch 731, Test Loss: 0.7290652990341187\n",
      "Epoch 5, Batch 732, Test Loss: 0.6022171974182129\n",
      "Epoch 5, Batch 733, Test Loss: 0.6012202501296997\n",
      "Epoch 5, Batch 734, Test Loss: 0.4990408718585968\n",
      "Epoch 5, Batch 735, Test Loss: 0.7239174246788025\n",
      "Epoch 5, Batch 736, Test Loss: 0.5932766199111938\n",
      "Epoch 5, Batch 737, Test Loss: 0.738327145576477\n",
      "Epoch 5, Batch 738, Test Loss: 0.5114099383354187\n",
      "Epoch 5, Batch 739, Test Loss: 0.42356961965560913\n",
      "Epoch 5, Batch 740, Test Loss: 0.5612622499465942\n",
      "Epoch 5, Batch 741, Test Loss: 0.6555267572402954\n",
      "Epoch 5, Batch 742, Test Loss: 0.6679406762123108\n",
      "Epoch 5, Batch 743, Test Loss: 0.5994135141372681\n",
      "Epoch 5, Batch 744, Test Loss: 0.49402284622192383\n",
      "Epoch 5, Batch 745, Test Loss: 0.696968138217926\n",
      "Epoch 5, Batch 746, Test Loss: 0.4478866755962372\n",
      "Epoch 5, Batch 747, Test Loss: 0.5418015718460083\n",
      "Epoch 5, Batch 748, Test Loss: 0.6814399361610413\n",
      "Epoch 5, Batch 749, Test Loss: 0.5550467371940613\n",
      "Epoch 5, Batch 750, Test Loss: 0.5363264083862305\n",
      "Epoch 5, Batch 751, Test Loss: 0.5210826992988586\n",
      "Epoch 5, Batch 752, Test Loss: 0.5649567246437073\n",
      "Epoch 5, Batch 753, Test Loss: 0.6721928119659424\n",
      "Epoch 5, Batch 754, Test Loss: 0.6400296092033386\n",
      "Epoch 5, Batch 755, Test Loss: 0.5550602078437805\n",
      "Epoch 5, Batch 756, Test Loss: 0.6948639750480652\n",
      "Epoch 5, Batch 757, Test Loss: 0.4892033338546753\n",
      "Epoch 5, Batch 758, Test Loss: 0.6032577157020569\n",
      "Epoch 5, Batch 759, Test Loss: 0.7879244089126587\n",
      "Epoch 5, Batch 760, Test Loss: 0.46959879994392395\n",
      "Epoch 5, Batch 761, Test Loss: 0.6265112161636353\n",
      "Epoch 5, Batch 762, Test Loss: 0.4922507405281067\n",
      "Epoch 5, Batch 763, Test Loss: 0.6333383321762085\n",
      "Epoch 5, Batch 764, Test Loss: 0.46752962470054626\n",
      "Epoch 5, Batch 765, Test Loss: 0.5372706651687622\n",
      "Epoch 5, Batch 766, Test Loss: 0.5483711361885071\n",
      "Epoch 5, Batch 767, Test Loss: 0.4676206707954407\n",
      "Epoch 5, Batch 768, Test Loss: 0.9095077514648438\n",
      "Epoch 5, Batch 769, Test Loss: 0.4904800057411194\n",
      "Epoch 5, Batch 770, Test Loss: 0.6587299704551697\n",
      "Epoch 5, Batch 771, Test Loss: 0.6008118391036987\n",
      "Epoch 5, Batch 772, Test Loss: 0.5203713178634644\n",
      "Epoch 5, Batch 773, Test Loss: 0.5474746227264404\n",
      "Epoch 5, Batch 774, Test Loss: 0.4802401661872864\n",
      "Epoch 5, Batch 775, Test Loss: 0.41007596254348755\n",
      "Epoch 5, Batch 776, Test Loss: 0.8906018137931824\n",
      "Epoch 5, Batch 777, Test Loss: 0.500727117061615\n",
      "Epoch 5, Batch 778, Test Loss: 0.5394483208656311\n",
      "Epoch 5, Batch 779, Test Loss: 0.6869550347328186\n",
      "Epoch 5, Batch 780, Test Loss: 0.7140165567398071\n",
      "Epoch 5, Batch 781, Test Loss: 0.5884595513343811\n",
      "Epoch 5, Batch 782, Test Loss: 0.5828701853752136\n",
      "Epoch 5, Batch 783, Test Loss: 0.5107339024543762\n",
      "Epoch 5, Batch 784, Test Loss: 0.5395207405090332\n",
      "Epoch 5, Batch 785, Test Loss: 0.5449371933937073\n",
      "Epoch 5, Batch 786, Test Loss: 0.6546905040740967\n",
      "Epoch 5, Batch 787, Test Loss: 0.7045113444328308\n",
      "Epoch 5, Batch 788, Test Loss: 0.7250685691833496\n",
      "Epoch 5, Batch 789, Test Loss: 0.5729864239692688\n",
      "Epoch 5, Batch 790, Test Loss: 0.45323866605758667\n",
      "Epoch 5, Batch 791, Test Loss: 0.463083952665329\n",
      "Epoch 5, Batch 792, Test Loss: 0.6688495874404907\n",
      "Epoch 5, Batch 793, Test Loss: 0.5618101954460144\n",
      "Epoch 5, Batch 794, Test Loss: 0.5548145771026611\n",
      "Epoch 5, Batch 795, Test Loss: 0.6269270777702332\n",
      "Epoch 5, Batch 796, Test Loss: 0.5762478709220886\n",
      "Epoch 5, Batch 797, Test Loss: 0.5528082251548767\n",
      "Epoch 5, Batch 798, Test Loss: 0.703009843826294\n",
      "Epoch 5, Batch 799, Test Loss: 0.6310006380081177\n",
      "Epoch 5, Batch 800, Test Loss: 0.6909114718437195\n",
      "Epoch 5, Batch 801, Test Loss: 0.5695917010307312\n",
      "Epoch 5, Batch 802, Test Loss: 0.5336045026779175\n",
      "Epoch 5, Batch 803, Test Loss: 0.644806981086731\n",
      "Epoch 5, Batch 804, Test Loss: 0.5611996650695801\n",
      "Epoch 5, Batch 805, Test Loss: 0.6275712251663208\n",
      "Epoch 5, Batch 806, Test Loss: 0.8025528192520142\n",
      "Epoch 5, Batch 807, Test Loss: 0.5845988988876343\n",
      "Epoch 5, Batch 808, Test Loss: 0.6200286149978638\n",
      "Epoch 5, Batch 809, Test Loss: 0.4745262861251831\n",
      "Epoch 5, Batch 810, Test Loss: 0.5558774471282959\n",
      "Epoch 5, Batch 811, Test Loss: 0.6761404275894165\n",
      "Epoch 5, Batch 812, Test Loss: 0.6176555156707764\n",
      "Epoch 5, Batch 813, Test Loss: 0.625177800655365\n",
      "Epoch 5, Batch 814, Test Loss: 0.7317484617233276\n",
      "Epoch 5, Batch 815, Test Loss: 0.572416365146637\n",
      "Epoch 5, Batch 816, Test Loss: 0.5614880919456482\n",
      "Epoch 5, Batch 817, Test Loss: 0.6229357123374939\n",
      "Epoch 5, Batch 818, Test Loss: 0.49193018674850464\n",
      "Epoch 5, Batch 819, Test Loss: 0.5290054082870483\n",
      "Epoch 5, Batch 820, Test Loss: 0.3532130718231201\n",
      "Epoch 5, Batch 821, Test Loss: 0.6785351634025574\n",
      "Epoch 5, Batch 822, Test Loss: 0.6747286319732666\n",
      "Epoch 5, Batch 823, Test Loss: 0.6288143396377563\n",
      "Epoch 5, Batch 824, Test Loss: 0.4824381172657013\n",
      "Epoch 5, Batch 825, Test Loss: 0.5951985716819763\n",
      "Epoch 5, Batch 826, Test Loss: 0.726841390132904\n",
      "Epoch 5, Batch 827, Test Loss: 0.655614972114563\n",
      "Epoch 5, Batch 828, Test Loss: 0.4875149130821228\n",
      "Epoch 5, Batch 829, Test Loss: 0.659786581993103\n",
      "Epoch 5, Batch 830, Test Loss: 0.6923921704292297\n",
      "Epoch 5, Batch 831, Test Loss: 0.5355485081672668\n",
      "Epoch 5, Batch 832, Test Loss: 0.6390140056610107\n",
      "Epoch 5, Batch 833, Test Loss: 0.5741639733314514\n",
      "Epoch 5, Batch 834, Test Loss: 0.7316064834594727\n",
      "Epoch 5, Batch 835, Test Loss: 0.5519870519638062\n",
      "Epoch 5, Batch 836, Test Loss: 0.5575906038284302\n",
      "Epoch 5, Batch 837, Test Loss: 0.5276066064834595\n",
      "Epoch 5, Batch 838, Test Loss: 0.677471935749054\n",
      "Epoch 5, Batch 839, Test Loss: 0.578813910484314\n",
      "Epoch 5, Batch 840, Test Loss: 0.7242783904075623\n",
      "Epoch 5, Batch 841, Test Loss: 0.5863803625106812\n",
      "Epoch 5, Batch 842, Test Loss: 0.6122473478317261\n",
      "Epoch 5, Batch 843, Test Loss: 0.5134484171867371\n",
      "Epoch 5, Batch 844, Test Loss: 0.5451371669769287\n",
      "Epoch 5, Batch 845, Test Loss: 0.9763294458389282\n",
      "Epoch 5, Batch 846, Test Loss: 0.509170651435852\n",
      "Epoch 5, Batch 847, Test Loss: 0.46511614322662354\n",
      "Epoch 5, Batch 848, Test Loss: 0.6398338079452515\n",
      "Epoch 5, Batch 849, Test Loss: 0.6295902132987976\n",
      "Epoch 5, Batch 850, Test Loss: 0.5101841688156128\n",
      "Epoch 5, Batch 851, Test Loss: 0.811201810836792\n",
      "Epoch 5, Batch 852, Test Loss: 0.6247915029525757\n",
      "Epoch 5, Batch 853, Test Loss: 0.43029889464378357\n",
      "Epoch 5, Batch 854, Test Loss: 0.9283719062805176\n",
      "Epoch 5, Batch 855, Test Loss: 0.5719680786132812\n",
      "Epoch 5, Batch 856, Test Loss: 0.6044403910636902\n",
      "Epoch 5, Batch 857, Test Loss: 0.522244393825531\n",
      "Epoch 5, Batch 858, Test Loss: 0.6042162179946899\n",
      "Epoch 5, Batch 859, Test Loss: 0.602297306060791\n",
      "Epoch 5, Batch 860, Test Loss: 0.5798032283782959\n",
      "Epoch 5, Batch 861, Test Loss: 0.5985546708106995\n",
      "Epoch 5, Batch 862, Test Loss: 0.4943348467350006\n",
      "Epoch 5, Batch 863, Test Loss: 0.665809690952301\n",
      "Epoch 5, Batch 864, Test Loss: 0.4794238209724426\n",
      "Epoch 5, Batch 865, Test Loss: 0.6777846813201904\n",
      "Epoch 5, Batch 866, Test Loss: 0.5909351706504822\n",
      "Epoch 5, Batch 867, Test Loss: 0.5058998465538025\n",
      "Epoch 5, Batch 868, Test Loss: 0.5821912884712219\n",
      "Epoch 5, Batch 869, Test Loss: 0.6288279891014099\n",
      "Epoch 5, Batch 870, Test Loss: 0.4469043016433716\n",
      "Epoch 5, Batch 871, Test Loss: 0.5595123767852783\n",
      "Epoch 5, Batch 872, Test Loss: 0.6873955726623535\n",
      "Epoch 5, Batch 873, Test Loss: 0.6151676177978516\n",
      "Epoch 5, Batch 874, Test Loss: 0.5123965740203857\n",
      "Epoch 5, Batch 875, Test Loss: 0.5368651747703552\n",
      "Epoch 5, Batch 876, Test Loss: 0.6184717416763306\n",
      "Epoch 5, Batch 877, Test Loss: 0.39232608675956726\n",
      "Epoch 5, Batch 878, Test Loss: 0.7914113402366638\n",
      "Epoch 5, Batch 879, Test Loss: 0.5852447152137756\n",
      "Epoch 5, Batch 880, Test Loss: 0.5119463801383972\n",
      "Epoch 5, Batch 881, Test Loss: 0.6749814748764038\n",
      "Epoch 5, Batch 882, Test Loss: 0.6001721620559692\n",
      "Epoch 5, Batch 883, Test Loss: 0.6243582367897034\n",
      "Epoch 5, Batch 884, Test Loss: 0.5603365302085876\n",
      "Epoch 5, Batch 885, Test Loss: 0.4433041512966156\n",
      "Epoch 5, Batch 886, Test Loss: 0.9209373593330383\n",
      "Epoch 5, Batch 887, Test Loss: 0.4975394010543823\n",
      "Epoch 5, Batch 888, Test Loss: 0.5201426148414612\n",
      "Epoch 5, Batch 889, Test Loss: 0.5719257593154907\n",
      "Epoch 5, Batch 890, Test Loss: 0.6201280951499939\n",
      "Epoch 5, Batch 891, Test Loss: 0.566143274307251\n",
      "Epoch 5, Batch 892, Test Loss: 0.558530330657959\n",
      "Epoch 5, Batch 893, Test Loss: 0.7377278208732605\n",
      "Epoch 5, Batch 894, Test Loss: 0.5501896142959595\n",
      "Epoch 5, Batch 895, Test Loss: 0.5586149096488953\n",
      "Epoch 5, Batch 896, Test Loss: 0.5553708672523499\n",
      "Epoch 5, Batch 897, Test Loss: 0.5603702664375305\n",
      "Epoch 5, Batch 898, Test Loss: 0.6479083895683289\n",
      "Epoch 5, Batch 899, Test Loss: 0.43870365619659424\n",
      "Epoch 5, Batch 900, Test Loss: 0.502987265586853\n",
      "Epoch 5, Batch 901, Test Loss: 0.36350589990615845\n",
      "Epoch 5, Batch 902, Test Loss: 0.6659418344497681\n",
      "Epoch 5, Batch 903, Test Loss: 0.498592734336853\n",
      "Epoch 5, Batch 904, Test Loss: 0.5544173717498779\n",
      "Epoch 5, Batch 905, Test Loss: 0.5655205845832825\n",
      "Epoch 5, Batch 906, Test Loss: 0.5744324922561646\n",
      "Epoch 5, Batch 907, Test Loss: 0.46723294258117676\n",
      "Epoch 5, Batch 908, Test Loss: 0.5713776350021362\n",
      "Epoch 5, Batch 909, Test Loss: 0.6364213824272156\n",
      "Epoch 5, Batch 910, Test Loss: 0.5627371072769165\n",
      "Epoch 5, Batch 911, Test Loss: 0.5556700229644775\n",
      "Epoch 5, Batch 912, Test Loss: 0.6310451030731201\n",
      "Epoch 5, Batch 913, Test Loss: 0.5365284085273743\n",
      "Epoch 5, Batch 914, Test Loss: 0.6876894235610962\n",
      "Epoch 5, Batch 915, Test Loss: 0.638193666934967\n",
      "Epoch 5, Batch 916, Test Loss: 0.6275186538696289\n",
      "Epoch 5, Batch 917, Test Loss: 0.7984938621520996\n",
      "Epoch 5, Batch 918, Test Loss: 0.5486880540847778\n",
      "Epoch 5, Batch 919, Test Loss: 0.5207870602607727\n",
      "Epoch 5, Batch 920, Test Loss: 0.5855713486671448\n",
      "Epoch 5, Batch 921, Test Loss: 0.6282808184623718\n",
      "Epoch 5, Batch 922, Test Loss: 0.4440791606903076\n",
      "Epoch 5, Batch 923, Test Loss: 0.5387194752693176\n",
      "Epoch 5, Batch 924, Test Loss: 0.5783553123474121\n",
      "Epoch 5, Batch 925, Test Loss: 0.5306826829910278\n",
      "Epoch 5, Batch 926, Test Loss: 0.5491019487380981\n",
      "Epoch 5, Batch 927, Test Loss: 0.5811859965324402\n",
      "Epoch 5, Batch 928, Test Loss: 0.5285841822624207\n",
      "Epoch 5, Batch 929, Test Loss: 0.7370322346687317\n",
      "Epoch 5, Batch 930, Test Loss: 0.4783151149749756\n",
      "Epoch 5, Batch 931, Test Loss: 0.7826387286186218\n",
      "Epoch 5, Batch 932, Test Loss: 0.6063236594200134\n",
      "Epoch 5, Batch 933, Test Loss: 0.452065646648407\n",
      "Epoch 5, Batch 934, Test Loss: 0.7033795118331909\n",
      "Epoch 5, Batch 935, Test Loss: 0.5522448420524597\n",
      "Epoch 5, Batch 936, Test Loss: 0.6470754742622375\n",
      "Epoch 5, Batch 937, Test Loss: 0.6633736491203308\n",
      "Epoch 5, Batch 938, Test Loss: 0.41128063201904297\n",
      "Accuracy of Test set: 0.7810666666666667\n",
      "Epoch 6, Batch 1, Loss: 0.6773697137832642\n",
      "Epoch 6, Batch 2, Loss: 0.6657148599624634\n",
      "Epoch 6, Batch 3, Loss: 0.5992234945297241\n",
      "Epoch 6, Batch 4, Loss: 0.5143446922302246\n",
      "Epoch 6, Batch 5, Loss: 0.503704309463501\n",
      "Epoch 6, Batch 6, Loss: 0.7059749364852905\n",
      "Epoch 6, Batch 7, Loss: 0.6735336184501648\n",
      "Epoch 6, Batch 8, Loss: 0.9336685538291931\n",
      "Epoch 6, Batch 9, Loss: 0.5441826581954956\n",
      "Epoch 6, Batch 10, Loss: 0.7673807144165039\n",
      "Epoch 6, Batch 11, Loss: 0.6096177697181702\n",
      "Epoch 6, Batch 12, Loss: 0.6222621202468872\n",
      "Epoch 6, Batch 13, Loss: 0.8066326975822449\n",
      "Epoch 6, Batch 14, Loss: 0.7015052437782288\n",
      "Epoch 6, Batch 15, Loss: 0.6625950336456299\n",
      "Epoch 6, Batch 16, Loss: 0.4447036385536194\n",
      "Epoch 6, Batch 17, Loss: 0.5231269598007202\n",
      "Epoch 6, Batch 18, Loss: 0.7339087128639221\n",
      "Epoch 6, Batch 19, Loss: 0.7075759172439575\n",
      "Epoch 6, Batch 20, Loss: 0.48426640033721924\n",
      "Epoch 6, Batch 21, Loss: 0.49973607063293457\n",
      "Epoch 6, Batch 22, Loss: 0.5936416387557983\n",
      "Epoch 6, Batch 23, Loss: 0.5283157229423523\n",
      "Epoch 6, Batch 24, Loss: 0.5913376212120056\n",
      "Epoch 6, Batch 25, Loss: 0.7541158199310303\n",
      "Epoch 6, Batch 26, Loss: 0.6601291298866272\n",
      "Epoch 6, Batch 27, Loss: 0.7310359477996826\n",
      "Epoch 6, Batch 28, Loss: 0.5457266569137573\n",
      "Epoch 6, Batch 29, Loss: 0.6515786647796631\n",
      "Epoch 6, Batch 30, Loss: 0.5316792130470276\n",
      "Epoch 6, Batch 31, Loss: 0.6300089359283447\n",
      "Epoch 6, Batch 32, Loss: 0.6640740036964417\n",
      "Epoch 6, Batch 33, Loss: 0.6638404130935669\n",
      "Epoch 6, Batch 34, Loss: 0.5601006150245667\n",
      "Epoch 6, Batch 35, Loss: 0.5500576496124268\n",
      "Epoch 6, Batch 36, Loss: 0.7171592116355896\n",
      "Epoch 6, Batch 37, Loss: 0.6662724614143372\n",
      "Epoch 6, Batch 38, Loss: 0.6668851971626282\n",
      "Epoch 6, Batch 39, Loss: 0.4518272280693054\n",
      "Epoch 6, Batch 40, Loss: 0.5710230469703674\n",
      "Epoch 6, Batch 41, Loss: 0.7048523426055908\n",
      "Epoch 6, Batch 42, Loss: 0.8384166359901428\n",
      "Epoch 6, Batch 43, Loss: 0.7419009208679199\n",
      "Epoch 6, Batch 44, Loss: 0.5657992362976074\n",
      "Epoch 6, Batch 45, Loss: 0.6512665748596191\n",
      "Epoch 6, Batch 46, Loss: 0.5615167021751404\n",
      "Epoch 6, Batch 47, Loss: 0.5756914615631104\n",
      "Epoch 6, Batch 48, Loss: 0.476891428232193\n",
      "Epoch 6, Batch 49, Loss: 0.8473656177520752\n",
      "Epoch 6, Batch 50, Loss: 0.6158741116523743\n",
      "Epoch 6, Batch 51, Loss: 0.8977689146995544\n",
      "Epoch 6, Batch 52, Loss: 0.7969490885734558\n",
      "Epoch 6, Batch 53, Loss: 0.6064438819885254\n",
      "Epoch 6, Batch 54, Loss: 0.626266360282898\n",
      "Epoch 6, Batch 55, Loss: 0.557549774646759\n",
      "Epoch 6, Batch 56, Loss: 0.852482795715332\n",
      "Epoch 6, Batch 57, Loss: 0.5366997122764587\n",
      "Epoch 6, Batch 58, Loss: 0.6315672397613525\n",
      "Epoch 6, Batch 59, Loss: 0.7863777875900269\n",
      "Epoch 6, Batch 60, Loss: 0.5275780558586121\n",
      "Epoch 6, Batch 61, Loss: 0.578414797782898\n",
      "Epoch 6, Batch 62, Loss: 0.6220507621765137\n",
      "Epoch 6, Batch 63, Loss: 0.5389868021011353\n",
      "Epoch 6, Batch 64, Loss: 0.6182518005371094\n",
      "Epoch 6, Batch 65, Loss: 0.5329931378364563\n",
      "Epoch 6, Batch 66, Loss: 0.6303684711456299\n",
      "Epoch 6, Batch 67, Loss: 0.5801737308502197\n",
      "Epoch 6, Batch 68, Loss: 0.5759768486022949\n",
      "Epoch 6, Batch 69, Loss: 0.6421483755111694\n",
      "Epoch 6, Batch 70, Loss: 0.40758824348449707\n",
      "Epoch 6, Batch 71, Loss: 0.5825062990188599\n",
      "Epoch 6, Batch 72, Loss: 0.6757281422615051\n",
      "Epoch 6, Batch 73, Loss: 0.5060099363327026\n",
      "Epoch 6, Batch 74, Loss: 0.5269523859024048\n",
      "Epoch 6, Batch 75, Loss: 0.6900207996368408\n",
      "Epoch 6, Batch 76, Loss: 0.5070463418960571\n",
      "Epoch 6, Batch 77, Loss: 0.41562214493751526\n",
      "Epoch 6, Batch 78, Loss: 0.6974362730979919\n",
      "Epoch 6, Batch 79, Loss: 0.5166292786598206\n",
      "Epoch 6, Batch 80, Loss: 0.40142467617988586\n",
      "Epoch 6, Batch 81, Loss: 0.5823304057121277\n",
      "Epoch 6, Batch 82, Loss: 0.6140810251235962\n",
      "Epoch 6, Batch 83, Loss: 0.5726395845413208\n",
      "Epoch 6, Batch 84, Loss: 0.5559510588645935\n",
      "Epoch 6, Batch 85, Loss: 0.6444006562232971\n",
      "Epoch 6, Batch 86, Loss: 0.4264923930168152\n",
      "Epoch 6, Batch 87, Loss: 0.482363224029541\n",
      "Epoch 6, Batch 88, Loss: 0.6338844299316406\n",
      "Epoch 6, Batch 89, Loss: 0.45432502031326294\n",
      "Epoch 6, Batch 90, Loss: 0.5722222328186035\n",
      "Epoch 6, Batch 91, Loss: 0.4749270975589752\n",
      "Epoch 6, Batch 92, Loss: 0.7055059671401978\n",
      "Epoch 6, Batch 93, Loss: 0.572817862033844\n",
      "Epoch 6, Batch 94, Loss: 0.3974580764770508\n",
      "Epoch 6, Batch 95, Loss: 0.7169350385665894\n",
      "Epoch 6, Batch 96, Loss: 0.7930698990821838\n",
      "Epoch 6, Batch 97, Loss: 0.5116707682609558\n",
      "Epoch 6, Batch 98, Loss: 0.5795233249664307\n",
      "Epoch 6, Batch 99, Loss: 0.6068053841590881\n",
      "Epoch 6, Batch 100, Loss: 0.5767136812210083\n",
      "Epoch 6, Batch 101, Loss: 0.7871857285499573\n",
      "Epoch 6, Batch 102, Loss: 0.48708009719848633\n",
      "Epoch 6, Batch 103, Loss: 0.6627948880195618\n",
      "Epoch 6, Batch 104, Loss: 0.53517746925354\n",
      "Epoch 6, Batch 105, Loss: 0.6369786262512207\n",
      "Epoch 6, Batch 106, Loss: 0.5596895813941956\n",
      "Epoch 6, Batch 107, Loss: 0.549609363079071\n",
      "Epoch 6, Batch 108, Loss: 0.6206485629081726\n",
      "Epoch 6, Batch 109, Loss: 0.3707661032676697\n",
      "Epoch 6, Batch 110, Loss: 0.5598974227905273\n",
      "Epoch 6, Batch 111, Loss: 0.3700554668903351\n",
      "Epoch 6, Batch 112, Loss: 0.5719919800758362\n",
      "Epoch 6, Batch 113, Loss: 0.7083580493927002\n",
      "Epoch 6, Batch 114, Loss: 0.7199698090553284\n",
      "Epoch 6, Batch 115, Loss: 0.6333763599395752\n",
      "Epoch 6, Batch 116, Loss: 0.7321712970733643\n",
      "Epoch 6, Batch 117, Loss: 0.801202654838562\n",
      "Epoch 6, Batch 118, Loss: 0.5122765302658081\n",
      "Epoch 6, Batch 119, Loss: 0.3939623236656189\n",
      "Epoch 6, Batch 120, Loss: 0.5805389881134033\n",
      "Epoch 6, Batch 121, Loss: 0.44139567017555237\n",
      "Epoch 6, Batch 122, Loss: 0.8053879737854004\n",
      "Epoch 6, Batch 123, Loss: 0.6684413552284241\n",
      "Epoch 6, Batch 124, Loss: 0.6561173796653748\n",
      "Epoch 6, Batch 125, Loss: 0.8481408357620239\n",
      "Epoch 6, Batch 126, Loss: 0.5265628099441528\n",
      "Epoch 6, Batch 127, Loss: 0.5083173513412476\n",
      "Epoch 6, Batch 128, Loss: 0.5791853070259094\n",
      "Epoch 6, Batch 129, Loss: 0.6884413957595825\n",
      "Epoch 6, Batch 130, Loss: 0.86733478307724\n",
      "Epoch 6, Batch 131, Loss: 0.6737637519836426\n",
      "Epoch 6, Batch 132, Loss: 0.5542935729026794\n",
      "Epoch 6, Batch 133, Loss: 0.6418823599815369\n",
      "Epoch 6, Batch 134, Loss: 0.6867432594299316\n",
      "Epoch 6, Batch 135, Loss: 0.635172426700592\n",
      "Epoch 6, Batch 136, Loss: 0.7848360538482666\n",
      "Epoch 6, Batch 137, Loss: 0.4708186984062195\n",
      "Epoch 6, Batch 138, Loss: 0.5547661781311035\n",
      "Epoch 6, Batch 139, Loss: 0.6326639652252197\n",
      "Epoch 6, Batch 140, Loss: 0.5780043601989746\n",
      "Epoch 6, Batch 141, Loss: 0.6821305751800537\n",
      "Epoch 6, Batch 142, Loss: 0.7074442505836487\n",
      "Epoch 6, Batch 143, Loss: 0.6232925653457642\n",
      "Epoch 6, Batch 144, Loss: 0.5261068940162659\n",
      "Epoch 6, Batch 145, Loss: 0.668599545955658\n",
      "Epoch 6, Batch 146, Loss: 0.5648460984230042\n",
      "Epoch 6, Batch 147, Loss: 0.927208662033081\n",
      "Epoch 6, Batch 148, Loss: 0.49535462260246277\n",
      "Epoch 6, Batch 149, Loss: 0.48843273520469666\n",
      "Epoch 6, Batch 150, Loss: 0.4189186692237854\n",
      "Epoch 6, Batch 151, Loss: 0.6438937187194824\n",
      "Epoch 6, Batch 152, Loss: 0.460856556892395\n",
      "Epoch 6, Batch 153, Loss: 0.5396067500114441\n",
      "Epoch 6, Batch 154, Loss: 0.5736674666404724\n",
      "Epoch 6, Batch 155, Loss: 0.5000542402267456\n",
      "Epoch 6, Batch 156, Loss: 0.5358786582946777\n",
      "Epoch 6, Batch 157, Loss: 0.4469228684902191\n",
      "Epoch 6, Batch 158, Loss: 0.5605580806732178\n",
      "Epoch 6, Batch 159, Loss: 0.6997062563896179\n",
      "Epoch 6, Batch 160, Loss: 0.5828434228897095\n",
      "Epoch 6, Batch 161, Loss: 0.5903178453445435\n",
      "Epoch 6, Batch 162, Loss: 0.41523072123527527\n",
      "Epoch 6, Batch 163, Loss: 0.5808948278427124\n",
      "Epoch 6, Batch 164, Loss: 0.6375691890716553\n",
      "Epoch 6, Batch 165, Loss: 0.7243202924728394\n",
      "Epoch 6, Batch 166, Loss: 0.7666956782341003\n",
      "Epoch 6, Batch 167, Loss: 0.44271591305732727\n",
      "Epoch 6, Batch 168, Loss: 0.5766430497169495\n",
      "Epoch 6, Batch 169, Loss: 0.6377210021018982\n",
      "Epoch 6, Batch 170, Loss: 0.8196324110031128\n",
      "Epoch 6, Batch 171, Loss: 0.5776033401489258\n",
      "Epoch 6, Batch 172, Loss: 0.5021100044250488\n",
      "Epoch 6, Batch 173, Loss: 0.7181074023246765\n",
      "Epoch 6, Batch 174, Loss: 0.6628079414367676\n",
      "Epoch 6, Batch 175, Loss: 0.5504762530326843\n",
      "Epoch 6, Batch 176, Loss: 0.7715591788291931\n",
      "Epoch 6, Batch 177, Loss: 0.6058182716369629\n",
      "Epoch 6, Batch 178, Loss: 0.3903650641441345\n",
      "Epoch 6, Batch 179, Loss: 0.41179153323173523\n",
      "Epoch 6, Batch 180, Loss: 0.778404176235199\n",
      "Epoch 6, Batch 181, Loss: 0.5073688626289368\n",
      "Epoch 6, Batch 182, Loss: 0.742948055267334\n",
      "Epoch 6, Batch 183, Loss: 0.5836293697357178\n",
      "Epoch 6, Batch 184, Loss: 0.6028620600700378\n",
      "Epoch 6, Batch 185, Loss: 0.5505504608154297\n",
      "Epoch 6, Batch 186, Loss: 0.6483122110366821\n",
      "Epoch 6, Batch 187, Loss: 0.5211213827133179\n",
      "Epoch 6, Batch 188, Loss: 0.6617336869239807\n",
      "Epoch 6, Batch 189, Loss: 0.6486434936523438\n",
      "Epoch 6, Batch 190, Loss: 0.7191101908683777\n",
      "Epoch 6, Batch 191, Loss: 0.6697568893432617\n",
      "Epoch 6, Batch 192, Loss: 0.5330661535263062\n",
      "Epoch 6, Batch 193, Loss: 0.4085371494293213\n",
      "Epoch 6, Batch 194, Loss: 0.3773897886276245\n",
      "Epoch 6, Batch 195, Loss: 0.6021914482116699\n",
      "Epoch 6, Batch 196, Loss: 0.489482045173645\n",
      "Epoch 6, Batch 197, Loss: 0.7456071376800537\n",
      "Epoch 6, Batch 198, Loss: 0.5566613674163818\n",
      "Epoch 6, Batch 199, Loss: 0.8501821756362915\n",
      "Epoch 6, Batch 200, Loss: 0.6699921488761902\n",
      "Epoch 6, Batch 201, Loss: 0.665182888507843\n",
      "Epoch 6, Batch 202, Loss: 0.5608765482902527\n",
      "Epoch 6, Batch 203, Loss: 0.557040810585022\n",
      "Epoch 6, Batch 204, Loss: 0.5747148394584656\n",
      "Epoch 6, Batch 205, Loss: 0.441013365983963\n",
      "Epoch 6, Batch 206, Loss: 0.5023676156997681\n",
      "Epoch 6, Batch 207, Loss: 0.6962172985076904\n",
      "Epoch 6, Batch 208, Loss: 0.5293792486190796\n",
      "Epoch 6, Batch 209, Loss: 0.579901933670044\n",
      "Epoch 6, Batch 210, Loss: 0.6087331771850586\n",
      "Epoch 6, Batch 211, Loss: 0.7726870179176331\n",
      "Epoch 6, Batch 212, Loss: 0.5821409821510315\n",
      "Epoch 6, Batch 213, Loss: 0.4207380712032318\n",
      "Epoch 6, Batch 214, Loss: 0.5164690613746643\n",
      "Epoch 6, Batch 215, Loss: 0.5606101751327515\n",
      "Epoch 6, Batch 216, Loss: 0.6474098563194275\n",
      "Epoch 6, Batch 217, Loss: 0.6058709621429443\n",
      "Epoch 6, Batch 218, Loss: 0.6866810321807861\n",
      "Epoch 6, Batch 219, Loss: 0.5027254819869995\n",
      "Epoch 6, Batch 220, Loss: 0.574161171913147\n",
      "Epoch 6, Batch 221, Loss: 0.49927398562431335\n",
      "Epoch 6, Batch 222, Loss: 0.5876477956771851\n",
      "Epoch 6, Batch 223, Loss: 0.5154334902763367\n",
      "Epoch 6, Batch 224, Loss: 0.5800039768218994\n",
      "Epoch 6, Batch 225, Loss: 0.4822860658168793\n",
      "Epoch 6, Batch 226, Loss: 0.42203420400619507\n",
      "Epoch 6, Batch 227, Loss: 0.5819432735443115\n",
      "Epoch 6, Batch 228, Loss: 0.73450767993927\n",
      "Epoch 6, Batch 229, Loss: 0.6577783226966858\n",
      "Epoch 6, Batch 230, Loss: 0.6556353569030762\n",
      "Epoch 6, Batch 231, Loss: 0.6141828298568726\n",
      "Epoch 6, Batch 232, Loss: 0.5221809148788452\n",
      "Epoch 6, Batch 233, Loss: 0.5256963968276978\n",
      "Epoch 6, Batch 234, Loss: 0.5407997369766235\n",
      "Epoch 6, Batch 235, Loss: 0.5186527371406555\n",
      "Epoch 6, Batch 236, Loss: 0.47298336029052734\n",
      "Epoch 6, Batch 237, Loss: 0.6222286224365234\n",
      "Epoch 6, Batch 238, Loss: 0.5404105186462402\n",
      "Epoch 6, Batch 239, Loss: 0.7435207962989807\n",
      "Epoch 6, Batch 240, Loss: 0.6283367276191711\n",
      "Epoch 6, Batch 241, Loss: 0.829052209854126\n",
      "Epoch 6, Batch 242, Loss: 0.4446812868118286\n",
      "Epoch 6, Batch 243, Loss: 0.7373999357223511\n",
      "Epoch 6, Batch 244, Loss: 0.5550798773765564\n",
      "Epoch 6, Batch 245, Loss: 0.4632411599159241\n",
      "Epoch 6, Batch 246, Loss: 0.44923239946365356\n",
      "Epoch 6, Batch 247, Loss: 0.6409968733787537\n",
      "Epoch 6, Batch 248, Loss: 0.8662264347076416\n",
      "Epoch 6, Batch 249, Loss: 0.4650685787200928\n",
      "Epoch 6, Batch 250, Loss: 0.5068725347518921\n",
      "Epoch 6, Batch 251, Loss: 0.4522625505924225\n",
      "Epoch 6, Batch 252, Loss: 0.4398876428604126\n",
      "Epoch 6, Batch 253, Loss: 0.6574837565422058\n",
      "Epoch 6, Batch 254, Loss: 0.6351084113121033\n",
      "Epoch 6, Batch 255, Loss: 0.714544951915741\n",
      "Epoch 6, Batch 256, Loss: 0.5065350532531738\n",
      "Epoch 6, Batch 257, Loss: 0.6438950300216675\n",
      "Epoch 6, Batch 258, Loss: 0.5513350367546082\n",
      "Epoch 6, Batch 259, Loss: 0.4998638331890106\n",
      "Epoch 6, Batch 260, Loss: 0.5573433637619019\n",
      "Epoch 6, Batch 261, Loss: 0.452095627784729\n",
      "Epoch 6, Batch 262, Loss: 0.4269300103187561\n",
      "Epoch 6, Batch 263, Loss: 0.4958723783493042\n",
      "Epoch 6, Batch 264, Loss: 0.4910229444503784\n",
      "Epoch 6, Batch 265, Loss: 0.4773252010345459\n",
      "Epoch 6, Batch 266, Loss: 0.6938005685806274\n",
      "Epoch 6, Batch 267, Loss: 0.4547269344329834\n",
      "Epoch 6, Batch 268, Loss: 0.6653525829315186\n",
      "Epoch 6, Batch 269, Loss: 0.9007183313369751\n",
      "Epoch 6, Batch 270, Loss: 0.5460814833641052\n",
      "Epoch 6, Batch 271, Loss: 0.5973919034004211\n",
      "Epoch 6, Batch 272, Loss: 0.5862621665000916\n",
      "Epoch 6, Batch 273, Loss: 0.7186154127120972\n",
      "Epoch 6, Batch 274, Loss: 0.774143397808075\n",
      "Epoch 6, Batch 275, Loss: 0.5519985556602478\n",
      "Epoch 6, Batch 276, Loss: 0.5126482248306274\n",
      "Epoch 6, Batch 277, Loss: 0.9140611886978149\n",
      "Epoch 6, Batch 278, Loss: 0.6776134371757507\n",
      "Epoch 6, Batch 279, Loss: 0.4838103950023651\n",
      "Epoch 6, Batch 280, Loss: 0.6216607093811035\n",
      "Epoch 6, Batch 281, Loss: 0.7542059421539307\n",
      "Epoch 6, Batch 282, Loss: 0.4606111943721771\n",
      "Epoch 6, Batch 283, Loss: 0.6585932970046997\n",
      "Epoch 6, Batch 284, Loss: 0.7509194612503052\n",
      "Epoch 6, Batch 285, Loss: 0.5705963969230652\n",
      "Epoch 6, Batch 286, Loss: 0.6520053744316101\n",
      "Epoch 6, Batch 287, Loss: 0.5692640542984009\n",
      "Epoch 6, Batch 288, Loss: 0.6815618276596069\n",
      "Epoch 6, Batch 289, Loss: 0.49353647232055664\n",
      "Epoch 6, Batch 290, Loss: 0.4693898856639862\n",
      "Epoch 6, Batch 291, Loss: 0.5624222755432129\n",
      "Epoch 6, Batch 292, Loss: 0.548090934753418\n",
      "Epoch 6, Batch 293, Loss: 0.3806324303150177\n",
      "Epoch 6, Batch 294, Loss: 0.6713818907737732\n",
      "Epoch 6, Batch 295, Loss: 0.666135847568512\n",
      "Epoch 6, Batch 296, Loss: 0.8575527667999268\n",
      "Epoch 6, Batch 297, Loss: 0.5703523755073547\n",
      "Epoch 6, Batch 298, Loss: 0.5879395604133606\n",
      "Epoch 6, Batch 299, Loss: 0.46510496735572815\n",
      "Epoch 6, Batch 300, Loss: 0.7159899473190308\n",
      "Epoch 6, Batch 301, Loss: 0.612909197807312\n",
      "Epoch 6, Batch 302, Loss: 0.7179797291755676\n",
      "Epoch 6, Batch 303, Loss: 0.756547749042511\n",
      "Epoch 6, Batch 304, Loss: 0.5337905287742615\n",
      "Epoch 6, Batch 305, Loss: 0.6371161937713623\n",
      "Epoch 6, Batch 306, Loss: 0.6855971813201904\n",
      "Epoch 6, Batch 307, Loss: 0.5519006252288818\n",
      "Epoch 6, Batch 308, Loss: 0.7212722301483154\n",
      "Epoch 6, Batch 309, Loss: 0.46988701820373535\n",
      "Epoch 6, Batch 310, Loss: 0.8957964181900024\n",
      "Epoch 6, Batch 311, Loss: 0.6542088389396667\n",
      "Epoch 6, Batch 312, Loss: 0.7154542803764343\n",
      "Epoch 6, Batch 313, Loss: 0.6343419551849365\n",
      "Epoch 6, Batch 314, Loss: 0.4861448407173157\n",
      "Epoch 6, Batch 315, Loss: 0.49438202381134033\n",
      "Epoch 6, Batch 316, Loss: 0.8111972212791443\n",
      "Epoch 6, Batch 317, Loss: 0.5352497696876526\n",
      "Epoch 6, Batch 318, Loss: 0.6352136731147766\n",
      "Epoch 6, Batch 319, Loss: 0.5197964310646057\n",
      "Epoch 6, Batch 320, Loss: 0.6330100893974304\n",
      "Epoch 6, Batch 321, Loss: 0.8250351548194885\n",
      "Epoch 6, Batch 322, Loss: 0.5874617695808411\n",
      "Epoch 6, Batch 323, Loss: 0.5057060122489929\n",
      "Epoch 6, Batch 324, Loss: 0.7567721605300903\n",
      "Epoch 6, Batch 325, Loss: 0.7494744062423706\n",
      "Epoch 6, Batch 326, Loss: 0.5688645243644714\n",
      "Epoch 6, Batch 327, Loss: 0.6069506406784058\n",
      "Epoch 6, Batch 328, Loss: 0.5256961584091187\n",
      "Epoch 6, Batch 329, Loss: 0.47822630405426025\n",
      "Epoch 6, Batch 330, Loss: 0.7878885269165039\n",
      "Epoch 6, Batch 331, Loss: 0.5998478531837463\n",
      "Epoch 6, Batch 332, Loss: 0.6947310566902161\n",
      "Epoch 6, Batch 333, Loss: 0.6195563077926636\n",
      "Epoch 6, Batch 334, Loss: 0.6589048504829407\n",
      "Epoch 6, Batch 335, Loss: 0.5122460126876831\n",
      "Epoch 6, Batch 336, Loss: 0.8224478960037231\n",
      "Epoch 6, Batch 337, Loss: 0.5685641765594482\n",
      "Epoch 6, Batch 338, Loss: 0.7342022061347961\n",
      "Epoch 6, Batch 339, Loss: 0.751935601234436\n",
      "Epoch 6, Batch 340, Loss: 0.7381213903427124\n",
      "Epoch 6, Batch 341, Loss: 0.6185686588287354\n",
      "Epoch 6, Batch 342, Loss: 0.6602937579154968\n",
      "Epoch 6, Batch 343, Loss: 0.5770801901817322\n",
      "Epoch 6, Batch 344, Loss: 0.6546412706375122\n",
      "Epoch 6, Batch 345, Loss: 0.5587239265441895\n",
      "Epoch 6, Batch 346, Loss: 0.6816871762275696\n",
      "Epoch 6, Batch 347, Loss: 0.48337626457214355\n",
      "Epoch 6, Batch 348, Loss: 0.551947295665741\n",
      "Epoch 6, Batch 349, Loss: 0.5467948913574219\n",
      "Epoch 6, Batch 350, Loss: 0.7112026810646057\n",
      "Epoch 6, Batch 351, Loss: 0.6417710781097412\n",
      "Epoch 6, Batch 352, Loss: 0.5470407009124756\n",
      "Epoch 6, Batch 353, Loss: 0.7073073387145996\n",
      "Epoch 6, Batch 354, Loss: 0.6046093702316284\n",
      "Epoch 6, Batch 355, Loss: 0.562898576259613\n",
      "Epoch 6, Batch 356, Loss: 0.6808407306671143\n",
      "Epoch 6, Batch 357, Loss: 0.5549342036247253\n",
      "Epoch 6, Batch 358, Loss: 0.7453716993331909\n",
      "Epoch 6, Batch 359, Loss: 0.7312546968460083\n",
      "Epoch 6, Batch 360, Loss: 0.44494253396987915\n",
      "Epoch 6, Batch 361, Loss: 0.6020858287811279\n",
      "Epoch 6, Batch 362, Loss: 0.7316303849220276\n",
      "Epoch 6, Batch 363, Loss: 0.7688336372375488\n",
      "Epoch 6, Batch 364, Loss: 0.43521320819854736\n",
      "Epoch 6, Batch 365, Loss: 0.6178647875785828\n",
      "Epoch 6, Batch 366, Loss: 0.5998501777648926\n",
      "Epoch 6, Batch 367, Loss: 0.5215317010879517\n",
      "Epoch 6, Batch 368, Loss: 0.4128546416759491\n",
      "Epoch 6, Batch 369, Loss: 0.4669784903526306\n",
      "Epoch 6, Batch 370, Loss: 0.48045381903648376\n",
      "Epoch 6, Batch 371, Loss: 0.7016512751579285\n",
      "Epoch 6, Batch 372, Loss: 0.6777472496032715\n",
      "Epoch 6, Batch 373, Loss: 0.581083357334137\n",
      "Epoch 6, Batch 374, Loss: 0.740497350692749\n",
      "Epoch 6, Batch 375, Loss: 0.7052246332168579\n",
      "Epoch 6, Batch 376, Loss: 0.44356340169906616\n",
      "Epoch 6, Batch 377, Loss: 0.7302047610282898\n",
      "Epoch 6, Batch 378, Loss: 0.6432463526725769\n",
      "Epoch 6, Batch 379, Loss: 0.4528108835220337\n",
      "Epoch 6, Batch 380, Loss: 0.6638278365135193\n",
      "Epoch 6, Batch 381, Loss: 0.5716954469680786\n",
      "Epoch 6, Batch 382, Loss: 0.5227094292640686\n",
      "Epoch 6, Batch 383, Loss: 0.6015636324882507\n",
      "Epoch 6, Batch 384, Loss: 0.5278047919273376\n",
      "Epoch 6, Batch 385, Loss: 0.4838796555995941\n",
      "Epoch 6, Batch 386, Loss: 0.6194855570793152\n",
      "Epoch 6, Batch 387, Loss: 0.4447307586669922\n",
      "Epoch 6, Batch 388, Loss: 0.5693491101264954\n",
      "Epoch 6, Batch 389, Loss: 0.7936623692512512\n",
      "Epoch 6, Batch 390, Loss: 0.7676373720169067\n",
      "Epoch 6, Batch 391, Loss: 0.6240178346633911\n",
      "Epoch 6, Batch 392, Loss: 0.6869356036186218\n",
      "Epoch 6, Batch 393, Loss: 0.5951094627380371\n",
      "Epoch 6, Batch 394, Loss: 0.7248759269714355\n",
      "Epoch 6, Batch 395, Loss: 0.6509031653404236\n",
      "Epoch 6, Batch 396, Loss: 0.5733941197395325\n",
      "Epoch 6, Batch 397, Loss: 0.5907766222953796\n",
      "Epoch 6, Batch 398, Loss: 0.711283802986145\n",
      "Epoch 6, Batch 399, Loss: 0.6992956399917603\n",
      "Epoch 6, Batch 400, Loss: 0.3925042450428009\n",
      "Epoch 6, Batch 401, Loss: 0.460326224565506\n",
      "Epoch 6, Batch 402, Loss: 0.37209585309028625\n",
      "Epoch 6, Batch 403, Loss: 0.6866762638092041\n",
      "Epoch 6, Batch 404, Loss: 0.5735979080200195\n",
      "Epoch 6, Batch 405, Loss: 0.6000741720199585\n",
      "Epoch 6, Batch 406, Loss: 0.5922636985778809\n",
      "Epoch 6, Batch 407, Loss: 0.6654520034790039\n",
      "Epoch 6, Batch 408, Loss: 0.5185383558273315\n",
      "Epoch 6, Batch 409, Loss: 0.607750415802002\n",
      "Epoch 6, Batch 410, Loss: 0.6583879590034485\n",
      "Epoch 6, Batch 411, Loss: 0.4605935513973236\n",
      "Epoch 6, Batch 412, Loss: 0.5146112442016602\n",
      "Epoch 6, Batch 413, Loss: 0.5242917537689209\n",
      "Epoch 6, Batch 414, Loss: 0.6849002838134766\n",
      "Epoch 6, Batch 415, Loss: 0.6494666934013367\n",
      "Epoch 6, Batch 416, Loss: 0.3420718312263489\n",
      "Epoch 6, Batch 417, Loss: 0.5720196962356567\n",
      "Epoch 6, Batch 418, Loss: 0.6713929176330566\n",
      "Epoch 6, Batch 419, Loss: 0.6487874388694763\n",
      "Epoch 6, Batch 420, Loss: 0.49289125204086304\n",
      "Epoch 6, Batch 421, Loss: 0.4586699903011322\n",
      "Epoch 6, Batch 422, Loss: 0.6764674186706543\n",
      "Epoch 6, Batch 423, Loss: 0.5307113528251648\n",
      "Epoch 6, Batch 424, Loss: 0.6747766733169556\n",
      "Epoch 6, Batch 425, Loss: 0.42720064520835876\n",
      "Epoch 6, Batch 426, Loss: 0.7380843162536621\n",
      "Epoch 6, Batch 427, Loss: 0.44039833545684814\n",
      "Epoch 6, Batch 428, Loss: 0.6941993236541748\n",
      "Epoch 6, Batch 429, Loss: 0.5015575885772705\n",
      "Epoch 6, Batch 430, Loss: 0.5503655076026917\n",
      "Epoch 6, Batch 431, Loss: 0.4667561948299408\n",
      "Epoch 6, Batch 432, Loss: 0.3820977509021759\n",
      "Epoch 6, Batch 433, Loss: 0.5494035482406616\n",
      "Epoch 6, Batch 434, Loss: 0.5846228003501892\n",
      "Epoch 6, Batch 435, Loss: 0.5418609380722046\n",
      "Epoch 6, Batch 436, Loss: 0.5766404867172241\n",
      "Epoch 6, Batch 437, Loss: 0.4253760278224945\n",
      "Epoch 6, Batch 438, Loss: 0.59493088722229\n",
      "Epoch 6, Batch 439, Loss: 0.5976900458335876\n",
      "Epoch 6, Batch 440, Loss: 0.5383843779563904\n",
      "Epoch 6, Batch 441, Loss: 0.5724742412567139\n",
      "Epoch 6, Batch 442, Loss: 0.5470798015594482\n",
      "Epoch 6, Batch 443, Loss: 0.6323693990707397\n",
      "Epoch 6, Batch 444, Loss: 0.7987328171730042\n",
      "Epoch 6, Batch 445, Loss: 0.4874241352081299\n",
      "Epoch 6, Batch 446, Loss: 0.5999654531478882\n",
      "Epoch 6, Batch 447, Loss: 0.6411060094833374\n",
      "Epoch 6, Batch 448, Loss: 0.5954204797744751\n",
      "Epoch 6, Batch 449, Loss: 0.43065524101257324\n",
      "Epoch 6, Batch 450, Loss: 0.7766556739807129\n",
      "Epoch 6, Batch 451, Loss: 0.4856458306312561\n",
      "Epoch 6, Batch 452, Loss: 0.599949836730957\n",
      "Epoch 6, Batch 453, Loss: 0.420714408159256\n",
      "Epoch 6, Batch 454, Loss: 0.5371167063713074\n",
      "Epoch 6, Batch 455, Loss: 0.652299702167511\n",
      "Epoch 6, Batch 456, Loss: 0.6429691314697266\n",
      "Epoch 6, Batch 457, Loss: 0.5244860053062439\n",
      "Epoch 6, Batch 458, Loss: 0.5276075601577759\n",
      "Epoch 6, Batch 459, Loss: 0.7593879103660583\n",
      "Epoch 6, Batch 460, Loss: 0.6316837668418884\n",
      "Epoch 6, Batch 461, Loss: 0.5981380343437195\n",
      "Epoch 6, Batch 462, Loss: 0.573887825012207\n",
      "Epoch 6, Batch 463, Loss: 0.63408362865448\n",
      "Epoch 6, Batch 464, Loss: 0.5484261512756348\n",
      "Epoch 6, Batch 465, Loss: 0.5607476234436035\n",
      "Epoch 6, Batch 466, Loss: 0.6852154731750488\n",
      "Epoch 6, Batch 467, Loss: 0.5325971841812134\n",
      "Epoch 6, Batch 468, Loss: 0.4571448266506195\n",
      "Epoch 6, Batch 469, Loss: 0.6809826493263245\n",
      "Epoch 6, Batch 470, Loss: 0.8546745777130127\n",
      "Epoch 6, Batch 471, Loss: 0.427167147397995\n",
      "Epoch 6, Batch 472, Loss: 0.43273821473121643\n",
      "Epoch 6, Batch 473, Loss: 0.6487124562263489\n",
      "Epoch 6, Batch 474, Loss: 0.5103841423988342\n",
      "Epoch 6, Batch 475, Loss: 0.618329644203186\n",
      "Epoch 6, Batch 476, Loss: 0.5960532426834106\n",
      "Epoch 6, Batch 477, Loss: 0.6447495222091675\n",
      "Epoch 6, Batch 478, Loss: 0.5459048748016357\n",
      "Epoch 6, Batch 479, Loss: 0.4942179024219513\n",
      "Epoch 6, Batch 480, Loss: 0.6155131459236145\n",
      "Epoch 6, Batch 481, Loss: 0.4680972993373871\n",
      "Epoch 6, Batch 482, Loss: 0.6051992774009705\n",
      "Epoch 6, Batch 483, Loss: 0.5893641710281372\n",
      "Epoch 6, Batch 484, Loss: 0.7366160154342651\n",
      "Epoch 6, Batch 485, Loss: 0.7376442551612854\n",
      "Epoch 6, Batch 486, Loss: 0.6140850186347961\n",
      "Epoch 6, Batch 487, Loss: 0.4993264079093933\n",
      "Epoch 6, Batch 488, Loss: 0.40026751160621643\n",
      "Epoch 6, Batch 489, Loss: 0.46574023365974426\n",
      "Epoch 6, Batch 490, Loss: 0.4332605004310608\n",
      "Epoch 6, Batch 491, Loss: 0.6641074419021606\n",
      "Epoch 6, Batch 492, Loss: 0.7179377675056458\n",
      "Epoch 6, Batch 493, Loss: 0.5373056530952454\n",
      "Epoch 6, Batch 494, Loss: 0.5948615074157715\n",
      "Epoch 6, Batch 495, Loss: 0.4971413314342499\n",
      "Epoch 6, Batch 496, Loss: 0.6319589614868164\n",
      "Epoch 6, Batch 497, Loss: 0.8680054545402527\n",
      "Epoch 6, Batch 498, Loss: 0.45681387186050415\n",
      "Epoch 6, Batch 499, Loss: 0.9956821799278259\n",
      "Epoch 6, Batch 500, Loss: 0.5620723366737366\n",
      "Epoch 6, Batch 501, Loss: 0.5322214365005493\n",
      "Epoch 6, Batch 502, Loss: 0.6314975023269653\n",
      "Epoch 6, Batch 503, Loss: 0.7000265717506409\n",
      "Epoch 6, Batch 504, Loss: 0.48343130946159363\n",
      "Epoch 6, Batch 505, Loss: 0.6019708514213562\n",
      "Epoch 6, Batch 506, Loss: 0.5988572835922241\n",
      "Epoch 6, Batch 507, Loss: 0.5443888902664185\n",
      "Epoch 6, Batch 508, Loss: 0.5221126079559326\n",
      "Epoch 6, Batch 509, Loss: 0.6910851001739502\n",
      "Epoch 6, Batch 510, Loss: 0.6204124689102173\n",
      "Epoch 6, Batch 511, Loss: 0.797382116317749\n",
      "Epoch 6, Batch 512, Loss: 0.42911338806152344\n",
      "Epoch 6, Batch 513, Loss: 0.6397382616996765\n",
      "Epoch 6, Batch 514, Loss: 0.7568734884262085\n",
      "Epoch 6, Batch 515, Loss: 0.5208370089530945\n",
      "Epoch 6, Batch 516, Loss: 0.6993649005889893\n",
      "Epoch 6, Batch 517, Loss: 0.5789928436279297\n",
      "Epoch 6, Batch 518, Loss: 0.3674698770046234\n",
      "Epoch 6, Batch 519, Loss: 0.5199530720710754\n",
      "Epoch 6, Batch 520, Loss: 0.44117993116378784\n",
      "Epoch 6, Batch 521, Loss: 0.5147817134857178\n",
      "Epoch 6, Batch 522, Loss: 0.6447481513023376\n",
      "Epoch 6, Batch 523, Loss: 0.662254810333252\n",
      "Epoch 6, Batch 524, Loss: 0.6217937469482422\n",
      "Epoch 6, Batch 525, Loss: 0.829748809337616\n",
      "Epoch 6, Batch 526, Loss: 0.6565341353416443\n",
      "Epoch 6, Batch 527, Loss: 0.5868956446647644\n",
      "Epoch 6, Batch 528, Loss: 0.4991686940193176\n",
      "Epoch 6, Batch 529, Loss: 0.5645046234130859\n",
      "Epoch 6, Batch 530, Loss: 0.6086228489875793\n",
      "Epoch 6, Batch 531, Loss: 0.7679070830345154\n",
      "Epoch 6, Batch 532, Loss: 0.675419807434082\n",
      "Epoch 6, Batch 533, Loss: 0.7130484580993652\n",
      "Epoch 6, Batch 534, Loss: 0.4738861322402954\n",
      "Epoch 6, Batch 535, Loss: 0.6915826797485352\n",
      "Epoch 6, Batch 536, Loss: 0.6570701003074646\n",
      "Epoch 6, Batch 537, Loss: 0.5488997101783752\n",
      "Epoch 6, Batch 538, Loss: 0.5270441770553589\n",
      "Epoch 6, Batch 539, Loss: 0.6700682640075684\n",
      "Epoch 6, Batch 540, Loss: 0.5248076915740967\n",
      "Epoch 6, Batch 541, Loss: 0.7274754047393799\n",
      "Epoch 6, Batch 542, Loss: 0.7499812245368958\n",
      "Epoch 6, Batch 543, Loss: 0.4871140122413635\n",
      "Epoch 6, Batch 544, Loss: 0.34036654233932495\n",
      "Epoch 6, Batch 545, Loss: 0.5325258374214172\n",
      "Epoch 6, Batch 546, Loss: 0.78504478931427\n",
      "Epoch 6, Batch 547, Loss: 0.6187939047813416\n",
      "Epoch 6, Batch 548, Loss: 0.508959949016571\n",
      "Epoch 6, Batch 549, Loss: 0.6101064682006836\n",
      "Epoch 6, Batch 550, Loss: 0.5903438329696655\n",
      "Epoch 6, Batch 551, Loss: 0.61790531873703\n",
      "Epoch 6, Batch 552, Loss: 0.5050264596939087\n",
      "Epoch 6, Batch 553, Loss: 0.587360143661499\n",
      "Epoch 6, Batch 554, Loss: 0.5891972780227661\n",
      "Epoch 6, Batch 555, Loss: 0.510688304901123\n",
      "Epoch 6, Batch 556, Loss: 0.5953890681266785\n",
      "Epoch 6, Batch 557, Loss: 0.9041152000427246\n",
      "Epoch 6, Batch 558, Loss: 0.7474114894866943\n",
      "Epoch 6, Batch 559, Loss: 0.5710158944129944\n",
      "Epoch 6, Batch 560, Loss: 0.6403152942657471\n",
      "Epoch 6, Batch 561, Loss: 0.5324475765228271\n",
      "Epoch 6, Batch 562, Loss: 0.5298241376876831\n",
      "Epoch 6, Batch 563, Loss: 0.4071599543094635\n",
      "Epoch 6, Batch 564, Loss: 0.5323368906974792\n",
      "Epoch 6, Batch 565, Loss: 0.6517097353935242\n",
      "Epoch 6, Batch 566, Loss: 0.685511589050293\n",
      "Epoch 6, Batch 567, Loss: 0.6548570394515991\n",
      "Epoch 6, Batch 568, Loss: 0.6601662635803223\n",
      "Epoch 6, Batch 569, Loss: 0.7139670252799988\n",
      "Epoch 6, Batch 570, Loss: 0.6482040286064148\n",
      "Epoch 6, Batch 571, Loss: 0.5908634066581726\n",
      "Epoch 6, Batch 572, Loss: 0.529699981212616\n",
      "Epoch 6, Batch 573, Loss: 0.3893522620201111\n",
      "Epoch 6, Batch 574, Loss: 0.5907849073410034\n",
      "Epoch 6, Batch 575, Loss: 0.5971757173538208\n",
      "Epoch 6, Batch 576, Loss: 0.40858736634254456\n",
      "Epoch 6, Batch 577, Loss: 0.7242866158485413\n",
      "Epoch 6, Batch 578, Loss: 0.4794887900352478\n",
      "Epoch 6, Batch 579, Loss: 0.5466418266296387\n",
      "Epoch 6, Batch 580, Loss: 0.6957299709320068\n",
      "Epoch 6, Batch 581, Loss: 0.6548125147819519\n",
      "Epoch 6, Batch 582, Loss: 0.6576270461082458\n",
      "Epoch 6, Batch 583, Loss: 0.567946195602417\n",
      "Epoch 6, Batch 584, Loss: 0.6363059878349304\n",
      "Epoch 6, Batch 585, Loss: 0.7094485759735107\n",
      "Epoch 6, Batch 586, Loss: 0.6658453345298767\n",
      "Epoch 6, Batch 587, Loss: 0.5747681856155396\n",
      "Epoch 6, Batch 588, Loss: 0.38408932089805603\n",
      "Epoch 6, Batch 589, Loss: 0.5775828957557678\n",
      "Epoch 6, Batch 590, Loss: 0.6168229579925537\n",
      "Epoch 6, Batch 591, Loss: 0.6855572462081909\n",
      "Epoch 6, Batch 592, Loss: 0.669529139995575\n",
      "Epoch 6, Batch 593, Loss: 0.4538419246673584\n",
      "Epoch 6, Batch 594, Loss: 0.5493432879447937\n",
      "Epoch 6, Batch 595, Loss: 0.533894419670105\n",
      "Epoch 6, Batch 596, Loss: 0.4379044771194458\n",
      "Epoch 6, Batch 597, Loss: 0.7448492646217346\n",
      "Epoch 6, Batch 598, Loss: 0.777866780757904\n",
      "Epoch 6, Batch 599, Loss: 0.4855683147907257\n",
      "Epoch 6, Batch 600, Loss: 0.44686275720596313\n",
      "Epoch 6, Batch 601, Loss: 0.7995581030845642\n",
      "Epoch 6, Batch 602, Loss: 0.6728180646896362\n",
      "Epoch 6, Batch 603, Loss: 0.43818581104278564\n",
      "Epoch 6, Batch 604, Loss: 0.7291373610496521\n",
      "Epoch 6, Batch 605, Loss: 0.6774685382843018\n",
      "Epoch 6, Batch 606, Loss: 0.5956007838249207\n",
      "Epoch 6, Batch 607, Loss: 0.4006481170654297\n",
      "Epoch 6, Batch 608, Loss: 0.575141191482544\n",
      "Epoch 6, Batch 609, Loss: 0.5890974998474121\n",
      "Epoch 6, Batch 610, Loss: 0.5932433009147644\n",
      "Epoch 6, Batch 611, Loss: 0.613659679889679\n",
      "Epoch 6, Batch 612, Loss: 0.6518921852111816\n",
      "Epoch 6, Batch 613, Loss: 0.8322804570198059\n",
      "Epoch 6, Batch 614, Loss: 0.7184446454048157\n",
      "Epoch 6, Batch 615, Loss: 0.529421329498291\n",
      "Epoch 6, Batch 616, Loss: 0.5427182912826538\n",
      "Epoch 6, Batch 617, Loss: 0.5320134162902832\n",
      "Epoch 6, Batch 618, Loss: 0.6683402061462402\n",
      "Epoch 6, Batch 619, Loss: 0.504585325717926\n",
      "Epoch 6, Batch 620, Loss: 0.6528716683387756\n",
      "Epoch 6, Batch 621, Loss: 0.7856727242469788\n",
      "Epoch 6, Batch 622, Loss: 0.5013887286186218\n",
      "Epoch 6, Batch 623, Loss: 0.6106228232383728\n",
      "Epoch 6, Batch 624, Loss: 0.6375635266304016\n",
      "Epoch 6, Batch 625, Loss: 0.6072326302528381\n",
      "Epoch 6, Batch 626, Loss: 0.5900101661682129\n",
      "Epoch 6, Batch 627, Loss: 0.6643056869506836\n",
      "Epoch 6, Batch 628, Loss: 0.37026113271713257\n",
      "Epoch 6, Batch 629, Loss: 0.7805243134498596\n",
      "Epoch 6, Batch 630, Loss: 0.5043047070503235\n",
      "Epoch 6, Batch 631, Loss: 0.576745867729187\n",
      "Epoch 6, Batch 632, Loss: 0.7558822631835938\n",
      "Epoch 6, Batch 633, Loss: 0.6235156059265137\n",
      "Epoch 6, Batch 634, Loss: 0.47988975048065186\n",
      "Epoch 6, Batch 635, Loss: 0.6090258359909058\n",
      "Epoch 6, Batch 636, Loss: 0.8015945553779602\n",
      "Epoch 6, Batch 637, Loss: 0.6926173567771912\n",
      "Epoch 6, Batch 638, Loss: 0.5725699663162231\n",
      "Epoch 6, Batch 639, Loss: 0.541348934173584\n",
      "Epoch 6, Batch 640, Loss: 0.5133964419364929\n",
      "Epoch 6, Batch 641, Loss: 0.7253299951553345\n",
      "Epoch 6, Batch 642, Loss: 0.7612422704696655\n",
      "Epoch 6, Batch 643, Loss: 0.7423263788223267\n",
      "Epoch 6, Batch 644, Loss: 0.6662368178367615\n",
      "Epoch 6, Batch 645, Loss: 0.695184588432312\n",
      "Epoch 6, Batch 646, Loss: 0.5583348274230957\n",
      "Epoch 6, Batch 647, Loss: 0.8414391875267029\n",
      "Epoch 6, Batch 648, Loss: 0.6575274467468262\n",
      "Epoch 6, Batch 649, Loss: 0.6692651510238647\n",
      "Epoch 6, Batch 650, Loss: 0.8173609972000122\n",
      "Epoch 6, Batch 651, Loss: 0.527083158493042\n",
      "Epoch 6, Batch 652, Loss: 0.5609421730041504\n",
      "Epoch 6, Batch 653, Loss: 0.6153796315193176\n",
      "Epoch 6, Batch 654, Loss: 0.5089450478553772\n",
      "Epoch 6, Batch 655, Loss: 0.9738010764122009\n",
      "Epoch 6, Batch 656, Loss: 0.6196925044059753\n",
      "Epoch 6, Batch 657, Loss: 0.5718677043914795\n",
      "Epoch 6, Batch 658, Loss: 0.4418419599533081\n",
      "Epoch 6, Batch 659, Loss: 0.41721564531326294\n",
      "Epoch 6, Batch 660, Loss: 0.4586029052734375\n",
      "Epoch 6, Batch 661, Loss: 0.6168103218078613\n",
      "Epoch 6, Batch 662, Loss: 0.587298572063446\n",
      "Epoch 6, Batch 663, Loss: 0.7357930541038513\n",
      "Epoch 6, Batch 664, Loss: 0.4485388696193695\n",
      "Epoch 6, Batch 665, Loss: 0.6175357699394226\n",
      "Epoch 6, Batch 666, Loss: 0.40195798873901367\n",
      "Epoch 6, Batch 667, Loss: 0.6260350942611694\n",
      "Epoch 6, Batch 668, Loss: 0.48653438687324524\n",
      "Epoch 6, Batch 669, Loss: 0.5780868530273438\n",
      "Epoch 6, Batch 670, Loss: 0.5321810245513916\n",
      "Epoch 6, Batch 671, Loss: 0.594750702381134\n",
      "Epoch 6, Batch 672, Loss: 0.5292986631393433\n",
      "Epoch 6, Batch 673, Loss: 0.5871093273162842\n",
      "Epoch 6, Batch 674, Loss: 0.5840626955032349\n",
      "Epoch 6, Batch 675, Loss: 0.5842939019203186\n",
      "Epoch 6, Batch 676, Loss: 0.7468865513801575\n",
      "Epoch 6, Batch 677, Loss: 0.5022414922714233\n",
      "Epoch 6, Batch 678, Loss: 0.6493502855300903\n",
      "Epoch 6, Batch 679, Loss: 0.76697838306427\n",
      "Epoch 6, Batch 680, Loss: 0.5880413055419922\n",
      "Epoch 6, Batch 681, Loss: 0.5102834105491638\n",
      "Epoch 6, Batch 682, Loss: 0.7216929793357849\n",
      "Epoch 6, Batch 683, Loss: 0.634740948677063\n",
      "Epoch 6, Batch 684, Loss: 0.5158141851425171\n",
      "Epoch 6, Batch 685, Loss: 0.3548511266708374\n",
      "Epoch 6, Batch 686, Loss: 0.735846757888794\n",
      "Epoch 6, Batch 687, Loss: 0.44827720522880554\n",
      "Epoch 6, Batch 688, Loss: 0.632570743560791\n",
      "Epoch 6, Batch 689, Loss: 0.5043889284133911\n",
      "Epoch 6, Batch 690, Loss: 0.5290011167526245\n",
      "Epoch 6, Batch 691, Loss: 0.701685905456543\n",
      "Epoch 6, Batch 692, Loss: 0.6380924582481384\n",
      "Epoch 6, Batch 693, Loss: 0.4666481614112854\n",
      "Epoch 6, Batch 694, Loss: 0.48610055446624756\n",
      "Epoch 6, Batch 695, Loss: 0.8402819633483887\n",
      "Epoch 6, Batch 696, Loss: 0.4104822278022766\n",
      "Epoch 6, Batch 697, Loss: 0.8568979501724243\n",
      "Epoch 6, Batch 698, Loss: 0.5394787788391113\n",
      "Epoch 6, Batch 699, Loss: 0.7225660085678101\n",
      "Epoch 6, Batch 700, Loss: 0.907288670539856\n",
      "Epoch 6, Batch 701, Loss: 0.8096630573272705\n",
      "Epoch 6, Batch 702, Loss: 0.9511171579360962\n",
      "Epoch 6, Batch 703, Loss: 0.8383740782737732\n",
      "Epoch 6, Batch 704, Loss: 0.466753751039505\n",
      "Epoch 6, Batch 705, Loss: 0.7701976299285889\n",
      "Epoch 6, Batch 706, Loss: 0.5792896747589111\n",
      "Epoch 6, Batch 707, Loss: 0.5418091416358948\n",
      "Epoch 6, Batch 708, Loss: 0.5505703091621399\n",
      "Epoch 6, Batch 709, Loss: 0.6478311419487\n",
      "Epoch 6, Batch 710, Loss: 0.719302773475647\n",
      "Epoch 6, Batch 711, Loss: 0.6022627949714661\n",
      "Epoch 6, Batch 712, Loss: 0.6564427614212036\n",
      "Epoch 6, Batch 713, Loss: 0.48351210355758667\n",
      "Epoch 6, Batch 714, Loss: 0.5186496376991272\n",
      "Epoch 6, Batch 715, Loss: 0.5762763023376465\n",
      "Epoch 6, Batch 716, Loss: 0.5950226783752441\n",
      "Epoch 6, Batch 717, Loss: 0.4015105068683624\n",
      "Epoch 6, Batch 718, Loss: 0.47765326499938965\n",
      "Epoch 6, Batch 719, Loss: 0.7064627408981323\n",
      "Epoch 6, Batch 720, Loss: 0.9014876484870911\n",
      "Epoch 6, Batch 721, Loss: 0.7258473634719849\n",
      "Epoch 6, Batch 722, Loss: 0.5889848470687866\n",
      "Epoch 6, Batch 723, Loss: 0.5464886426925659\n",
      "Epoch 6, Batch 724, Loss: 0.4366181790828705\n",
      "Epoch 6, Batch 725, Loss: 0.5310139656066895\n",
      "Epoch 6, Batch 726, Loss: 0.4969603419303894\n",
      "Epoch 6, Batch 727, Loss: 0.736628532409668\n",
      "Epoch 6, Batch 728, Loss: 0.5536665320396423\n",
      "Epoch 6, Batch 729, Loss: 0.4799911379814148\n",
      "Epoch 6, Batch 730, Loss: 0.6033821105957031\n",
      "Epoch 6, Batch 731, Loss: 0.523794412612915\n",
      "Epoch 6, Batch 732, Loss: 0.48051464557647705\n",
      "Epoch 6, Batch 733, Loss: 0.5472341775894165\n",
      "Epoch 6, Batch 734, Loss: 0.4367532432079315\n",
      "Epoch 6, Batch 735, Loss: 0.6124100685119629\n",
      "Epoch 6, Batch 736, Loss: 0.5961275696754456\n",
      "Epoch 6, Batch 737, Loss: 0.43535616993904114\n",
      "Epoch 6, Batch 738, Loss: 0.4477965533733368\n",
      "Epoch 6, Batch 739, Loss: 0.6079112887382507\n",
      "Epoch 6, Batch 740, Loss: 0.6602535247802734\n",
      "Epoch 6, Batch 741, Loss: 0.8518329858779907\n",
      "Epoch 6, Batch 742, Loss: 0.509604811668396\n",
      "Epoch 6, Batch 743, Loss: 0.698976457118988\n",
      "Epoch 6, Batch 744, Loss: 0.4707787334918976\n",
      "Epoch 6, Batch 745, Loss: 0.567446768283844\n",
      "Epoch 6, Batch 746, Loss: 0.6399707794189453\n",
      "Epoch 6, Batch 747, Loss: 0.39138418436050415\n",
      "Epoch 6, Batch 748, Loss: 0.49547505378723145\n",
      "Epoch 6, Batch 749, Loss: 0.8380879163742065\n",
      "Epoch 6, Batch 750, Loss: 0.5284947156906128\n",
      "Epoch 6, Batch 751, Loss: 0.5305946469306946\n",
      "Epoch 6, Batch 752, Loss: 0.6337114572525024\n",
      "Epoch 6, Batch 753, Loss: 0.6748851537704468\n",
      "Epoch 6, Batch 754, Loss: 0.3637486398220062\n",
      "Epoch 6, Batch 755, Loss: 0.4925078749656677\n",
      "Epoch 6, Batch 756, Loss: 0.59736567735672\n",
      "Epoch 6, Batch 757, Loss: 0.5028848648071289\n",
      "Epoch 6, Batch 758, Loss: 0.713040828704834\n",
      "Epoch 6, Batch 759, Loss: 0.5237686038017273\n",
      "Epoch 6, Batch 760, Loss: 0.6808842420578003\n",
      "Epoch 6, Batch 761, Loss: 0.5887076258659363\n",
      "Epoch 6, Batch 762, Loss: 0.7586946487426758\n",
      "Epoch 6, Batch 763, Loss: 0.6754801273345947\n",
      "Epoch 6, Batch 764, Loss: 0.6090748906135559\n",
      "Epoch 6, Batch 765, Loss: 0.5105685591697693\n",
      "Epoch 6, Batch 766, Loss: 0.6128292083740234\n",
      "Epoch 6, Batch 767, Loss: 0.6118738651275635\n",
      "Epoch 6, Batch 768, Loss: 0.7805517911911011\n",
      "Epoch 6, Batch 769, Loss: 0.5300568342208862\n",
      "Epoch 6, Batch 770, Loss: 0.5772178769111633\n",
      "Epoch 6, Batch 771, Loss: 0.6349280476570129\n",
      "Epoch 6, Batch 772, Loss: 0.4728451371192932\n",
      "Epoch 6, Batch 773, Loss: 0.5178548097610474\n",
      "Epoch 6, Batch 774, Loss: 0.3526124954223633\n",
      "Epoch 6, Batch 775, Loss: 0.4353713095188141\n",
      "Epoch 6, Batch 776, Loss: 0.5472415685653687\n",
      "Epoch 6, Batch 777, Loss: 0.6240096092224121\n",
      "Epoch 6, Batch 778, Loss: 0.49297767877578735\n",
      "Epoch 6, Batch 779, Loss: 0.6290756464004517\n",
      "Epoch 6, Batch 780, Loss: 0.6276602149009705\n",
      "Epoch 6, Batch 781, Loss: 0.42054063081741333\n",
      "Epoch 6, Batch 782, Loss: 0.7293519973754883\n",
      "Epoch 6, Batch 783, Loss: 0.5718338489532471\n",
      "Epoch 6, Batch 784, Loss: 0.6254514455795288\n",
      "Epoch 6, Batch 785, Loss: 0.6406167149543762\n",
      "Epoch 6, Batch 786, Loss: 0.5489280819892883\n",
      "Epoch 6, Batch 787, Loss: 0.4108629524707794\n",
      "Epoch 6, Batch 788, Loss: 0.5876378417015076\n",
      "Epoch 6, Batch 789, Loss: 0.855792760848999\n",
      "Epoch 6, Batch 790, Loss: 0.5230345129966736\n",
      "Epoch 6, Batch 791, Loss: 0.681405782699585\n",
      "Epoch 6, Batch 792, Loss: 0.6868862509727478\n",
      "Epoch 6, Batch 793, Loss: 0.6127843856811523\n",
      "Epoch 6, Batch 794, Loss: 0.6698962450027466\n",
      "Epoch 6, Batch 795, Loss: 0.7125714421272278\n",
      "Epoch 6, Batch 796, Loss: 0.8403870463371277\n",
      "Epoch 6, Batch 797, Loss: 0.4098469614982605\n",
      "Epoch 6, Batch 798, Loss: 0.5258393287658691\n",
      "Epoch 6, Batch 799, Loss: 0.7331818342208862\n",
      "Epoch 6, Batch 800, Loss: 0.4282723665237427\n",
      "Epoch 6, Batch 801, Loss: 0.49673551321029663\n",
      "Epoch 6, Batch 802, Loss: 0.49023324251174927\n",
      "Epoch 6, Batch 803, Loss: 0.662958025932312\n",
      "Epoch 6, Batch 804, Loss: 0.5707621574401855\n",
      "Epoch 6, Batch 805, Loss: 0.4813655614852905\n",
      "Epoch 6, Batch 806, Loss: 0.529022216796875\n",
      "Epoch 6, Batch 807, Loss: 0.604192316532135\n",
      "Epoch 6, Batch 808, Loss: 0.5959879159927368\n",
      "Epoch 6, Batch 809, Loss: 0.5918890237808228\n",
      "Epoch 6, Batch 810, Loss: 0.4530814290046692\n",
      "Epoch 6, Batch 811, Loss: 0.6714844703674316\n",
      "Epoch 6, Batch 812, Loss: 0.44099926948547363\n",
      "Epoch 6, Batch 813, Loss: 0.5966516733169556\n",
      "Epoch 6, Batch 814, Loss: 0.5894255042076111\n",
      "Epoch 6, Batch 815, Loss: 0.6241687536239624\n",
      "Epoch 6, Batch 816, Loss: 0.6850154399871826\n",
      "Epoch 6, Batch 817, Loss: 0.5647546052932739\n",
      "Epoch 6, Batch 818, Loss: 0.626625120639801\n",
      "Epoch 6, Batch 819, Loss: 0.46975958347320557\n",
      "Epoch 6, Batch 820, Loss: 0.5863440632820129\n",
      "Epoch 6, Batch 821, Loss: 0.7326319217681885\n",
      "Epoch 6, Batch 822, Loss: 0.552905797958374\n",
      "Epoch 6, Batch 823, Loss: 0.48071810603141785\n",
      "Epoch 6, Batch 824, Loss: 0.7031345367431641\n",
      "Epoch 6, Batch 825, Loss: 0.5590083599090576\n",
      "Epoch 6, Batch 826, Loss: 0.7208840847015381\n",
      "Epoch 6, Batch 827, Loss: 0.6538653373718262\n",
      "Epoch 6, Batch 828, Loss: 0.760184645652771\n",
      "Epoch 6, Batch 829, Loss: 0.4470251798629761\n",
      "Epoch 6, Batch 830, Loss: 0.7081128358840942\n",
      "Epoch 6, Batch 831, Loss: 0.717657208442688\n",
      "Epoch 6, Batch 832, Loss: 0.5589073300361633\n",
      "Epoch 6, Batch 833, Loss: 0.5261765718460083\n",
      "Epoch 6, Batch 834, Loss: 0.563941478729248\n",
      "Epoch 6, Batch 835, Loss: 0.8740687370300293\n",
      "Epoch 6, Batch 836, Loss: 0.7976570129394531\n",
      "Epoch 6, Batch 837, Loss: 0.7256158590316772\n",
      "Epoch 6, Batch 838, Loss: 0.6797505617141724\n",
      "Epoch 6, Batch 839, Loss: 0.6062820553779602\n",
      "Epoch 6, Batch 840, Loss: 0.4573603868484497\n",
      "Epoch 6, Batch 841, Loss: 0.6866682171821594\n",
      "Epoch 6, Batch 842, Loss: 0.5666402578353882\n",
      "Epoch 6, Batch 843, Loss: 0.6088493466377258\n",
      "Epoch 6, Batch 844, Loss: 0.5381648540496826\n",
      "Epoch 6, Batch 845, Loss: 0.5326550006866455\n",
      "Epoch 6, Batch 846, Loss: 0.6235732436180115\n",
      "Epoch 6, Batch 847, Loss: 0.4969825744628906\n",
      "Epoch 6, Batch 848, Loss: 0.7015120387077332\n",
      "Epoch 6, Batch 849, Loss: 0.6376317143440247\n",
      "Epoch 6, Batch 850, Loss: 0.7749025821685791\n",
      "Epoch 6, Batch 851, Loss: 0.5798935294151306\n",
      "Epoch 6, Batch 852, Loss: 0.5442114472389221\n",
      "Epoch 6, Batch 853, Loss: 0.49332401156425476\n",
      "Epoch 6, Batch 854, Loss: 0.6180499792098999\n",
      "Epoch 6, Batch 855, Loss: 0.4853656589984894\n",
      "Epoch 6, Batch 856, Loss: 0.6244741678237915\n",
      "Epoch 6, Batch 857, Loss: 0.5868968963623047\n",
      "Epoch 6, Batch 858, Loss: 0.6206058859825134\n",
      "Epoch 6, Batch 859, Loss: 0.5391053557395935\n",
      "Epoch 6, Batch 860, Loss: 0.5447725057601929\n",
      "Epoch 6, Batch 861, Loss: 0.5053970217704773\n",
      "Epoch 6, Batch 862, Loss: 0.6298248767852783\n",
      "Epoch 6, Batch 863, Loss: 0.7202125191688538\n",
      "Epoch 6, Batch 864, Loss: 0.5482777953147888\n",
      "Epoch 6, Batch 865, Loss: 0.6350524425506592\n",
      "Epoch 6, Batch 866, Loss: 0.5750330686569214\n",
      "Epoch 6, Batch 867, Loss: 0.5591742992401123\n",
      "Epoch 6, Batch 868, Loss: 0.3986830711364746\n",
      "Epoch 6, Batch 869, Loss: 0.5774763822555542\n",
      "Epoch 6, Batch 870, Loss: 0.5374907851219177\n",
      "Epoch 6, Batch 871, Loss: 0.7252752780914307\n",
      "Epoch 6, Batch 872, Loss: 0.5679281949996948\n",
      "Epoch 6, Batch 873, Loss: 0.4655091464519501\n",
      "Epoch 6, Batch 874, Loss: 0.5591073632240295\n",
      "Epoch 6, Batch 875, Loss: 0.43064549565315247\n",
      "Epoch 6, Batch 876, Loss: 0.5873197913169861\n",
      "Epoch 6, Batch 877, Loss: 0.8435093760490417\n",
      "Epoch 6, Batch 878, Loss: 0.5765074491500854\n",
      "Epoch 6, Batch 879, Loss: 0.7899386286735535\n",
      "Epoch 6, Batch 880, Loss: 0.503267765045166\n",
      "Epoch 6, Batch 881, Loss: 0.5749617218971252\n",
      "Epoch 6, Batch 882, Loss: 0.6320667266845703\n",
      "Epoch 6, Batch 883, Loss: 0.4968826174736023\n",
      "Epoch 6, Batch 884, Loss: 0.6296961903572083\n",
      "Epoch 6, Batch 885, Loss: 0.49598586559295654\n",
      "Epoch 6, Batch 886, Loss: 0.7625857591629028\n",
      "Epoch 6, Batch 887, Loss: 0.4525984823703766\n",
      "Epoch 6, Batch 888, Loss: 0.40716779232025146\n",
      "Epoch 6, Batch 889, Loss: 0.49705037474632263\n",
      "Epoch 6, Batch 890, Loss: 0.730178713798523\n",
      "Epoch 6, Batch 891, Loss: 0.5720509886741638\n",
      "Epoch 6, Batch 892, Loss: 0.4810579717159271\n",
      "Epoch 6, Batch 893, Loss: 0.6288092136383057\n",
      "Epoch 6, Batch 894, Loss: 0.6665512919425964\n",
      "Epoch 6, Batch 895, Loss: 0.48537635803222656\n",
      "Epoch 6, Batch 896, Loss: 0.505145788192749\n",
      "Epoch 6, Batch 897, Loss: 0.6255431175231934\n",
      "Epoch 6, Batch 898, Loss: 0.7912839651107788\n",
      "Epoch 6, Batch 899, Loss: 0.6524326205253601\n",
      "Epoch 6, Batch 900, Loss: 0.6028041839599609\n",
      "Epoch 6, Batch 901, Loss: 0.6349058151245117\n",
      "Epoch 6, Batch 902, Loss: 0.6243419051170349\n",
      "Epoch 6, Batch 903, Loss: 0.5827740430831909\n",
      "Epoch 6, Batch 904, Loss: 0.6382061839103699\n",
      "Epoch 6, Batch 905, Loss: 0.8007811307907104\n",
      "Epoch 6, Batch 906, Loss: 0.5802944302558899\n",
      "Epoch 6, Batch 907, Loss: 0.5850716829299927\n",
      "Epoch 6, Batch 908, Loss: 0.3878890573978424\n",
      "Epoch 6, Batch 909, Loss: 0.4224574565887451\n",
      "Epoch 6, Batch 910, Loss: 0.5919075608253479\n",
      "Epoch 6, Batch 911, Loss: 0.7489228248596191\n",
      "Epoch 6, Batch 912, Loss: 0.5441290140151978\n",
      "Epoch 6, Batch 913, Loss: 0.5018063187599182\n",
      "Epoch 6, Batch 914, Loss: 0.5977215766906738\n",
      "Epoch 6, Batch 915, Loss: 0.6113654375076294\n",
      "Epoch 6, Batch 916, Loss: 0.548992395401001\n",
      "Epoch 6, Batch 917, Loss: 0.8269151449203491\n",
      "Epoch 6, Batch 918, Loss: 0.494052916765213\n",
      "Epoch 6, Batch 919, Loss: 0.5057456493377686\n",
      "Epoch 6, Batch 920, Loss: 0.5409661531448364\n",
      "Epoch 6, Batch 921, Loss: 0.4836078882217407\n",
      "Epoch 6, Batch 922, Loss: 0.7629625797271729\n",
      "Epoch 6, Batch 923, Loss: 0.39891788363456726\n",
      "Epoch 6, Batch 924, Loss: 0.4658346176147461\n",
      "Epoch 6, Batch 925, Loss: 0.670515775680542\n",
      "Epoch 6, Batch 926, Loss: 0.617035448551178\n",
      "Epoch 6, Batch 927, Loss: 0.4756677746772766\n",
      "Epoch 6, Batch 928, Loss: 0.6071614623069763\n",
      "Epoch 6, Batch 929, Loss: 0.7698184847831726\n",
      "Epoch 6, Batch 930, Loss: 0.6264081001281738\n",
      "Epoch 6, Batch 931, Loss: 0.5132635831832886\n",
      "Epoch 6, Batch 932, Loss: 0.7305436730384827\n",
      "Epoch 6, Batch 933, Loss: 0.8143850564956665\n",
      "Epoch 6, Batch 934, Loss: 0.5881495475769043\n",
      "Epoch 6, Batch 935, Loss: 0.5383533835411072\n",
      "Epoch 6, Batch 936, Loss: 0.6232919096946716\n",
      "Epoch 6, Batch 937, Loss: 0.3305027484893799\n",
      "Epoch 6, Batch 938, Loss: 0.5270391702651978\n",
      "Accuracy of train set: 0.7853166666666667\n",
      "Epoch 6, Batch 1, Test Loss: 0.4636363685131073\n",
      "Epoch 6, Batch 2, Test Loss: 0.45106950402259827\n",
      "Epoch 6, Batch 3, Test Loss: 0.7264494299888611\n",
      "Epoch 6, Batch 4, Test Loss: 0.5061675310134888\n",
      "Epoch 6, Batch 5, Test Loss: 0.6044498085975647\n",
      "Epoch 6, Batch 6, Test Loss: 0.6662405133247375\n",
      "Epoch 6, Batch 7, Test Loss: 0.6627765893936157\n",
      "Epoch 6, Batch 8, Test Loss: 0.9656219482421875\n",
      "Epoch 6, Batch 9, Test Loss: 0.8304925560951233\n",
      "Epoch 6, Batch 10, Test Loss: 0.5617383718490601\n",
      "Epoch 6, Batch 11, Test Loss: 0.548956573009491\n",
      "Epoch 6, Batch 12, Test Loss: 0.6048159599304199\n",
      "Epoch 6, Batch 13, Test Loss: 0.5583686828613281\n",
      "Epoch 6, Batch 14, Test Loss: 0.8799306154251099\n",
      "Epoch 6, Batch 15, Test Loss: 0.5444047451019287\n",
      "Epoch 6, Batch 16, Test Loss: 0.4582078456878662\n",
      "Epoch 6, Batch 17, Test Loss: 0.5485226511955261\n",
      "Epoch 6, Batch 18, Test Loss: 0.48898884654045105\n",
      "Epoch 6, Batch 19, Test Loss: 0.4866403043270111\n",
      "Epoch 6, Batch 20, Test Loss: 0.7500468492507935\n",
      "Epoch 6, Batch 21, Test Loss: 0.6480389833450317\n",
      "Epoch 6, Batch 22, Test Loss: 0.6766126155853271\n",
      "Epoch 6, Batch 23, Test Loss: 0.6871165633201599\n",
      "Epoch 6, Batch 24, Test Loss: 0.5621201395988464\n",
      "Epoch 6, Batch 25, Test Loss: 0.641840398311615\n",
      "Epoch 6, Batch 26, Test Loss: 0.7497923374176025\n",
      "Epoch 6, Batch 27, Test Loss: 0.5656797289848328\n",
      "Epoch 6, Batch 28, Test Loss: 0.5308285355567932\n",
      "Epoch 6, Batch 29, Test Loss: 0.7689048647880554\n",
      "Epoch 6, Batch 30, Test Loss: 0.5393062233924866\n",
      "Epoch 6, Batch 31, Test Loss: 0.5450232625007629\n",
      "Epoch 6, Batch 32, Test Loss: 0.6115846037864685\n",
      "Epoch 6, Batch 33, Test Loss: 0.5974980592727661\n",
      "Epoch 6, Batch 34, Test Loss: 0.7410114407539368\n",
      "Epoch 6, Batch 35, Test Loss: 0.6082318425178528\n",
      "Epoch 6, Batch 36, Test Loss: 0.6927649974822998\n",
      "Epoch 6, Batch 37, Test Loss: 0.9182955026626587\n",
      "Epoch 6, Batch 38, Test Loss: 0.7145129442214966\n",
      "Epoch 6, Batch 39, Test Loss: 0.8080595135688782\n",
      "Epoch 6, Batch 40, Test Loss: 0.5174674987792969\n",
      "Epoch 6, Batch 41, Test Loss: 0.4885970652103424\n",
      "Epoch 6, Batch 42, Test Loss: 0.48136016726493835\n",
      "Epoch 6, Batch 43, Test Loss: 0.6611545085906982\n",
      "Epoch 6, Batch 44, Test Loss: 0.48662975430488586\n",
      "Epoch 6, Batch 45, Test Loss: 0.6198099255561829\n",
      "Epoch 6, Batch 46, Test Loss: 0.4986414909362793\n",
      "Epoch 6, Batch 47, Test Loss: 0.5187259912490845\n",
      "Epoch 6, Batch 48, Test Loss: 0.5171211957931519\n",
      "Epoch 6, Batch 49, Test Loss: 0.5477941036224365\n",
      "Epoch 6, Batch 50, Test Loss: 0.6069477796554565\n",
      "Epoch 6, Batch 51, Test Loss: 0.5682654976844788\n",
      "Epoch 6, Batch 52, Test Loss: 0.7308960556983948\n",
      "Epoch 6, Batch 53, Test Loss: 0.5386210083961487\n",
      "Epoch 6, Batch 54, Test Loss: 0.4812397360801697\n",
      "Epoch 6, Batch 55, Test Loss: 0.5190348625183105\n",
      "Epoch 6, Batch 56, Test Loss: 0.7156816720962524\n",
      "Epoch 6, Batch 57, Test Loss: 0.6036577820777893\n",
      "Epoch 6, Batch 58, Test Loss: 0.5971747636795044\n",
      "Epoch 6, Batch 59, Test Loss: 0.5884310603141785\n",
      "Epoch 6, Batch 60, Test Loss: 0.5544235110282898\n",
      "Epoch 6, Batch 61, Test Loss: 0.5551418662071228\n",
      "Epoch 6, Batch 62, Test Loss: 0.7860003709793091\n",
      "Epoch 6, Batch 63, Test Loss: 0.5793408751487732\n",
      "Epoch 6, Batch 64, Test Loss: 0.5659434199333191\n",
      "Epoch 6, Batch 65, Test Loss: 0.6032754182815552\n",
      "Epoch 6, Batch 66, Test Loss: 0.752299964427948\n",
      "Epoch 6, Batch 67, Test Loss: 0.6610479950904846\n",
      "Epoch 6, Batch 68, Test Loss: 0.5657224655151367\n",
      "Epoch 6, Batch 69, Test Loss: 0.7119584083557129\n",
      "Epoch 6, Batch 70, Test Loss: 0.6106407642364502\n",
      "Epoch 6, Batch 71, Test Loss: 0.7662441730499268\n",
      "Epoch 6, Batch 72, Test Loss: 0.7835775017738342\n",
      "Epoch 6, Batch 73, Test Loss: 0.5044397711753845\n",
      "Epoch 6, Batch 74, Test Loss: 0.6790887713432312\n",
      "Epoch 6, Batch 75, Test Loss: 0.5234962701797485\n",
      "Epoch 6, Batch 76, Test Loss: 0.6879884004592896\n",
      "Epoch 6, Batch 77, Test Loss: 0.5095996260643005\n",
      "Epoch 6, Batch 78, Test Loss: 0.4524379372596741\n",
      "Epoch 6, Batch 79, Test Loss: 0.890786349773407\n",
      "Epoch 6, Batch 80, Test Loss: 0.5135231018066406\n",
      "Epoch 6, Batch 81, Test Loss: 0.6272093653678894\n",
      "Epoch 6, Batch 82, Test Loss: 0.6088263988494873\n",
      "Epoch 6, Batch 83, Test Loss: 0.7042197585105896\n",
      "Epoch 6, Batch 84, Test Loss: 0.7706325650215149\n",
      "Epoch 6, Batch 85, Test Loss: 0.6063251495361328\n",
      "Epoch 6, Batch 86, Test Loss: 0.7378509640693665\n",
      "Epoch 6, Batch 87, Test Loss: 0.6158960461616516\n",
      "Epoch 6, Batch 88, Test Loss: 0.5479695796966553\n",
      "Epoch 6, Batch 89, Test Loss: 0.5032898783683777\n",
      "Epoch 6, Batch 90, Test Loss: 0.5596851110458374\n",
      "Epoch 6, Batch 91, Test Loss: 0.3834760785102844\n",
      "Epoch 6, Batch 92, Test Loss: 0.6436100006103516\n",
      "Epoch 6, Batch 93, Test Loss: 0.5905898213386536\n",
      "Epoch 6, Batch 94, Test Loss: 0.5207099318504333\n",
      "Epoch 6, Batch 95, Test Loss: 0.5269413590431213\n",
      "Epoch 6, Batch 96, Test Loss: 0.40097060799598694\n",
      "Epoch 6, Batch 97, Test Loss: 0.4684787392616272\n",
      "Epoch 6, Batch 98, Test Loss: 0.7487403154373169\n",
      "Epoch 6, Batch 99, Test Loss: 0.8408231735229492\n",
      "Epoch 6, Batch 100, Test Loss: 0.5448196530342102\n",
      "Epoch 6, Batch 101, Test Loss: 0.579860508441925\n",
      "Epoch 6, Batch 102, Test Loss: 0.6079940795898438\n",
      "Epoch 6, Batch 103, Test Loss: 0.7405374646186829\n",
      "Epoch 6, Batch 104, Test Loss: 0.7033305764198303\n",
      "Epoch 6, Batch 105, Test Loss: 0.5699359178543091\n",
      "Epoch 6, Batch 106, Test Loss: 0.529517412185669\n",
      "Epoch 6, Batch 107, Test Loss: 0.6605706810951233\n",
      "Epoch 6, Batch 108, Test Loss: 0.7500580549240112\n",
      "Epoch 6, Batch 109, Test Loss: 0.6622462868690491\n",
      "Epoch 6, Batch 110, Test Loss: 0.6270893812179565\n",
      "Epoch 6, Batch 111, Test Loss: 0.7767700552940369\n",
      "Epoch 6, Batch 112, Test Loss: 0.7442972660064697\n",
      "Epoch 6, Batch 113, Test Loss: 0.7029045820236206\n",
      "Epoch 6, Batch 114, Test Loss: 0.5810351967811584\n",
      "Epoch 6, Batch 115, Test Loss: 0.5041832327842712\n",
      "Epoch 6, Batch 116, Test Loss: 0.6057847738265991\n",
      "Epoch 6, Batch 117, Test Loss: 0.6689115166664124\n",
      "Epoch 6, Batch 118, Test Loss: 0.46679943799972534\n",
      "Epoch 6, Batch 119, Test Loss: 0.5885312557220459\n",
      "Epoch 6, Batch 120, Test Loss: 0.7031179070472717\n",
      "Epoch 6, Batch 121, Test Loss: 0.5281050205230713\n",
      "Epoch 6, Batch 122, Test Loss: 0.6745811700820923\n",
      "Epoch 6, Batch 123, Test Loss: 0.48196786642074585\n",
      "Epoch 6, Batch 124, Test Loss: 0.5559263825416565\n",
      "Epoch 6, Batch 125, Test Loss: 0.5493311882019043\n",
      "Epoch 6, Batch 126, Test Loss: 0.48605501651763916\n",
      "Epoch 6, Batch 127, Test Loss: 0.45419231057167053\n",
      "Epoch 6, Batch 128, Test Loss: 0.5164790153503418\n",
      "Epoch 6, Batch 129, Test Loss: 1.0246814489364624\n",
      "Epoch 6, Batch 130, Test Loss: 0.7365490198135376\n",
      "Epoch 6, Batch 131, Test Loss: 0.6892634034156799\n",
      "Epoch 6, Batch 132, Test Loss: 0.4965970814228058\n",
      "Epoch 6, Batch 133, Test Loss: 0.5548326969146729\n",
      "Epoch 6, Batch 134, Test Loss: 0.6203253269195557\n",
      "Epoch 6, Batch 135, Test Loss: 0.5306257605552673\n",
      "Epoch 6, Batch 136, Test Loss: 0.5570563673973083\n",
      "Epoch 6, Batch 137, Test Loss: 0.6126310229301453\n",
      "Epoch 6, Batch 138, Test Loss: 0.6044008731842041\n",
      "Epoch 6, Batch 139, Test Loss: 0.504949688911438\n",
      "Epoch 6, Batch 140, Test Loss: 0.5069389939308167\n",
      "Epoch 6, Batch 141, Test Loss: 0.7379010319709778\n",
      "Epoch 6, Batch 142, Test Loss: 0.37600278854370117\n",
      "Epoch 6, Batch 143, Test Loss: 0.703409731388092\n",
      "Epoch 6, Batch 144, Test Loss: 0.6463854312896729\n",
      "Epoch 6, Batch 145, Test Loss: 0.5725997090339661\n",
      "Epoch 6, Batch 146, Test Loss: 0.6274542808532715\n",
      "Epoch 6, Batch 147, Test Loss: 0.7398214340209961\n",
      "Epoch 6, Batch 148, Test Loss: 0.6256702542304993\n",
      "Epoch 6, Batch 149, Test Loss: 0.5289948582649231\n",
      "Epoch 6, Batch 150, Test Loss: 0.6312673091888428\n",
      "Epoch 6, Batch 151, Test Loss: 0.9316482543945312\n",
      "Epoch 6, Batch 152, Test Loss: 0.5195640325546265\n",
      "Epoch 6, Batch 153, Test Loss: 0.4866667091846466\n",
      "Epoch 6, Batch 154, Test Loss: 0.4939438998699188\n",
      "Epoch 6, Batch 155, Test Loss: 0.6196925640106201\n",
      "Epoch 6, Batch 156, Test Loss: 0.578432559967041\n",
      "Epoch 6, Batch 157, Test Loss: 0.76191246509552\n",
      "Epoch 6, Batch 158, Test Loss: 0.5350942015647888\n",
      "Epoch 6, Batch 159, Test Loss: 0.6593060493469238\n",
      "Epoch 6, Batch 160, Test Loss: 0.7721887826919556\n",
      "Epoch 6, Batch 161, Test Loss: 0.6188821792602539\n",
      "Epoch 6, Batch 162, Test Loss: 0.5872069597244263\n",
      "Epoch 6, Batch 163, Test Loss: 1.0676451921463013\n",
      "Epoch 6, Batch 164, Test Loss: 0.7411941289901733\n",
      "Epoch 6, Batch 165, Test Loss: 0.7237814664840698\n",
      "Epoch 6, Batch 166, Test Loss: 0.6195206046104431\n",
      "Epoch 6, Batch 167, Test Loss: 0.609169602394104\n",
      "Epoch 6, Batch 168, Test Loss: 0.4526589810848236\n",
      "Epoch 6, Batch 169, Test Loss: 0.5969123840332031\n",
      "Epoch 6, Batch 170, Test Loss: 0.453407883644104\n",
      "Epoch 6, Batch 171, Test Loss: 0.48050183057785034\n",
      "Epoch 6, Batch 172, Test Loss: 0.7438288927078247\n",
      "Epoch 6, Batch 173, Test Loss: 0.5243527889251709\n",
      "Epoch 6, Batch 174, Test Loss: 0.7384878993034363\n",
      "Epoch 6, Batch 175, Test Loss: 0.6326173543930054\n",
      "Epoch 6, Batch 176, Test Loss: 0.5472425818443298\n",
      "Epoch 6, Batch 177, Test Loss: 0.6306781768798828\n",
      "Epoch 6, Batch 178, Test Loss: 0.532208263874054\n",
      "Epoch 6, Batch 179, Test Loss: 0.5127284526824951\n",
      "Epoch 6, Batch 180, Test Loss: 0.4806670546531677\n",
      "Epoch 6, Batch 181, Test Loss: 0.5397582650184631\n",
      "Epoch 6, Batch 182, Test Loss: 0.8146297335624695\n",
      "Epoch 6, Batch 183, Test Loss: 0.5186569690704346\n",
      "Epoch 6, Batch 184, Test Loss: 0.6153686046600342\n",
      "Epoch 6, Batch 185, Test Loss: 0.5878192782402039\n",
      "Epoch 6, Batch 186, Test Loss: 0.826130747795105\n",
      "Epoch 6, Batch 187, Test Loss: 0.5904262661933899\n",
      "Epoch 6, Batch 188, Test Loss: 0.5390208959579468\n",
      "Epoch 6, Batch 189, Test Loss: 0.5981296300888062\n",
      "Epoch 6, Batch 190, Test Loss: 0.5138307809829712\n",
      "Epoch 6, Batch 191, Test Loss: 0.5177215337753296\n",
      "Epoch 6, Batch 192, Test Loss: 0.4453476071357727\n",
      "Epoch 6, Batch 193, Test Loss: 0.8258484601974487\n",
      "Epoch 6, Batch 194, Test Loss: 0.5994985103607178\n",
      "Epoch 6, Batch 195, Test Loss: 0.47780120372772217\n",
      "Epoch 6, Batch 196, Test Loss: 0.7686518430709839\n",
      "Epoch 6, Batch 197, Test Loss: 0.550521194934845\n",
      "Epoch 6, Batch 198, Test Loss: 0.5533150434494019\n",
      "Epoch 6, Batch 199, Test Loss: 0.5624605417251587\n",
      "Epoch 6, Batch 200, Test Loss: 0.5006524324417114\n",
      "Epoch 6, Batch 201, Test Loss: 0.6234357953071594\n",
      "Epoch 6, Batch 202, Test Loss: 0.6672274470329285\n",
      "Epoch 6, Batch 203, Test Loss: 0.6417420506477356\n",
      "Epoch 6, Batch 204, Test Loss: 0.6316964626312256\n",
      "Epoch 6, Batch 205, Test Loss: 0.9778012633323669\n",
      "Epoch 6, Batch 206, Test Loss: 0.5438278913497925\n",
      "Epoch 6, Batch 207, Test Loss: 0.5107158422470093\n",
      "Epoch 6, Batch 208, Test Loss: 0.7592149972915649\n",
      "Epoch 6, Batch 209, Test Loss: 0.7678406834602356\n",
      "Epoch 6, Batch 210, Test Loss: 0.5275468826293945\n",
      "Epoch 6, Batch 211, Test Loss: 0.882269024848938\n",
      "Epoch 6, Batch 212, Test Loss: 0.4802265167236328\n",
      "Epoch 6, Batch 213, Test Loss: 0.757611870765686\n",
      "Epoch 6, Batch 214, Test Loss: 0.7684900760650635\n",
      "Epoch 6, Batch 215, Test Loss: 0.7487435340881348\n",
      "Epoch 6, Batch 216, Test Loss: 0.4300268888473511\n",
      "Epoch 6, Batch 217, Test Loss: 0.6716783046722412\n",
      "Epoch 6, Batch 218, Test Loss: 0.6724557280540466\n",
      "Epoch 6, Batch 219, Test Loss: 0.45741987228393555\n",
      "Epoch 6, Batch 220, Test Loss: 0.7197365164756775\n",
      "Epoch 6, Batch 221, Test Loss: 0.8152697086334229\n",
      "Epoch 6, Batch 222, Test Loss: 0.4721417725086212\n",
      "Epoch 6, Batch 223, Test Loss: 0.6439445614814758\n",
      "Epoch 6, Batch 224, Test Loss: 0.5975331664085388\n",
      "Epoch 6, Batch 225, Test Loss: 0.7225633263587952\n",
      "Epoch 6, Batch 226, Test Loss: 0.6715250611305237\n",
      "Epoch 6, Batch 227, Test Loss: 0.6593560576438904\n",
      "Epoch 6, Batch 228, Test Loss: 0.4741725027561188\n",
      "Epoch 6, Batch 229, Test Loss: 0.6073035001754761\n",
      "Epoch 6, Batch 230, Test Loss: 0.5795040130615234\n",
      "Epoch 6, Batch 231, Test Loss: 0.5682024359703064\n",
      "Epoch 6, Batch 232, Test Loss: 0.7462785243988037\n",
      "Epoch 6, Batch 233, Test Loss: 0.7286272048950195\n",
      "Epoch 6, Batch 234, Test Loss: 0.6000569462776184\n",
      "Epoch 6, Batch 235, Test Loss: 0.504752516746521\n",
      "Epoch 6, Batch 236, Test Loss: 0.6514381170272827\n",
      "Epoch 6, Batch 237, Test Loss: 0.5261560082435608\n",
      "Epoch 6, Batch 238, Test Loss: 0.6394585967063904\n",
      "Epoch 6, Batch 239, Test Loss: 0.7808367013931274\n",
      "Epoch 6, Batch 240, Test Loss: 0.691686749458313\n",
      "Epoch 6, Batch 241, Test Loss: 0.5643852949142456\n",
      "Epoch 6, Batch 242, Test Loss: 0.44548937678337097\n",
      "Epoch 6, Batch 243, Test Loss: 0.5926132798194885\n",
      "Epoch 6, Batch 244, Test Loss: 0.506756067276001\n",
      "Epoch 6, Batch 245, Test Loss: 0.6062196493148804\n",
      "Epoch 6, Batch 246, Test Loss: 0.6401816010475159\n",
      "Epoch 6, Batch 247, Test Loss: 0.3564729392528534\n",
      "Epoch 6, Batch 248, Test Loss: 0.5868648290634155\n",
      "Epoch 6, Batch 249, Test Loss: 0.9313591122627258\n",
      "Epoch 6, Batch 250, Test Loss: 0.8086298108100891\n",
      "Epoch 6, Batch 251, Test Loss: 0.6025344729423523\n",
      "Epoch 6, Batch 252, Test Loss: 0.5085034370422363\n",
      "Epoch 6, Batch 253, Test Loss: 0.6962264776229858\n",
      "Epoch 6, Batch 254, Test Loss: 0.5811136364936829\n",
      "Epoch 6, Batch 255, Test Loss: 0.7332338094711304\n",
      "Epoch 6, Batch 256, Test Loss: 0.7723708152770996\n",
      "Epoch 6, Batch 257, Test Loss: 0.6992583274841309\n",
      "Epoch 6, Batch 258, Test Loss: 0.49430370330810547\n",
      "Epoch 6, Batch 259, Test Loss: 0.7323792576789856\n",
      "Epoch 6, Batch 260, Test Loss: 0.6876453757286072\n",
      "Epoch 6, Batch 261, Test Loss: 0.5768548250198364\n",
      "Epoch 6, Batch 262, Test Loss: 0.5366636514663696\n",
      "Epoch 6, Batch 263, Test Loss: 0.5271320343017578\n",
      "Epoch 6, Batch 264, Test Loss: 0.5606644749641418\n",
      "Epoch 6, Batch 265, Test Loss: 0.7621972560882568\n",
      "Epoch 6, Batch 266, Test Loss: 0.504030168056488\n",
      "Epoch 6, Batch 267, Test Loss: 0.5169110298156738\n",
      "Epoch 6, Batch 268, Test Loss: 0.6034701466560364\n",
      "Epoch 6, Batch 269, Test Loss: 0.7610198855400085\n",
      "Epoch 6, Batch 270, Test Loss: 0.6683840155601501\n",
      "Epoch 6, Batch 271, Test Loss: 0.5379835367202759\n",
      "Epoch 6, Batch 272, Test Loss: 0.7387881875038147\n",
      "Epoch 6, Batch 273, Test Loss: 0.427478164434433\n",
      "Epoch 6, Batch 274, Test Loss: 0.675640344619751\n",
      "Epoch 6, Batch 275, Test Loss: 0.6992778778076172\n",
      "Epoch 6, Batch 276, Test Loss: 0.6544809937477112\n",
      "Epoch 6, Batch 277, Test Loss: 0.5832345485687256\n",
      "Epoch 6, Batch 278, Test Loss: 0.8302606344223022\n",
      "Epoch 6, Batch 279, Test Loss: 0.5237589478492737\n",
      "Epoch 6, Batch 280, Test Loss: 0.6853805184364319\n",
      "Epoch 6, Batch 281, Test Loss: 0.4378527104854584\n",
      "Epoch 6, Batch 282, Test Loss: 0.8432924747467041\n",
      "Epoch 6, Batch 283, Test Loss: 0.7523798942565918\n",
      "Epoch 6, Batch 284, Test Loss: 0.5781509876251221\n",
      "Epoch 6, Batch 285, Test Loss: 0.6228657364845276\n",
      "Epoch 6, Batch 286, Test Loss: 0.5339446663856506\n",
      "Epoch 6, Batch 287, Test Loss: 0.5793664455413818\n",
      "Epoch 6, Batch 288, Test Loss: 0.834100067615509\n",
      "Epoch 6, Batch 289, Test Loss: 0.5532316565513611\n",
      "Epoch 6, Batch 290, Test Loss: 0.4419185221195221\n",
      "Epoch 6, Batch 291, Test Loss: 0.6574034094810486\n",
      "Epoch 6, Batch 292, Test Loss: 0.42229217290878296\n",
      "Epoch 6, Batch 293, Test Loss: 0.5764886140823364\n",
      "Epoch 6, Batch 294, Test Loss: 0.4614790380001068\n",
      "Epoch 6, Batch 295, Test Loss: 0.5672885179519653\n",
      "Epoch 6, Batch 296, Test Loss: 0.6988822221755981\n",
      "Epoch 6, Batch 297, Test Loss: 0.5663952231407166\n",
      "Epoch 6, Batch 298, Test Loss: 0.5924009084701538\n",
      "Epoch 6, Batch 299, Test Loss: 0.6001718044281006\n",
      "Epoch 6, Batch 300, Test Loss: 0.5509282946586609\n",
      "Epoch 6, Batch 301, Test Loss: 0.6736257076263428\n",
      "Epoch 6, Batch 302, Test Loss: 0.634489119052887\n",
      "Epoch 6, Batch 303, Test Loss: 0.8023145198822021\n",
      "Epoch 6, Batch 304, Test Loss: 0.8871950507164001\n",
      "Epoch 6, Batch 305, Test Loss: 0.5200071930885315\n",
      "Epoch 6, Batch 306, Test Loss: 0.7201244235038757\n",
      "Epoch 6, Batch 307, Test Loss: 0.62623131275177\n",
      "Epoch 6, Batch 308, Test Loss: 0.6784192323684692\n",
      "Epoch 6, Batch 309, Test Loss: 0.6913695931434631\n",
      "Epoch 6, Batch 310, Test Loss: 0.7504652142524719\n",
      "Epoch 6, Batch 311, Test Loss: 0.9931024312973022\n",
      "Epoch 6, Batch 312, Test Loss: 0.6186869144439697\n",
      "Epoch 6, Batch 313, Test Loss: 0.6225690245628357\n",
      "Epoch 6, Batch 314, Test Loss: 0.7582060098648071\n",
      "Epoch 6, Batch 315, Test Loss: 0.9287841320037842\n",
      "Epoch 6, Batch 316, Test Loss: 0.40652602910995483\n",
      "Epoch 6, Batch 317, Test Loss: 0.5863824486732483\n",
      "Epoch 6, Batch 318, Test Loss: 0.7022183537483215\n",
      "Epoch 6, Batch 319, Test Loss: 0.5562143325805664\n",
      "Epoch 6, Batch 320, Test Loss: 0.6073328256607056\n",
      "Epoch 6, Batch 321, Test Loss: 0.444290429353714\n",
      "Epoch 6, Batch 322, Test Loss: 0.6347782015800476\n",
      "Epoch 6, Batch 323, Test Loss: 0.680147647857666\n",
      "Epoch 6, Batch 324, Test Loss: 0.800112247467041\n",
      "Epoch 6, Batch 325, Test Loss: 0.41936761140823364\n",
      "Epoch 6, Batch 326, Test Loss: 0.8621729612350464\n",
      "Epoch 6, Batch 327, Test Loss: 0.8567587733268738\n",
      "Epoch 6, Batch 328, Test Loss: 0.7848243117332458\n",
      "Epoch 6, Batch 329, Test Loss: 0.7617785930633545\n",
      "Epoch 6, Batch 330, Test Loss: 0.7512305974960327\n",
      "Epoch 6, Batch 331, Test Loss: 0.4673924148082733\n",
      "Epoch 6, Batch 332, Test Loss: 0.3968746066093445\n",
      "Epoch 6, Batch 333, Test Loss: 0.7429345846176147\n",
      "Epoch 6, Batch 334, Test Loss: 0.5079560279846191\n",
      "Epoch 6, Batch 335, Test Loss: 0.7203012108802795\n",
      "Epoch 6, Batch 336, Test Loss: 0.7611624598503113\n",
      "Epoch 6, Batch 337, Test Loss: 0.6242783665657043\n",
      "Epoch 6, Batch 338, Test Loss: 0.6144156455993652\n",
      "Epoch 6, Batch 339, Test Loss: 0.5765825510025024\n",
      "Epoch 6, Batch 340, Test Loss: 0.6859018206596375\n",
      "Epoch 6, Batch 341, Test Loss: 0.5541369915008545\n",
      "Epoch 6, Batch 342, Test Loss: 0.7433371543884277\n",
      "Epoch 6, Batch 343, Test Loss: 0.7110174298286438\n",
      "Epoch 6, Batch 344, Test Loss: 0.5802584886550903\n",
      "Epoch 6, Batch 345, Test Loss: 0.5266399383544922\n",
      "Epoch 6, Batch 346, Test Loss: 0.6428289413452148\n",
      "Epoch 6, Batch 347, Test Loss: 0.9041681289672852\n",
      "Epoch 6, Batch 348, Test Loss: 0.7132477760314941\n",
      "Epoch 6, Batch 349, Test Loss: 0.7653055191040039\n",
      "Epoch 6, Batch 350, Test Loss: 0.678955078125\n",
      "Epoch 6, Batch 351, Test Loss: 0.7499916553497314\n",
      "Epoch 6, Batch 352, Test Loss: 0.6333779692649841\n",
      "Epoch 6, Batch 353, Test Loss: 0.7094267010688782\n",
      "Epoch 6, Batch 354, Test Loss: 0.4898950457572937\n",
      "Epoch 6, Batch 355, Test Loss: 0.7317993640899658\n",
      "Epoch 6, Batch 356, Test Loss: 0.6848042607307434\n",
      "Epoch 6, Batch 357, Test Loss: 0.6377829909324646\n",
      "Epoch 6, Batch 358, Test Loss: 0.7267436981201172\n",
      "Epoch 6, Batch 359, Test Loss: 0.572982907295227\n",
      "Epoch 6, Batch 360, Test Loss: 0.5845751762390137\n",
      "Epoch 6, Batch 361, Test Loss: 0.550841212272644\n",
      "Epoch 6, Batch 362, Test Loss: 0.6592472195625305\n",
      "Epoch 6, Batch 363, Test Loss: 0.620315670967102\n",
      "Epoch 6, Batch 364, Test Loss: 0.4352312386035919\n",
      "Epoch 6, Batch 365, Test Loss: 0.574288010597229\n",
      "Epoch 6, Batch 366, Test Loss: 0.6355427503585815\n",
      "Epoch 6, Batch 367, Test Loss: 0.502895176410675\n",
      "Epoch 6, Batch 368, Test Loss: 0.7522807121276855\n",
      "Epoch 6, Batch 369, Test Loss: 0.67032790184021\n",
      "Epoch 6, Batch 370, Test Loss: 0.5630075931549072\n",
      "Epoch 6, Batch 371, Test Loss: 0.790160059928894\n",
      "Epoch 6, Batch 372, Test Loss: 0.5562255382537842\n",
      "Epoch 6, Batch 373, Test Loss: 0.4667762815952301\n",
      "Epoch 6, Batch 374, Test Loss: 0.8689699172973633\n",
      "Epoch 6, Batch 375, Test Loss: 0.349277138710022\n",
      "Epoch 6, Batch 376, Test Loss: 0.6967008113861084\n",
      "Epoch 6, Batch 377, Test Loss: 0.4536631107330322\n",
      "Epoch 6, Batch 378, Test Loss: 0.5066890120506287\n",
      "Epoch 6, Batch 379, Test Loss: 0.7398368716239929\n",
      "Epoch 6, Batch 380, Test Loss: 0.6525163650512695\n",
      "Epoch 6, Batch 381, Test Loss: 0.7026535272598267\n",
      "Epoch 6, Batch 382, Test Loss: 0.7825117707252502\n",
      "Epoch 6, Batch 383, Test Loss: 0.48294734954833984\n",
      "Epoch 6, Batch 384, Test Loss: 0.792354941368103\n",
      "Epoch 6, Batch 385, Test Loss: 0.6525413393974304\n",
      "Epoch 6, Batch 386, Test Loss: 0.6281780004501343\n",
      "Epoch 6, Batch 387, Test Loss: 0.5993386507034302\n",
      "Epoch 6, Batch 388, Test Loss: 0.7101094722747803\n",
      "Epoch 6, Batch 389, Test Loss: 0.6446363925933838\n",
      "Epoch 6, Batch 390, Test Loss: 0.5483096837997437\n",
      "Epoch 6, Batch 391, Test Loss: 0.672943115234375\n",
      "Epoch 6, Batch 392, Test Loss: 0.5589698553085327\n",
      "Epoch 6, Batch 393, Test Loss: 0.7170079946517944\n",
      "Epoch 6, Batch 394, Test Loss: 0.5760349631309509\n",
      "Epoch 6, Batch 395, Test Loss: 0.5808521509170532\n",
      "Epoch 6, Batch 396, Test Loss: 0.7796074748039246\n",
      "Epoch 6, Batch 397, Test Loss: 0.7045682072639465\n",
      "Epoch 6, Batch 398, Test Loss: 0.4340432584285736\n",
      "Epoch 6, Batch 399, Test Loss: 0.7507442831993103\n",
      "Epoch 6, Batch 400, Test Loss: 0.5640535354614258\n",
      "Epoch 6, Batch 401, Test Loss: 0.4674282670021057\n",
      "Epoch 6, Batch 402, Test Loss: 0.6024772524833679\n",
      "Epoch 6, Batch 403, Test Loss: 0.7334760427474976\n",
      "Epoch 6, Batch 404, Test Loss: 0.6098067760467529\n",
      "Epoch 6, Batch 405, Test Loss: 0.5392122268676758\n",
      "Epoch 6, Batch 406, Test Loss: 0.6272932887077332\n",
      "Epoch 6, Batch 407, Test Loss: 0.49866053462028503\n",
      "Epoch 6, Batch 408, Test Loss: 0.4900038242340088\n",
      "Epoch 6, Batch 409, Test Loss: 0.6634030342102051\n",
      "Epoch 6, Batch 410, Test Loss: 0.9134235978126526\n",
      "Epoch 6, Batch 411, Test Loss: 0.637687087059021\n",
      "Epoch 6, Batch 412, Test Loss: 0.6137415170669556\n",
      "Epoch 6, Batch 413, Test Loss: 0.6669321656227112\n",
      "Epoch 6, Batch 414, Test Loss: 0.6097809672355652\n",
      "Epoch 6, Batch 415, Test Loss: 0.5546436309814453\n",
      "Epoch 6, Batch 416, Test Loss: 0.5157463550567627\n",
      "Epoch 6, Batch 417, Test Loss: 0.6910114884376526\n",
      "Epoch 6, Batch 418, Test Loss: 0.43597856163978577\n",
      "Epoch 6, Batch 419, Test Loss: 0.5231841802597046\n",
      "Epoch 6, Batch 420, Test Loss: 0.5584036707878113\n",
      "Epoch 6, Batch 421, Test Loss: 0.47349813580513\n",
      "Epoch 6, Batch 422, Test Loss: 0.8209882974624634\n",
      "Epoch 6, Batch 423, Test Loss: 0.5274531245231628\n",
      "Epoch 6, Batch 424, Test Loss: 0.767202615737915\n",
      "Epoch 6, Batch 425, Test Loss: 0.4854339361190796\n",
      "Epoch 6, Batch 426, Test Loss: 0.7874405980110168\n",
      "Epoch 6, Batch 427, Test Loss: 0.6283972859382629\n",
      "Epoch 6, Batch 428, Test Loss: 0.6337116360664368\n",
      "Epoch 6, Batch 429, Test Loss: 0.5204426050186157\n",
      "Epoch 6, Batch 430, Test Loss: 0.588434636592865\n",
      "Epoch 6, Batch 431, Test Loss: 0.6283444762229919\n",
      "Epoch 6, Batch 432, Test Loss: 0.9508823156356812\n",
      "Epoch 6, Batch 433, Test Loss: 0.7645972967147827\n",
      "Epoch 6, Batch 434, Test Loss: 0.5699561238288879\n",
      "Epoch 6, Batch 435, Test Loss: 0.547845184803009\n",
      "Epoch 6, Batch 436, Test Loss: 0.5141797065734863\n",
      "Epoch 6, Batch 437, Test Loss: 0.6198182106018066\n",
      "Epoch 6, Batch 438, Test Loss: 0.48256275057792664\n",
      "Epoch 6, Batch 439, Test Loss: 0.6290432810783386\n",
      "Epoch 6, Batch 440, Test Loss: 0.5542592406272888\n",
      "Epoch 6, Batch 441, Test Loss: 0.5487844944000244\n",
      "Epoch 6, Batch 442, Test Loss: 0.512622058391571\n",
      "Epoch 6, Batch 443, Test Loss: 0.5088289976119995\n",
      "Epoch 6, Batch 444, Test Loss: 0.6588638424873352\n",
      "Epoch 6, Batch 445, Test Loss: 0.49515825510025024\n",
      "Epoch 6, Batch 446, Test Loss: 0.4477289319038391\n",
      "Epoch 6, Batch 447, Test Loss: 0.6039943695068359\n",
      "Epoch 6, Batch 448, Test Loss: 0.4277738928794861\n",
      "Epoch 6, Batch 449, Test Loss: 0.6961740851402283\n",
      "Epoch 6, Batch 450, Test Loss: 0.5565589666366577\n",
      "Epoch 6, Batch 451, Test Loss: 0.7070721387863159\n",
      "Epoch 6, Batch 452, Test Loss: 0.5176599025726318\n",
      "Epoch 6, Batch 453, Test Loss: 0.5424063205718994\n",
      "Epoch 6, Batch 454, Test Loss: 0.750648558139801\n",
      "Epoch 6, Batch 455, Test Loss: 0.6161150932312012\n",
      "Epoch 6, Batch 456, Test Loss: 0.651853084564209\n",
      "Epoch 6, Batch 457, Test Loss: 0.4941402077674866\n",
      "Epoch 6, Batch 458, Test Loss: 0.5291070938110352\n",
      "Epoch 6, Batch 459, Test Loss: 0.5640468001365662\n",
      "Epoch 6, Batch 460, Test Loss: 0.46107175946235657\n",
      "Epoch 6, Batch 461, Test Loss: 0.6620374917984009\n",
      "Epoch 6, Batch 462, Test Loss: 0.5659229159355164\n",
      "Epoch 6, Batch 463, Test Loss: 0.6761521100997925\n",
      "Epoch 6, Batch 464, Test Loss: 0.8003793954849243\n",
      "Epoch 6, Batch 465, Test Loss: 0.5760072469711304\n",
      "Epoch 6, Batch 466, Test Loss: 0.709812581539154\n",
      "Epoch 6, Batch 467, Test Loss: 0.5182344317436218\n",
      "Epoch 6, Batch 468, Test Loss: 0.5855439901351929\n",
      "Epoch 6, Batch 469, Test Loss: 0.581917941570282\n",
      "Epoch 6, Batch 470, Test Loss: 0.5461086630821228\n",
      "Epoch 6, Batch 471, Test Loss: 0.46357792615890503\n",
      "Epoch 6, Batch 472, Test Loss: 0.645699143409729\n",
      "Epoch 6, Batch 473, Test Loss: 0.7729670405387878\n",
      "Epoch 6, Batch 474, Test Loss: 0.85421222448349\n",
      "Epoch 6, Batch 475, Test Loss: 0.6534897089004517\n",
      "Epoch 6, Batch 476, Test Loss: 0.6068872213363647\n",
      "Epoch 6, Batch 477, Test Loss: 0.4516512155532837\n",
      "Epoch 6, Batch 478, Test Loss: 0.4984162449836731\n",
      "Epoch 6, Batch 479, Test Loss: 0.5479512214660645\n",
      "Epoch 6, Batch 480, Test Loss: 0.5407673716545105\n",
      "Epoch 6, Batch 481, Test Loss: 0.6100144982337952\n",
      "Epoch 6, Batch 482, Test Loss: 0.5393694043159485\n",
      "Epoch 6, Batch 483, Test Loss: 0.6097306609153748\n",
      "Epoch 6, Batch 484, Test Loss: 0.7916573286056519\n",
      "Epoch 6, Batch 485, Test Loss: 0.6894204616546631\n",
      "Epoch 6, Batch 486, Test Loss: 0.6902817487716675\n",
      "Epoch 6, Batch 487, Test Loss: 0.6623993515968323\n",
      "Epoch 6, Batch 488, Test Loss: 0.4927021861076355\n",
      "Epoch 6, Batch 489, Test Loss: 0.6052029728889465\n",
      "Epoch 6, Batch 490, Test Loss: 0.44487789273262024\n",
      "Epoch 6, Batch 491, Test Loss: 0.4923051595687866\n",
      "Epoch 6, Batch 492, Test Loss: 0.42444175481796265\n",
      "Epoch 6, Batch 493, Test Loss: 0.48066893219947815\n",
      "Epoch 6, Batch 494, Test Loss: 0.4461820721626282\n",
      "Epoch 6, Batch 495, Test Loss: 0.6555927395820618\n",
      "Epoch 6, Batch 496, Test Loss: 0.8085330724716187\n",
      "Epoch 6, Batch 497, Test Loss: 0.9362499713897705\n",
      "Epoch 6, Batch 498, Test Loss: 0.6415184140205383\n",
      "Epoch 6, Batch 499, Test Loss: 0.7768208384513855\n",
      "Epoch 6, Batch 500, Test Loss: 0.621742844581604\n",
      "Epoch 6, Batch 501, Test Loss: 0.655124306678772\n",
      "Epoch 6, Batch 502, Test Loss: 0.7130928039550781\n",
      "Epoch 6, Batch 503, Test Loss: 0.5079447627067566\n",
      "Epoch 6, Batch 504, Test Loss: 0.6069527268409729\n",
      "Epoch 6, Batch 505, Test Loss: 0.5954720973968506\n",
      "Epoch 6, Batch 506, Test Loss: 0.5679207444190979\n",
      "Epoch 6, Batch 507, Test Loss: 0.7484015226364136\n",
      "Epoch 6, Batch 508, Test Loss: 0.5069855451583862\n",
      "Epoch 6, Batch 509, Test Loss: 0.5335049629211426\n",
      "Epoch 6, Batch 510, Test Loss: 0.7530044317245483\n",
      "Epoch 6, Batch 511, Test Loss: 0.5194639563560486\n",
      "Epoch 6, Batch 512, Test Loss: 0.6973560452461243\n",
      "Epoch 6, Batch 513, Test Loss: 0.5049822330474854\n",
      "Epoch 6, Batch 514, Test Loss: 0.6965593099594116\n",
      "Epoch 6, Batch 515, Test Loss: 0.6813439726829529\n",
      "Epoch 6, Batch 516, Test Loss: 0.6560177803039551\n",
      "Epoch 6, Batch 517, Test Loss: 0.40909087657928467\n",
      "Epoch 6, Batch 518, Test Loss: 0.5647415518760681\n",
      "Epoch 6, Batch 519, Test Loss: 0.5677391290664673\n",
      "Epoch 6, Batch 520, Test Loss: 0.5585112571716309\n",
      "Epoch 6, Batch 521, Test Loss: 0.48759084939956665\n",
      "Epoch 6, Batch 522, Test Loss: 0.5685924887657166\n",
      "Epoch 6, Batch 523, Test Loss: 0.51458340883255\n",
      "Epoch 6, Batch 524, Test Loss: 0.5441737771034241\n",
      "Epoch 6, Batch 525, Test Loss: 0.6016849875450134\n",
      "Epoch 6, Batch 526, Test Loss: 0.37937045097351074\n",
      "Epoch 6, Batch 527, Test Loss: 0.6929923892021179\n",
      "Epoch 6, Batch 528, Test Loss: 0.8101688623428345\n",
      "Epoch 6, Batch 529, Test Loss: 0.7822152376174927\n",
      "Epoch 6, Batch 530, Test Loss: 0.5758825540542603\n",
      "Epoch 6, Batch 531, Test Loss: 0.5936912298202515\n",
      "Epoch 6, Batch 532, Test Loss: 0.5745078325271606\n",
      "Epoch 6, Batch 533, Test Loss: 0.5540172457695007\n",
      "Epoch 6, Batch 534, Test Loss: 0.6361852884292603\n",
      "Epoch 6, Batch 535, Test Loss: 0.8852723836898804\n",
      "Epoch 6, Batch 536, Test Loss: 0.8007513284683228\n",
      "Epoch 6, Batch 537, Test Loss: 0.7031439542770386\n",
      "Epoch 6, Batch 538, Test Loss: 0.5227964520454407\n",
      "Epoch 6, Batch 539, Test Loss: 0.6089367866516113\n",
      "Epoch 6, Batch 540, Test Loss: 0.7276062369346619\n",
      "Epoch 6, Batch 541, Test Loss: 0.7360652685165405\n",
      "Epoch 6, Batch 542, Test Loss: 0.6706895232200623\n",
      "Epoch 6, Batch 543, Test Loss: 0.7205303311347961\n",
      "Epoch 6, Batch 544, Test Loss: 0.4630276560783386\n",
      "Epoch 6, Batch 545, Test Loss: 0.6839162707328796\n",
      "Epoch 6, Batch 546, Test Loss: 0.5926566123962402\n",
      "Epoch 6, Batch 547, Test Loss: 0.9003278613090515\n",
      "Epoch 6, Batch 548, Test Loss: 0.6269248723983765\n",
      "Epoch 6, Batch 549, Test Loss: 0.5432291030883789\n",
      "Epoch 6, Batch 550, Test Loss: 0.6983066201210022\n",
      "Epoch 6, Batch 551, Test Loss: 0.5794135332107544\n",
      "Epoch 6, Batch 552, Test Loss: 0.7859333157539368\n",
      "Epoch 6, Batch 553, Test Loss: 0.5407113432884216\n",
      "Epoch 6, Batch 554, Test Loss: 0.5128264427185059\n",
      "Epoch 6, Batch 555, Test Loss: 0.814140796661377\n",
      "Epoch 6, Batch 556, Test Loss: 0.6052616834640503\n",
      "Epoch 6, Batch 557, Test Loss: 0.43964552879333496\n",
      "Epoch 6, Batch 558, Test Loss: 0.8445218801498413\n",
      "Epoch 6, Batch 559, Test Loss: 0.5813595056533813\n",
      "Epoch 6, Batch 560, Test Loss: 0.5856233835220337\n",
      "Epoch 6, Batch 561, Test Loss: 0.3941216468811035\n",
      "Epoch 6, Batch 562, Test Loss: 0.5824978351593018\n",
      "Epoch 6, Batch 563, Test Loss: 0.7081258893013\n",
      "Epoch 6, Batch 564, Test Loss: 0.5763512849807739\n",
      "Epoch 6, Batch 565, Test Loss: 0.5831705331802368\n",
      "Epoch 6, Batch 566, Test Loss: 0.6184281706809998\n",
      "Epoch 6, Batch 567, Test Loss: 0.5651959180831909\n",
      "Epoch 6, Batch 568, Test Loss: 0.7674373984336853\n",
      "Epoch 6, Batch 569, Test Loss: 0.6224064230918884\n",
      "Epoch 6, Batch 570, Test Loss: 0.7657841444015503\n",
      "Epoch 6, Batch 571, Test Loss: 0.7018794417381287\n",
      "Epoch 6, Batch 572, Test Loss: 0.470122754573822\n",
      "Epoch 6, Batch 573, Test Loss: 0.6106711030006409\n",
      "Epoch 6, Batch 574, Test Loss: 0.5528612732887268\n",
      "Epoch 6, Batch 575, Test Loss: 0.789502739906311\n",
      "Epoch 6, Batch 576, Test Loss: 0.6910133957862854\n",
      "Epoch 6, Batch 577, Test Loss: 0.686162531375885\n",
      "Epoch 6, Batch 578, Test Loss: 0.6858168244361877\n",
      "Epoch 6, Batch 579, Test Loss: 0.5754928588867188\n",
      "Epoch 6, Batch 580, Test Loss: 0.5472684502601624\n",
      "Epoch 6, Batch 581, Test Loss: 0.7571462392807007\n",
      "Epoch 6, Batch 582, Test Loss: 0.6286223530769348\n",
      "Epoch 6, Batch 583, Test Loss: 0.7451465129852295\n",
      "Epoch 6, Batch 584, Test Loss: 0.5949418544769287\n",
      "Epoch 6, Batch 585, Test Loss: 0.6235456466674805\n",
      "Epoch 6, Batch 586, Test Loss: 0.818384051322937\n",
      "Epoch 6, Batch 587, Test Loss: 0.8338810205459595\n",
      "Epoch 6, Batch 588, Test Loss: 0.6553311347961426\n",
      "Epoch 6, Batch 589, Test Loss: 0.5968328714370728\n",
      "Epoch 6, Batch 590, Test Loss: 0.927044153213501\n",
      "Epoch 6, Batch 591, Test Loss: 0.5277568697929382\n",
      "Epoch 6, Batch 592, Test Loss: 0.6810965538024902\n",
      "Epoch 6, Batch 593, Test Loss: 0.5196117758750916\n",
      "Epoch 6, Batch 594, Test Loss: 0.7923462390899658\n",
      "Epoch 6, Batch 595, Test Loss: 0.6547383666038513\n",
      "Epoch 6, Batch 596, Test Loss: 0.759257972240448\n",
      "Epoch 6, Batch 597, Test Loss: 0.578685462474823\n",
      "Epoch 6, Batch 598, Test Loss: 0.6314259171485901\n",
      "Epoch 6, Batch 599, Test Loss: 0.6354159712791443\n",
      "Epoch 6, Batch 600, Test Loss: 0.5829079151153564\n",
      "Epoch 6, Batch 601, Test Loss: 0.4951655864715576\n",
      "Epoch 6, Batch 602, Test Loss: 0.7251613140106201\n",
      "Epoch 6, Batch 603, Test Loss: 0.45379582047462463\n",
      "Epoch 6, Batch 604, Test Loss: 0.6647926568984985\n",
      "Epoch 6, Batch 605, Test Loss: 0.4092447757720947\n",
      "Epoch 6, Batch 606, Test Loss: 0.4179568588733673\n",
      "Epoch 6, Batch 607, Test Loss: 0.4963206648826599\n",
      "Epoch 6, Batch 608, Test Loss: 0.8489717841148376\n",
      "Epoch 6, Batch 609, Test Loss: 0.6298454403877258\n",
      "Epoch 6, Batch 610, Test Loss: 0.6317460536956787\n",
      "Epoch 6, Batch 611, Test Loss: 0.8032273054122925\n",
      "Epoch 6, Batch 612, Test Loss: 0.5492311120033264\n",
      "Epoch 6, Batch 613, Test Loss: 0.7667598128318787\n",
      "Epoch 6, Batch 614, Test Loss: 0.792251706123352\n",
      "Epoch 6, Batch 615, Test Loss: 0.6383902430534363\n",
      "Epoch 6, Batch 616, Test Loss: 0.5471251010894775\n",
      "Epoch 6, Batch 617, Test Loss: 0.7198658585548401\n",
      "Epoch 6, Batch 618, Test Loss: 0.7892881035804749\n",
      "Epoch 6, Batch 619, Test Loss: 0.5110466480255127\n",
      "Epoch 6, Batch 620, Test Loss: 0.6688441634178162\n",
      "Epoch 6, Batch 621, Test Loss: 0.8161694407463074\n",
      "Epoch 6, Batch 622, Test Loss: 0.5436452627182007\n",
      "Epoch 6, Batch 623, Test Loss: 0.733980119228363\n",
      "Epoch 6, Batch 624, Test Loss: 0.696213960647583\n",
      "Epoch 6, Batch 625, Test Loss: 0.7460097074508667\n",
      "Epoch 6, Batch 626, Test Loss: 0.7783840894699097\n",
      "Epoch 6, Batch 627, Test Loss: 0.7102192044258118\n",
      "Epoch 6, Batch 628, Test Loss: 0.7212662100791931\n",
      "Epoch 6, Batch 629, Test Loss: 0.6648095846176147\n",
      "Epoch 6, Batch 630, Test Loss: 0.7237940430641174\n",
      "Epoch 6, Batch 631, Test Loss: 0.8675171136856079\n",
      "Epoch 6, Batch 632, Test Loss: 0.5090696811676025\n",
      "Epoch 6, Batch 633, Test Loss: 0.9094538688659668\n",
      "Epoch 6, Batch 634, Test Loss: 0.5162319540977478\n",
      "Epoch 6, Batch 635, Test Loss: 0.546239972114563\n",
      "Epoch 6, Batch 636, Test Loss: 0.5030584931373596\n",
      "Epoch 6, Batch 637, Test Loss: 0.4739200472831726\n",
      "Epoch 6, Batch 638, Test Loss: 0.6629990339279175\n",
      "Epoch 6, Batch 639, Test Loss: 0.5661081075668335\n",
      "Epoch 6, Batch 640, Test Loss: 0.8074817657470703\n",
      "Epoch 6, Batch 641, Test Loss: 0.8045951128005981\n",
      "Epoch 6, Batch 642, Test Loss: 0.7060215473175049\n",
      "Epoch 6, Batch 643, Test Loss: 0.48522865772247314\n",
      "Epoch 6, Batch 644, Test Loss: 0.5224241018295288\n",
      "Epoch 6, Batch 645, Test Loss: 0.6942839622497559\n",
      "Epoch 6, Batch 646, Test Loss: 0.6294305920600891\n",
      "Epoch 6, Batch 647, Test Loss: 0.5162240266799927\n",
      "Epoch 6, Batch 648, Test Loss: 0.657581090927124\n",
      "Epoch 6, Batch 649, Test Loss: 0.564053475856781\n",
      "Epoch 6, Batch 650, Test Loss: 0.7269887924194336\n",
      "Epoch 6, Batch 651, Test Loss: 0.6463375091552734\n",
      "Epoch 6, Batch 652, Test Loss: 0.5328429341316223\n",
      "Epoch 6, Batch 653, Test Loss: 0.7215381860733032\n",
      "Epoch 6, Batch 654, Test Loss: 0.6749793291091919\n",
      "Epoch 6, Batch 655, Test Loss: 0.5690112113952637\n",
      "Epoch 6, Batch 656, Test Loss: 0.8484877943992615\n",
      "Epoch 6, Batch 657, Test Loss: 0.6514759659767151\n",
      "Epoch 6, Batch 658, Test Loss: 0.6301606893539429\n",
      "Epoch 6, Batch 659, Test Loss: 0.4976804852485657\n",
      "Epoch 6, Batch 660, Test Loss: 0.5535964965820312\n",
      "Epoch 6, Batch 661, Test Loss: 0.6420170068740845\n",
      "Epoch 6, Batch 662, Test Loss: 0.6110447645187378\n",
      "Epoch 6, Batch 663, Test Loss: 0.6162861585617065\n",
      "Epoch 6, Batch 664, Test Loss: 0.6144309043884277\n",
      "Epoch 6, Batch 665, Test Loss: 0.79997318983078\n",
      "Epoch 6, Batch 666, Test Loss: 0.6428691744804382\n",
      "Epoch 6, Batch 667, Test Loss: 0.6681802272796631\n",
      "Epoch 6, Batch 668, Test Loss: 0.7167111039161682\n",
      "Epoch 6, Batch 669, Test Loss: 0.6358592510223389\n",
      "Epoch 6, Batch 670, Test Loss: 0.7365686297416687\n",
      "Epoch 6, Batch 671, Test Loss: 0.4472769796848297\n",
      "Epoch 6, Batch 672, Test Loss: 0.7816194295883179\n",
      "Epoch 6, Batch 673, Test Loss: 0.695177435874939\n",
      "Epoch 6, Batch 674, Test Loss: 0.6233973503112793\n",
      "Epoch 6, Batch 675, Test Loss: 0.6688113212585449\n",
      "Epoch 6, Batch 676, Test Loss: 0.6459769010543823\n",
      "Epoch 6, Batch 677, Test Loss: 0.6365997791290283\n",
      "Epoch 6, Batch 678, Test Loss: 0.7561795115470886\n",
      "Epoch 6, Batch 679, Test Loss: 0.6883043646812439\n",
      "Epoch 6, Batch 680, Test Loss: 0.6629320383071899\n",
      "Epoch 6, Batch 681, Test Loss: 0.5043975114822388\n",
      "Epoch 6, Batch 682, Test Loss: 0.5527344346046448\n",
      "Epoch 6, Batch 683, Test Loss: 0.6547831296920776\n",
      "Epoch 6, Batch 684, Test Loss: 0.479922890663147\n",
      "Epoch 6, Batch 685, Test Loss: 0.5205996036529541\n",
      "Epoch 6, Batch 686, Test Loss: 0.7049278020858765\n",
      "Epoch 6, Batch 687, Test Loss: 0.8437314629554749\n",
      "Epoch 6, Batch 688, Test Loss: 0.5561022758483887\n",
      "Epoch 6, Batch 689, Test Loss: 0.5503807663917542\n",
      "Epoch 6, Batch 690, Test Loss: 0.577326774597168\n",
      "Epoch 6, Batch 691, Test Loss: 0.7166675925254822\n",
      "Epoch 6, Batch 692, Test Loss: 0.643646240234375\n",
      "Epoch 6, Batch 693, Test Loss: 0.561272382736206\n",
      "Epoch 6, Batch 694, Test Loss: 0.6860331892967224\n",
      "Epoch 6, Batch 695, Test Loss: 0.5019953846931458\n",
      "Epoch 6, Batch 696, Test Loss: 0.5858730673789978\n",
      "Epoch 6, Batch 697, Test Loss: 0.6290472149848938\n",
      "Epoch 6, Batch 698, Test Loss: 0.5183353424072266\n",
      "Epoch 6, Batch 699, Test Loss: 0.5620802044868469\n",
      "Epoch 6, Batch 700, Test Loss: 0.7463259100914001\n",
      "Epoch 6, Batch 701, Test Loss: 0.5917470455169678\n",
      "Epoch 6, Batch 702, Test Loss: 0.6574037075042725\n",
      "Epoch 6, Batch 703, Test Loss: 0.34829258918762207\n",
      "Epoch 6, Batch 704, Test Loss: 0.7241052389144897\n",
      "Epoch 6, Batch 705, Test Loss: 0.6456968784332275\n",
      "Epoch 6, Batch 706, Test Loss: 0.607867956161499\n",
      "Epoch 6, Batch 707, Test Loss: 0.8715575337409973\n",
      "Epoch 6, Batch 708, Test Loss: 0.6000357866287231\n",
      "Epoch 6, Batch 709, Test Loss: 0.8301927447319031\n",
      "Epoch 6, Batch 710, Test Loss: 0.8873654007911682\n",
      "Epoch 6, Batch 711, Test Loss: 0.6902516484260559\n",
      "Epoch 6, Batch 712, Test Loss: 0.591963529586792\n",
      "Epoch 6, Batch 713, Test Loss: 0.5363568067550659\n",
      "Epoch 6, Batch 714, Test Loss: 0.5076708793640137\n",
      "Epoch 6, Batch 715, Test Loss: 0.7417316436767578\n",
      "Epoch 6, Batch 716, Test Loss: 0.5764898061752319\n",
      "Epoch 6, Batch 717, Test Loss: 0.4653953015804291\n",
      "Epoch 6, Batch 718, Test Loss: 0.5815005898475647\n",
      "Epoch 6, Batch 719, Test Loss: 0.747896671295166\n",
      "Epoch 6, Batch 720, Test Loss: 0.3674847483634949\n",
      "Epoch 6, Batch 721, Test Loss: 0.6036044955253601\n",
      "Epoch 6, Batch 722, Test Loss: 0.466447651386261\n",
      "Epoch 6, Batch 723, Test Loss: 0.801125705242157\n",
      "Epoch 6, Batch 724, Test Loss: 0.6025267839431763\n",
      "Epoch 6, Batch 725, Test Loss: 0.5550770163536072\n",
      "Epoch 6, Batch 726, Test Loss: 0.8290664553642273\n",
      "Epoch 6, Batch 727, Test Loss: 0.6202095150947571\n",
      "Epoch 6, Batch 728, Test Loss: 0.8939661383628845\n",
      "Epoch 6, Batch 729, Test Loss: 0.6058297157287598\n",
      "Epoch 6, Batch 730, Test Loss: 0.5169642567634583\n",
      "Epoch 6, Batch 731, Test Loss: 0.7995523810386658\n",
      "Epoch 6, Batch 732, Test Loss: 0.6143916249275208\n",
      "Epoch 6, Batch 733, Test Loss: 0.4750358462333679\n",
      "Epoch 6, Batch 734, Test Loss: 0.570490300655365\n",
      "Epoch 6, Batch 735, Test Loss: 0.8047176599502563\n",
      "Epoch 6, Batch 736, Test Loss: 0.5843750238418579\n",
      "Epoch 6, Batch 737, Test Loss: 0.7553391456604004\n",
      "Epoch 6, Batch 738, Test Loss: 0.9408866167068481\n",
      "Epoch 6, Batch 739, Test Loss: 0.645937979221344\n",
      "Epoch 6, Batch 740, Test Loss: 0.721815824508667\n",
      "Epoch 6, Batch 741, Test Loss: 1.0947233438491821\n",
      "Epoch 6, Batch 742, Test Loss: 0.5483540296554565\n",
      "Epoch 6, Batch 743, Test Loss: 0.8574861884117126\n",
      "Epoch 6, Batch 744, Test Loss: 0.6472935676574707\n",
      "Epoch 6, Batch 745, Test Loss: 0.5863473415374756\n",
      "Epoch 6, Batch 746, Test Loss: 0.5044474601745605\n",
      "Epoch 6, Batch 747, Test Loss: 0.6874331831932068\n",
      "Epoch 6, Batch 748, Test Loss: 0.52423095703125\n",
      "Epoch 6, Batch 749, Test Loss: 0.6619877219200134\n",
      "Epoch 6, Batch 750, Test Loss: 0.6488233208656311\n",
      "Epoch 6, Batch 751, Test Loss: 0.6485194563865662\n",
      "Epoch 6, Batch 752, Test Loss: 0.7310356497764587\n",
      "Epoch 6, Batch 753, Test Loss: 0.588219165802002\n",
      "Epoch 6, Batch 754, Test Loss: 0.6585776805877686\n",
      "Epoch 6, Batch 755, Test Loss: 0.49930083751678467\n",
      "Epoch 6, Batch 756, Test Loss: 0.5657102465629578\n",
      "Epoch 6, Batch 757, Test Loss: 0.6763383746147156\n",
      "Epoch 6, Batch 758, Test Loss: 0.641533374786377\n",
      "Epoch 6, Batch 759, Test Loss: 0.5163458585739136\n",
      "Epoch 6, Batch 760, Test Loss: 0.5416972637176514\n",
      "Epoch 6, Batch 761, Test Loss: 0.7126865983009338\n",
      "Epoch 6, Batch 762, Test Loss: 0.5907976627349854\n",
      "Epoch 6, Batch 763, Test Loss: 0.5005490183830261\n",
      "Epoch 6, Batch 764, Test Loss: 0.6745873093605042\n",
      "Epoch 6, Batch 765, Test Loss: 0.5237501263618469\n",
      "Epoch 6, Batch 766, Test Loss: 0.5743077397346497\n",
      "Epoch 6, Batch 767, Test Loss: 0.6536471843719482\n",
      "Epoch 6, Batch 768, Test Loss: 0.6280215978622437\n",
      "Epoch 6, Batch 769, Test Loss: 0.5395470857620239\n",
      "Epoch 6, Batch 770, Test Loss: 0.7519192099571228\n",
      "Epoch 6, Batch 771, Test Loss: 0.5347867012023926\n",
      "Epoch 6, Batch 772, Test Loss: 0.6499037742614746\n",
      "Epoch 6, Batch 773, Test Loss: 0.6996438503265381\n",
      "Epoch 6, Batch 774, Test Loss: 0.5939617156982422\n",
      "Epoch 6, Batch 775, Test Loss: 0.6489453315734863\n",
      "Epoch 6, Batch 776, Test Loss: 0.7467355728149414\n",
      "Epoch 6, Batch 777, Test Loss: 0.6414839625358582\n",
      "Epoch 6, Batch 778, Test Loss: 0.6636372804641724\n",
      "Epoch 6, Batch 779, Test Loss: 0.6138876080513\n",
      "Epoch 6, Batch 780, Test Loss: 0.750905454158783\n",
      "Epoch 6, Batch 781, Test Loss: 0.5173819065093994\n",
      "Epoch 6, Batch 782, Test Loss: 0.5893193483352661\n",
      "Epoch 6, Batch 783, Test Loss: 0.4705803394317627\n",
      "Epoch 6, Batch 784, Test Loss: 0.7599471211433411\n",
      "Epoch 6, Batch 785, Test Loss: 0.3885003328323364\n",
      "Epoch 6, Batch 786, Test Loss: 0.6364808082580566\n",
      "Epoch 6, Batch 787, Test Loss: 0.6390596628189087\n",
      "Epoch 6, Batch 788, Test Loss: 0.5472087264060974\n",
      "Epoch 6, Batch 789, Test Loss: 0.686340868473053\n",
      "Epoch 6, Batch 790, Test Loss: 0.5738162994384766\n",
      "Epoch 6, Batch 791, Test Loss: 0.7309514284133911\n",
      "Epoch 6, Batch 792, Test Loss: 0.8776944875717163\n",
      "Epoch 6, Batch 793, Test Loss: 0.7519195675849915\n",
      "Epoch 6, Batch 794, Test Loss: 0.6441217064857483\n",
      "Epoch 6, Batch 795, Test Loss: 0.6213639974594116\n",
      "Epoch 6, Batch 796, Test Loss: 0.6879820823669434\n",
      "Epoch 6, Batch 797, Test Loss: 0.5406506657600403\n",
      "Epoch 6, Batch 798, Test Loss: 0.6670491695404053\n",
      "Epoch 6, Batch 799, Test Loss: 0.7180109024047852\n",
      "Epoch 6, Batch 800, Test Loss: 0.48564010858535767\n",
      "Epoch 6, Batch 801, Test Loss: 0.6008391976356506\n",
      "Epoch 6, Batch 802, Test Loss: 0.6454062461853027\n",
      "Epoch 6, Batch 803, Test Loss: 0.5647527575492859\n",
      "Epoch 6, Batch 804, Test Loss: 0.6704050898551941\n",
      "Epoch 6, Batch 805, Test Loss: 0.6671524047851562\n",
      "Epoch 6, Batch 806, Test Loss: 0.7328534126281738\n",
      "Epoch 6, Batch 807, Test Loss: 0.6678032875061035\n",
      "Epoch 6, Batch 808, Test Loss: 0.7600442171096802\n",
      "Epoch 6, Batch 809, Test Loss: 0.6635775566101074\n",
      "Epoch 6, Batch 810, Test Loss: 0.7757226228713989\n",
      "Epoch 6, Batch 811, Test Loss: 0.6261679530143738\n",
      "Epoch 6, Batch 812, Test Loss: 0.6046385169029236\n",
      "Epoch 6, Batch 813, Test Loss: 0.485649436712265\n",
      "Epoch 6, Batch 814, Test Loss: 0.5965883731842041\n",
      "Epoch 6, Batch 815, Test Loss: 0.5316646099090576\n",
      "Epoch 6, Batch 816, Test Loss: 0.5785842537879944\n",
      "Epoch 6, Batch 817, Test Loss: 0.5170098543167114\n",
      "Epoch 6, Batch 818, Test Loss: 0.6169207692146301\n",
      "Epoch 6, Batch 819, Test Loss: 0.619794487953186\n",
      "Epoch 6, Batch 820, Test Loss: 0.5557758212089539\n",
      "Epoch 6, Batch 821, Test Loss: 0.6828746795654297\n",
      "Epoch 6, Batch 822, Test Loss: 0.626664400100708\n",
      "Epoch 6, Batch 823, Test Loss: 0.4810335636138916\n",
      "Epoch 6, Batch 824, Test Loss: 0.8491789698600769\n",
      "Epoch 6, Batch 825, Test Loss: 0.5163038969039917\n",
      "Epoch 6, Batch 826, Test Loss: 0.7994972467422485\n",
      "Epoch 6, Batch 827, Test Loss: 0.5750601291656494\n",
      "Epoch 6, Batch 828, Test Loss: 0.6606208086013794\n",
      "Epoch 6, Batch 829, Test Loss: 0.6001484394073486\n",
      "Epoch 6, Batch 830, Test Loss: 0.8069272041320801\n",
      "Epoch 6, Batch 831, Test Loss: 0.526950478553772\n",
      "Epoch 6, Batch 832, Test Loss: 0.5582330226898193\n",
      "Epoch 6, Batch 833, Test Loss: 0.6020631790161133\n",
      "Epoch 6, Batch 834, Test Loss: 0.7747761607170105\n",
      "Epoch 6, Batch 835, Test Loss: 0.459594190120697\n",
      "Epoch 6, Batch 836, Test Loss: 0.49677547812461853\n",
      "Epoch 6, Batch 837, Test Loss: 0.48566341400146484\n",
      "Epoch 6, Batch 838, Test Loss: 0.6428449153900146\n",
      "Epoch 6, Batch 839, Test Loss: 0.577363908290863\n",
      "Epoch 6, Batch 840, Test Loss: 0.5652527809143066\n",
      "Epoch 6, Batch 841, Test Loss: 0.7336642146110535\n",
      "Epoch 6, Batch 842, Test Loss: 0.5158562660217285\n",
      "Epoch 6, Batch 843, Test Loss: 0.49084213376045227\n",
      "Epoch 6, Batch 844, Test Loss: 0.7931370735168457\n",
      "Epoch 6, Batch 845, Test Loss: 0.468788743019104\n",
      "Epoch 6, Batch 846, Test Loss: 0.8446831703186035\n",
      "Epoch 6, Batch 847, Test Loss: 0.6346441507339478\n",
      "Epoch 6, Batch 848, Test Loss: 0.45093825459480286\n",
      "Epoch 6, Batch 849, Test Loss: 0.7107260823249817\n",
      "Epoch 6, Batch 850, Test Loss: 0.5420553684234619\n",
      "Epoch 6, Batch 851, Test Loss: 0.6258880496025085\n",
      "Epoch 6, Batch 852, Test Loss: 0.6699934005737305\n",
      "Epoch 6, Batch 853, Test Loss: 0.5247246623039246\n",
      "Epoch 6, Batch 854, Test Loss: 0.7084936499595642\n",
      "Epoch 6, Batch 855, Test Loss: 0.43719181418418884\n",
      "Epoch 6, Batch 856, Test Loss: 0.5679358839988708\n",
      "Epoch 6, Batch 857, Test Loss: 0.78377765417099\n",
      "Epoch 6, Batch 858, Test Loss: 0.40005186200141907\n",
      "Epoch 6, Batch 859, Test Loss: 0.5439526438713074\n",
      "Epoch 6, Batch 860, Test Loss: 0.5851789116859436\n",
      "Epoch 6, Batch 861, Test Loss: 0.5830326080322266\n",
      "Epoch 6, Batch 862, Test Loss: 0.5789665579795837\n",
      "Epoch 6, Batch 863, Test Loss: 0.6957674622535706\n",
      "Epoch 6, Batch 864, Test Loss: 0.6634961366653442\n",
      "Epoch 6, Batch 865, Test Loss: 0.533438503742218\n",
      "Epoch 6, Batch 866, Test Loss: 0.5822309255599976\n",
      "Epoch 6, Batch 867, Test Loss: 0.6198270916938782\n",
      "Epoch 6, Batch 868, Test Loss: 0.7456544637680054\n",
      "Epoch 6, Batch 869, Test Loss: 0.801053524017334\n",
      "Epoch 6, Batch 870, Test Loss: 0.6544159650802612\n",
      "Epoch 6, Batch 871, Test Loss: 0.8783593773841858\n",
      "Epoch 6, Batch 872, Test Loss: 0.9760251045227051\n",
      "Epoch 6, Batch 873, Test Loss: 0.6238831281661987\n",
      "Epoch 6, Batch 874, Test Loss: 0.7271919846534729\n",
      "Epoch 6, Batch 875, Test Loss: 0.4938344955444336\n",
      "Epoch 6, Batch 876, Test Loss: 0.5853726863861084\n",
      "Epoch 6, Batch 877, Test Loss: 0.6842119693756104\n",
      "Epoch 6, Batch 878, Test Loss: 0.5042179822921753\n",
      "Epoch 6, Batch 879, Test Loss: 0.8372824192047119\n",
      "Epoch 6, Batch 880, Test Loss: 0.5493708252906799\n",
      "Epoch 6, Batch 881, Test Loss: 0.5842275023460388\n",
      "Epoch 6, Batch 882, Test Loss: 0.5868375301361084\n",
      "Epoch 6, Batch 883, Test Loss: 0.3893369734287262\n",
      "Epoch 6, Batch 884, Test Loss: 0.7669674754142761\n",
      "Epoch 6, Batch 885, Test Loss: 0.6821584701538086\n",
      "Epoch 6, Batch 886, Test Loss: 0.7024635672569275\n",
      "Epoch 6, Batch 887, Test Loss: 0.5347651243209839\n",
      "Epoch 6, Batch 888, Test Loss: 0.7556977868080139\n",
      "Epoch 6, Batch 889, Test Loss: 0.575559675693512\n",
      "Epoch 6, Batch 890, Test Loss: 0.5434923768043518\n",
      "Epoch 6, Batch 891, Test Loss: 0.5951102375984192\n",
      "Epoch 6, Batch 892, Test Loss: 0.8131468296051025\n",
      "Epoch 6, Batch 893, Test Loss: 0.8770024180412292\n",
      "Epoch 6, Batch 894, Test Loss: 0.6323857307434082\n",
      "Epoch 6, Batch 895, Test Loss: 0.6950188279151917\n",
      "Epoch 6, Batch 896, Test Loss: 0.6519975066184998\n",
      "Epoch 6, Batch 897, Test Loss: 0.5690098404884338\n",
      "Epoch 6, Batch 898, Test Loss: 0.6633225083351135\n",
      "Epoch 6, Batch 899, Test Loss: 0.8380365967750549\n",
      "Epoch 6, Batch 900, Test Loss: 0.597459077835083\n",
      "Epoch 6, Batch 901, Test Loss: 0.7171558141708374\n",
      "Epoch 6, Batch 902, Test Loss: 0.6541950106620789\n",
      "Epoch 6, Batch 903, Test Loss: 0.43643730878829956\n",
      "Epoch 6, Batch 904, Test Loss: 0.6642565131187439\n",
      "Epoch 6, Batch 905, Test Loss: 0.45239824056625366\n",
      "Epoch 6, Batch 906, Test Loss: 0.8728245496749878\n",
      "Epoch 6, Batch 907, Test Loss: 0.6322307586669922\n",
      "Epoch 6, Batch 908, Test Loss: 0.7688038945198059\n",
      "Epoch 6, Batch 909, Test Loss: 0.4771565794944763\n",
      "Epoch 6, Batch 910, Test Loss: 0.6273581981658936\n",
      "Epoch 6, Batch 911, Test Loss: 0.7379288673400879\n",
      "Epoch 6, Batch 912, Test Loss: 0.44778770208358765\n",
      "Epoch 6, Batch 913, Test Loss: 0.684310257434845\n",
      "Epoch 6, Batch 914, Test Loss: 0.6040365695953369\n",
      "Epoch 6, Batch 915, Test Loss: 0.34248363971710205\n",
      "Epoch 6, Batch 916, Test Loss: 0.83296138048172\n",
      "Epoch 6, Batch 917, Test Loss: 0.6030485033988953\n",
      "Epoch 6, Batch 918, Test Loss: 0.6467590928077698\n",
      "Epoch 6, Batch 919, Test Loss: 0.7615172266960144\n",
      "Epoch 6, Batch 920, Test Loss: 0.5099060535430908\n",
      "Epoch 6, Batch 921, Test Loss: 0.7060985565185547\n",
      "Epoch 6, Batch 922, Test Loss: 0.45780619978904724\n",
      "Epoch 6, Batch 923, Test Loss: 0.72825688123703\n",
      "Epoch 6, Batch 924, Test Loss: 0.6879989504814148\n",
      "Epoch 6, Batch 925, Test Loss: 0.6117733120918274\n",
      "Epoch 6, Batch 926, Test Loss: 0.6448885202407837\n",
      "Epoch 6, Batch 927, Test Loss: 0.5165839791297913\n",
      "Epoch 6, Batch 928, Test Loss: 0.688654899597168\n",
      "Epoch 6, Batch 929, Test Loss: 0.5868619084358215\n",
      "Epoch 6, Batch 930, Test Loss: 0.5345790386199951\n",
      "Epoch 6, Batch 931, Test Loss: 0.6453493237495422\n",
      "Epoch 6, Batch 932, Test Loss: 0.4409075379371643\n",
      "Epoch 6, Batch 933, Test Loss: 0.6072496175765991\n",
      "Epoch 6, Batch 934, Test Loss: 0.6603663563728333\n",
      "Epoch 6, Batch 935, Test Loss: 0.42078182101249695\n",
      "Epoch 6, Batch 936, Test Loss: 0.5694260597229004\n",
      "Epoch 6, Batch 937, Test Loss: 0.6383318305015564\n",
      "Epoch 6, Batch 938, Test Loss: 0.6806439757347107\n",
      "Accuracy of Test set: 0.77285\n",
      "Epoch 7, Batch 1, Loss: 0.5537229180335999\n",
      "Epoch 7, Batch 2, Loss: 0.6554708480834961\n",
      "Epoch 7, Batch 3, Loss: 0.6384112238883972\n",
      "Epoch 7, Batch 4, Loss: 0.5694480538368225\n",
      "Epoch 7, Batch 5, Loss: 0.6514845490455627\n",
      "Epoch 7, Batch 6, Loss: 0.7029339075088501\n",
      "Epoch 7, Batch 7, Loss: 0.6094815135002136\n",
      "Epoch 7, Batch 8, Loss: 0.5659855604171753\n",
      "Epoch 7, Batch 9, Loss: 0.7095237970352173\n",
      "Epoch 7, Batch 10, Loss: 0.6037618517875671\n",
      "Epoch 7, Batch 11, Loss: 0.7587810754776001\n",
      "Epoch 7, Batch 12, Loss: 0.4211035668849945\n",
      "Epoch 7, Batch 13, Loss: 0.6957094669342041\n",
      "Epoch 7, Batch 14, Loss: 0.7199792265892029\n",
      "Epoch 7, Batch 15, Loss: 0.42252832651138306\n",
      "Epoch 7, Batch 16, Loss: 0.47167640924453735\n",
      "Epoch 7, Batch 17, Loss: 0.7235199809074402\n",
      "Epoch 7, Batch 18, Loss: 0.6205267906188965\n",
      "Epoch 7, Batch 19, Loss: 0.5783946514129639\n",
      "Epoch 7, Batch 20, Loss: 0.5328101515769958\n",
      "Epoch 7, Batch 21, Loss: 0.5727993249893188\n",
      "Epoch 7, Batch 22, Loss: 0.776309609413147\n",
      "Epoch 7, Batch 23, Loss: 0.584784209728241\n",
      "Epoch 7, Batch 24, Loss: 0.680867075920105\n",
      "Epoch 7, Batch 25, Loss: 0.6395503878593445\n",
      "Epoch 7, Batch 26, Loss: 0.526046872138977\n",
      "Epoch 7, Batch 27, Loss: 0.5552177429199219\n",
      "Epoch 7, Batch 28, Loss: 0.6876562833786011\n",
      "Epoch 7, Batch 29, Loss: 0.7782764434814453\n",
      "Epoch 7, Batch 30, Loss: 0.6107917428016663\n",
      "Epoch 7, Batch 31, Loss: 0.542742908000946\n",
      "Epoch 7, Batch 32, Loss: 0.5940557718276978\n",
      "Epoch 7, Batch 33, Loss: 0.5501827597618103\n",
      "Epoch 7, Batch 34, Loss: 0.6138310432434082\n",
      "Epoch 7, Batch 35, Loss: 0.4738313555717468\n",
      "Epoch 7, Batch 36, Loss: 0.5053022503852844\n",
      "Epoch 7, Batch 37, Loss: 0.651528537273407\n",
      "Epoch 7, Batch 38, Loss: 0.7123277187347412\n",
      "Epoch 7, Batch 39, Loss: 0.6328072547912598\n",
      "Epoch 7, Batch 40, Loss: 0.49522656202316284\n",
      "Epoch 7, Batch 41, Loss: 0.5717703700065613\n",
      "Epoch 7, Batch 42, Loss: 0.6424942016601562\n",
      "Epoch 7, Batch 43, Loss: 0.599976122379303\n",
      "Epoch 7, Batch 44, Loss: 0.6721614003181458\n",
      "Epoch 7, Batch 45, Loss: 0.5016576647758484\n",
      "Epoch 7, Batch 46, Loss: 0.6287091374397278\n",
      "Epoch 7, Batch 47, Loss: 0.6515807509422302\n",
      "Epoch 7, Batch 48, Loss: 0.6303953528404236\n",
      "Epoch 7, Batch 49, Loss: 0.6335939168930054\n",
      "Epoch 7, Batch 50, Loss: 0.9191315770149231\n",
      "Epoch 7, Batch 51, Loss: 0.6908358335494995\n",
      "Epoch 7, Batch 52, Loss: 0.29256483912467957\n",
      "Epoch 7, Batch 53, Loss: 0.557455837726593\n",
      "Epoch 7, Batch 54, Loss: 0.649333119392395\n",
      "Epoch 7, Batch 55, Loss: 0.6082978248596191\n",
      "Epoch 7, Batch 56, Loss: 0.5294771790504456\n",
      "Epoch 7, Batch 57, Loss: 0.5739012956619263\n",
      "Epoch 7, Batch 58, Loss: 0.43911510705947876\n",
      "Epoch 7, Batch 59, Loss: 0.5310100317001343\n",
      "Epoch 7, Batch 60, Loss: 0.6384832262992859\n",
      "Epoch 7, Batch 61, Loss: 0.6122235655784607\n",
      "Epoch 7, Batch 62, Loss: 0.58721923828125\n",
      "Epoch 7, Batch 63, Loss: 0.7889358997344971\n",
      "Epoch 7, Batch 64, Loss: 0.5686329007148743\n",
      "Epoch 7, Batch 65, Loss: 0.699751615524292\n",
      "Epoch 7, Batch 66, Loss: 0.5591654777526855\n",
      "Epoch 7, Batch 67, Loss: 0.5345352292060852\n",
      "Epoch 7, Batch 68, Loss: 0.6991598606109619\n",
      "Epoch 7, Batch 69, Loss: 0.4511384665966034\n",
      "Epoch 7, Batch 70, Loss: 0.741349995136261\n",
      "Epoch 7, Batch 71, Loss: 0.5709146857261658\n",
      "Epoch 7, Batch 72, Loss: 0.5365613698959351\n",
      "Epoch 7, Batch 73, Loss: 0.7722717523574829\n",
      "Epoch 7, Batch 74, Loss: 0.5515501499176025\n",
      "Epoch 7, Batch 75, Loss: 0.6450359225273132\n",
      "Epoch 7, Batch 76, Loss: 0.535701334476471\n",
      "Epoch 7, Batch 77, Loss: 0.5717437863349915\n",
      "Epoch 7, Batch 78, Loss: 0.6321340799331665\n",
      "Epoch 7, Batch 79, Loss: 0.6083011627197266\n",
      "Epoch 7, Batch 80, Loss: 0.6608096361160278\n",
      "Epoch 7, Batch 81, Loss: 0.6271195411682129\n",
      "Epoch 7, Batch 82, Loss: 0.6270495057106018\n",
      "Epoch 7, Batch 83, Loss: 0.5539461374282837\n",
      "Epoch 7, Batch 84, Loss: 0.49622491002082825\n",
      "Epoch 7, Batch 85, Loss: 0.47571513056755066\n",
      "Epoch 7, Batch 86, Loss: 0.9002951383590698\n",
      "Epoch 7, Batch 87, Loss: 0.5984813570976257\n",
      "Epoch 7, Batch 88, Loss: 0.739883542060852\n",
      "Epoch 7, Batch 89, Loss: 0.603080153465271\n",
      "Epoch 7, Batch 90, Loss: 0.4121057391166687\n",
      "Epoch 7, Batch 91, Loss: 0.684145450592041\n",
      "Epoch 7, Batch 92, Loss: 0.5204583406448364\n",
      "Epoch 7, Batch 93, Loss: 0.3944288492202759\n",
      "Epoch 7, Batch 94, Loss: 0.633508563041687\n",
      "Epoch 7, Batch 95, Loss: 0.6918355822563171\n",
      "Epoch 7, Batch 96, Loss: 0.6060590744018555\n",
      "Epoch 7, Batch 97, Loss: 0.4720972180366516\n",
      "Epoch 7, Batch 98, Loss: 0.8289108872413635\n",
      "Epoch 7, Batch 99, Loss: 0.364023357629776\n",
      "Epoch 7, Batch 100, Loss: 0.7299821972846985\n",
      "Epoch 7, Batch 101, Loss: 0.5697146058082581\n",
      "Epoch 7, Batch 102, Loss: 0.4953030049800873\n",
      "Epoch 7, Batch 103, Loss: 0.5673379898071289\n",
      "Epoch 7, Batch 104, Loss: 0.4064817726612091\n",
      "Epoch 7, Batch 105, Loss: 0.7637836933135986\n",
      "Epoch 7, Batch 106, Loss: 0.40097329020500183\n",
      "Epoch 7, Batch 107, Loss: 0.48153576254844666\n",
      "Epoch 7, Batch 108, Loss: 0.7365257740020752\n",
      "Epoch 7, Batch 109, Loss: 0.4855036735534668\n",
      "Epoch 7, Batch 110, Loss: 0.6189554929733276\n",
      "Epoch 7, Batch 111, Loss: 0.3590133786201477\n",
      "Epoch 7, Batch 112, Loss: 0.4510839581489563\n",
      "Epoch 7, Batch 113, Loss: 0.6568489670753479\n",
      "Epoch 7, Batch 114, Loss: 0.5724971294403076\n",
      "Epoch 7, Batch 115, Loss: 0.6543850302696228\n",
      "Epoch 7, Batch 116, Loss: 0.45332738757133484\n",
      "Epoch 7, Batch 117, Loss: 0.43287670612335205\n",
      "Epoch 7, Batch 118, Loss: 0.5907531380653381\n",
      "Epoch 7, Batch 119, Loss: 0.7024226188659668\n",
      "Epoch 7, Batch 120, Loss: 0.4979431927204132\n",
      "Epoch 7, Batch 121, Loss: 0.41012245416641235\n",
      "Epoch 7, Batch 122, Loss: 0.7282605171203613\n",
      "Epoch 7, Batch 123, Loss: 0.6864518523216248\n",
      "Epoch 7, Batch 124, Loss: 0.5371297001838684\n",
      "Epoch 7, Batch 125, Loss: 0.6100966930389404\n",
      "Epoch 7, Batch 126, Loss: 0.6449170708656311\n",
      "Epoch 7, Batch 127, Loss: 0.4601750671863556\n",
      "Epoch 7, Batch 128, Loss: 0.7322317957878113\n",
      "Epoch 7, Batch 129, Loss: 0.6638038158416748\n",
      "Epoch 7, Batch 130, Loss: 0.4499981105327606\n",
      "Epoch 7, Batch 131, Loss: 0.41652828454971313\n",
      "Epoch 7, Batch 132, Loss: 0.5361413359642029\n",
      "Epoch 7, Batch 133, Loss: 0.3777417242527008\n",
      "Epoch 7, Batch 134, Loss: 0.5612342953681946\n",
      "Epoch 7, Batch 135, Loss: 0.43127650022506714\n",
      "Epoch 7, Batch 136, Loss: 0.54490065574646\n",
      "Epoch 7, Batch 137, Loss: 0.706916093826294\n",
      "Epoch 7, Batch 138, Loss: 0.6203188300132751\n",
      "Epoch 7, Batch 139, Loss: 0.643865168094635\n",
      "Epoch 7, Batch 140, Loss: 0.6357300281524658\n",
      "Epoch 7, Batch 141, Loss: 0.5715273022651672\n",
      "Epoch 7, Batch 142, Loss: 0.5354080200195312\n",
      "Epoch 7, Batch 143, Loss: 0.5930182337760925\n",
      "Epoch 7, Batch 144, Loss: 0.6195982694625854\n",
      "Epoch 7, Batch 145, Loss: 0.48903965950012207\n",
      "Epoch 7, Batch 146, Loss: 0.6476954817771912\n",
      "Epoch 7, Batch 147, Loss: 0.5369285941123962\n",
      "Epoch 7, Batch 148, Loss: 0.5182188749313354\n",
      "Epoch 7, Batch 149, Loss: 0.49172285199165344\n",
      "Epoch 7, Batch 150, Loss: 0.7336239814758301\n",
      "Epoch 7, Batch 151, Loss: 0.5426040887832642\n",
      "Epoch 7, Batch 152, Loss: 0.5009533762931824\n",
      "Epoch 7, Batch 153, Loss: 0.5963348746299744\n",
      "Epoch 7, Batch 154, Loss: 0.6601174473762512\n",
      "Epoch 7, Batch 155, Loss: 0.704074501991272\n",
      "Epoch 7, Batch 156, Loss: 0.4543735980987549\n",
      "Epoch 7, Batch 157, Loss: 0.49389544129371643\n",
      "Epoch 7, Batch 158, Loss: 0.6117039322853088\n",
      "Epoch 7, Batch 159, Loss: 0.6891095042228699\n",
      "Epoch 7, Batch 160, Loss: 0.4979139268398285\n",
      "Epoch 7, Batch 161, Loss: 0.5769938230514526\n",
      "Epoch 7, Batch 162, Loss: 0.5252141356468201\n",
      "Epoch 7, Batch 163, Loss: 0.46049806475639343\n",
      "Epoch 7, Batch 164, Loss: 0.42474985122680664\n",
      "Epoch 7, Batch 165, Loss: 0.5297585129737854\n",
      "Epoch 7, Batch 166, Loss: 0.41233423352241516\n",
      "Epoch 7, Batch 167, Loss: 0.5718629360198975\n",
      "Epoch 7, Batch 168, Loss: 0.4809858202934265\n",
      "Epoch 7, Batch 169, Loss: 0.5332609415054321\n",
      "Epoch 7, Batch 170, Loss: 0.598885715007782\n",
      "Epoch 7, Batch 171, Loss: 0.6468233466148376\n",
      "Epoch 7, Batch 172, Loss: 0.47984620928764343\n",
      "Epoch 7, Batch 173, Loss: 0.5064003467559814\n",
      "Epoch 7, Batch 174, Loss: 0.6309192776679993\n",
      "Epoch 7, Batch 175, Loss: 0.5415478944778442\n",
      "Epoch 7, Batch 176, Loss: 0.5096766948699951\n",
      "Epoch 7, Batch 177, Loss: 0.49267256259918213\n",
      "Epoch 7, Batch 178, Loss: 0.6365071535110474\n",
      "Epoch 7, Batch 179, Loss: 0.6457858085632324\n",
      "Epoch 7, Batch 180, Loss: 0.5370178818702698\n",
      "Epoch 7, Batch 181, Loss: 0.5230582356452942\n",
      "Epoch 7, Batch 182, Loss: 0.6270285844802856\n",
      "Epoch 7, Batch 183, Loss: 0.49439892172813416\n",
      "Epoch 7, Batch 184, Loss: 0.6153092980384827\n",
      "Epoch 7, Batch 185, Loss: 0.5891488790512085\n",
      "Epoch 7, Batch 186, Loss: 0.6317103505134583\n",
      "Epoch 7, Batch 187, Loss: 0.8139797449111938\n",
      "Epoch 7, Batch 188, Loss: 0.7534387111663818\n",
      "Epoch 7, Batch 189, Loss: 0.6201093196868896\n",
      "Epoch 7, Batch 190, Loss: 0.5512751936912537\n",
      "Epoch 7, Batch 191, Loss: 0.4277477562427521\n",
      "Epoch 7, Batch 192, Loss: 0.7272931337356567\n",
      "Epoch 7, Batch 193, Loss: 0.6896815299987793\n",
      "Epoch 7, Batch 194, Loss: 0.5726766586303711\n",
      "Epoch 7, Batch 195, Loss: 0.539373517036438\n",
      "Epoch 7, Batch 196, Loss: 0.49128618836402893\n",
      "Epoch 7, Batch 197, Loss: 0.8866155743598938\n",
      "Epoch 7, Batch 198, Loss: 0.614833414554596\n",
      "Epoch 7, Batch 199, Loss: 0.4492397904396057\n",
      "Epoch 7, Batch 200, Loss: 0.6114581823348999\n",
      "Epoch 7, Batch 201, Loss: 0.5673529505729675\n",
      "Epoch 7, Batch 202, Loss: 0.5037579536437988\n",
      "Epoch 7, Batch 203, Loss: 0.5516173839569092\n",
      "Epoch 7, Batch 204, Loss: 0.5933293104171753\n",
      "Epoch 7, Batch 205, Loss: 0.4913088083267212\n",
      "Epoch 7, Batch 206, Loss: 0.5227141976356506\n",
      "Epoch 7, Batch 207, Loss: 0.4980822801589966\n",
      "Epoch 7, Batch 208, Loss: 0.6599880456924438\n",
      "Epoch 7, Batch 209, Loss: 0.6773502826690674\n",
      "Epoch 7, Batch 210, Loss: 0.6494943499565125\n",
      "Epoch 7, Batch 211, Loss: 0.47885265946388245\n",
      "Epoch 7, Batch 212, Loss: 0.6263771057128906\n",
      "Epoch 7, Batch 213, Loss: 0.5579394698143005\n",
      "Epoch 7, Batch 214, Loss: 0.5922605991363525\n",
      "Epoch 7, Batch 215, Loss: 0.6053140163421631\n",
      "Epoch 7, Batch 216, Loss: 0.7510703802108765\n",
      "Epoch 7, Batch 217, Loss: 0.5341347455978394\n",
      "Epoch 7, Batch 218, Loss: 0.5589359402656555\n",
      "Epoch 7, Batch 219, Loss: 0.5868035554885864\n",
      "Epoch 7, Batch 220, Loss: 0.7299713492393494\n",
      "Epoch 7, Batch 221, Loss: 0.5553136467933655\n",
      "Epoch 7, Batch 222, Loss: 0.515198826789856\n",
      "Epoch 7, Batch 223, Loss: 0.6804283857345581\n",
      "Epoch 7, Batch 224, Loss: 0.35579460859298706\n",
      "Epoch 7, Batch 225, Loss: 0.44959568977355957\n",
      "Epoch 7, Batch 226, Loss: 0.685546875\n",
      "Epoch 7, Batch 227, Loss: 0.5520408153533936\n",
      "Epoch 7, Batch 228, Loss: 0.7265556454658508\n",
      "Epoch 7, Batch 229, Loss: 0.545606255531311\n",
      "Epoch 7, Batch 230, Loss: 0.5556789636611938\n",
      "Epoch 7, Batch 231, Loss: 0.9059225916862488\n",
      "Epoch 7, Batch 232, Loss: 0.6657285094261169\n",
      "Epoch 7, Batch 233, Loss: 0.5069287419319153\n",
      "Epoch 7, Batch 234, Loss: 0.5593056678771973\n",
      "Epoch 7, Batch 235, Loss: 0.6340097188949585\n",
      "Epoch 7, Batch 236, Loss: 0.5292239785194397\n",
      "Epoch 7, Batch 237, Loss: 0.31540313363075256\n",
      "Epoch 7, Batch 238, Loss: 0.5581921339035034\n",
      "Epoch 7, Batch 239, Loss: 0.6750341653823853\n",
      "Epoch 7, Batch 240, Loss: 0.5576781034469604\n",
      "Epoch 7, Batch 241, Loss: 0.5848955512046814\n",
      "Epoch 7, Batch 242, Loss: 0.6309420466423035\n",
      "Epoch 7, Batch 243, Loss: 0.4019675850868225\n",
      "Epoch 7, Batch 244, Loss: 0.605099081993103\n",
      "Epoch 7, Batch 245, Loss: 0.3583053648471832\n",
      "Epoch 7, Batch 246, Loss: 0.6126715540885925\n",
      "Epoch 7, Batch 247, Loss: 0.46963217854499817\n",
      "Epoch 7, Batch 248, Loss: 0.6710531115531921\n",
      "Epoch 7, Batch 249, Loss: 0.6913726329803467\n",
      "Epoch 7, Batch 250, Loss: 0.5197361707687378\n",
      "Epoch 7, Batch 251, Loss: 0.44042345881462097\n",
      "Epoch 7, Batch 252, Loss: 0.40226003527641296\n",
      "Epoch 7, Batch 253, Loss: 0.6230436563491821\n",
      "Epoch 7, Batch 254, Loss: 0.6353560090065002\n",
      "Epoch 7, Batch 255, Loss: 0.6959273815155029\n",
      "Epoch 7, Batch 256, Loss: 0.6407113671302795\n",
      "Epoch 7, Batch 257, Loss: 0.43976038694381714\n",
      "Epoch 7, Batch 258, Loss: 0.6873664259910583\n",
      "Epoch 7, Batch 259, Loss: 0.41911962628364563\n",
      "Epoch 7, Batch 260, Loss: 0.48615333437919617\n",
      "Epoch 7, Batch 261, Loss: 0.5918437242507935\n",
      "Epoch 7, Batch 262, Loss: 0.5148220062255859\n",
      "Epoch 7, Batch 263, Loss: 0.5693796277046204\n",
      "Epoch 7, Batch 264, Loss: 0.779719352722168\n",
      "Epoch 7, Batch 265, Loss: 0.600902795791626\n",
      "Epoch 7, Batch 266, Loss: 0.6568828821182251\n",
      "Epoch 7, Batch 267, Loss: 0.6603640913963318\n",
      "Epoch 7, Batch 268, Loss: 0.483346164226532\n",
      "Epoch 7, Batch 269, Loss: 0.4380187690258026\n",
      "Epoch 7, Batch 270, Loss: 0.6781174540519714\n",
      "Epoch 7, Batch 271, Loss: 0.5456407070159912\n",
      "Epoch 7, Batch 272, Loss: 0.5554306507110596\n",
      "Epoch 7, Batch 273, Loss: 0.570874810218811\n",
      "Epoch 7, Batch 274, Loss: 0.4387836456298828\n",
      "Epoch 7, Batch 275, Loss: 0.5503029227256775\n",
      "Epoch 7, Batch 276, Loss: 0.6816127300262451\n",
      "Epoch 7, Batch 277, Loss: 0.4668411612510681\n",
      "Epoch 7, Batch 278, Loss: 0.48051220178604126\n",
      "Epoch 7, Batch 279, Loss: 0.4019224941730499\n",
      "Epoch 7, Batch 280, Loss: 0.5593997240066528\n",
      "Epoch 7, Batch 281, Loss: 0.5460668802261353\n",
      "Epoch 7, Batch 282, Loss: 0.6681645512580872\n",
      "Epoch 7, Batch 283, Loss: 0.5595456957817078\n",
      "Epoch 7, Batch 284, Loss: 0.6440563797950745\n",
      "Epoch 7, Batch 285, Loss: 0.4998322129249573\n",
      "Epoch 7, Batch 286, Loss: 0.5001305937767029\n",
      "Epoch 7, Batch 287, Loss: 0.4296325147151947\n",
      "Epoch 7, Batch 288, Loss: 0.49870896339416504\n",
      "Epoch 7, Batch 289, Loss: 0.554325520992279\n",
      "Epoch 7, Batch 290, Loss: 0.4295113682746887\n",
      "Epoch 7, Batch 291, Loss: 0.5110592842102051\n",
      "Epoch 7, Batch 292, Loss: 0.49570560455322266\n",
      "Epoch 7, Batch 293, Loss: 0.6067800521850586\n",
      "Epoch 7, Batch 294, Loss: 0.42416226863861084\n",
      "Epoch 7, Batch 295, Loss: 0.5116786360740662\n",
      "Epoch 7, Batch 296, Loss: 0.32260727882385254\n",
      "Epoch 7, Batch 297, Loss: 0.5466256737709045\n",
      "Epoch 7, Batch 298, Loss: 0.496883749961853\n",
      "Epoch 7, Batch 299, Loss: 0.4511276185512543\n",
      "Epoch 7, Batch 300, Loss: 0.5295929908752441\n",
      "Epoch 7, Batch 301, Loss: 0.5151838064193726\n",
      "Epoch 7, Batch 302, Loss: 0.5551583766937256\n",
      "Epoch 7, Batch 303, Loss: 0.6094224452972412\n",
      "Epoch 7, Batch 304, Loss: 0.7215620875358582\n",
      "Epoch 7, Batch 305, Loss: 0.7231842875480652\n",
      "Epoch 7, Batch 306, Loss: 0.597579836845398\n",
      "Epoch 7, Batch 307, Loss: 0.5798861980438232\n",
      "Epoch 7, Batch 308, Loss: 0.6235350370407104\n",
      "Epoch 7, Batch 309, Loss: 0.49810129404067993\n",
      "Epoch 7, Batch 310, Loss: 0.6073647737503052\n",
      "Epoch 7, Batch 311, Loss: 0.5366555452346802\n",
      "Epoch 7, Batch 312, Loss: 0.6008496284484863\n",
      "Epoch 7, Batch 313, Loss: 0.5709075927734375\n",
      "Epoch 7, Batch 314, Loss: 0.4852977693080902\n",
      "Epoch 7, Batch 315, Loss: 0.5716531276702881\n",
      "Epoch 7, Batch 316, Loss: 0.7040330171585083\n",
      "Epoch 7, Batch 317, Loss: 0.5739714503288269\n",
      "Epoch 7, Batch 318, Loss: 0.7176125645637512\n",
      "Epoch 7, Batch 319, Loss: 0.5895224809646606\n",
      "Epoch 7, Batch 320, Loss: 0.571442723274231\n",
      "Epoch 7, Batch 321, Loss: 0.6199200749397278\n",
      "Epoch 7, Batch 322, Loss: 0.4991384744644165\n",
      "Epoch 7, Batch 323, Loss: 0.5482458472251892\n",
      "Epoch 7, Batch 324, Loss: 0.8265721797943115\n",
      "Epoch 7, Batch 325, Loss: 0.6472679376602173\n",
      "Epoch 7, Batch 326, Loss: 0.5577224493026733\n",
      "Epoch 7, Batch 327, Loss: 0.5650447010993958\n",
      "Epoch 7, Batch 328, Loss: 0.7406328916549683\n",
      "Epoch 7, Batch 329, Loss: 0.5909566879272461\n",
      "Epoch 7, Batch 330, Loss: 0.6216366291046143\n",
      "Epoch 7, Batch 331, Loss: 0.5709255337715149\n",
      "Epoch 7, Batch 332, Loss: 0.24461296200752258\n",
      "Epoch 7, Batch 333, Loss: 0.6217538118362427\n",
      "Epoch 7, Batch 334, Loss: 0.46686112880706787\n",
      "Epoch 7, Batch 335, Loss: 0.6975180506706238\n",
      "Epoch 7, Batch 336, Loss: 0.5193216800689697\n",
      "Epoch 7, Batch 337, Loss: 0.5757260322570801\n",
      "Epoch 7, Batch 338, Loss: 0.7140541672706604\n",
      "Epoch 7, Batch 339, Loss: 0.6072589755058289\n",
      "Epoch 7, Batch 340, Loss: 0.5754759907722473\n",
      "Epoch 7, Batch 341, Loss: 0.7194232940673828\n",
      "Epoch 7, Batch 342, Loss: 0.5608491897583008\n",
      "Epoch 7, Batch 343, Loss: 0.49733608961105347\n",
      "Epoch 7, Batch 344, Loss: 0.6052502393722534\n",
      "Epoch 7, Batch 345, Loss: 0.7295621037483215\n",
      "Epoch 7, Batch 346, Loss: 0.6581319570541382\n",
      "Epoch 7, Batch 347, Loss: 0.3547571301460266\n",
      "Epoch 7, Batch 348, Loss: 0.7650942206382751\n",
      "Epoch 7, Batch 349, Loss: 0.6062215566635132\n",
      "Epoch 7, Batch 350, Loss: 0.5407758951187134\n",
      "Epoch 7, Batch 351, Loss: 0.5646055340766907\n",
      "Epoch 7, Batch 352, Loss: 0.6948498487472534\n",
      "Epoch 7, Batch 353, Loss: 0.5074154138565063\n",
      "Epoch 7, Batch 354, Loss: 0.7804434895515442\n",
      "Epoch 7, Batch 355, Loss: 0.8505891561508179\n",
      "Epoch 7, Batch 356, Loss: 0.734064519405365\n",
      "Epoch 7, Batch 357, Loss: 0.5787966251373291\n",
      "Epoch 7, Batch 358, Loss: 0.7627272605895996\n",
      "Epoch 7, Batch 359, Loss: 0.526277482509613\n",
      "Epoch 7, Batch 360, Loss: 0.5723724961280823\n",
      "Epoch 7, Batch 361, Loss: 0.6098111867904663\n",
      "Epoch 7, Batch 362, Loss: 0.7286873459815979\n",
      "Epoch 7, Batch 363, Loss: 0.6291981339454651\n",
      "Epoch 7, Batch 364, Loss: 0.44599491357803345\n",
      "Epoch 7, Batch 365, Loss: 0.5199286937713623\n",
      "Epoch 7, Batch 366, Loss: 0.621120035648346\n",
      "Epoch 7, Batch 367, Loss: 0.5087831020355225\n",
      "Epoch 7, Batch 368, Loss: 0.5471058487892151\n",
      "Epoch 7, Batch 369, Loss: 0.5837153792381287\n",
      "Epoch 7, Batch 370, Loss: 0.6621513962745667\n",
      "Epoch 7, Batch 371, Loss: 0.5630699992179871\n",
      "Epoch 7, Batch 372, Loss: 0.5368914008140564\n",
      "Epoch 7, Batch 373, Loss: 0.62018221616745\n",
      "Epoch 7, Batch 374, Loss: 0.7028332352638245\n",
      "Epoch 7, Batch 375, Loss: 0.7310695052146912\n",
      "Epoch 7, Batch 376, Loss: 0.6638978123664856\n",
      "Epoch 7, Batch 377, Loss: 0.5509213805198669\n",
      "Epoch 7, Batch 378, Loss: 0.6544959545135498\n",
      "Epoch 7, Batch 379, Loss: 0.7658458352088928\n",
      "Epoch 7, Batch 380, Loss: 0.7556842565536499\n",
      "Epoch 7, Batch 381, Loss: 0.5151920914649963\n",
      "Epoch 7, Batch 382, Loss: 0.5027601718902588\n",
      "Epoch 7, Batch 383, Loss: 0.6535085439682007\n",
      "Epoch 7, Batch 384, Loss: 0.6007015705108643\n",
      "Epoch 7, Batch 385, Loss: 0.5100632309913635\n",
      "Epoch 7, Batch 386, Loss: 0.5657161474227905\n",
      "Epoch 7, Batch 387, Loss: 0.512006402015686\n",
      "Epoch 7, Batch 388, Loss: 0.5919137001037598\n",
      "Epoch 7, Batch 389, Loss: 0.45298415422439575\n",
      "Epoch 7, Batch 390, Loss: 0.4131518006324768\n",
      "Epoch 7, Batch 391, Loss: 0.5110113024711609\n",
      "Epoch 7, Batch 392, Loss: 0.39243727922439575\n",
      "Epoch 7, Batch 393, Loss: 0.5785109996795654\n",
      "Epoch 7, Batch 394, Loss: 0.44050636887550354\n",
      "Epoch 7, Batch 395, Loss: 0.4296858310699463\n",
      "Epoch 7, Batch 396, Loss: 0.40751153230667114\n",
      "Epoch 7, Batch 397, Loss: 0.4835691452026367\n",
      "Epoch 7, Batch 398, Loss: 0.5668455958366394\n",
      "Epoch 7, Batch 399, Loss: 0.5748451352119446\n",
      "Epoch 7, Batch 400, Loss: 0.43075740337371826\n",
      "Epoch 7, Batch 401, Loss: 0.5933746695518494\n",
      "Epoch 7, Batch 402, Loss: 0.7068637013435364\n",
      "Epoch 7, Batch 403, Loss: 0.6754449605941772\n",
      "Epoch 7, Batch 404, Loss: 0.563413679599762\n",
      "Epoch 7, Batch 405, Loss: 0.7038190364837646\n",
      "Epoch 7, Batch 406, Loss: 0.3853020668029785\n",
      "Epoch 7, Batch 407, Loss: 0.3560185134410858\n",
      "Epoch 7, Batch 408, Loss: 0.3747725784778595\n",
      "Epoch 7, Batch 409, Loss: 0.5025893449783325\n",
      "Epoch 7, Batch 410, Loss: 0.5354981422424316\n",
      "Epoch 7, Batch 411, Loss: 0.4432513117790222\n",
      "Epoch 7, Batch 412, Loss: 0.5282539129257202\n",
      "Epoch 7, Batch 413, Loss: 0.7085974216461182\n",
      "Epoch 7, Batch 414, Loss: 0.4805764853954315\n",
      "Epoch 7, Batch 415, Loss: 0.45259836316108704\n",
      "Epoch 7, Batch 416, Loss: 0.7804214954376221\n",
      "Epoch 7, Batch 417, Loss: 0.5249348282814026\n",
      "Epoch 7, Batch 418, Loss: 0.6331168413162231\n",
      "Epoch 7, Batch 419, Loss: 0.612508237361908\n",
      "Epoch 7, Batch 420, Loss: 0.43336042761802673\n",
      "Epoch 7, Batch 421, Loss: 0.7254144549369812\n",
      "Epoch 7, Batch 422, Loss: 0.5657662153244019\n",
      "Epoch 7, Batch 423, Loss: 0.5848954319953918\n",
      "Epoch 7, Batch 424, Loss: 0.4438868761062622\n",
      "Epoch 7, Batch 425, Loss: 0.5245029330253601\n",
      "Epoch 7, Batch 426, Loss: 0.4830591380596161\n",
      "Epoch 7, Batch 427, Loss: 0.5659846663475037\n",
      "Epoch 7, Batch 428, Loss: 0.48575031757354736\n",
      "Epoch 7, Batch 429, Loss: 0.4826735854148865\n",
      "Epoch 7, Batch 430, Loss: 0.626908004283905\n",
      "Epoch 7, Batch 431, Loss: 0.4082622826099396\n",
      "Epoch 7, Batch 432, Loss: 0.5407657027244568\n",
      "Epoch 7, Batch 433, Loss: 0.6236307621002197\n",
      "Epoch 7, Batch 434, Loss: 0.5286750197410583\n",
      "Epoch 7, Batch 435, Loss: 0.6758277416229248\n",
      "Epoch 7, Batch 436, Loss: 0.7152347564697266\n",
      "Epoch 7, Batch 437, Loss: 0.5468388199806213\n",
      "Epoch 7, Batch 438, Loss: 0.6273484230041504\n",
      "Epoch 7, Batch 439, Loss: 0.6106063723564148\n",
      "Epoch 7, Batch 440, Loss: 0.5547332763671875\n",
      "Epoch 7, Batch 441, Loss: 0.6031301021575928\n",
      "Epoch 7, Batch 442, Loss: 0.5126644968986511\n",
      "Epoch 7, Batch 443, Loss: 0.5673086047172546\n",
      "Epoch 7, Batch 444, Loss: 0.5975996255874634\n",
      "Epoch 7, Batch 445, Loss: 0.4334731996059418\n",
      "Epoch 7, Batch 446, Loss: 0.7088049650192261\n",
      "Epoch 7, Batch 447, Loss: 0.7360590696334839\n",
      "Epoch 7, Batch 448, Loss: 0.4771715998649597\n",
      "Epoch 7, Batch 449, Loss: 0.5257014036178589\n",
      "Epoch 7, Batch 450, Loss: 0.9349249601364136\n",
      "Epoch 7, Batch 451, Loss: 0.41087302565574646\n",
      "Epoch 7, Batch 452, Loss: 0.5391596555709839\n",
      "Epoch 7, Batch 453, Loss: 0.6412121057510376\n",
      "Epoch 7, Batch 454, Loss: 0.6259556412696838\n",
      "Epoch 7, Batch 455, Loss: 0.5620676279067993\n",
      "Epoch 7, Batch 456, Loss: 0.5523358583450317\n",
      "Epoch 7, Batch 457, Loss: 0.4452364444732666\n",
      "Epoch 7, Batch 458, Loss: 0.5994372367858887\n",
      "Epoch 7, Batch 459, Loss: 0.6626101732254028\n",
      "Epoch 7, Batch 460, Loss: 0.5927222967147827\n",
      "Epoch 7, Batch 461, Loss: 0.6433078646659851\n",
      "Epoch 7, Batch 462, Loss: 0.4794873595237732\n",
      "Epoch 7, Batch 463, Loss: 0.5281952023506165\n",
      "Epoch 7, Batch 464, Loss: 0.6113904118537903\n",
      "Epoch 7, Batch 465, Loss: 0.6197175979614258\n",
      "Epoch 7, Batch 466, Loss: 0.6136029362678528\n",
      "Epoch 7, Batch 467, Loss: 0.5832827091217041\n",
      "Epoch 7, Batch 468, Loss: 0.3438829183578491\n",
      "Epoch 7, Batch 469, Loss: 0.4387812316417694\n",
      "Epoch 7, Batch 470, Loss: 0.6181817650794983\n",
      "Epoch 7, Batch 471, Loss: 0.5661461353302002\n",
      "Epoch 7, Batch 472, Loss: 0.5761935710906982\n",
      "Epoch 7, Batch 473, Loss: 0.513892650604248\n",
      "Epoch 7, Batch 474, Loss: 0.5998218059539795\n",
      "Epoch 7, Batch 475, Loss: 0.5342168211936951\n",
      "Epoch 7, Batch 476, Loss: 0.8100982308387756\n",
      "Epoch 7, Batch 477, Loss: 0.5445199608802795\n",
      "Epoch 7, Batch 478, Loss: 0.5175426006317139\n",
      "Epoch 7, Batch 479, Loss: 0.5224531888961792\n",
      "Epoch 7, Batch 480, Loss: 0.617900550365448\n",
      "Epoch 7, Batch 481, Loss: 0.5167455077171326\n",
      "Epoch 7, Batch 482, Loss: 0.8036291599273682\n",
      "Epoch 7, Batch 483, Loss: 0.46030938625335693\n",
      "Epoch 7, Batch 484, Loss: 0.6624071002006531\n",
      "Epoch 7, Batch 485, Loss: 0.3627568483352661\n",
      "Epoch 7, Batch 486, Loss: 0.46814781427383423\n",
      "Epoch 7, Batch 487, Loss: 0.5382425785064697\n",
      "Epoch 7, Batch 488, Loss: 0.5962234139442444\n",
      "Epoch 7, Batch 489, Loss: 0.493693470954895\n",
      "Epoch 7, Batch 490, Loss: 0.799512505531311\n",
      "Epoch 7, Batch 491, Loss: 0.5269489288330078\n",
      "Epoch 7, Batch 492, Loss: 0.6092867851257324\n",
      "Epoch 7, Batch 493, Loss: 0.4937753975391388\n",
      "Epoch 7, Batch 494, Loss: 0.5681425333023071\n",
      "Epoch 7, Batch 495, Loss: 0.4766238033771515\n",
      "Epoch 7, Batch 496, Loss: 0.6470784544944763\n",
      "Epoch 7, Batch 497, Loss: 0.6803461313247681\n",
      "Epoch 7, Batch 498, Loss: 0.4409390985965729\n",
      "Epoch 7, Batch 499, Loss: 0.3461915850639343\n",
      "Epoch 7, Batch 500, Loss: 0.7706397771835327\n",
      "Epoch 7, Batch 501, Loss: 0.4581836760044098\n",
      "Epoch 7, Batch 502, Loss: 0.5192920565605164\n",
      "Epoch 7, Batch 503, Loss: 0.6639425754547119\n",
      "Epoch 7, Batch 504, Loss: 0.5511589646339417\n",
      "Epoch 7, Batch 505, Loss: 0.45464980602264404\n",
      "Epoch 7, Batch 506, Loss: 0.4609981179237366\n",
      "Epoch 7, Batch 507, Loss: 0.5058420300483704\n",
      "Epoch 7, Batch 508, Loss: 0.6404519081115723\n",
      "Epoch 7, Batch 509, Loss: 0.5269563794136047\n",
      "Epoch 7, Batch 510, Loss: 0.7226698994636536\n",
      "Epoch 7, Batch 511, Loss: 0.7109216451644897\n",
      "Epoch 7, Batch 512, Loss: 0.5949035286903381\n",
      "Epoch 7, Batch 513, Loss: 0.5789250135421753\n",
      "Epoch 7, Batch 514, Loss: 0.6582432985305786\n",
      "Epoch 7, Batch 515, Loss: 0.38026490807533264\n",
      "Epoch 7, Batch 516, Loss: 0.3984796404838562\n",
      "Epoch 7, Batch 517, Loss: 0.9972003698348999\n",
      "Epoch 7, Batch 518, Loss: 0.4998830556869507\n",
      "Epoch 7, Batch 519, Loss: 0.7586147785186768\n",
      "Epoch 7, Batch 520, Loss: 0.7037186026573181\n",
      "Epoch 7, Batch 521, Loss: 0.3784985840320587\n",
      "Epoch 7, Batch 522, Loss: 0.45812171697616577\n",
      "Epoch 7, Batch 523, Loss: 0.5144006013870239\n",
      "Epoch 7, Batch 524, Loss: 0.6621639728546143\n",
      "Epoch 7, Batch 525, Loss: 0.5574933290481567\n",
      "Epoch 7, Batch 526, Loss: 0.5447831153869629\n",
      "Epoch 7, Batch 527, Loss: 0.3873881697654724\n",
      "Epoch 7, Batch 528, Loss: 0.3891982436180115\n",
      "Epoch 7, Batch 529, Loss: 0.47588828206062317\n",
      "Epoch 7, Batch 530, Loss: 0.6022743582725525\n",
      "Epoch 7, Batch 531, Loss: 0.6232233643531799\n",
      "Epoch 7, Batch 532, Loss: 0.6571747660636902\n",
      "Epoch 7, Batch 533, Loss: 0.5711606740951538\n",
      "Epoch 7, Batch 534, Loss: 0.5662719011306763\n",
      "Epoch 7, Batch 535, Loss: 0.5529184937477112\n",
      "Epoch 7, Batch 536, Loss: 0.8482394218444824\n",
      "Epoch 7, Batch 537, Loss: 0.4604901969432831\n",
      "Epoch 7, Batch 538, Loss: 0.503588080406189\n",
      "Epoch 7, Batch 539, Loss: 0.5323064923286438\n",
      "Epoch 7, Batch 540, Loss: 0.6246099472045898\n",
      "Epoch 7, Batch 541, Loss: 0.4622275233268738\n",
      "Epoch 7, Batch 542, Loss: 0.4678218364715576\n",
      "Epoch 7, Batch 543, Loss: 0.4125087261199951\n",
      "Epoch 7, Batch 544, Loss: 0.6797908544540405\n",
      "Epoch 7, Batch 545, Loss: 0.6959351897239685\n",
      "Epoch 7, Batch 546, Loss: 0.6088919043540955\n",
      "Epoch 7, Batch 547, Loss: 0.7372797131538391\n",
      "Epoch 7, Batch 548, Loss: 0.5475738048553467\n",
      "Epoch 7, Batch 549, Loss: 0.8290283679962158\n",
      "Epoch 7, Batch 550, Loss: 0.7484695911407471\n",
      "Epoch 7, Batch 551, Loss: 0.6630218029022217\n",
      "Epoch 7, Batch 552, Loss: 0.4966699481010437\n",
      "Epoch 7, Batch 553, Loss: 0.43864694237709045\n",
      "Epoch 7, Batch 554, Loss: 0.6495625972747803\n",
      "Epoch 7, Batch 555, Loss: 0.510890543460846\n",
      "Epoch 7, Batch 556, Loss: 0.4239259958267212\n",
      "Epoch 7, Batch 557, Loss: 0.41033631563186646\n",
      "Epoch 7, Batch 558, Loss: 0.5041537880897522\n",
      "Epoch 7, Batch 559, Loss: 0.5019960403442383\n",
      "Epoch 7, Batch 560, Loss: 0.5412850975990295\n",
      "Epoch 7, Batch 561, Loss: 0.8419550061225891\n",
      "Epoch 7, Batch 562, Loss: 0.559799075126648\n",
      "Epoch 7, Batch 563, Loss: 0.5152946710586548\n",
      "Epoch 7, Batch 564, Loss: 0.4570280909538269\n",
      "Epoch 7, Batch 565, Loss: 0.5980581641197205\n",
      "Epoch 7, Batch 566, Loss: 0.6068830490112305\n",
      "Epoch 7, Batch 567, Loss: 0.5112297534942627\n",
      "Epoch 7, Batch 568, Loss: 0.4980056881904602\n",
      "Epoch 7, Batch 569, Loss: 0.6482282876968384\n",
      "Epoch 7, Batch 570, Loss: 0.571444034576416\n",
      "Epoch 7, Batch 571, Loss: 0.6389634609222412\n",
      "Epoch 7, Batch 572, Loss: 0.561578631401062\n",
      "Epoch 7, Batch 573, Loss: 0.4250836670398712\n",
      "Epoch 7, Batch 574, Loss: 0.5040401220321655\n",
      "Epoch 7, Batch 575, Loss: 0.5315264463424683\n",
      "Epoch 7, Batch 576, Loss: 0.8008745908737183\n",
      "Epoch 7, Batch 577, Loss: 0.45205822587013245\n",
      "Epoch 7, Batch 578, Loss: 0.5327515006065369\n",
      "Epoch 7, Batch 579, Loss: 0.8343176245689392\n",
      "Epoch 7, Batch 580, Loss: 0.5177835822105408\n",
      "Epoch 7, Batch 581, Loss: 0.3658469319343567\n",
      "Epoch 7, Batch 582, Loss: 0.37660712003707886\n",
      "Epoch 7, Batch 583, Loss: 0.396196186542511\n",
      "Epoch 7, Batch 584, Loss: 0.4744894504547119\n",
      "Epoch 7, Batch 585, Loss: 0.6885206699371338\n",
      "Epoch 7, Batch 586, Loss: 0.6261056661605835\n",
      "Epoch 7, Batch 587, Loss: 0.5657244920730591\n",
      "Epoch 7, Batch 588, Loss: 0.7536251544952393\n",
      "Epoch 7, Batch 589, Loss: 0.5540562272071838\n",
      "Epoch 7, Batch 590, Loss: 0.6632272005081177\n",
      "Epoch 7, Batch 591, Loss: 0.5959597826004028\n",
      "Epoch 7, Batch 592, Loss: 0.4966185688972473\n",
      "Epoch 7, Batch 593, Loss: 0.7136186361312866\n",
      "Epoch 7, Batch 594, Loss: 0.5578690767288208\n",
      "Epoch 7, Batch 595, Loss: 0.4857161343097687\n",
      "Epoch 7, Batch 596, Loss: 0.3756343722343445\n",
      "Epoch 7, Batch 597, Loss: 0.4764813780784607\n",
      "Epoch 7, Batch 598, Loss: 0.5497224926948547\n",
      "Epoch 7, Batch 599, Loss: 0.5409637689590454\n",
      "Epoch 7, Batch 600, Loss: 0.5651625990867615\n",
      "Epoch 7, Batch 601, Loss: 0.9045659899711609\n",
      "Epoch 7, Batch 602, Loss: 0.6036969423294067\n",
      "Epoch 7, Batch 603, Loss: 0.45435547828674316\n",
      "Epoch 7, Batch 604, Loss: 0.506019651889801\n",
      "Epoch 7, Batch 605, Loss: 0.613879382610321\n",
      "Epoch 7, Batch 606, Loss: 0.6908786296844482\n",
      "Epoch 7, Batch 607, Loss: 0.7140196561813354\n",
      "Epoch 7, Batch 608, Loss: 0.7309666872024536\n",
      "Epoch 7, Batch 609, Loss: 0.45824795961380005\n",
      "Epoch 7, Batch 610, Loss: 0.6011846661567688\n",
      "Epoch 7, Batch 611, Loss: 0.5986965894699097\n",
      "Epoch 7, Batch 612, Loss: 0.47951650619506836\n",
      "Epoch 7, Batch 613, Loss: 0.4462106227874756\n",
      "Epoch 7, Batch 614, Loss: 0.7480217218399048\n",
      "Epoch 7, Batch 615, Loss: 0.5633931159973145\n",
      "Epoch 7, Batch 616, Loss: 0.5455491542816162\n",
      "Epoch 7, Batch 617, Loss: 0.41280514001846313\n",
      "Epoch 7, Batch 618, Loss: 0.49591636657714844\n",
      "Epoch 7, Batch 619, Loss: 0.46528398990631104\n",
      "Epoch 7, Batch 620, Loss: 0.4426904618740082\n",
      "Epoch 7, Batch 621, Loss: 0.5109654068946838\n",
      "Epoch 7, Batch 622, Loss: 0.4758358299732208\n",
      "Epoch 7, Batch 623, Loss: 0.5270150303840637\n",
      "Epoch 7, Batch 624, Loss: 0.9494802951812744\n",
      "Epoch 7, Batch 625, Loss: 0.39902710914611816\n",
      "Epoch 7, Batch 626, Loss: 0.6390223503112793\n",
      "Epoch 7, Batch 627, Loss: 0.7160279154777527\n",
      "Epoch 7, Batch 628, Loss: 0.6789972186088562\n",
      "Epoch 7, Batch 629, Loss: 0.5406227111816406\n",
      "Epoch 7, Batch 630, Loss: 0.4311578869819641\n",
      "Epoch 7, Batch 631, Loss: 0.6036937236785889\n",
      "Epoch 7, Batch 632, Loss: 0.5968365669250488\n",
      "Epoch 7, Batch 633, Loss: 0.483629047870636\n",
      "Epoch 7, Batch 634, Loss: 0.6340333223342896\n",
      "Epoch 7, Batch 635, Loss: 0.512942910194397\n",
      "Epoch 7, Batch 636, Loss: 0.48492878675460815\n",
      "Epoch 7, Batch 637, Loss: 0.46996062994003296\n",
      "Epoch 7, Batch 638, Loss: 0.44914233684539795\n",
      "Epoch 7, Batch 639, Loss: 0.46156907081604004\n",
      "Epoch 7, Batch 640, Loss: 0.5803595185279846\n",
      "Epoch 7, Batch 641, Loss: 0.5290127992630005\n",
      "Epoch 7, Batch 642, Loss: 0.5083191394805908\n",
      "Epoch 7, Batch 643, Loss: 0.4022306799888611\n",
      "Epoch 7, Batch 644, Loss: 0.645293653011322\n",
      "Epoch 7, Batch 645, Loss: 0.6162787079811096\n",
      "Epoch 7, Batch 646, Loss: 0.5163805484771729\n",
      "Epoch 7, Batch 647, Loss: 0.5429738759994507\n",
      "Epoch 7, Batch 648, Loss: 0.4478472173213959\n",
      "Epoch 7, Batch 649, Loss: 0.9514947533607483\n",
      "Epoch 7, Batch 650, Loss: 0.5870808959007263\n",
      "Epoch 7, Batch 651, Loss: 0.5152063369750977\n",
      "Epoch 7, Batch 652, Loss: 0.5359985828399658\n",
      "Epoch 7, Batch 653, Loss: 0.44987672567367554\n",
      "Epoch 7, Batch 654, Loss: 0.6090275645256042\n",
      "Epoch 7, Batch 655, Loss: 0.5503342151641846\n",
      "Epoch 7, Batch 656, Loss: 0.586142897605896\n",
      "Epoch 7, Batch 657, Loss: 0.619959831237793\n",
      "Epoch 7, Batch 658, Loss: 0.5700757503509521\n",
      "Epoch 7, Batch 659, Loss: 0.5276982188224792\n",
      "Epoch 7, Batch 660, Loss: 0.6294056177139282\n",
      "Epoch 7, Batch 661, Loss: 0.5174384117126465\n",
      "Epoch 7, Batch 662, Loss: 0.6096631288528442\n",
      "Epoch 7, Batch 663, Loss: 0.5881383419036865\n",
      "Epoch 7, Batch 664, Loss: 0.47826939821243286\n",
      "Epoch 7, Batch 665, Loss: 0.42009344696998596\n",
      "Epoch 7, Batch 666, Loss: 0.5657073855400085\n",
      "Epoch 7, Batch 667, Loss: 0.6069427728652954\n",
      "Epoch 7, Batch 668, Loss: 0.6714154481887817\n",
      "Epoch 7, Batch 669, Loss: 0.6843531131744385\n",
      "Epoch 7, Batch 670, Loss: 0.44478291273117065\n",
      "Epoch 7, Batch 671, Loss: 0.510236382484436\n",
      "Epoch 7, Batch 672, Loss: 0.6713123321533203\n",
      "Epoch 7, Batch 673, Loss: 0.5285599827766418\n",
      "Epoch 7, Batch 674, Loss: 0.6119570732116699\n",
      "Epoch 7, Batch 675, Loss: 0.6708484292030334\n",
      "Epoch 7, Batch 676, Loss: 0.7663408517837524\n",
      "Epoch 7, Batch 677, Loss: 0.4815913438796997\n",
      "Epoch 7, Batch 678, Loss: 0.48030924797058105\n",
      "Epoch 7, Batch 679, Loss: 0.7264600992202759\n",
      "Epoch 7, Batch 680, Loss: 0.4198947846889496\n",
      "Epoch 7, Batch 681, Loss: 0.6125615835189819\n",
      "Epoch 7, Batch 682, Loss: 0.8796981573104858\n",
      "Epoch 7, Batch 683, Loss: 0.5940086245536804\n",
      "Epoch 7, Batch 684, Loss: 0.6931222081184387\n",
      "Epoch 7, Batch 685, Loss: 0.5197426080703735\n",
      "Epoch 7, Batch 686, Loss: 0.4820724427700043\n",
      "Epoch 7, Batch 687, Loss: 0.7792369723320007\n",
      "Epoch 7, Batch 688, Loss: 0.556891143321991\n",
      "Epoch 7, Batch 689, Loss: 0.5410177111625671\n",
      "Epoch 7, Batch 690, Loss: 0.4655781388282776\n",
      "Epoch 7, Batch 691, Loss: 0.44196009635925293\n",
      "Epoch 7, Batch 692, Loss: 0.49487680196762085\n",
      "Epoch 7, Batch 693, Loss: 0.45216700434684753\n",
      "Epoch 7, Batch 694, Loss: 0.8448083996772766\n",
      "Epoch 7, Batch 695, Loss: 0.6537165641784668\n",
      "Epoch 7, Batch 696, Loss: 0.5507017970085144\n",
      "Epoch 7, Batch 697, Loss: 0.8187344074249268\n",
      "Epoch 7, Batch 698, Loss: 0.5787590742111206\n",
      "Epoch 7, Batch 699, Loss: 0.6150963306427002\n",
      "Epoch 7, Batch 700, Loss: 0.5630696415901184\n",
      "Epoch 7, Batch 701, Loss: 0.6624212861061096\n",
      "Epoch 7, Batch 702, Loss: 0.6457387208938599\n",
      "Epoch 7, Batch 703, Loss: 0.4491008520126343\n",
      "Epoch 7, Batch 704, Loss: 0.5487949848175049\n",
      "Epoch 7, Batch 705, Loss: 0.41224318742752075\n",
      "Epoch 7, Batch 706, Loss: 0.6311805248260498\n",
      "Epoch 7, Batch 707, Loss: 0.3712613582611084\n",
      "Epoch 7, Batch 708, Loss: 0.69084632396698\n",
      "Epoch 7, Batch 709, Loss: 0.367886483669281\n",
      "Epoch 7, Batch 710, Loss: 0.5797951221466064\n",
      "Epoch 7, Batch 711, Loss: 0.419476717710495\n",
      "Epoch 7, Batch 712, Loss: 0.5067377686500549\n",
      "Epoch 7, Batch 713, Loss: 0.8197574615478516\n",
      "Epoch 7, Batch 714, Loss: 0.589352011680603\n",
      "Epoch 7, Batch 715, Loss: 0.46761244535446167\n",
      "Epoch 7, Batch 716, Loss: 0.6617593765258789\n",
      "Epoch 7, Batch 717, Loss: 0.6893432140350342\n",
      "Epoch 7, Batch 718, Loss: 0.5215359330177307\n",
      "Epoch 7, Batch 719, Loss: 0.42454415559768677\n",
      "Epoch 7, Batch 720, Loss: 0.6034146547317505\n",
      "Epoch 7, Batch 721, Loss: 0.483798086643219\n",
      "Epoch 7, Batch 722, Loss: 0.48215052485466003\n",
      "Epoch 7, Batch 723, Loss: 0.74014812707901\n",
      "Epoch 7, Batch 724, Loss: 0.6517022848129272\n",
      "Epoch 7, Batch 725, Loss: 0.5049766302108765\n",
      "Epoch 7, Batch 726, Loss: 0.5236387252807617\n",
      "Epoch 7, Batch 727, Loss: 0.7968136668205261\n",
      "Epoch 7, Batch 728, Loss: 0.5162478685379028\n",
      "Epoch 7, Batch 729, Loss: 0.43640947341918945\n",
      "Epoch 7, Batch 730, Loss: 0.6031203269958496\n",
      "Epoch 7, Batch 731, Loss: 0.6869513988494873\n",
      "Epoch 7, Batch 732, Loss: 0.35628125071525574\n",
      "Epoch 7, Batch 733, Loss: 0.5912171602249146\n",
      "Epoch 7, Batch 734, Loss: 0.4831213057041168\n",
      "Epoch 7, Batch 735, Loss: 0.4738505482673645\n",
      "Epoch 7, Batch 736, Loss: 0.9303977489471436\n",
      "Epoch 7, Batch 737, Loss: 0.6739013195037842\n",
      "Epoch 7, Batch 738, Loss: 0.5674355030059814\n",
      "Epoch 7, Batch 739, Loss: 0.5556600093841553\n",
      "Epoch 7, Batch 740, Loss: 0.47832921147346497\n",
      "Epoch 7, Batch 741, Loss: 0.6627804636955261\n",
      "Epoch 7, Batch 742, Loss: 0.5137401819229126\n",
      "Epoch 7, Batch 743, Loss: 0.5447474718093872\n",
      "Epoch 7, Batch 744, Loss: 0.5634837746620178\n",
      "Epoch 7, Batch 745, Loss: 0.5273366570472717\n",
      "Epoch 7, Batch 746, Loss: 0.7584332823753357\n",
      "Epoch 7, Batch 747, Loss: 0.6447029113769531\n",
      "Epoch 7, Batch 748, Loss: 0.7638092041015625\n",
      "Epoch 7, Batch 749, Loss: 0.6163027882575989\n",
      "Epoch 7, Batch 750, Loss: 0.5885676741600037\n",
      "Epoch 7, Batch 751, Loss: 0.6548709869384766\n",
      "Epoch 7, Batch 752, Loss: 0.504406213760376\n",
      "Epoch 7, Batch 753, Loss: 0.5380542874336243\n",
      "Epoch 7, Batch 754, Loss: 0.44465821981430054\n",
      "Epoch 7, Batch 755, Loss: 0.6054702401161194\n",
      "Epoch 7, Batch 756, Loss: 0.725854754447937\n",
      "Epoch 7, Batch 757, Loss: 0.6109827160835266\n",
      "Epoch 7, Batch 758, Loss: 0.4843057692050934\n",
      "Epoch 7, Batch 759, Loss: 0.3839234709739685\n",
      "Epoch 7, Batch 760, Loss: 0.4944279193878174\n",
      "Epoch 7, Batch 761, Loss: 0.48505860567092896\n",
      "Epoch 7, Batch 762, Loss: 0.6002081036567688\n",
      "Epoch 7, Batch 763, Loss: 0.5503196716308594\n",
      "Epoch 7, Batch 764, Loss: 0.6762794852256775\n",
      "Epoch 7, Batch 765, Loss: 0.4502297639846802\n",
      "Epoch 7, Batch 766, Loss: 0.5021070837974548\n",
      "Epoch 7, Batch 767, Loss: 0.6503386497497559\n",
      "Epoch 7, Batch 768, Loss: 0.5637232661247253\n",
      "Epoch 7, Batch 769, Loss: 0.712999701499939\n",
      "Epoch 7, Batch 770, Loss: 0.7308045029640198\n",
      "Epoch 7, Batch 771, Loss: 0.6292539238929749\n",
      "Epoch 7, Batch 772, Loss: 0.48980140686035156\n",
      "Epoch 7, Batch 773, Loss: 0.584680438041687\n",
      "Epoch 7, Batch 774, Loss: 0.6296838521957397\n",
      "Epoch 7, Batch 775, Loss: 0.535343587398529\n",
      "Epoch 7, Batch 776, Loss: 0.4630824327468872\n",
      "Epoch 7, Batch 777, Loss: 0.6771234273910522\n",
      "Epoch 7, Batch 778, Loss: 0.7449074983596802\n",
      "Epoch 7, Batch 779, Loss: 0.600082516670227\n",
      "Epoch 7, Batch 780, Loss: 0.5653057098388672\n",
      "Epoch 7, Batch 781, Loss: 0.8259813189506531\n",
      "Epoch 7, Batch 782, Loss: 0.5732165575027466\n",
      "Epoch 7, Batch 783, Loss: 0.6464143991470337\n",
      "Epoch 7, Batch 784, Loss: 0.9322751760482788\n",
      "Epoch 7, Batch 785, Loss: 0.37721091508865356\n",
      "Epoch 7, Batch 786, Loss: 0.5319316387176514\n",
      "Epoch 7, Batch 787, Loss: 0.5752239227294922\n",
      "Epoch 7, Batch 788, Loss: 0.46885520219802856\n",
      "Epoch 7, Batch 789, Loss: 0.6439739465713501\n",
      "Epoch 7, Batch 790, Loss: 0.57728511095047\n",
      "Epoch 7, Batch 791, Loss: 0.5567305088043213\n",
      "Epoch 7, Batch 792, Loss: 0.6155173778533936\n",
      "Epoch 7, Batch 793, Loss: 0.6654587984085083\n",
      "Epoch 7, Batch 794, Loss: 0.4427722692489624\n",
      "Epoch 7, Batch 795, Loss: 0.7503920793533325\n",
      "Epoch 7, Batch 796, Loss: 0.6768390536308289\n",
      "Epoch 7, Batch 797, Loss: 0.5128569602966309\n",
      "Epoch 7, Batch 798, Loss: 0.42517709732055664\n",
      "Epoch 7, Batch 799, Loss: 0.4501580595970154\n",
      "Epoch 7, Batch 800, Loss: 0.39716553688049316\n",
      "Epoch 7, Batch 801, Loss: 0.6725869178771973\n",
      "Epoch 7, Batch 802, Loss: 0.7151457071304321\n",
      "Epoch 7, Batch 803, Loss: 0.4304511845111847\n",
      "Epoch 7, Batch 804, Loss: 0.5390624403953552\n",
      "Epoch 7, Batch 805, Loss: 0.614359438419342\n",
      "Epoch 7, Batch 806, Loss: 0.6570354700088501\n",
      "Epoch 7, Batch 807, Loss: 0.53721022605896\n",
      "Epoch 7, Batch 808, Loss: 0.6504826545715332\n",
      "Epoch 7, Batch 809, Loss: 0.3438384532928467\n",
      "Epoch 7, Batch 810, Loss: 0.5406079292297363\n",
      "Epoch 7, Batch 811, Loss: 0.37778037786483765\n",
      "Epoch 7, Batch 812, Loss: 0.44928523898124695\n",
      "Epoch 7, Batch 813, Loss: 0.5591643452644348\n",
      "Epoch 7, Batch 814, Loss: 0.47217780351638794\n",
      "Epoch 7, Batch 815, Loss: 0.5095873475074768\n",
      "Epoch 7, Batch 816, Loss: 0.759541928768158\n",
      "Epoch 7, Batch 817, Loss: 0.4462008476257324\n",
      "Epoch 7, Batch 818, Loss: 0.586732029914856\n",
      "Epoch 7, Batch 819, Loss: 0.46450865268707275\n",
      "Epoch 7, Batch 820, Loss: 0.5562371015548706\n",
      "Epoch 7, Batch 821, Loss: 0.6066155433654785\n",
      "Epoch 7, Batch 822, Loss: 0.6873502731323242\n",
      "Epoch 7, Batch 823, Loss: 0.5142247080802917\n",
      "Epoch 7, Batch 824, Loss: 0.533115565776825\n",
      "Epoch 7, Batch 825, Loss: 0.5849652290344238\n",
      "Epoch 7, Batch 826, Loss: 0.5791864991188049\n",
      "Epoch 7, Batch 827, Loss: 0.6763302087783813\n",
      "Epoch 7, Batch 828, Loss: 0.4550526440143585\n",
      "Epoch 7, Batch 829, Loss: 0.6914162039756775\n",
      "Epoch 7, Batch 830, Loss: 0.47906193137168884\n",
      "Epoch 7, Batch 831, Loss: 0.8030197024345398\n",
      "Epoch 7, Batch 832, Loss: 0.8006249070167542\n",
      "Epoch 7, Batch 833, Loss: 0.3753795027732849\n",
      "Epoch 7, Batch 834, Loss: 0.6612215042114258\n",
      "Epoch 7, Batch 835, Loss: 0.7740640044212341\n",
      "Epoch 7, Batch 836, Loss: 0.5628001093864441\n",
      "Epoch 7, Batch 837, Loss: 0.5375615954399109\n",
      "Epoch 7, Batch 838, Loss: 0.539982259273529\n",
      "Epoch 7, Batch 839, Loss: 0.6907440423965454\n",
      "Epoch 7, Batch 840, Loss: 0.6083977818489075\n",
      "Epoch 7, Batch 841, Loss: 0.5741088390350342\n",
      "Epoch 7, Batch 842, Loss: 0.3918907642364502\n",
      "Epoch 7, Batch 843, Loss: 0.4943857789039612\n",
      "Epoch 7, Batch 844, Loss: 0.4232129454612732\n",
      "Epoch 7, Batch 845, Loss: 0.6331445574760437\n",
      "Epoch 7, Batch 846, Loss: 0.5711506605148315\n",
      "Epoch 7, Batch 847, Loss: 0.7088562846183777\n",
      "Epoch 7, Batch 848, Loss: 0.5574610829353333\n",
      "Epoch 7, Batch 849, Loss: 0.49664005637168884\n",
      "Epoch 7, Batch 850, Loss: 0.5751368999481201\n",
      "Epoch 7, Batch 851, Loss: 0.5908187627792358\n",
      "Epoch 7, Batch 852, Loss: 0.6000487208366394\n",
      "Epoch 7, Batch 853, Loss: 0.3947221636772156\n",
      "Epoch 7, Batch 854, Loss: 0.4994373321533203\n",
      "Epoch 7, Batch 855, Loss: 0.6314151287078857\n",
      "Epoch 7, Batch 856, Loss: 0.3367827236652374\n",
      "Epoch 7, Batch 857, Loss: 0.687282383441925\n",
      "Epoch 7, Batch 858, Loss: 0.7317532896995544\n",
      "Epoch 7, Batch 859, Loss: 0.6008990406990051\n",
      "Epoch 7, Batch 860, Loss: 0.49695929884910583\n",
      "Epoch 7, Batch 861, Loss: 0.4734838604927063\n",
      "Epoch 7, Batch 862, Loss: 0.5436990857124329\n",
      "Epoch 7, Batch 863, Loss: 0.5146980285644531\n",
      "Epoch 7, Batch 864, Loss: 0.6629283428192139\n",
      "Epoch 7, Batch 865, Loss: 0.5060157775878906\n",
      "Epoch 7, Batch 866, Loss: 0.9668092727661133\n",
      "Epoch 7, Batch 867, Loss: 0.8022742867469788\n",
      "Epoch 7, Batch 868, Loss: 0.5027107000350952\n",
      "Epoch 7, Batch 869, Loss: 0.6125900149345398\n",
      "Epoch 7, Batch 870, Loss: 0.6549524068832397\n",
      "Epoch 7, Batch 871, Loss: 0.48066869378089905\n",
      "Epoch 7, Batch 872, Loss: 0.3523440361022949\n",
      "Epoch 7, Batch 873, Loss: 0.8257534503936768\n",
      "Epoch 7, Batch 874, Loss: 0.5930596590042114\n",
      "Epoch 7, Batch 875, Loss: 0.558015763759613\n",
      "Epoch 7, Batch 876, Loss: 0.7572776079177856\n",
      "Epoch 7, Batch 877, Loss: 0.5309498310089111\n",
      "Epoch 7, Batch 878, Loss: 0.7809542417526245\n",
      "Epoch 7, Batch 879, Loss: 0.5033553838729858\n",
      "Epoch 7, Batch 880, Loss: 0.5861470699310303\n",
      "Epoch 7, Batch 881, Loss: 0.4730599522590637\n",
      "Epoch 7, Batch 882, Loss: 0.7276399731636047\n",
      "Epoch 7, Batch 883, Loss: 0.4505629539489746\n",
      "Epoch 7, Batch 884, Loss: 0.42045390605926514\n",
      "Epoch 7, Batch 885, Loss: 0.6101906895637512\n",
      "Epoch 7, Batch 886, Loss: 0.6078797578811646\n",
      "Epoch 7, Batch 887, Loss: 0.5412527918815613\n",
      "Epoch 7, Batch 888, Loss: 0.6724469661712646\n",
      "Epoch 7, Batch 889, Loss: 0.5032761693000793\n",
      "Epoch 7, Batch 890, Loss: 0.507697582244873\n",
      "Epoch 7, Batch 891, Loss: 0.7102484703063965\n",
      "Epoch 7, Batch 892, Loss: 0.5170196890830994\n",
      "Epoch 7, Batch 893, Loss: 0.5024945139884949\n",
      "Epoch 7, Batch 894, Loss: 0.5720453262329102\n",
      "Epoch 7, Batch 895, Loss: 0.5298393964767456\n",
      "Epoch 7, Batch 896, Loss: 0.913923442363739\n",
      "Epoch 7, Batch 897, Loss: 0.6459165215492249\n",
      "Epoch 7, Batch 898, Loss: 0.6543144583702087\n",
      "Epoch 7, Batch 899, Loss: 0.4705135226249695\n",
      "Epoch 7, Batch 900, Loss: 0.5686500072479248\n",
      "Epoch 7, Batch 901, Loss: 0.41634848713874817\n",
      "Epoch 7, Batch 902, Loss: 0.7689800262451172\n",
      "Epoch 7, Batch 903, Loss: 0.7102263569831848\n",
      "Epoch 7, Batch 904, Loss: 0.4684821665287018\n",
      "Epoch 7, Batch 905, Loss: 0.42656245827674866\n",
      "Epoch 7, Batch 906, Loss: 0.40813586115837097\n",
      "Epoch 7, Batch 907, Loss: 0.6118208169937134\n",
      "Epoch 7, Batch 908, Loss: 0.5298103094100952\n",
      "Epoch 7, Batch 909, Loss: 0.6064778566360474\n",
      "Epoch 7, Batch 910, Loss: 0.5559201240539551\n",
      "Epoch 7, Batch 911, Loss: 0.5840457081794739\n",
      "Epoch 7, Batch 912, Loss: 0.6229854822158813\n",
      "Epoch 7, Batch 913, Loss: 0.4937196671962738\n",
      "Epoch 7, Batch 914, Loss: 0.6801274418830872\n",
      "Epoch 7, Batch 915, Loss: 0.601203203201294\n",
      "Epoch 7, Batch 916, Loss: 0.5635349750518799\n",
      "Epoch 7, Batch 917, Loss: 0.7761949300765991\n",
      "Epoch 7, Batch 918, Loss: 0.6081833243370056\n",
      "Epoch 7, Batch 919, Loss: 0.5884992480278015\n",
      "Epoch 7, Batch 920, Loss: 0.4811352491378784\n",
      "Epoch 7, Batch 921, Loss: 0.6725653409957886\n",
      "Epoch 7, Batch 922, Loss: 0.5030772686004639\n",
      "Epoch 7, Batch 923, Loss: 0.6083062291145325\n",
      "Epoch 7, Batch 924, Loss: 0.8004173636436462\n",
      "Epoch 7, Batch 925, Loss: 0.5166598558425903\n",
      "Epoch 7, Batch 926, Loss: 0.5436316132545471\n",
      "Epoch 7, Batch 927, Loss: 0.44593554735183716\n",
      "Epoch 7, Batch 928, Loss: 0.4994169771671295\n",
      "Epoch 7, Batch 929, Loss: 0.5227513313293457\n",
      "Epoch 7, Batch 930, Loss: 0.5933071970939636\n",
      "Epoch 7, Batch 931, Loss: 0.4837389886379242\n",
      "Epoch 7, Batch 932, Loss: 0.5265487432479858\n",
      "Epoch 7, Batch 933, Loss: 0.5140315294265747\n",
      "Epoch 7, Batch 934, Loss: 0.42214342951774597\n",
      "Epoch 7, Batch 935, Loss: 0.6292868256568909\n",
      "Epoch 7, Batch 936, Loss: 0.421802282333374\n",
      "Epoch 7, Batch 937, Loss: 0.5081230401992798\n",
      "Epoch 7, Batch 938, Loss: 0.7379056215286255\n",
      "Accuracy of train set: 0.79745\n",
      "Epoch 7, Batch 1, Test Loss: 0.6685667037963867\n",
      "Epoch 7, Batch 2, Test Loss: 0.5925148725509644\n",
      "Epoch 7, Batch 3, Test Loss: 0.5886889696121216\n",
      "Epoch 7, Batch 4, Test Loss: 0.5413325428962708\n",
      "Epoch 7, Batch 5, Test Loss: 0.4710039496421814\n",
      "Epoch 7, Batch 6, Test Loss: 0.5449968576431274\n",
      "Epoch 7, Batch 7, Test Loss: 0.6543238759040833\n",
      "Epoch 7, Batch 8, Test Loss: 0.7686084508895874\n",
      "Epoch 7, Batch 9, Test Loss: 0.4656156897544861\n",
      "Epoch 7, Batch 10, Test Loss: 0.5325016975402832\n",
      "Epoch 7, Batch 11, Test Loss: 0.776605486869812\n",
      "Epoch 7, Batch 12, Test Loss: 0.7367364764213562\n",
      "Epoch 7, Batch 13, Test Loss: 0.49781525135040283\n",
      "Epoch 7, Batch 14, Test Loss: 0.4653858244419098\n",
      "Epoch 7, Batch 15, Test Loss: 0.5764766931533813\n",
      "Epoch 7, Batch 16, Test Loss: 0.5201812982559204\n",
      "Epoch 7, Batch 17, Test Loss: 0.5031669735908508\n",
      "Epoch 7, Batch 18, Test Loss: 0.5949214100837708\n",
      "Epoch 7, Batch 19, Test Loss: 0.6033862233161926\n",
      "Epoch 7, Batch 20, Test Loss: 0.7552835941314697\n",
      "Epoch 7, Batch 21, Test Loss: 0.5710869431495667\n",
      "Epoch 7, Batch 22, Test Loss: 0.7537452578544617\n",
      "Epoch 7, Batch 23, Test Loss: 0.30412137508392334\n",
      "Epoch 7, Batch 24, Test Loss: 0.5824288129806519\n",
      "Epoch 7, Batch 25, Test Loss: 0.5284594297409058\n",
      "Epoch 7, Batch 26, Test Loss: 0.5542875528335571\n",
      "Epoch 7, Batch 27, Test Loss: 0.731772780418396\n",
      "Epoch 7, Batch 28, Test Loss: 0.7405478954315186\n",
      "Epoch 7, Batch 29, Test Loss: 0.2929782569408417\n",
      "Epoch 7, Batch 30, Test Loss: 0.5270116329193115\n",
      "Epoch 7, Batch 31, Test Loss: 0.413318932056427\n",
      "Epoch 7, Batch 32, Test Loss: 0.6327745914459229\n",
      "Epoch 7, Batch 33, Test Loss: 0.6668839454650879\n",
      "Epoch 7, Batch 34, Test Loss: 0.6512919664382935\n",
      "Epoch 7, Batch 35, Test Loss: 0.571802020072937\n",
      "Epoch 7, Batch 36, Test Loss: 0.5309979915618896\n",
      "Epoch 7, Batch 37, Test Loss: 0.545127272605896\n",
      "Epoch 7, Batch 38, Test Loss: 0.6881915330886841\n",
      "Epoch 7, Batch 39, Test Loss: 0.4618108868598938\n",
      "Epoch 7, Batch 40, Test Loss: 0.6414790749549866\n",
      "Epoch 7, Batch 41, Test Loss: 0.5001435279846191\n",
      "Epoch 7, Batch 42, Test Loss: 0.5118357539176941\n",
      "Epoch 7, Batch 43, Test Loss: 0.652397632598877\n",
      "Epoch 7, Batch 44, Test Loss: 0.511125385761261\n",
      "Epoch 7, Batch 45, Test Loss: 0.5599154233932495\n",
      "Epoch 7, Batch 46, Test Loss: 0.35185670852661133\n",
      "Epoch 7, Batch 47, Test Loss: 0.5474385023117065\n",
      "Epoch 7, Batch 48, Test Loss: 0.8263686895370483\n",
      "Epoch 7, Batch 49, Test Loss: 0.559310793876648\n",
      "Epoch 7, Batch 50, Test Loss: 0.5252862572669983\n",
      "Epoch 7, Batch 51, Test Loss: 0.6663771271705627\n",
      "Epoch 7, Batch 52, Test Loss: 0.5184766054153442\n",
      "Epoch 7, Batch 53, Test Loss: 0.5404819250106812\n",
      "Epoch 7, Batch 54, Test Loss: 0.5163240432739258\n",
      "Epoch 7, Batch 55, Test Loss: 0.7922494411468506\n",
      "Epoch 7, Batch 56, Test Loss: 0.752869725227356\n",
      "Epoch 7, Batch 57, Test Loss: 0.5868394374847412\n",
      "Epoch 7, Batch 58, Test Loss: 0.4654884338378906\n",
      "Epoch 7, Batch 59, Test Loss: 0.6213245987892151\n",
      "Epoch 7, Batch 60, Test Loss: 0.5362862348556519\n",
      "Epoch 7, Batch 61, Test Loss: 0.5533108711242676\n",
      "Epoch 7, Batch 62, Test Loss: 0.5932912230491638\n",
      "Epoch 7, Batch 63, Test Loss: 0.43679532408714294\n",
      "Epoch 7, Batch 64, Test Loss: 0.4453110098838806\n",
      "Epoch 7, Batch 65, Test Loss: 0.6084771156311035\n",
      "Epoch 7, Batch 66, Test Loss: 0.42072129249572754\n",
      "Epoch 7, Batch 67, Test Loss: 0.4068792462348938\n",
      "Epoch 7, Batch 68, Test Loss: 0.5150718688964844\n",
      "Epoch 7, Batch 69, Test Loss: 0.48986268043518066\n",
      "Epoch 7, Batch 70, Test Loss: 0.4964960515499115\n",
      "Epoch 7, Batch 71, Test Loss: 0.5782233476638794\n",
      "Epoch 7, Batch 72, Test Loss: 0.6582704782485962\n",
      "Epoch 7, Batch 73, Test Loss: 0.6213561296463013\n",
      "Epoch 7, Batch 74, Test Loss: 0.7529810667037964\n",
      "Epoch 7, Batch 75, Test Loss: 0.40149614214897156\n",
      "Epoch 7, Batch 76, Test Loss: 0.5017752647399902\n",
      "Epoch 7, Batch 77, Test Loss: 0.5739094614982605\n",
      "Epoch 7, Batch 78, Test Loss: 0.6269564628601074\n",
      "Epoch 7, Batch 79, Test Loss: 0.5270500183105469\n",
      "Epoch 7, Batch 80, Test Loss: 0.7302025556564331\n",
      "Epoch 7, Batch 81, Test Loss: 0.5304072499275208\n",
      "Epoch 7, Batch 82, Test Loss: 0.7582517266273499\n",
      "Epoch 7, Batch 83, Test Loss: 0.6377054452896118\n",
      "Epoch 7, Batch 84, Test Loss: 0.6122391223907471\n",
      "Epoch 7, Batch 85, Test Loss: 0.6319660544395447\n",
      "Epoch 7, Batch 86, Test Loss: 0.6715317964553833\n",
      "Epoch 7, Batch 87, Test Loss: 0.7721389532089233\n",
      "Epoch 7, Batch 88, Test Loss: 0.6170815229415894\n",
      "Epoch 7, Batch 89, Test Loss: 0.6505888104438782\n",
      "Epoch 7, Batch 90, Test Loss: 0.501393735408783\n",
      "Epoch 7, Batch 91, Test Loss: 0.4504038095474243\n",
      "Epoch 7, Batch 92, Test Loss: 0.7025243639945984\n",
      "Epoch 7, Batch 93, Test Loss: 0.9420441389083862\n",
      "Epoch 7, Batch 94, Test Loss: 0.5371956825256348\n",
      "Epoch 7, Batch 95, Test Loss: 0.5657738447189331\n",
      "Epoch 7, Batch 96, Test Loss: 0.6165050268173218\n",
      "Epoch 7, Batch 97, Test Loss: 0.3718366026878357\n",
      "Epoch 7, Batch 98, Test Loss: 0.40502893924713135\n",
      "Epoch 7, Batch 99, Test Loss: 0.6864580512046814\n",
      "Epoch 7, Batch 100, Test Loss: 0.3911013603210449\n",
      "Epoch 7, Batch 101, Test Loss: 0.5444928407669067\n",
      "Epoch 7, Batch 102, Test Loss: 0.6146734952926636\n",
      "Epoch 7, Batch 103, Test Loss: 0.6743724346160889\n",
      "Epoch 7, Batch 104, Test Loss: 0.5328028202056885\n",
      "Epoch 7, Batch 105, Test Loss: 0.7542470693588257\n",
      "Epoch 7, Batch 106, Test Loss: 0.6062008142471313\n",
      "Epoch 7, Batch 107, Test Loss: 0.4931010603904724\n",
      "Epoch 7, Batch 108, Test Loss: 0.7753439545631409\n",
      "Epoch 7, Batch 109, Test Loss: 0.7083412408828735\n",
      "Epoch 7, Batch 110, Test Loss: 0.5680647492408752\n",
      "Epoch 7, Batch 111, Test Loss: 0.6385869383811951\n",
      "Epoch 7, Batch 112, Test Loss: 0.43321236968040466\n",
      "Epoch 7, Batch 113, Test Loss: 0.5945752263069153\n",
      "Epoch 7, Batch 114, Test Loss: 0.5587024688720703\n",
      "Epoch 7, Batch 115, Test Loss: 0.6408339738845825\n",
      "Epoch 7, Batch 116, Test Loss: 0.6524189114570618\n",
      "Epoch 7, Batch 117, Test Loss: 0.3849208652973175\n",
      "Epoch 7, Batch 118, Test Loss: 0.5982884168624878\n",
      "Epoch 7, Batch 119, Test Loss: 0.5470944046974182\n",
      "Epoch 7, Batch 120, Test Loss: 0.5662126541137695\n",
      "Epoch 7, Batch 121, Test Loss: 0.6027388572692871\n",
      "Epoch 7, Batch 122, Test Loss: 0.5413537621498108\n",
      "Epoch 7, Batch 123, Test Loss: 0.5713736414909363\n",
      "Epoch 7, Batch 124, Test Loss: 0.3911705017089844\n",
      "Epoch 7, Batch 125, Test Loss: 0.3466379642486572\n",
      "Epoch 7, Batch 126, Test Loss: 0.7142505049705505\n",
      "Epoch 7, Batch 127, Test Loss: 0.8241401314735413\n",
      "Epoch 7, Batch 128, Test Loss: 0.44951605796813965\n",
      "Epoch 7, Batch 129, Test Loss: 0.5076316595077515\n",
      "Epoch 7, Batch 130, Test Loss: 0.6109036803245544\n",
      "Epoch 7, Batch 131, Test Loss: 0.501884937286377\n",
      "Epoch 7, Batch 132, Test Loss: 0.511478841304779\n",
      "Epoch 7, Batch 133, Test Loss: 0.5255133509635925\n",
      "Epoch 7, Batch 134, Test Loss: 0.453837513923645\n",
      "Epoch 7, Batch 135, Test Loss: 0.45989036560058594\n",
      "Epoch 7, Batch 136, Test Loss: 0.7396640181541443\n",
      "Epoch 7, Batch 137, Test Loss: 0.5278898477554321\n",
      "Epoch 7, Batch 138, Test Loss: 0.7061047554016113\n",
      "Epoch 7, Batch 139, Test Loss: 0.6722753047943115\n",
      "Epoch 7, Batch 140, Test Loss: 0.47846460342407227\n",
      "Epoch 7, Batch 141, Test Loss: 0.7021288275718689\n",
      "Epoch 7, Batch 142, Test Loss: 0.5831320285797119\n",
      "Epoch 7, Batch 143, Test Loss: 0.32760384678840637\n",
      "Epoch 7, Batch 144, Test Loss: 0.460185706615448\n",
      "Epoch 7, Batch 145, Test Loss: 0.4844360947608948\n",
      "Epoch 7, Batch 146, Test Loss: 0.5024657249450684\n",
      "Epoch 7, Batch 147, Test Loss: 0.5580030679702759\n",
      "Epoch 7, Batch 148, Test Loss: 0.4485785663127899\n",
      "Epoch 7, Batch 149, Test Loss: 0.5023954510688782\n",
      "Epoch 7, Batch 150, Test Loss: 0.6817450523376465\n",
      "Epoch 7, Batch 151, Test Loss: 0.7395809292793274\n",
      "Epoch 7, Batch 152, Test Loss: 0.38617196679115295\n",
      "Epoch 7, Batch 153, Test Loss: 0.38472044467926025\n",
      "Epoch 7, Batch 154, Test Loss: 0.4810580015182495\n",
      "Epoch 7, Batch 155, Test Loss: 0.48712971806526184\n",
      "Epoch 7, Batch 156, Test Loss: 0.5186401605606079\n",
      "Epoch 7, Batch 157, Test Loss: 0.5235268473625183\n",
      "Epoch 7, Batch 158, Test Loss: 0.5691460967063904\n",
      "Epoch 7, Batch 159, Test Loss: 0.41811302304267883\n",
      "Epoch 7, Batch 160, Test Loss: 0.5010151267051697\n",
      "Epoch 7, Batch 161, Test Loss: 0.7935285568237305\n",
      "Epoch 7, Batch 162, Test Loss: 0.5621005892753601\n",
      "Epoch 7, Batch 163, Test Loss: 0.5158053636550903\n",
      "Epoch 7, Batch 164, Test Loss: 0.5871685147285461\n",
      "Epoch 7, Batch 165, Test Loss: 0.9120877981185913\n",
      "Epoch 7, Batch 166, Test Loss: 0.5340978503227234\n",
      "Epoch 7, Batch 167, Test Loss: 0.4663495123386383\n",
      "Epoch 7, Batch 168, Test Loss: 0.48834604024887085\n",
      "Epoch 7, Batch 169, Test Loss: 0.5332686305046082\n",
      "Epoch 7, Batch 170, Test Loss: 0.5276415944099426\n",
      "Epoch 7, Batch 171, Test Loss: 0.7301310896873474\n",
      "Epoch 7, Batch 172, Test Loss: 0.4386870861053467\n",
      "Epoch 7, Batch 173, Test Loss: 0.5137013792991638\n",
      "Epoch 7, Batch 174, Test Loss: 0.5874311923980713\n",
      "Epoch 7, Batch 175, Test Loss: 0.3061657249927521\n",
      "Epoch 7, Batch 176, Test Loss: 0.5692474842071533\n",
      "Epoch 7, Batch 177, Test Loss: 0.5787699818611145\n",
      "Epoch 7, Batch 178, Test Loss: 0.36794406175613403\n",
      "Epoch 7, Batch 179, Test Loss: 0.7002381086349487\n",
      "Epoch 7, Batch 180, Test Loss: 0.6928062438964844\n",
      "Epoch 7, Batch 181, Test Loss: 0.5129354596138\n",
      "Epoch 7, Batch 182, Test Loss: 0.5270900726318359\n",
      "Epoch 7, Batch 183, Test Loss: 0.45086750388145447\n",
      "Epoch 7, Batch 184, Test Loss: 0.47985905408859253\n",
      "Epoch 7, Batch 185, Test Loss: 0.4925061762332916\n",
      "Epoch 7, Batch 186, Test Loss: 0.6402832865715027\n",
      "Epoch 7, Batch 187, Test Loss: 0.6058297157287598\n",
      "Epoch 7, Batch 188, Test Loss: 0.56028151512146\n",
      "Epoch 7, Batch 189, Test Loss: 0.5824251174926758\n",
      "Epoch 7, Batch 190, Test Loss: 0.7287335991859436\n",
      "Epoch 7, Batch 191, Test Loss: 0.5830865502357483\n",
      "Epoch 7, Batch 192, Test Loss: 0.579123854637146\n",
      "Epoch 7, Batch 193, Test Loss: 0.32092076539993286\n",
      "Epoch 7, Batch 194, Test Loss: 0.49802154302597046\n",
      "Epoch 7, Batch 195, Test Loss: 0.5608882308006287\n",
      "Epoch 7, Batch 196, Test Loss: 0.5578765273094177\n",
      "Epoch 7, Batch 197, Test Loss: 0.4194919466972351\n",
      "Epoch 7, Batch 198, Test Loss: 0.6236128807067871\n",
      "Epoch 7, Batch 199, Test Loss: 0.7000133991241455\n",
      "Epoch 7, Batch 200, Test Loss: 0.5996732711791992\n",
      "Epoch 7, Batch 201, Test Loss: 0.6569657921791077\n",
      "Epoch 7, Batch 202, Test Loss: 0.9269120097160339\n",
      "Epoch 7, Batch 203, Test Loss: 0.577114462852478\n",
      "Epoch 7, Batch 204, Test Loss: 0.3756311535835266\n",
      "Epoch 7, Batch 205, Test Loss: 0.5135825872421265\n",
      "Epoch 7, Batch 206, Test Loss: 0.5943096876144409\n",
      "Epoch 7, Batch 207, Test Loss: 0.6661841869354248\n",
      "Epoch 7, Batch 208, Test Loss: 0.579090416431427\n",
      "Epoch 7, Batch 209, Test Loss: 0.5472773313522339\n",
      "Epoch 7, Batch 210, Test Loss: 0.5222245454788208\n",
      "Epoch 7, Batch 211, Test Loss: 0.6504123210906982\n",
      "Epoch 7, Batch 212, Test Loss: 0.3829469084739685\n",
      "Epoch 7, Batch 213, Test Loss: 0.4689745008945465\n",
      "Epoch 7, Batch 214, Test Loss: 0.44171303510665894\n",
      "Epoch 7, Batch 215, Test Loss: 0.6334400177001953\n",
      "Epoch 7, Batch 216, Test Loss: 0.5454779863357544\n",
      "Epoch 7, Batch 217, Test Loss: 0.45347923040390015\n",
      "Epoch 7, Batch 218, Test Loss: 0.5523933172225952\n",
      "Epoch 7, Batch 219, Test Loss: 0.38777029514312744\n",
      "Epoch 7, Batch 220, Test Loss: 0.7250214219093323\n",
      "Epoch 7, Batch 221, Test Loss: 0.4587758779525757\n",
      "Epoch 7, Batch 222, Test Loss: 0.5410553812980652\n",
      "Epoch 7, Batch 223, Test Loss: 0.44210588932037354\n",
      "Epoch 7, Batch 224, Test Loss: 0.5824883580207825\n",
      "Epoch 7, Batch 225, Test Loss: 0.5990685820579529\n",
      "Epoch 7, Batch 226, Test Loss: 0.624407172203064\n",
      "Epoch 7, Batch 227, Test Loss: 0.4848640561103821\n",
      "Epoch 7, Batch 228, Test Loss: 0.6859790682792664\n",
      "Epoch 7, Batch 229, Test Loss: 0.5102054476737976\n",
      "Epoch 7, Batch 230, Test Loss: 0.49507027864456177\n",
      "Epoch 7, Batch 231, Test Loss: 0.6729029417037964\n",
      "Epoch 7, Batch 232, Test Loss: 0.5888475775718689\n",
      "Epoch 7, Batch 233, Test Loss: 0.5070090293884277\n",
      "Epoch 7, Batch 234, Test Loss: 0.5660175085067749\n",
      "Epoch 7, Batch 235, Test Loss: 0.5808765888214111\n",
      "Epoch 7, Batch 236, Test Loss: 0.6698322892189026\n",
      "Epoch 7, Batch 237, Test Loss: 0.43588265776634216\n",
      "Epoch 7, Batch 238, Test Loss: 0.6290541887283325\n",
      "Epoch 7, Batch 239, Test Loss: 0.5706654787063599\n",
      "Epoch 7, Batch 240, Test Loss: 0.4596010744571686\n",
      "Epoch 7, Batch 241, Test Loss: 0.583087146282196\n",
      "Epoch 7, Batch 242, Test Loss: 0.5155764818191528\n",
      "Epoch 7, Batch 243, Test Loss: 0.44458067417144775\n",
      "Epoch 7, Batch 244, Test Loss: 0.9277133941650391\n",
      "Epoch 7, Batch 245, Test Loss: 0.7101659774780273\n",
      "Epoch 7, Batch 246, Test Loss: 0.49229273200035095\n",
      "Epoch 7, Batch 247, Test Loss: 0.5601769685745239\n",
      "Epoch 7, Batch 248, Test Loss: 0.4274175465106964\n",
      "Epoch 7, Batch 249, Test Loss: 0.6710294485092163\n",
      "Epoch 7, Batch 250, Test Loss: 0.42506086826324463\n",
      "Epoch 7, Batch 251, Test Loss: 0.6250945329666138\n",
      "Epoch 7, Batch 252, Test Loss: 0.5597696304321289\n",
      "Epoch 7, Batch 253, Test Loss: 0.7158281207084656\n",
      "Epoch 7, Batch 254, Test Loss: 0.5165855884552002\n",
      "Epoch 7, Batch 255, Test Loss: 0.6287142634391785\n",
      "Epoch 7, Batch 256, Test Loss: 0.42471757531166077\n",
      "Epoch 7, Batch 257, Test Loss: 0.47155532240867615\n",
      "Epoch 7, Batch 258, Test Loss: 0.4577847719192505\n",
      "Epoch 7, Batch 259, Test Loss: 0.6432085037231445\n",
      "Epoch 7, Batch 260, Test Loss: 0.5109599232673645\n",
      "Epoch 7, Batch 261, Test Loss: 0.5254597067832947\n",
      "Epoch 7, Batch 262, Test Loss: 0.4376833438873291\n",
      "Epoch 7, Batch 263, Test Loss: 0.6317657232284546\n",
      "Epoch 7, Batch 264, Test Loss: 0.6626318097114563\n",
      "Epoch 7, Batch 265, Test Loss: 0.5557087659835815\n",
      "Epoch 7, Batch 266, Test Loss: 0.5231060981750488\n",
      "Epoch 7, Batch 267, Test Loss: 0.6599189639091492\n",
      "Epoch 7, Batch 268, Test Loss: 0.4615595042705536\n",
      "Epoch 7, Batch 269, Test Loss: 0.4667608141899109\n",
      "Epoch 7, Batch 270, Test Loss: 0.4553516209125519\n",
      "Epoch 7, Batch 271, Test Loss: 0.840391218662262\n",
      "Epoch 7, Batch 272, Test Loss: 0.5498691201210022\n",
      "Epoch 7, Batch 273, Test Loss: 0.5118529200553894\n",
      "Epoch 7, Batch 274, Test Loss: 0.6123942136764526\n",
      "Epoch 7, Batch 275, Test Loss: 0.5729376673698425\n",
      "Epoch 7, Batch 276, Test Loss: 0.5373795032501221\n",
      "Epoch 7, Batch 277, Test Loss: 0.7274110317230225\n",
      "Epoch 7, Batch 278, Test Loss: 0.39879193902015686\n",
      "Epoch 7, Batch 279, Test Loss: 0.3377678096294403\n",
      "Epoch 7, Batch 280, Test Loss: 0.4436255097389221\n",
      "Epoch 7, Batch 281, Test Loss: 0.46442899107933044\n",
      "Epoch 7, Batch 282, Test Loss: 0.28153908252716064\n",
      "Epoch 7, Batch 283, Test Loss: 0.637588620185852\n",
      "Epoch 7, Batch 284, Test Loss: 0.6613773703575134\n",
      "Epoch 7, Batch 285, Test Loss: 0.441516250371933\n",
      "Epoch 7, Batch 286, Test Loss: 0.57505863904953\n",
      "Epoch 7, Batch 287, Test Loss: 0.42536142468452454\n",
      "Epoch 7, Batch 288, Test Loss: 0.6466434597969055\n",
      "Epoch 7, Batch 289, Test Loss: 0.5027923583984375\n",
      "Epoch 7, Batch 290, Test Loss: 0.7532367706298828\n",
      "Epoch 7, Batch 291, Test Loss: 0.37392657995224\n",
      "Epoch 7, Batch 292, Test Loss: 0.6797980666160583\n",
      "Epoch 7, Batch 293, Test Loss: 0.5507838726043701\n",
      "Epoch 7, Batch 294, Test Loss: 0.6291247606277466\n",
      "Epoch 7, Batch 295, Test Loss: 0.43731722235679626\n",
      "Epoch 7, Batch 296, Test Loss: 0.7296146154403687\n",
      "Epoch 7, Batch 297, Test Loss: 0.54068523645401\n",
      "Epoch 7, Batch 298, Test Loss: 0.5877426266670227\n",
      "Epoch 7, Batch 299, Test Loss: 0.4599153995513916\n",
      "Epoch 7, Batch 300, Test Loss: 0.42965853214263916\n",
      "Epoch 7, Batch 301, Test Loss: 0.530839741230011\n",
      "Epoch 7, Batch 302, Test Loss: 0.3984510898590088\n",
      "Epoch 7, Batch 303, Test Loss: 0.602281391620636\n",
      "Epoch 7, Batch 304, Test Loss: 0.5214307904243469\n",
      "Epoch 7, Batch 305, Test Loss: 0.4287954270839691\n",
      "Epoch 7, Batch 306, Test Loss: 0.5963358879089355\n",
      "Epoch 7, Batch 307, Test Loss: 0.5402926802635193\n",
      "Epoch 7, Batch 308, Test Loss: 0.4364737570285797\n",
      "Epoch 7, Batch 309, Test Loss: 0.5677788257598877\n",
      "Epoch 7, Batch 310, Test Loss: 0.4927334785461426\n",
      "Epoch 7, Batch 311, Test Loss: 0.84404456615448\n",
      "Epoch 7, Batch 312, Test Loss: 0.6092613339424133\n",
      "Epoch 7, Batch 313, Test Loss: 0.6823006272315979\n",
      "Epoch 7, Batch 314, Test Loss: 0.719184935092926\n",
      "Epoch 7, Batch 315, Test Loss: 0.5269249081611633\n",
      "Epoch 7, Batch 316, Test Loss: 0.5516121983528137\n",
      "Epoch 7, Batch 317, Test Loss: 0.6174129843711853\n",
      "Epoch 7, Batch 318, Test Loss: 0.5874830484390259\n",
      "Epoch 7, Batch 319, Test Loss: 0.46741238236427307\n",
      "Epoch 7, Batch 320, Test Loss: 0.5464299917221069\n",
      "Epoch 7, Batch 321, Test Loss: 0.5609930157661438\n",
      "Epoch 7, Batch 322, Test Loss: 0.5659555196762085\n",
      "Epoch 7, Batch 323, Test Loss: 0.5532882213592529\n",
      "Epoch 7, Batch 324, Test Loss: 0.6553621888160706\n",
      "Epoch 7, Batch 325, Test Loss: 0.820706307888031\n",
      "Epoch 7, Batch 326, Test Loss: 0.5408065319061279\n",
      "Epoch 7, Batch 327, Test Loss: 0.45377156138420105\n",
      "Epoch 7, Batch 328, Test Loss: 0.5282930135726929\n",
      "Epoch 7, Batch 329, Test Loss: 0.7878591418266296\n",
      "Epoch 7, Batch 330, Test Loss: 0.5951337218284607\n",
      "Epoch 7, Batch 331, Test Loss: 0.715201735496521\n",
      "Epoch 7, Batch 332, Test Loss: 0.4430887699127197\n",
      "Epoch 7, Batch 333, Test Loss: 0.40164557099342346\n",
      "Epoch 7, Batch 334, Test Loss: 0.7757866382598877\n",
      "Epoch 7, Batch 335, Test Loss: 0.6031021475791931\n",
      "Epoch 7, Batch 336, Test Loss: 0.6038930416107178\n",
      "Epoch 7, Batch 337, Test Loss: 0.5828801393508911\n",
      "Epoch 7, Batch 338, Test Loss: 0.4551193416118622\n",
      "Epoch 7, Batch 339, Test Loss: 0.5937051177024841\n",
      "Epoch 7, Batch 340, Test Loss: 0.5669163465499878\n",
      "Epoch 7, Batch 341, Test Loss: 0.5997040867805481\n",
      "Epoch 7, Batch 342, Test Loss: 0.5528457164764404\n",
      "Epoch 7, Batch 343, Test Loss: 0.691205620765686\n",
      "Epoch 7, Batch 344, Test Loss: 0.4936392307281494\n",
      "Epoch 7, Batch 345, Test Loss: 0.4112660884857178\n",
      "Epoch 7, Batch 346, Test Loss: 0.5481579303741455\n",
      "Epoch 7, Batch 347, Test Loss: 0.5286251902580261\n",
      "Epoch 7, Batch 348, Test Loss: 0.4882216453552246\n",
      "Epoch 7, Batch 349, Test Loss: 0.6977748274803162\n",
      "Epoch 7, Batch 350, Test Loss: 0.4087105989456177\n",
      "Epoch 7, Batch 351, Test Loss: 0.33232608437538147\n",
      "Epoch 7, Batch 352, Test Loss: 0.5345901846885681\n",
      "Epoch 7, Batch 353, Test Loss: 0.5843921899795532\n",
      "Epoch 7, Batch 354, Test Loss: 0.42487409710884094\n",
      "Epoch 7, Batch 355, Test Loss: 0.6148251891136169\n",
      "Epoch 7, Batch 356, Test Loss: 0.4689658284187317\n",
      "Epoch 7, Batch 357, Test Loss: 0.7408269643783569\n",
      "Epoch 7, Batch 358, Test Loss: 0.4713376462459564\n",
      "Epoch 7, Batch 359, Test Loss: 0.6545848846435547\n",
      "Epoch 7, Batch 360, Test Loss: 0.5660273432731628\n",
      "Epoch 7, Batch 361, Test Loss: 0.465071439743042\n",
      "Epoch 7, Batch 362, Test Loss: 0.5360803604125977\n",
      "Epoch 7, Batch 363, Test Loss: 0.5305195450782776\n",
      "Epoch 7, Batch 364, Test Loss: 0.5760253667831421\n",
      "Epoch 7, Batch 365, Test Loss: 0.5136584639549255\n",
      "Epoch 7, Batch 366, Test Loss: 0.5622937083244324\n",
      "Epoch 7, Batch 367, Test Loss: 0.7037093639373779\n",
      "Epoch 7, Batch 368, Test Loss: 0.6721192598342896\n",
      "Epoch 7, Batch 369, Test Loss: 0.8092734217643738\n",
      "Epoch 7, Batch 370, Test Loss: 0.7528794407844543\n",
      "Epoch 7, Batch 371, Test Loss: 0.552638828754425\n",
      "Epoch 7, Batch 372, Test Loss: 0.49656885862350464\n",
      "Epoch 7, Batch 373, Test Loss: 0.7527830600738525\n",
      "Epoch 7, Batch 374, Test Loss: 0.5411556959152222\n",
      "Epoch 7, Batch 375, Test Loss: 0.3981018364429474\n",
      "Epoch 7, Batch 376, Test Loss: 0.650571882724762\n",
      "Epoch 7, Batch 377, Test Loss: 0.5894635915756226\n",
      "Epoch 7, Batch 378, Test Loss: 0.7158064246177673\n",
      "Epoch 7, Batch 379, Test Loss: 0.6910972595214844\n",
      "Epoch 7, Batch 380, Test Loss: 0.5718048214912415\n",
      "Epoch 7, Batch 381, Test Loss: 0.5130727291107178\n",
      "Epoch 7, Batch 382, Test Loss: 0.46594953536987305\n",
      "Epoch 7, Batch 383, Test Loss: 0.7847364544868469\n",
      "Epoch 7, Batch 384, Test Loss: 0.4978550672531128\n",
      "Epoch 7, Batch 385, Test Loss: 0.44039106369018555\n",
      "Epoch 7, Batch 386, Test Loss: 0.5324693918228149\n",
      "Epoch 7, Batch 387, Test Loss: 0.6336574554443359\n",
      "Epoch 7, Batch 388, Test Loss: 0.5053156614303589\n",
      "Epoch 7, Batch 389, Test Loss: 0.5268877148628235\n",
      "Epoch 7, Batch 390, Test Loss: 0.6288833022117615\n",
      "Epoch 7, Batch 391, Test Loss: 0.5734567642211914\n",
      "Epoch 7, Batch 392, Test Loss: 0.5262666344642639\n",
      "Epoch 7, Batch 393, Test Loss: 0.47880619764328003\n",
      "Epoch 7, Batch 394, Test Loss: 0.6634480357170105\n",
      "Epoch 7, Batch 395, Test Loss: 0.654433012008667\n",
      "Epoch 7, Batch 396, Test Loss: 0.6527824997901917\n",
      "Epoch 7, Batch 397, Test Loss: 0.7085497975349426\n",
      "Epoch 7, Batch 398, Test Loss: 0.47428518533706665\n",
      "Epoch 7, Batch 399, Test Loss: 0.6255077719688416\n",
      "Epoch 7, Batch 400, Test Loss: 0.5088550448417664\n",
      "Epoch 7, Batch 401, Test Loss: 0.5097118616104126\n",
      "Epoch 7, Batch 402, Test Loss: 0.3994852304458618\n",
      "Epoch 7, Batch 403, Test Loss: 0.5676936507225037\n",
      "Epoch 7, Batch 404, Test Loss: 0.7682031989097595\n",
      "Epoch 7, Batch 405, Test Loss: 0.5489423274993896\n",
      "Epoch 7, Batch 406, Test Loss: 0.5296644568443298\n",
      "Epoch 7, Batch 407, Test Loss: 0.8096777200698853\n",
      "Epoch 7, Batch 408, Test Loss: 0.49714672565460205\n",
      "Epoch 7, Batch 409, Test Loss: 0.42481696605682373\n",
      "Epoch 7, Batch 410, Test Loss: 0.898964524269104\n",
      "Epoch 7, Batch 411, Test Loss: 0.47769713401794434\n",
      "Epoch 7, Batch 412, Test Loss: 0.3766968548297882\n",
      "Epoch 7, Batch 413, Test Loss: 0.5001703500747681\n",
      "Epoch 7, Batch 414, Test Loss: 0.48077356815338135\n",
      "Epoch 7, Batch 415, Test Loss: 0.6729380488395691\n",
      "Epoch 7, Batch 416, Test Loss: 0.5242904424667358\n",
      "Epoch 7, Batch 417, Test Loss: 0.5209535360336304\n",
      "Epoch 7, Batch 418, Test Loss: 0.5991054773330688\n",
      "Epoch 7, Batch 419, Test Loss: 0.62105792760849\n",
      "Epoch 7, Batch 420, Test Loss: 0.601889967918396\n",
      "Epoch 7, Batch 421, Test Loss: 0.5284376740455627\n",
      "Epoch 7, Batch 422, Test Loss: 0.45183324813842773\n",
      "Epoch 7, Batch 423, Test Loss: 0.5479113459587097\n",
      "Epoch 7, Batch 424, Test Loss: 0.41126883029937744\n",
      "Epoch 7, Batch 425, Test Loss: 0.6902564167976379\n",
      "Epoch 7, Batch 426, Test Loss: 0.5231823325157166\n",
      "Epoch 7, Batch 427, Test Loss: 0.7224369645118713\n",
      "Epoch 7, Batch 428, Test Loss: 0.49378734827041626\n",
      "Epoch 7, Batch 429, Test Loss: 0.4951423108577728\n",
      "Epoch 7, Batch 430, Test Loss: 0.5922785997390747\n",
      "Epoch 7, Batch 431, Test Loss: 0.7555709481239319\n",
      "Epoch 7, Batch 432, Test Loss: 0.507908284664154\n",
      "Epoch 7, Batch 433, Test Loss: 0.7080739736557007\n",
      "Epoch 7, Batch 434, Test Loss: 0.9237024784088135\n",
      "Epoch 7, Batch 435, Test Loss: 0.420028418302536\n",
      "Epoch 7, Batch 436, Test Loss: 0.5097790360450745\n",
      "Epoch 7, Batch 437, Test Loss: 0.6639493703842163\n",
      "Epoch 7, Batch 438, Test Loss: 0.4524475038051605\n",
      "Epoch 7, Batch 439, Test Loss: 0.5751827955245972\n",
      "Epoch 7, Batch 440, Test Loss: 0.4673430621623993\n",
      "Epoch 7, Batch 441, Test Loss: 0.5280458331108093\n",
      "Epoch 7, Batch 442, Test Loss: 0.6796655058860779\n",
      "Epoch 7, Batch 443, Test Loss: 0.8205700516700745\n",
      "Epoch 7, Batch 444, Test Loss: 0.5019709467887878\n",
      "Epoch 7, Batch 445, Test Loss: 0.5327883958816528\n",
      "Epoch 7, Batch 446, Test Loss: 0.7819871306419373\n",
      "Epoch 7, Batch 447, Test Loss: 0.4419546127319336\n",
      "Epoch 7, Batch 448, Test Loss: 0.4809647500514984\n",
      "Epoch 7, Batch 449, Test Loss: 0.6081733703613281\n",
      "Epoch 7, Batch 450, Test Loss: 0.5053511261940002\n",
      "Epoch 7, Batch 451, Test Loss: 0.7105597257614136\n",
      "Epoch 7, Batch 452, Test Loss: 0.5829775929450989\n",
      "Epoch 7, Batch 453, Test Loss: 0.6825271844863892\n",
      "Epoch 7, Batch 454, Test Loss: 0.5569697618484497\n",
      "Epoch 7, Batch 455, Test Loss: 0.47331398725509644\n",
      "Epoch 7, Batch 456, Test Loss: 0.6975720524787903\n",
      "Epoch 7, Batch 457, Test Loss: 0.7860774397850037\n",
      "Epoch 7, Batch 458, Test Loss: 0.5973657369613647\n",
      "Epoch 7, Batch 459, Test Loss: 0.35770174860954285\n",
      "Epoch 7, Batch 460, Test Loss: 0.7200093269348145\n",
      "Epoch 7, Batch 461, Test Loss: 0.6985613107681274\n",
      "Epoch 7, Batch 462, Test Loss: 0.7402076125144958\n",
      "Epoch 7, Batch 463, Test Loss: 0.568308413028717\n",
      "Epoch 7, Batch 464, Test Loss: 0.5783571004867554\n",
      "Epoch 7, Batch 465, Test Loss: 0.6034717559814453\n",
      "Epoch 7, Batch 466, Test Loss: 0.5530577898025513\n",
      "Epoch 7, Batch 467, Test Loss: 0.48812589049339294\n",
      "Epoch 7, Batch 468, Test Loss: 0.5778490304946899\n",
      "Epoch 7, Batch 469, Test Loss: 0.43782368302345276\n",
      "Epoch 7, Batch 470, Test Loss: 0.42692631483078003\n",
      "Epoch 7, Batch 471, Test Loss: 0.5395491719245911\n",
      "Epoch 7, Batch 472, Test Loss: 0.43229007720947266\n",
      "Epoch 7, Batch 473, Test Loss: 0.5024868845939636\n",
      "Epoch 7, Batch 474, Test Loss: 0.6752185225486755\n",
      "Epoch 7, Batch 475, Test Loss: 0.6783408522605896\n",
      "Epoch 7, Batch 476, Test Loss: 0.5343447923660278\n",
      "Epoch 7, Batch 477, Test Loss: 0.7261035442352295\n",
      "Epoch 7, Batch 478, Test Loss: 0.589637279510498\n",
      "Epoch 7, Batch 479, Test Loss: 0.4966767728328705\n",
      "Epoch 7, Batch 480, Test Loss: 0.5824382901191711\n",
      "Epoch 7, Batch 481, Test Loss: 0.4872090518474579\n",
      "Epoch 7, Batch 482, Test Loss: 0.5722199082374573\n",
      "Epoch 7, Batch 483, Test Loss: 0.3700431287288666\n",
      "Epoch 7, Batch 484, Test Loss: 0.6513480544090271\n",
      "Epoch 7, Batch 485, Test Loss: 0.5607132911682129\n",
      "Epoch 7, Batch 486, Test Loss: 0.42378923296928406\n",
      "Epoch 7, Batch 487, Test Loss: 0.651555061340332\n",
      "Epoch 7, Batch 488, Test Loss: 0.4716240167617798\n",
      "Epoch 7, Batch 489, Test Loss: 0.5322650671005249\n",
      "Epoch 7, Batch 490, Test Loss: 0.4095875322818756\n",
      "Epoch 7, Batch 491, Test Loss: 0.6911607980728149\n",
      "Epoch 7, Batch 492, Test Loss: 0.8611587882041931\n",
      "Epoch 7, Batch 493, Test Loss: 0.6461980938911438\n",
      "Epoch 7, Batch 494, Test Loss: 0.5130643844604492\n",
      "Epoch 7, Batch 495, Test Loss: 0.5884037017822266\n",
      "Epoch 7, Batch 496, Test Loss: 0.5269281268119812\n",
      "Epoch 7, Batch 497, Test Loss: 0.5740866661071777\n",
      "Epoch 7, Batch 498, Test Loss: 0.5236141085624695\n",
      "Epoch 7, Batch 499, Test Loss: 0.6931411027908325\n",
      "Epoch 7, Batch 500, Test Loss: 0.5435231924057007\n",
      "Epoch 7, Batch 501, Test Loss: 0.5029007792472839\n",
      "Epoch 7, Batch 502, Test Loss: 0.6224185824394226\n",
      "Epoch 7, Batch 503, Test Loss: 0.5964115262031555\n",
      "Epoch 7, Batch 504, Test Loss: 0.5797815918922424\n",
      "Epoch 7, Batch 505, Test Loss: 0.6007517576217651\n",
      "Epoch 7, Batch 506, Test Loss: 0.39781615138053894\n",
      "Epoch 7, Batch 507, Test Loss: 0.533126175403595\n",
      "Epoch 7, Batch 508, Test Loss: 0.6243869066238403\n",
      "Epoch 7, Batch 509, Test Loss: 0.44287073612213135\n",
      "Epoch 7, Batch 510, Test Loss: 0.5218503475189209\n",
      "Epoch 7, Batch 511, Test Loss: 0.5377546548843384\n",
      "Epoch 7, Batch 512, Test Loss: 0.6063363552093506\n",
      "Epoch 7, Batch 513, Test Loss: 0.42122209072113037\n",
      "Epoch 7, Batch 514, Test Loss: 0.6525208950042725\n",
      "Epoch 7, Batch 515, Test Loss: 0.443257600069046\n",
      "Epoch 7, Batch 516, Test Loss: 0.543962299823761\n",
      "Epoch 7, Batch 517, Test Loss: 0.3820335865020752\n",
      "Epoch 7, Batch 518, Test Loss: 0.5690955519676208\n",
      "Epoch 7, Batch 519, Test Loss: 0.8542044162750244\n",
      "Epoch 7, Batch 520, Test Loss: 0.5473113059997559\n",
      "Epoch 7, Batch 521, Test Loss: 0.6747279763221741\n",
      "Epoch 7, Batch 522, Test Loss: 0.628592312335968\n",
      "Epoch 7, Batch 523, Test Loss: 0.5976678729057312\n",
      "Epoch 7, Batch 524, Test Loss: 0.3918973207473755\n",
      "Epoch 7, Batch 525, Test Loss: 0.7048043012619019\n",
      "Epoch 7, Batch 526, Test Loss: 0.39374080300331116\n",
      "Epoch 7, Batch 527, Test Loss: 0.5151932239532471\n",
      "Epoch 7, Batch 528, Test Loss: 0.5756491422653198\n",
      "Epoch 7, Batch 529, Test Loss: 0.7617419362068176\n",
      "Epoch 7, Batch 530, Test Loss: 0.5539514422416687\n",
      "Epoch 7, Batch 531, Test Loss: 0.5438488721847534\n",
      "Epoch 7, Batch 532, Test Loss: 0.5203360319137573\n",
      "Epoch 7, Batch 533, Test Loss: 0.6394185423851013\n",
      "Epoch 7, Batch 534, Test Loss: 0.5034868121147156\n",
      "Epoch 7, Batch 535, Test Loss: 0.664492130279541\n",
      "Epoch 7, Batch 536, Test Loss: 0.6381159424781799\n",
      "Epoch 7, Batch 537, Test Loss: 0.5805217027664185\n",
      "Epoch 7, Batch 538, Test Loss: 0.629435122013092\n",
      "Epoch 7, Batch 539, Test Loss: 0.6823046207427979\n",
      "Epoch 7, Batch 540, Test Loss: 0.6368651986122131\n",
      "Epoch 7, Batch 541, Test Loss: 0.6332290172576904\n",
      "Epoch 7, Batch 542, Test Loss: 0.6263526678085327\n",
      "Epoch 7, Batch 543, Test Loss: 0.4996175169944763\n",
      "Epoch 7, Batch 544, Test Loss: 0.4399472773075104\n",
      "Epoch 7, Batch 545, Test Loss: 0.6736943125724792\n",
      "Epoch 7, Batch 546, Test Loss: 0.6941230893135071\n",
      "Epoch 7, Batch 547, Test Loss: 0.42651131749153137\n",
      "Epoch 7, Batch 548, Test Loss: 0.46693259477615356\n",
      "Epoch 7, Batch 549, Test Loss: 0.7001959085464478\n",
      "Epoch 7, Batch 550, Test Loss: 0.5383732914924622\n",
      "Epoch 7, Batch 551, Test Loss: 0.6585206985473633\n",
      "Epoch 7, Batch 552, Test Loss: 0.566613495349884\n",
      "Epoch 7, Batch 553, Test Loss: 0.511987030506134\n",
      "Epoch 7, Batch 554, Test Loss: 0.4965246021747589\n",
      "Epoch 7, Batch 555, Test Loss: 0.5697790384292603\n",
      "Epoch 7, Batch 556, Test Loss: 0.4808352589607239\n",
      "Epoch 7, Batch 557, Test Loss: 0.47651100158691406\n",
      "Epoch 7, Batch 558, Test Loss: 0.5655723810195923\n",
      "Epoch 7, Batch 559, Test Loss: 0.5706208348274231\n",
      "Epoch 7, Batch 560, Test Loss: 0.7373637557029724\n",
      "Epoch 7, Batch 561, Test Loss: 0.6266037821769714\n",
      "Epoch 7, Batch 562, Test Loss: 0.5689664483070374\n",
      "Epoch 7, Batch 563, Test Loss: 0.7131261229515076\n",
      "Epoch 7, Batch 564, Test Loss: 0.3575543165206909\n",
      "Epoch 7, Batch 565, Test Loss: 0.4422767758369446\n",
      "Epoch 7, Batch 566, Test Loss: 0.5307105183601379\n",
      "Epoch 7, Batch 567, Test Loss: 0.4930309057235718\n",
      "Epoch 7, Batch 568, Test Loss: 0.5929257869720459\n",
      "Epoch 7, Batch 569, Test Loss: 0.5172801613807678\n",
      "Epoch 7, Batch 570, Test Loss: 0.5828402042388916\n",
      "Epoch 7, Batch 571, Test Loss: 0.5816884636878967\n",
      "Epoch 7, Batch 572, Test Loss: 0.26283499598503113\n",
      "Epoch 7, Batch 573, Test Loss: 0.5932599306106567\n",
      "Epoch 7, Batch 574, Test Loss: 0.5414135456085205\n",
      "Epoch 7, Batch 575, Test Loss: 0.5144364237785339\n",
      "Epoch 7, Batch 576, Test Loss: 0.5326941013336182\n",
      "Epoch 7, Batch 577, Test Loss: 0.6339461803436279\n",
      "Epoch 7, Batch 578, Test Loss: 0.49754858016967773\n",
      "Epoch 7, Batch 579, Test Loss: 0.5991871953010559\n",
      "Epoch 7, Batch 580, Test Loss: 0.5420116782188416\n",
      "Epoch 7, Batch 581, Test Loss: 0.7562292814254761\n",
      "Epoch 7, Batch 582, Test Loss: 0.5722668766975403\n",
      "Epoch 7, Batch 583, Test Loss: 0.5944128632545471\n",
      "Epoch 7, Batch 584, Test Loss: 0.7746246457099915\n",
      "Epoch 7, Batch 585, Test Loss: 0.7345359921455383\n",
      "Epoch 7, Batch 586, Test Loss: 0.721721887588501\n",
      "Epoch 7, Batch 587, Test Loss: 0.6426636576652527\n",
      "Epoch 7, Batch 588, Test Loss: 0.4917971193790436\n",
      "Epoch 7, Batch 589, Test Loss: 0.8073412179946899\n",
      "Epoch 7, Batch 590, Test Loss: 0.843827486038208\n",
      "Epoch 7, Batch 591, Test Loss: 0.6362496018409729\n",
      "Epoch 7, Batch 592, Test Loss: 0.4253426790237427\n",
      "Epoch 7, Batch 593, Test Loss: 0.586135983467102\n",
      "Epoch 7, Batch 594, Test Loss: 0.5457504391670227\n",
      "Epoch 7, Batch 595, Test Loss: 0.694579005241394\n",
      "Epoch 7, Batch 596, Test Loss: 0.6194687485694885\n",
      "Epoch 7, Batch 597, Test Loss: 0.7127006649971008\n",
      "Epoch 7, Batch 598, Test Loss: 0.6142470836639404\n",
      "Epoch 7, Batch 599, Test Loss: 0.5676361918449402\n",
      "Epoch 7, Batch 600, Test Loss: 0.6376953125\n",
      "Epoch 7, Batch 601, Test Loss: 0.6908500790596008\n",
      "Epoch 7, Batch 602, Test Loss: 0.5430291891098022\n",
      "Epoch 7, Batch 603, Test Loss: 0.603225827217102\n",
      "Epoch 7, Batch 604, Test Loss: 0.4362972676753998\n",
      "Epoch 7, Batch 605, Test Loss: 0.5138546824455261\n",
      "Epoch 7, Batch 606, Test Loss: 0.6923380494117737\n",
      "Epoch 7, Batch 607, Test Loss: 0.5700711011886597\n",
      "Epoch 7, Batch 608, Test Loss: 0.6236186623573303\n",
      "Epoch 7, Batch 609, Test Loss: 0.5194860100746155\n",
      "Epoch 7, Batch 610, Test Loss: 0.36421993374824524\n",
      "Epoch 7, Batch 611, Test Loss: 0.4843897223472595\n",
      "Epoch 7, Batch 612, Test Loss: 0.4162106215953827\n",
      "Epoch 7, Batch 613, Test Loss: 0.30791449546813965\n",
      "Epoch 7, Batch 614, Test Loss: 0.4911211431026459\n",
      "Epoch 7, Batch 615, Test Loss: 0.667716383934021\n",
      "Epoch 7, Batch 616, Test Loss: 0.7176355123519897\n",
      "Epoch 7, Batch 617, Test Loss: 0.4160931706428528\n",
      "Epoch 7, Batch 618, Test Loss: 0.5356065034866333\n",
      "Epoch 7, Batch 619, Test Loss: 0.35460934042930603\n",
      "Epoch 7, Batch 620, Test Loss: 0.5595995187759399\n",
      "Epoch 7, Batch 621, Test Loss: 0.6643772125244141\n",
      "Epoch 7, Batch 622, Test Loss: 0.4864455759525299\n",
      "Epoch 7, Batch 623, Test Loss: 0.48670822381973267\n",
      "Epoch 7, Batch 624, Test Loss: 0.5646467804908752\n",
      "Epoch 7, Batch 625, Test Loss: 0.5909016132354736\n",
      "Epoch 7, Batch 626, Test Loss: 0.6228814721107483\n",
      "Epoch 7, Batch 627, Test Loss: 0.573317289352417\n",
      "Epoch 7, Batch 628, Test Loss: 0.5053896903991699\n",
      "Epoch 7, Batch 629, Test Loss: 0.44932955503463745\n",
      "Epoch 7, Batch 630, Test Loss: 0.7659479975700378\n",
      "Epoch 7, Batch 631, Test Loss: 0.6989752650260925\n",
      "Epoch 7, Batch 632, Test Loss: 0.8305644392967224\n",
      "Epoch 7, Batch 633, Test Loss: 0.4767158329486847\n",
      "Epoch 7, Batch 634, Test Loss: 0.4938797950744629\n",
      "Epoch 7, Batch 635, Test Loss: 0.4552444517612457\n",
      "Epoch 7, Batch 636, Test Loss: 0.7622653245925903\n",
      "Epoch 7, Batch 637, Test Loss: 0.4566046893596649\n",
      "Epoch 7, Batch 638, Test Loss: 0.47850745916366577\n",
      "Epoch 7, Batch 639, Test Loss: 0.6015780568122864\n",
      "Epoch 7, Batch 640, Test Loss: 0.41479572653770447\n",
      "Epoch 7, Batch 641, Test Loss: 0.46970319747924805\n",
      "Epoch 7, Batch 642, Test Loss: 0.4414222240447998\n",
      "Epoch 7, Batch 643, Test Loss: 0.5239593982696533\n",
      "Epoch 7, Batch 644, Test Loss: 0.5013596415519714\n",
      "Epoch 7, Batch 645, Test Loss: 0.46947455406188965\n",
      "Epoch 7, Batch 646, Test Loss: 0.7567138075828552\n",
      "Epoch 7, Batch 647, Test Loss: 0.570155918598175\n",
      "Epoch 7, Batch 648, Test Loss: 0.6300650835037231\n",
      "Epoch 7, Batch 649, Test Loss: 0.6381155252456665\n",
      "Epoch 7, Batch 650, Test Loss: 0.33202552795410156\n",
      "Epoch 7, Batch 651, Test Loss: 0.6602307558059692\n",
      "Epoch 7, Batch 652, Test Loss: 0.46911710500717163\n",
      "Epoch 7, Batch 653, Test Loss: 0.48870500922203064\n",
      "Epoch 7, Batch 654, Test Loss: 0.6974394917488098\n",
      "Epoch 7, Batch 655, Test Loss: 0.4954652190208435\n",
      "Epoch 7, Batch 656, Test Loss: 0.6354073882102966\n",
      "Epoch 7, Batch 657, Test Loss: 0.49019935727119446\n",
      "Epoch 7, Batch 658, Test Loss: 0.541886568069458\n",
      "Epoch 7, Batch 659, Test Loss: 0.5972386002540588\n",
      "Epoch 7, Batch 660, Test Loss: 0.4704570472240448\n",
      "Epoch 7, Batch 661, Test Loss: 0.44182413816452026\n",
      "Epoch 7, Batch 662, Test Loss: 0.6456925272941589\n",
      "Epoch 7, Batch 663, Test Loss: 0.7278812527656555\n",
      "Epoch 7, Batch 664, Test Loss: 0.5094031691551208\n",
      "Epoch 7, Batch 665, Test Loss: 0.6053193807601929\n",
      "Epoch 7, Batch 666, Test Loss: 0.5430911183357239\n",
      "Epoch 7, Batch 667, Test Loss: 0.5304176211357117\n",
      "Epoch 7, Batch 668, Test Loss: 0.42830103635787964\n",
      "Epoch 7, Batch 669, Test Loss: 0.6825894117355347\n",
      "Epoch 7, Batch 670, Test Loss: 0.47651970386505127\n",
      "Epoch 7, Batch 671, Test Loss: 0.4060482978820801\n",
      "Epoch 7, Batch 672, Test Loss: 0.4914337396621704\n",
      "Epoch 7, Batch 673, Test Loss: 0.5346322059631348\n",
      "Epoch 7, Batch 674, Test Loss: 0.412066787481308\n",
      "Epoch 7, Batch 675, Test Loss: 0.761928379535675\n",
      "Epoch 7, Batch 676, Test Loss: 0.4635895788669586\n",
      "Epoch 7, Batch 677, Test Loss: 0.4988390803337097\n",
      "Epoch 7, Batch 678, Test Loss: 0.5197417140007019\n",
      "Epoch 7, Batch 679, Test Loss: 0.48995500802993774\n",
      "Epoch 7, Batch 680, Test Loss: 0.6387485861778259\n",
      "Epoch 7, Batch 681, Test Loss: 0.5420190095901489\n",
      "Epoch 7, Batch 682, Test Loss: 0.5429438948631287\n",
      "Epoch 7, Batch 683, Test Loss: 0.5343961715698242\n",
      "Epoch 7, Batch 684, Test Loss: 0.6595268845558167\n",
      "Epoch 7, Batch 685, Test Loss: 0.6281245946884155\n",
      "Epoch 7, Batch 686, Test Loss: 0.6310418844223022\n",
      "Epoch 7, Batch 687, Test Loss: 0.48806053400039673\n",
      "Epoch 7, Batch 688, Test Loss: 0.5089353322982788\n",
      "Epoch 7, Batch 689, Test Loss: 0.4278312921524048\n",
      "Epoch 7, Batch 690, Test Loss: 0.6713815331459045\n",
      "Epoch 7, Batch 691, Test Loss: 0.624104380607605\n",
      "Epoch 7, Batch 692, Test Loss: 0.3837398290634155\n",
      "Epoch 7, Batch 693, Test Loss: 0.5287606716156006\n",
      "Epoch 7, Batch 694, Test Loss: 0.7195684909820557\n",
      "Epoch 7, Batch 695, Test Loss: 0.5323821306228638\n",
      "Epoch 7, Batch 696, Test Loss: 0.51505047082901\n",
      "Epoch 7, Batch 697, Test Loss: 0.47302931547164917\n",
      "Epoch 7, Batch 698, Test Loss: 0.4995564818382263\n",
      "Epoch 7, Batch 699, Test Loss: 0.5753636956214905\n",
      "Epoch 7, Batch 700, Test Loss: 0.47494858503341675\n",
      "Epoch 7, Batch 701, Test Loss: 0.7745634317398071\n",
      "Epoch 7, Batch 702, Test Loss: 0.45806196331977844\n",
      "Epoch 7, Batch 703, Test Loss: 0.6315703392028809\n",
      "Epoch 7, Batch 704, Test Loss: 0.5442225337028503\n",
      "Epoch 7, Batch 705, Test Loss: 0.4761256277561188\n",
      "Epoch 7, Batch 706, Test Loss: 0.5781444311141968\n",
      "Epoch 7, Batch 707, Test Loss: 0.5529395937919617\n",
      "Epoch 7, Batch 708, Test Loss: 0.6085776090621948\n",
      "Epoch 7, Batch 709, Test Loss: 0.7055702209472656\n",
      "Epoch 7, Batch 710, Test Loss: 0.5711401700973511\n",
      "Epoch 7, Batch 711, Test Loss: 0.5049883723258972\n",
      "Epoch 7, Batch 712, Test Loss: 0.5252787470817566\n",
      "Epoch 7, Batch 713, Test Loss: 0.5158743858337402\n",
      "Epoch 7, Batch 714, Test Loss: 0.6725239753723145\n",
      "Epoch 7, Batch 715, Test Loss: 0.6729519367218018\n",
      "Epoch 7, Batch 716, Test Loss: 0.4519716799259186\n",
      "Epoch 7, Batch 717, Test Loss: 0.5753571391105652\n",
      "Epoch 7, Batch 718, Test Loss: 0.7545909285545349\n",
      "Epoch 7, Batch 719, Test Loss: 0.5563220977783203\n",
      "Epoch 7, Batch 720, Test Loss: 0.8056324124336243\n",
      "Epoch 7, Batch 721, Test Loss: 0.542258620262146\n",
      "Epoch 7, Batch 722, Test Loss: 0.6240114569664001\n",
      "Epoch 7, Batch 723, Test Loss: 0.58062744140625\n",
      "Epoch 7, Batch 724, Test Loss: 0.6264678835868835\n",
      "Epoch 7, Batch 725, Test Loss: 0.6297619938850403\n",
      "Epoch 7, Batch 726, Test Loss: 0.5758636593818665\n",
      "Epoch 7, Batch 727, Test Loss: 0.4419232904911041\n",
      "Epoch 7, Batch 728, Test Loss: 0.5715881586074829\n",
      "Epoch 7, Batch 729, Test Loss: 0.5994754433631897\n",
      "Epoch 7, Batch 730, Test Loss: 0.42319050431251526\n",
      "Epoch 7, Batch 731, Test Loss: 0.6475196480751038\n",
      "Epoch 7, Batch 732, Test Loss: 0.4478488266468048\n",
      "Epoch 7, Batch 733, Test Loss: 0.6404146552085876\n",
      "Epoch 7, Batch 734, Test Loss: 0.36931273341178894\n",
      "Epoch 7, Batch 735, Test Loss: 0.6797585487365723\n",
      "Epoch 7, Batch 736, Test Loss: 0.9149174094200134\n",
      "Epoch 7, Batch 737, Test Loss: 0.4049377143383026\n",
      "Epoch 7, Batch 738, Test Loss: 0.3732700049877167\n",
      "Epoch 7, Batch 739, Test Loss: 0.43066391348838806\n",
      "Epoch 7, Batch 740, Test Loss: 0.6073627471923828\n",
      "Epoch 7, Batch 741, Test Loss: 0.5377267599105835\n",
      "Epoch 7, Batch 742, Test Loss: 0.45141613483428955\n",
      "Epoch 7, Batch 743, Test Loss: 0.7370518445968628\n",
      "Epoch 7, Batch 744, Test Loss: 0.694703221321106\n",
      "Epoch 7, Batch 745, Test Loss: 0.43142080307006836\n",
      "Epoch 7, Batch 746, Test Loss: 0.5149441957473755\n",
      "Epoch 7, Batch 747, Test Loss: 0.5942897200584412\n",
      "Epoch 7, Batch 748, Test Loss: 0.38238781690597534\n",
      "Epoch 7, Batch 749, Test Loss: 0.610113799571991\n",
      "Epoch 7, Batch 750, Test Loss: 0.5027559399604797\n",
      "Epoch 7, Batch 751, Test Loss: 0.47968989610671997\n",
      "Epoch 7, Batch 752, Test Loss: 0.6538439393043518\n",
      "Epoch 7, Batch 753, Test Loss: 0.7450146079063416\n",
      "Epoch 7, Batch 754, Test Loss: 0.4633653163909912\n",
      "Epoch 7, Batch 755, Test Loss: 0.4905303716659546\n",
      "Epoch 7, Batch 756, Test Loss: 0.6180009245872498\n",
      "Epoch 7, Batch 757, Test Loss: 0.5169457793235779\n",
      "Epoch 7, Batch 758, Test Loss: 0.5025808215141296\n",
      "Epoch 7, Batch 759, Test Loss: 0.688869059085846\n",
      "Epoch 7, Batch 760, Test Loss: 0.5011224746704102\n",
      "Epoch 7, Batch 761, Test Loss: 0.4647616446018219\n",
      "Epoch 7, Batch 762, Test Loss: 0.5388166308403015\n",
      "Epoch 7, Batch 763, Test Loss: 0.4437198340892792\n",
      "Epoch 7, Batch 764, Test Loss: 0.4994496703147888\n",
      "Epoch 7, Batch 765, Test Loss: 0.7526870369911194\n",
      "Epoch 7, Batch 766, Test Loss: 0.5357190370559692\n",
      "Epoch 7, Batch 767, Test Loss: 0.48001939058303833\n",
      "Epoch 7, Batch 768, Test Loss: 1.0341198444366455\n",
      "Epoch 7, Batch 769, Test Loss: 0.686768651008606\n",
      "Epoch 7, Batch 770, Test Loss: 0.6047155261039734\n",
      "Epoch 7, Batch 771, Test Loss: 0.5611244440078735\n",
      "Epoch 7, Batch 772, Test Loss: 0.6851799488067627\n",
      "Epoch 7, Batch 773, Test Loss: 0.5241310596466064\n",
      "Epoch 7, Batch 774, Test Loss: 0.4445699453353882\n",
      "Epoch 7, Batch 775, Test Loss: 0.7210963368415833\n",
      "Epoch 7, Batch 776, Test Loss: 0.7626161575317383\n",
      "Epoch 7, Batch 777, Test Loss: 0.6041433811187744\n",
      "Epoch 7, Batch 778, Test Loss: 0.30741333961486816\n",
      "Epoch 7, Batch 779, Test Loss: 0.5941286683082581\n",
      "Epoch 7, Batch 780, Test Loss: 0.5257246494293213\n",
      "Epoch 7, Batch 781, Test Loss: 0.6699211597442627\n",
      "Epoch 7, Batch 782, Test Loss: 0.5355535745620728\n",
      "Epoch 7, Batch 783, Test Loss: 0.5123232007026672\n",
      "Epoch 7, Batch 784, Test Loss: 0.5465911626815796\n",
      "Epoch 7, Batch 785, Test Loss: 0.5439843535423279\n",
      "Epoch 7, Batch 786, Test Loss: 0.6514664888381958\n",
      "Epoch 7, Batch 787, Test Loss: 0.45975005626678467\n",
      "Epoch 7, Batch 788, Test Loss: 0.435735285282135\n",
      "Epoch 7, Batch 789, Test Loss: 0.6017704606056213\n",
      "Epoch 7, Batch 790, Test Loss: 0.5668028593063354\n",
      "Epoch 7, Batch 791, Test Loss: 0.6380108594894409\n",
      "Epoch 7, Batch 792, Test Loss: 0.5101441144943237\n",
      "Epoch 7, Batch 793, Test Loss: 0.570290744304657\n",
      "Epoch 7, Batch 794, Test Loss: 0.49853748083114624\n",
      "Epoch 7, Batch 795, Test Loss: 0.7505409717559814\n",
      "Epoch 7, Batch 796, Test Loss: 0.5561107397079468\n",
      "Epoch 7, Batch 797, Test Loss: 0.559990406036377\n",
      "Epoch 7, Batch 798, Test Loss: 0.6351745128631592\n",
      "Epoch 7, Batch 799, Test Loss: 0.6270205974578857\n",
      "Epoch 7, Batch 800, Test Loss: 0.6611572504043579\n",
      "Epoch 7, Batch 801, Test Loss: 0.6496740579605103\n",
      "Epoch 7, Batch 802, Test Loss: 0.6753976345062256\n",
      "Epoch 7, Batch 803, Test Loss: 0.6510317325592041\n",
      "Epoch 7, Batch 804, Test Loss: 0.5448774099349976\n",
      "Epoch 7, Batch 805, Test Loss: 0.41032618284225464\n",
      "Epoch 7, Batch 806, Test Loss: 0.5837123394012451\n",
      "Epoch 7, Batch 807, Test Loss: 0.6529542803764343\n",
      "Epoch 7, Batch 808, Test Loss: 0.5992462038993835\n",
      "Epoch 7, Batch 809, Test Loss: 0.48605382442474365\n",
      "Epoch 7, Batch 810, Test Loss: 0.44538113474845886\n",
      "Epoch 7, Batch 811, Test Loss: 0.6519995331764221\n",
      "Epoch 7, Batch 812, Test Loss: 0.5850352644920349\n",
      "Epoch 7, Batch 813, Test Loss: 0.48870763182640076\n",
      "Epoch 7, Batch 814, Test Loss: 0.6839656233787537\n",
      "Epoch 7, Batch 815, Test Loss: 0.7657961249351501\n",
      "Epoch 7, Batch 816, Test Loss: 0.6584034562110901\n",
      "Epoch 7, Batch 817, Test Loss: 0.5458202362060547\n",
      "Epoch 7, Batch 818, Test Loss: 0.4525489807128906\n",
      "Epoch 7, Batch 819, Test Loss: 0.5142238736152649\n",
      "Epoch 7, Batch 820, Test Loss: 0.6719229817390442\n",
      "Epoch 7, Batch 821, Test Loss: 0.46149060130119324\n",
      "Epoch 7, Batch 822, Test Loss: 0.6109152436256409\n",
      "Epoch 7, Batch 823, Test Loss: 0.4974943697452545\n",
      "Epoch 7, Batch 824, Test Loss: 0.4485098719596863\n",
      "Epoch 7, Batch 825, Test Loss: 0.5903317332267761\n",
      "Epoch 7, Batch 826, Test Loss: 0.610356330871582\n",
      "Epoch 7, Batch 827, Test Loss: 0.6607709527015686\n",
      "Epoch 7, Batch 828, Test Loss: 0.4827672243118286\n",
      "Epoch 7, Batch 829, Test Loss: 0.5308548808097839\n",
      "Epoch 7, Batch 830, Test Loss: 0.5319792032241821\n",
      "Epoch 7, Batch 831, Test Loss: 0.428866982460022\n",
      "Epoch 7, Batch 832, Test Loss: 0.5614991188049316\n",
      "Epoch 7, Batch 833, Test Loss: 0.4764542877674103\n",
      "Epoch 7, Batch 834, Test Loss: 0.7595452070236206\n",
      "Epoch 7, Batch 835, Test Loss: 0.4162457287311554\n",
      "Epoch 7, Batch 836, Test Loss: 0.44716060161590576\n",
      "Epoch 7, Batch 837, Test Loss: 0.5305924415588379\n",
      "Epoch 7, Batch 838, Test Loss: 0.5325785875320435\n",
      "Epoch 7, Batch 839, Test Loss: 0.5963683128356934\n",
      "Epoch 7, Batch 840, Test Loss: 0.6699120402336121\n",
      "Epoch 7, Batch 841, Test Loss: 0.643439769744873\n",
      "Epoch 7, Batch 842, Test Loss: 0.5011897683143616\n",
      "Epoch 7, Batch 843, Test Loss: 0.6388294100761414\n",
      "Epoch 7, Batch 844, Test Loss: 0.41762101650238037\n",
      "Epoch 7, Batch 845, Test Loss: 0.48243916034698486\n",
      "Epoch 7, Batch 846, Test Loss: 0.6026889085769653\n",
      "Epoch 7, Batch 847, Test Loss: 0.653138279914856\n",
      "Epoch 7, Batch 848, Test Loss: 0.6292962431907654\n",
      "Epoch 7, Batch 849, Test Loss: 0.4992988705635071\n",
      "Epoch 7, Batch 850, Test Loss: 0.562656581401825\n",
      "Epoch 7, Batch 851, Test Loss: 0.6501162648200989\n",
      "Epoch 7, Batch 852, Test Loss: 0.5143709182739258\n",
      "Epoch 7, Batch 853, Test Loss: 0.6999927163124084\n",
      "Epoch 7, Batch 854, Test Loss: 0.8283711075782776\n",
      "Epoch 7, Batch 855, Test Loss: 0.5363165140151978\n",
      "Epoch 7, Batch 856, Test Loss: 0.6292715668678284\n",
      "Epoch 7, Batch 857, Test Loss: 0.49837979674339294\n",
      "Epoch 7, Batch 858, Test Loss: 0.6513811945915222\n",
      "Epoch 7, Batch 859, Test Loss: 0.5355450510978699\n",
      "Epoch 7, Batch 860, Test Loss: 0.5635995268821716\n",
      "Epoch 7, Batch 861, Test Loss: 0.47280794382095337\n",
      "Epoch 7, Batch 862, Test Loss: 0.5421613454818726\n",
      "Epoch 7, Batch 863, Test Loss: 0.8072900176048279\n",
      "Epoch 7, Batch 864, Test Loss: 0.5666747093200684\n",
      "Epoch 7, Batch 865, Test Loss: 0.6776071786880493\n",
      "Epoch 7, Batch 866, Test Loss: 0.5610122084617615\n",
      "Epoch 7, Batch 867, Test Loss: 0.45341184735298157\n",
      "Epoch 7, Batch 868, Test Loss: 0.6116287112236023\n",
      "Epoch 7, Batch 869, Test Loss: 0.4286709725856781\n",
      "Epoch 7, Batch 870, Test Loss: 0.602535605430603\n",
      "Epoch 7, Batch 871, Test Loss: 0.45145541429519653\n",
      "Epoch 7, Batch 872, Test Loss: 0.5957764387130737\n",
      "Epoch 7, Batch 873, Test Loss: 0.6325842142105103\n",
      "Epoch 7, Batch 874, Test Loss: 0.6266286373138428\n",
      "Epoch 7, Batch 875, Test Loss: 0.5269474983215332\n",
      "Epoch 7, Batch 876, Test Loss: 0.5403169989585876\n",
      "Epoch 7, Batch 877, Test Loss: 0.6809228658676147\n",
      "Epoch 7, Batch 878, Test Loss: 0.6202279329299927\n",
      "Epoch 7, Batch 879, Test Loss: 0.4364030659198761\n",
      "Epoch 7, Batch 880, Test Loss: 0.4156242609024048\n",
      "Epoch 7, Batch 881, Test Loss: 0.5295641422271729\n",
      "Epoch 7, Batch 882, Test Loss: 0.47986307740211487\n",
      "Epoch 7, Batch 883, Test Loss: 0.3791860342025757\n",
      "Epoch 7, Batch 884, Test Loss: 0.6430330872535706\n",
      "Epoch 7, Batch 885, Test Loss: 0.80976402759552\n",
      "Epoch 7, Batch 886, Test Loss: 0.6244529485702515\n",
      "Epoch 7, Batch 887, Test Loss: 0.5132418274879456\n",
      "Epoch 7, Batch 888, Test Loss: 0.47815531492233276\n",
      "Epoch 7, Batch 889, Test Loss: 0.645500123500824\n",
      "Epoch 7, Batch 890, Test Loss: 0.648758590221405\n",
      "Epoch 7, Batch 891, Test Loss: 0.5095393061637878\n",
      "Epoch 7, Batch 892, Test Loss: 0.6699867844581604\n",
      "Epoch 7, Batch 893, Test Loss: 0.7692860960960388\n",
      "Epoch 7, Batch 894, Test Loss: 0.6707907915115356\n",
      "Epoch 7, Batch 895, Test Loss: 0.48534390330314636\n",
      "Epoch 7, Batch 896, Test Loss: 0.47336286306381226\n",
      "Epoch 7, Batch 897, Test Loss: 0.3956955671310425\n",
      "Epoch 7, Batch 898, Test Loss: 0.5166412591934204\n",
      "Epoch 7, Batch 899, Test Loss: 0.6171560883522034\n",
      "Epoch 7, Batch 900, Test Loss: 0.48445504903793335\n",
      "Epoch 7, Batch 901, Test Loss: 0.39609792828559875\n",
      "Epoch 7, Batch 902, Test Loss: 0.7164035439491272\n",
      "Epoch 7, Batch 903, Test Loss: 0.4957011938095093\n",
      "Epoch 7, Batch 904, Test Loss: 0.5146530866622925\n",
      "Epoch 7, Batch 905, Test Loss: 0.7221680283546448\n",
      "Epoch 7, Batch 906, Test Loss: 0.71628338098526\n",
      "Epoch 7, Batch 907, Test Loss: 0.4253373444080353\n",
      "Epoch 7, Batch 908, Test Loss: 0.7056961059570312\n",
      "Epoch 7, Batch 909, Test Loss: 0.8064911961555481\n",
      "Epoch 7, Batch 910, Test Loss: 0.6201406717300415\n",
      "Epoch 7, Batch 911, Test Loss: 0.5362005829811096\n",
      "Epoch 7, Batch 912, Test Loss: 0.6711744070053101\n",
      "Epoch 7, Batch 913, Test Loss: 0.9364326596260071\n",
      "Epoch 7, Batch 914, Test Loss: 0.48969316482543945\n",
      "Epoch 7, Batch 915, Test Loss: 0.6025370359420776\n",
      "Epoch 7, Batch 916, Test Loss: 0.4785236120223999\n",
      "Epoch 7, Batch 917, Test Loss: 0.5496317148208618\n",
      "Epoch 7, Batch 918, Test Loss: 0.5390125513076782\n",
      "Epoch 7, Batch 919, Test Loss: 0.7296977043151855\n",
      "Epoch 7, Batch 920, Test Loss: 0.4517103433609009\n",
      "Epoch 7, Batch 921, Test Loss: 0.5495193600654602\n",
      "Epoch 7, Batch 922, Test Loss: 0.6321109533309937\n",
      "Epoch 7, Batch 923, Test Loss: 0.4006302058696747\n",
      "Epoch 7, Batch 924, Test Loss: 0.4836752414703369\n",
      "Epoch 7, Batch 925, Test Loss: 0.5943270921707153\n",
      "Epoch 7, Batch 926, Test Loss: 0.5903565287590027\n",
      "Epoch 7, Batch 927, Test Loss: 0.4897545874118805\n",
      "Epoch 7, Batch 928, Test Loss: 0.5462642908096313\n",
      "Epoch 7, Batch 929, Test Loss: 0.43264734745025635\n",
      "Epoch 7, Batch 930, Test Loss: 0.4897880256175995\n",
      "Epoch 7, Batch 931, Test Loss: 0.5410189628601074\n",
      "Epoch 7, Batch 932, Test Loss: 0.8175272345542908\n",
      "Epoch 7, Batch 933, Test Loss: 0.6562958359718323\n",
      "Epoch 7, Batch 934, Test Loss: 0.6552242040634155\n",
      "Epoch 7, Batch 935, Test Loss: 0.584296703338623\n",
      "Epoch 7, Batch 936, Test Loss: 0.6259925961494446\n",
      "Epoch 7, Batch 937, Test Loss: 0.6290378570556641\n",
      "Epoch 7, Batch 938, Test Loss: 0.48686912655830383\n",
      "Accuracy of Test set: 0.7992166666666667\n",
      "Epoch 8, Batch 1, Loss: 0.5569694638252258\n",
      "Epoch 8, Batch 2, Loss: 0.5021100640296936\n",
      "Epoch 8, Batch 3, Loss: 0.7377067804336548\n",
      "Epoch 8, Batch 4, Loss: 0.6011472344398499\n",
      "Epoch 8, Batch 5, Loss: 0.9011253714561462\n",
      "Epoch 8, Batch 6, Loss: 0.5664082169532776\n",
      "Epoch 8, Batch 7, Loss: 0.5897042751312256\n",
      "Epoch 8, Batch 8, Loss: 0.3952280580997467\n",
      "Epoch 8, Batch 9, Loss: 0.36941495537757874\n",
      "Epoch 8, Batch 10, Loss: 0.43988102674484253\n",
      "Epoch 8, Batch 11, Loss: 0.5874578952789307\n",
      "Epoch 8, Batch 12, Loss: 0.5639135241508484\n",
      "Epoch 8, Batch 13, Loss: 0.6834408044815063\n",
      "Epoch 8, Batch 14, Loss: 0.472938597202301\n",
      "Epoch 8, Batch 15, Loss: 0.442630410194397\n",
      "Epoch 8, Batch 16, Loss: 0.5454742312431335\n",
      "Epoch 8, Batch 17, Loss: 0.48697975277900696\n",
      "Epoch 8, Batch 18, Loss: 0.6047921776771545\n",
      "Epoch 8, Batch 19, Loss: 0.6256532669067383\n",
      "Epoch 8, Batch 20, Loss: 0.6181979775428772\n",
      "Epoch 8, Batch 21, Loss: 0.6371378898620605\n",
      "Epoch 8, Batch 22, Loss: 0.684490978717804\n",
      "Epoch 8, Batch 23, Loss: 0.5390563011169434\n",
      "Epoch 8, Batch 24, Loss: 0.5810224413871765\n",
      "Epoch 8, Batch 25, Loss: 0.6019580960273743\n",
      "Epoch 8, Batch 26, Loss: 0.4318584203720093\n",
      "Epoch 8, Batch 27, Loss: 0.564571738243103\n",
      "Epoch 8, Batch 28, Loss: 0.4817378520965576\n",
      "Epoch 8, Batch 29, Loss: 0.5386103391647339\n",
      "Epoch 8, Batch 30, Loss: 0.8023881912231445\n",
      "Epoch 8, Batch 31, Loss: 0.5485811829566956\n",
      "Epoch 8, Batch 32, Loss: 0.46331003308296204\n",
      "Epoch 8, Batch 33, Loss: 0.4789813160896301\n",
      "Epoch 8, Batch 34, Loss: 0.5847178101539612\n",
      "Epoch 8, Batch 35, Loss: 0.4268515110015869\n",
      "Epoch 8, Batch 36, Loss: 0.4235014319419861\n",
      "Epoch 8, Batch 37, Loss: 0.6523457765579224\n",
      "Epoch 8, Batch 38, Loss: 0.5449262857437134\n",
      "Epoch 8, Batch 39, Loss: 0.6516439318656921\n",
      "Epoch 8, Batch 40, Loss: 0.6083641648292542\n",
      "Epoch 8, Batch 41, Loss: 0.37046411633491516\n",
      "Epoch 8, Batch 42, Loss: 0.6743589043617249\n",
      "Epoch 8, Batch 43, Loss: 0.7181853652000427\n",
      "Epoch 8, Batch 44, Loss: 0.45741933584213257\n",
      "Epoch 8, Batch 45, Loss: 0.5498589873313904\n",
      "Epoch 8, Batch 46, Loss: 0.558131754398346\n",
      "Epoch 8, Batch 47, Loss: 0.4915471076965332\n",
      "Epoch 8, Batch 48, Loss: 0.5340992212295532\n",
      "Epoch 8, Batch 49, Loss: 0.46304312348365784\n",
      "Epoch 8, Batch 50, Loss: 0.7533286809921265\n",
      "Epoch 8, Batch 51, Loss: 0.6075301766395569\n",
      "Epoch 8, Batch 52, Loss: 0.4284871816635132\n",
      "Epoch 8, Batch 53, Loss: 0.5342644453048706\n",
      "Epoch 8, Batch 54, Loss: 0.5640349984169006\n",
      "Epoch 8, Batch 55, Loss: 0.6649514436721802\n",
      "Epoch 8, Batch 56, Loss: 0.565239429473877\n",
      "Epoch 8, Batch 57, Loss: 0.3060893416404724\n",
      "Epoch 8, Batch 58, Loss: 0.6583554744720459\n",
      "Epoch 8, Batch 59, Loss: 0.6355496048927307\n",
      "Epoch 8, Batch 60, Loss: 0.5452262759208679\n",
      "Epoch 8, Batch 61, Loss: 0.4099849760532379\n",
      "Epoch 8, Batch 62, Loss: 0.6061662435531616\n",
      "Epoch 8, Batch 63, Loss: 0.7156469821929932\n",
      "Epoch 8, Batch 64, Loss: 0.5023908615112305\n",
      "Epoch 8, Batch 65, Loss: 0.43990784883499146\n",
      "Epoch 8, Batch 66, Loss: 0.3479706048965454\n",
      "Epoch 8, Batch 67, Loss: 0.4151969254016876\n",
      "Epoch 8, Batch 68, Loss: 0.6783391237258911\n",
      "Epoch 8, Batch 69, Loss: 0.5892869830131531\n",
      "Epoch 8, Batch 70, Loss: 0.6368996500968933\n",
      "Epoch 8, Batch 71, Loss: 0.46728020906448364\n",
      "Epoch 8, Batch 72, Loss: 0.6553679704666138\n",
      "Epoch 8, Batch 73, Loss: 0.6397153735160828\n",
      "Epoch 8, Batch 74, Loss: 0.8049569129943848\n",
      "Epoch 8, Batch 75, Loss: 0.6472251415252686\n",
      "Epoch 8, Batch 76, Loss: 0.7953507900238037\n",
      "Epoch 8, Batch 77, Loss: 0.5758305788040161\n",
      "Epoch 8, Batch 78, Loss: 0.5316454172134399\n",
      "Epoch 8, Batch 79, Loss: 0.4550020098686218\n",
      "Epoch 8, Batch 80, Loss: 0.8313260674476624\n",
      "Epoch 8, Batch 81, Loss: 0.5679295063018799\n",
      "Epoch 8, Batch 82, Loss: 0.6368776559829712\n",
      "Epoch 8, Batch 83, Loss: 0.6429744362831116\n",
      "Epoch 8, Batch 84, Loss: 0.6511995196342468\n",
      "Epoch 8, Batch 85, Loss: 0.6608201861381531\n",
      "Epoch 8, Batch 86, Loss: 0.5244541168212891\n",
      "Epoch 8, Batch 87, Loss: 0.6109820604324341\n",
      "Epoch 8, Batch 88, Loss: 0.7264087796211243\n",
      "Epoch 8, Batch 89, Loss: 0.542974591255188\n",
      "Epoch 8, Batch 90, Loss: 0.5093098282814026\n",
      "Epoch 8, Batch 91, Loss: 0.4359159469604492\n",
      "Epoch 8, Batch 92, Loss: 0.43996939063072205\n",
      "Epoch 8, Batch 93, Loss: 0.39946770668029785\n",
      "Epoch 8, Batch 94, Loss: 0.4474866986274719\n",
      "Epoch 8, Batch 95, Loss: 0.7103807926177979\n",
      "Epoch 8, Batch 96, Loss: 0.5224781632423401\n",
      "Epoch 8, Batch 97, Loss: 0.5926883220672607\n",
      "Epoch 8, Batch 98, Loss: 0.6991335153579712\n",
      "Epoch 8, Batch 99, Loss: 0.5318804979324341\n",
      "Epoch 8, Batch 100, Loss: 0.7329905033111572\n",
      "Epoch 8, Batch 101, Loss: 0.4649762511253357\n",
      "Epoch 8, Batch 102, Loss: 0.6400399208068848\n",
      "Epoch 8, Batch 103, Loss: 0.4933894872665405\n",
      "Epoch 8, Batch 104, Loss: 0.5522496104240417\n",
      "Epoch 8, Batch 105, Loss: 0.5106124877929688\n",
      "Epoch 8, Batch 106, Loss: 0.5521864295005798\n",
      "Epoch 8, Batch 107, Loss: 0.4858711063861847\n",
      "Epoch 8, Batch 108, Loss: 0.7738688588142395\n",
      "Epoch 8, Batch 109, Loss: 0.3970153331756592\n",
      "Epoch 8, Batch 110, Loss: 0.4794200658798218\n",
      "Epoch 8, Batch 111, Loss: 0.6335211396217346\n",
      "Epoch 8, Batch 112, Loss: 0.6875115633010864\n",
      "Epoch 8, Batch 113, Loss: 0.5340981483459473\n",
      "Epoch 8, Batch 114, Loss: 0.5898927450180054\n",
      "Epoch 8, Batch 115, Loss: 0.7397409081459045\n",
      "Epoch 8, Batch 116, Loss: 0.3995547890663147\n",
      "Epoch 8, Batch 117, Loss: 0.4598270654678345\n",
      "Epoch 8, Batch 118, Loss: 0.6988454461097717\n",
      "Epoch 8, Batch 119, Loss: 0.5669943690299988\n",
      "Epoch 8, Batch 120, Loss: 0.6479136943817139\n",
      "Epoch 8, Batch 121, Loss: 0.46074026823043823\n",
      "Epoch 8, Batch 122, Loss: 0.4657849669456482\n",
      "Epoch 8, Batch 123, Loss: 0.5838311910629272\n",
      "Epoch 8, Batch 124, Loss: 0.559734582901001\n",
      "Epoch 8, Batch 125, Loss: 0.5090829730033875\n",
      "Epoch 8, Batch 126, Loss: 0.4202023446559906\n",
      "Epoch 8, Batch 127, Loss: 0.5648673176765442\n",
      "Epoch 8, Batch 128, Loss: 0.4006890058517456\n",
      "Epoch 8, Batch 129, Loss: 0.5059707164764404\n",
      "Epoch 8, Batch 130, Loss: 0.4752424955368042\n",
      "Epoch 8, Batch 131, Loss: 0.9189479351043701\n",
      "Epoch 8, Batch 132, Loss: 0.39036551117897034\n",
      "Epoch 8, Batch 133, Loss: 0.40525537729263306\n",
      "Epoch 8, Batch 134, Loss: 0.2547820210456848\n",
      "Epoch 8, Batch 135, Loss: 0.46342241764068604\n",
      "Epoch 8, Batch 136, Loss: 0.5498704314231873\n",
      "Epoch 8, Batch 137, Loss: 0.6879640221595764\n",
      "Epoch 8, Batch 138, Loss: 0.5552005171775818\n",
      "Epoch 8, Batch 139, Loss: 0.42302340269088745\n",
      "Epoch 8, Batch 140, Loss: 0.4746183753013611\n",
      "Epoch 8, Batch 141, Loss: 0.36327168345451355\n",
      "Epoch 8, Batch 142, Loss: 0.5315423011779785\n",
      "Epoch 8, Batch 143, Loss: 0.5655885934829712\n",
      "Epoch 8, Batch 144, Loss: 0.6715370416641235\n",
      "Epoch 8, Batch 145, Loss: 0.5936824083328247\n",
      "Epoch 8, Batch 146, Loss: 0.6574150323867798\n",
      "Epoch 8, Batch 147, Loss: 0.4257616400718689\n",
      "Epoch 8, Batch 148, Loss: 0.45908549427986145\n",
      "Epoch 8, Batch 149, Loss: 0.406380832195282\n",
      "Epoch 8, Batch 150, Loss: 0.5065950751304626\n",
      "Epoch 8, Batch 151, Loss: 0.5918967723846436\n",
      "Epoch 8, Batch 152, Loss: 0.5874007344245911\n",
      "Epoch 8, Batch 153, Loss: 0.6373791098594666\n",
      "Epoch 8, Batch 154, Loss: 0.6461310386657715\n",
      "Epoch 8, Batch 155, Loss: 0.5634585618972778\n",
      "Epoch 8, Batch 156, Loss: 0.38250359892845154\n",
      "Epoch 8, Batch 157, Loss: 0.6473566293716431\n",
      "Epoch 8, Batch 158, Loss: 0.5887264013290405\n",
      "Epoch 8, Batch 159, Loss: 0.5499330163002014\n",
      "Epoch 8, Batch 160, Loss: 0.5852030515670776\n",
      "Epoch 8, Batch 161, Loss: 0.5224537253379822\n",
      "Epoch 8, Batch 162, Loss: 0.4601609706878662\n",
      "Epoch 8, Batch 163, Loss: 0.3026793301105499\n",
      "Epoch 8, Batch 164, Loss: 0.5246850848197937\n",
      "Epoch 8, Batch 165, Loss: 0.5742145776748657\n",
      "Epoch 8, Batch 166, Loss: 0.47632086277008057\n",
      "Epoch 8, Batch 167, Loss: 0.7645572423934937\n",
      "Epoch 8, Batch 168, Loss: 0.6619176268577576\n",
      "Epoch 8, Batch 169, Loss: 0.5234085917472839\n",
      "Epoch 8, Batch 170, Loss: 0.43667930364608765\n",
      "Epoch 8, Batch 171, Loss: 0.5124679803848267\n",
      "Epoch 8, Batch 172, Loss: 0.46275225281715393\n",
      "Epoch 8, Batch 173, Loss: 0.5708227753639221\n",
      "Epoch 8, Batch 174, Loss: 0.700481653213501\n",
      "Epoch 8, Batch 175, Loss: 0.5478606820106506\n",
      "Epoch 8, Batch 176, Loss: 0.5709084272384644\n",
      "Epoch 8, Batch 177, Loss: 0.5428385734558105\n",
      "Epoch 8, Batch 178, Loss: 0.49881696701049805\n",
      "Epoch 8, Batch 179, Loss: 0.6018844246864319\n",
      "Epoch 8, Batch 180, Loss: 0.5371863842010498\n",
      "Epoch 8, Batch 181, Loss: 0.6602525115013123\n",
      "Epoch 8, Batch 182, Loss: 0.49518021941185\n",
      "Epoch 8, Batch 183, Loss: 0.4794560670852661\n",
      "Epoch 8, Batch 184, Loss: 0.7209420204162598\n",
      "Epoch 8, Batch 185, Loss: 0.4388508200645447\n",
      "Epoch 8, Batch 186, Loss: 0.4333595633506775\n",
      "Epoch 8, Batch 187, Loss: 0.4838390648365021\n",
      "Epoch 8, Batch 188, Loss: 0.5881309509277344\n",
      "Epoch 8, Batch 189, Loss: 0.6071517467498779\n",
      "Epoch 8, Batch 190, Loss: 0.4423058032989502\n",
      "Epoch 8, Batch 191, Loss: 0.5044968724250793\n",
      "Epoch 8, Batch 192, Loss: 0.3128446638584137\n",
      "Epoch 8, Batch 193, Loss: 0.536276638507843\n",
      "Epoch 8, Batch 194, Loss: 0.8274368643760681\n",
      "Epoch 8, Batch 195, Loss: 0.4907152056694031\n",
      "Epoch 8, Batch 196, Loss: 0.733913779258728\n",
      "Epoch 8, Batch 197, Loss: 0.4200829863548279\n",
      "Epoch 8, Batch 198, Loss: 0.8519515991210938\n",
      "Epoch 8, Batch 199, Loss: 0.43738794326782227\n",
      "Epoch 8, Batch 200, Loss: 0.47592735290527344\n",
      "Epoch 8, Batch 201, Loss: 0.7154244780540466\n",
      "Epoch 8, Batch 202, Loss: 0.389993280172348\n",
      "Epoch 8, Batch 203, Loss: 0.5398899912834167\n",
      "Epoch 8, Batch 204, Loss: 0.49051612615585327\n",
      "Epoch 8, Batch 205, Loss: 0.5103606581687927\n",
      "Epoch 8, Batch 206, Loss: 0.6822356581687927\n",
      "Epoch 8, Batch 207, Loss: 0.4551659822463989\n",
      "Epoch 8, Batch 208, Loss: 0.5423584580421448\n",
      "Epoch 8, Batch 209, Loss: 0.7009292244911194\n",
      "Epoch 8, Batch 210, Loss: 0.5987008810043335\n",
      "Epoch 8, Batch 211, Loss: 0.5570784211158752\n",
      "Epoch 8, Batch 212, Loss: 0.6302297711372375\n",
      "Epoch 8, Batch 213, Loss: 0.5660780072212219\n",
      "Epoch 8, Batch 214, Loss: 0.36841997504234314\n",
      "Epoch 8, Batch 215, Loss: 0.7101858854293823\n",
      "Epoch 8, Batch 216, Loss: 0.5297112464904785\n",
      "Epoch 8, Batch 217, Loss: 0.6065387725830078\n",
      "Epoch 8, Batch 218, Loss: 0.5646888613700867\n",
      "Epoch 8, Batch 219, Loss: 0.3923734128475189\n",
      "Epoch 8, Batch 220, Loss: 0.5640259981155396\n",
      "Epoch 8, Batch 221, Loss: 0.4434484541416168\n",
      "Epoch 8, Batch 222, Loss: 0.38744544982910156\n",
      "Epoch 8, Batch 223, Loss: 0.538099467754364\n",
      "Epoch 8, Batch 224, Loss: 0.7246267199516296\n",
      "Epoch 8, Batch 225, Loss: 0.46107205748558044\n",
      "Epoch 8, Batch 226, Loss: 0.45956215262413025\n",
      "Epoch 8, Batch 227, Loss: 0.6708019971847534\n",
      "Epoch 8, Batch 228, Loss: 0.4663459062576294\n",
      "Epoch 8, Batch 229, Loss: 0.5792754888534546\n",
      "Epoch 8, Batch 230, Loss: 0.7943236231803894\n",
      "Epoch 8, Batch 231, Loss: 0.494070827960968\n",
      "Epoch 8, Batch 232, Loss: 0.6096968650817871\n",
      "Epoch 8, Batch 233, Loss: 0.5687392950057983\n",
      "Epoch 8, Batch 234, Loss: 0.44828134775161743\n",
      "Epoch 8, Batch 235, Loss: 0.7207329869270325\n",
      "Epoch 8, Batch 236, Loss: 0.4061942994594574\n",
      "Epoch 8, Batch 237, Loss: 0.8386709094047546\n",
      "Epoch 8, Batch 238, Loss: 0.5852898955345154\n",
      "Epoch 8, Batch 239, Loss: 0.44439437985420227\n",
      "Epoch 8, Batch 240, Loss: 0.6029795408248901\n",
      "Epoch 8, Batch 241, Loss: 0.7335541248321533\n",
      "Epoch 8, Batch 242, Loss: 1.0076221227645874\n",
      "Epoch 8, Batch 243, Loss: 0.44874900579452515\n",
      "Epoch 8, Batch 244, Loss: 0.489528626203537\n",
      "Epoch 8, Batch 245, Loss: 0.5247432589530945\n",
      "Epoch 8, Batch 246, Loss: 0.6498677134513855\n",
      "Epoch 8, Batch 247, Loss: 0.6771873235702515\n",
      "Epoch 8, Batch 248, Loss: 0.4646328091621399\n",
      "Epoch 8, Batch 249, Loss: 0.7284358143806458\n",
      "Epoch 8, Batch 250, Loss: 0.963808536529541\n",
      "Epoch 8, Batch 251, Loss: 0.6956987380981445\n",
      "Epoch 8, Batch 252, Loss: 0.3442316949367523\n",
      "Epoch 8, Batch 253, Loss: 0.6174821257591248\n",
      "Epoch 8, Batch 254, Loss: 0.7026343941688538\n",
      "Epoch 8, Batch 255, Loss: 0.5158941149711609\n",
      "Epoch 8, Batch 256, Loss: 0.6592048406600952\n",
      "Epoch 8, Batch 257, Loss: 0.6587010622024536\n",
      "Epoch 8, Batch 258, Loss: 0.6766055822372437\n",
      "Epoch 8, Batch 259, Loss: 0.8959050178527832\n",
      "Epoch 8, Batch 260, Loss: 0.7619437575340271\n",
      "Epoch 8, Batch 261, Loss: 0.5934630036354065\n",
      "Epoch 8, Batch 262, Loss: 0.413274347782135\n",
      "Epoch 8, Batch 263, Loss: 0.649024486541748\n",
      "Epoch 8, Batch 264, Loss: 0.613082230091095\n",
      "Epoch 8, Batch 265, Loss: 0.4875504970550537\n",
      "Epoch 8, Batch 266, Loss: 0.5786725282669067\n",
      "Epoch 8, Batch 267, Loss: 0.6117743253707886\n",
      "Epoch 8, Batch 268, Loss: 0.7817516326904297\n",
      "Epoch 8, Batch 269, Loss: 0.5904830098152161\n",
      "Epoch 8, Batch 270, Loss: 0.6108564138412476\n",
      "Epoch 8, Batch 271, Loss: 0.6311182379722595\n",
      "Epoch 8, Batch 272, Loss: 0.4041903614997864\n",
      "Epoch 8, Batch 273, Loss: 0.5882977843284607\n",
      "Epoch 8, Batch 274, Loss: 0.5687270164489746\n",
      "Epoch 8, Batch 275, Loss: 0.6199326515197754\n",
      "Epoch 8, Batch 276, Loss: 0.5444855093955994\n",
      "Epoch 8, Batch 277, Loss: 0.6224376559257507\n",
      "Epoch 8, Batch 278, Loss: 0.5324887037277222\n",
      "Epoch 8, Batch 279, Loss: 0.440426766872406\n",
      "Epoch 8, Batch 280, Loss: 0.5112791061401367\n",
      "Epoch 8, Batch 281, Loss: 0.5765202045440674\n",
      "Epoch 8, Batch 282, Loss: 0.4242442846298218\n",
      "Epoch 8, Batch 283, Loss: 0.4473586976528168\n",
      "Epoch 8, Batch 284, Loss: 0.3707928955554962\n",
      "Epoch 8, Batch 285, Loss: 0.5571094751358032\n",
      "Epoch 8, Batch 286, Loss: 0.46298396587371826\n",
      "Epoch 8, Batch 287, Loss: 0.4826362133026123\n",
      "Epoch 8, Batch 288, Loss: 0.575901448726654\n",
      "Epoch 8, Batch 289, Loss: 0.5167877674102783\n",
      "Epoch 8, Batch 290, Loss: 0.48910513520240784\n",
      "Epoch 8, Batch 291, Loss: 0.5133209824562073\n",
      "Epoch 8, Batch 292, Loss: 0.5354557037353516\n",
      "Epoch 8, Batch 293, Loss: 0.5708511471748352\n",
      "Epoch 8, Batch 294, Loss: 0.6385540962219238\n",
      "Epoch 8, Batch 295, Loss: 0.5458139777183533\n",
      "Epoch 8, Batch 296, Loss: 0.571081817150116\n",
      "Epoch 8, Batch 297, Loss: 0.4229118227958679\n",
      "Epoch 8, Batch 298, Loss: 0.7372737526893616\n",
      "Epoch 8, Batch 299, Loss: 0.797864556312561\n",
      "Epoch 8, Batch 300, Loss: 0.6108698844909668\n",
      "Epoch 8, Batch 301, Loss: 0.5705825686454773\n",
      "Epoch 8, Batch 302, Loss: 0.5780119299888611\n",
      "Epoch 8, Batch 303, Loss: 0.8314499258995056\n",
      "Epoch 8, Batch 304, Loss: 0.5899816751480103\n",
      "Epoch 8, Batch 305, Loss: 0.587477445602417\n",
      "Epoch 8, Batch 306, Loss: 0.3953770399093628\n",
      "Epoch 8, Batch 307, Loss: 0.584629237651825\n",
      "Epoch 8, Batch 308, Loss: 0.38124480843544006\n",
      "Epoch 8, Batch 309, Loss: 0.718717098236084\n",
      "Epoch 8, Batch 310, Loss: 0.6694933772087097\n",
      "Epoch 8, Batch 311, Loss: 0.5048946738243103\n",
      "Epoch 8, Batch 312, Loss: 0.6166774034500122\n",
      "Epoch 8, Batch 313, Loss: 0.45075899362564087\n",
      "Epoch 8, Batch 314, Loss: 0.5542007088661194\n",
      "Epoch 8, Batch 315, Loss: 0.4882108271121979\n",
      "Epoch 8, Batch 316, Loss: 0.6142871975898743\n",
      "Epoch 8, Batch 317, Loss: 0.5917438268661499\n",
      "Epoch 8, Batch 318, Loss: 0.5401503443717957\n",
      "Epoch 8, Batch 319, Loss: 0.5031101703643799\n",
      "Epoch 8, Batch 320, Loss: 0.6144053936004639\n",
      "Epoch 8, Batch 321, Loss: 0.6011633276939392\n",
      "Epoch 8, Batch 322, Loss: 0.511099636554718\n",
      "Epoch 8, Batch 323, Loss: 0.6886592507362366\n",
      "Epoch 8, Batch 324, Loss: 0.7151210904121399\n",
      "Epoch 8, Batch 325, Loss: 0.439155638217926\n",
      "Epoch 8, Batch 326, Loss: 0.5884391069412231\n",
      "Epoch 8, Batch 327, Loss: 0.6790204048156738\n",
      "Epoch 8, Batch 328, Loss: 0.7706310749053955\n",
      "Epoch 8, Batch 329, Loss: 0.38406670093536377\n",
      "Epoch 8, Batch 330, Loss: 0.48581960797309875\n",
      "Epoch 8, Batch 331, Loss: 0.5653181672096252\n",
      "Epoch 8, Batch 332, Loss: 0.5710690021514893\n",
      "Epoch 8, Batch 333, Loss: 0.5913127064704895\n",
      "Epoch 8, Batch 334, Loss: 0.5770520567893982\n",
      "Epoch 8, Batch 335, Loss: 0.5054003596305847\n",
      "Epoch 8, Batch 336, Loss: 0.7316026091575623\n",
      "Epoch 8, Batch 337, Loss: 0.5748925805091858\n",
      "Epoch 8, Batch 338, Loss: 0.5633546710014343\n",
      "Epoch 8, Batch 339, Loss: 0.6107376217842102\n",
      "Epoch 8, Batch 340, Loss: 0.5115609765052795\n",
      "Epoch 8, Batch 341, Loss: 0.6378061175346375\n",
      "Epoch 8, Batch 342, Loss: 0.38176289200782776\n",
      "Epoch 8, Batch 343, Loss: 0.4200619161128998\n",
      "Epoch 8, Batch 344, Loss: 0.6631201505661011\n",
      "Epoch 8, Batch 345, Loss: 0.4384053349494934\n",
      "Epoch 8, Batch 346, Loss: 0.4095408022403717\n",
      "Epoch 8, Batch 347, Loss: 0.5230869054794312\n",
      "Epoch 8, Batch 348, Loss: 0.7808555364608765\n",
      "Epoch 8, Batch 349, Loss: 0.5231522917747498\n",
      "Epoch 8, Batch 350, Loss: 0.8787257671356201\n",
      "Epoch 8, Batch 351, Loss: 0.5263117551803589\n",
      "Epoch 8, Batch 352, Loss: 0.40285927057266235\n",
      "Epoch 8, Batch 353, Loss: 0.5209876894950867\n",
      "Epoch 8, Batch 354, Loss: 0.4601980447769165\n",
      "Epoch 8, Batch 355, Loss: 0.6336601376533508\n",
      "Epoch 8, Batch 356, Loss: 0.6443011164665222\n",
      "Epoch 8, Batch 357, Loss: 0.7702769041061401\n",
      "Epoch 8, Batch 358, Loss: 0.4292280077934265\n",
      "Epoch 8, Batch 359, Loss: 0.5762040019035339\n",
      "Epoch 8, Batch 360, Loss: 0.5118505954742432\n",
      "Epoch 8, Batch 361, Loss: 0.6375865936279297\n",
      "Epoch 8, Batch 362, Loss: 0.34977734088897705\n",
      "Epoch 8, Batch 363, Loss: 0.5340005159378052\n",
      "Epoch 8, Batch 364, Loss: 0.5132427215576172\n",
      "Epoch 8, Batch 365, Loss: 0.43352195620536804\n",
      "Epoch 8, Batch 366, Loss: 0.43964913487434387\n",
      "Epoch 8, Batch 367, Loss: 0.47068336606025696\n",
      "Epoch 8, Batch 368, Loss: 0.5127797722816467\n",
      "Epoch 8, Batch 369, Loss: 0.5663766264915466\n",
      "Epoch 8, Batch 370, Loss: 0.48387792706489563\n",
      "Epoch 8, Batch 371, Loss: 0.7429665327072144\n",
      "Epoch 8, Batch 372, Loss: 0.7620764374732971\n",
      "Epoch 8, Batch 373, Loss: 0.6902062296867371\n",
      "Epoch 8, Batch 374, Loss: 0.5692742466926575\n",
      "Epoch 8, Batch 375, Loss: 0.43757447600364685\n",
      "Epoch 8, Batch 376, Loss: 0.4742850661277771\n",
      "Epoch 8, Batch 377, Loss: 0.3971174359321594\n",
      "Epoch 8, Batch 378, Loss: 0.5803982615470886\n",
      "Epoch 8, Batch 379, Loss: 0.4780217409133911\n",
      "Epoch 8, Batch 380, Loss: 0.6301639676094055\n",
      "Epoch 8, Batch 381, Loss: 0.6560549736022949\n",
      "Epoch 8, Batch 382, Loss: 0.3758760690689087\n",
      "Epoch 8, Batch 383, Loss: 0.40616682171821594\n",
      "Epoch 8, Batch 384, Loss: 0.4548582136631012\n",
      "Epoch 8, Batch 385, Loss: 0.6934582591056824\n",
      "Epoch 8, Batch 386, Loss: 0.5669557452201843\n",
      "Epoch 8, Batch 387, Loss: 0.31500062346458435\n",
      "Epoch 8, Batch 388, Loss: 0.3813953399658203\n",
      "Epoch 8, Batch 389, Loss: 0.47218772768974304\n",
      "Epoch 8, Batch 390, Loss: 0.6098492741584778\n",
      "Epoch 8, Batch 391, Loss: 0.38093942403793335\n",
      "Epoch 8, Batch 392, Loss: 0.5154941082000732\n",
      "Epoch 8, Batch 393, Loss: 0.5346226692199707\n",
      "Epoch 8, Batch 394, Loss: 0.9714141488075256\n",
      "Epoch 8, Batch 395, Loss: 0.349433958530426\n",
      "Epoch 8, Batch 396, Loss: 0.6520524024963379\n",
      "Epoch 8, Batch 397, Loss: 0.5583414435386658\n",
      "Epoch 8, Batch 398, Loss: 0.49700188636779785\n",
      "Epoch 8, Batch 399, Loss: 0.5453137755393982\n",
      "Epoch 8, Batch 400, Loss: 0.5032228827476501\n",
      "Epoch 8, Batch 401, Loss: 0.3028278946876526\n",
      "Epoch 8, Batch 402, Loss: 0.4834263026714325\n",
      "Epoch 8, Batch 403, Loss: 0.5531822443008423\n",
      "Epoch 8, Batch 404, Loss: 0.5683985352516174\n",
      "Epoch 8, Batch 405, Loss: 0.49209460616111755\n",
      "Epoch 8, Batch 406, Loss: 0.730734646320343\n",
      "Epoch 8, Batch 407, Loss: 0.47731220722198486\n",
      "Epoch 8, Batch 408, Loss: 0.4692511558532715\n",
      "Epoch 8, Batch 409, Loss: 0.5875914096832275\n",
      "Epoch 8, Batch 410, Loss: 0.5842273831367493\n",
      "Epoch 8, Batch 411, Loss: 0.6672650575637817\n",
      "Epoch 8, Batch 412, Loss: 0.587591290473938\n",
      "Epoch 8, Batch 413, Loss: 0.64600670337677\n",
      "Epoch 8, Batch 414, Loss: 0.5496907830238342\n",
      "Epoch 8, Batch 415, Loss: 0.6888432502746582\n",
      "Epoch 8, Batch 416, Loss: 0.4381808340549469\n",
      "Epoch 8, Batch 417, Loss: 0.5156040191650391\n",
      "Epoch 8, Batch 418, Loss: 0.5072649717330933\n",
      "Epoch 8, Batch 419, Loss: 0.4261581301689148\n",
      "Epoch 8, Batch 420, Loss: 0.5297322273254395\n",
      "Epoch 8, Batch 421, Loss: 0.38118866086006165\n",
      "Epoch 8, Batch 422, Loss: 0.6792939901351929\n",
      "Epoch 8, Batch 423, Loss: 0.3778398334980011\n",
      "Epoch 8, Batch 424, Loss: 0.550960898399353\n",
      "Epoch 8, Batch 425, Loss: 0.9083674550056458\n",
      "Epoch 8, Batch 426, Loss: 0.4749692976474762\n",
      "Epoch 8, Batch 427, Loss: 0.6105650663375854\n",
      "Epoch 8, Batch 428, Loss: 0.5318571329116821\n",
      "Epoch 8, Batch 429, Loss: 0.5368250012397766\n",
      "Epoch 8, Batch 430, Loss: 0.5936374068260193\n",
      "Epoch 8, Batch 431, Loss: 0.3353848457336426\n",
      "Epoch 8, Batch 432, Loss: 0.5603257417678833\n",
      "Epoch 8, Batch 433, Loss: 0.4410901367664337\n",
      "Epoch 8, Batch 434, Loss: 0.6706152558326721\n",
      "Epoch 8, Batch 435, Loss: 0.6110252141952515\n",
      "Epoch 8, Batch 436, Loss: 0.6767745018005371\n",
      "Epoch 8, Batch 437, Loss: 0.6063913106918335\n",
      "Epoch 8, Batch 438, Loss: 0.3839222490787506\n",
      "Epoch 8, Batch 439, Loss: 0.49161863327026367\n",
      "Epoch 8, Batch 440, Loss: 0.4334751069545746\n",
      "Epoch 8, Batch 441, Loss: 0.45410218834877014\n",
      "Epoch 8, Batch 442, Loss: 0.439159095287323\n",
      "Epoch 8, Batch 443, Loss: 0.5786798000335693\n",
      "Epoch 8, Batch 444, Loss: 0.5100505352020264\n",
      "Epoch 8, Batch 445, Loss: 0.8270608186721802\n",
      "Epoch 8, Batch 446, Loss: 0.5385898351669312\n",
      "Epoch 8, Batch 447, Loss: 0.45589762926101685\n",
      "Epoch 8, Batch 448, Loss: 0.3879002630710602\n",
      "Epoch 8, Batch 449, Loss: 0.68263840675354\n",
      "Epoch 8, Batch 450, Loss: 0.6106056571006775\n",
      "Epoch 8, Batch 451, Loss: 0.43369966745376587\n",
      "Epoch 8, Batch 452, Loss: 0.7267491817474365\n",
      "Epoch 8, Batch 453, Loss: 0.703482985496521\n",
      "Epoch 8, Batch 454, Loss: 0.599529504776001\n",
      "Epoch 8, Batch 455, Loss: 0.6145148277282715\n",
      "Epoch 8, Batch 456, Loss: 0.5182506442070007\n",
      "Epoch 8, Batch 457, Loss: 0.5426782369613647\n",
      "Epoch 8, Batch 458, Loss: 0.5450803637504578\n",
      "Epoch 8, Batch 459, Loss: 0.7324855327606201\n",
      "Epoch 8, Batch 460, Loss: 0.5042197108268738\n",
      "Epoch 8, Batch 461, Loss: 0.6242913007736206\n",
      "Epoch 8, Batch 462, Loss: 0.439039021730423\n",
      "Epoch 8, Batch 463, Loss: 0.5840429067611694\n",
      "Epoch 8, Batch 464, Loss: 0.9438647031784058\n",
      "Epoch 8, Batch 465, Loss: 0.5132189393043518\n",
      "Epoch 8, Batch 466, Loss: 0.5639222860336304\n",
      "Epoch 8, Batch 467, Loss: 0.432096004486084\n",
      "Epoch 8, Batch 468, Loss: 0.515072226524353\n",
      "Epoch 8, Batch 469, Loss: 0.5030993223190308\n",
      "Epoch 8, Batch 470, Loss: 0.6023032665252686\n",
      "Epoch 8, Batch 471, Loss: 0.8247160315513611\n",
      "Epoch 8, Batch 472, Loss: 0.5135539770126343\n",
      "Epoch 8, Batch 473, Loss: 0.44041186571121216\n",
      "Epoch 8, Batch 474, Loss: 0.6969321966171265\n",
      "Epoch 8, Batch 475, Loss: 0.5654928684234619\n",
      "Epoch 8, Batch 476, Loss: 0.32445719838142395\n",
      "Epoch 8, Batch 477, Loss: 0.6222256422042847\n",
      "Epoch 8, Batch 478, Loss: 0.4111887514591217\n",
      "Epoch 8, Batch 479, Loss: 0.6079095602035522\n",
      "Epoch 8, Batch 480, Loss: 0.5799388289451599\n",
      "Epoch 8, Batch 481, Loss: 0.44512778520584106\n",
      "Epoch 8, Batch 482, Loss: 0.45152679085731506\n",
      "Epoch 8, Batch 483, Loss: 0.6141437292098999\n",
      "Epoch 8, Batch 484, Loss: 0.5686641931533813\n",
      "Epoch 8, Batch 485, Loss: 0.4232698380947113\n",
      "Epoch 8, Batch 486, Loss: 0.6399903297424316\n",
      "Epoch 8, Batch 487, Loss: 0.5203512907028198\n",
      "Epoch 8, Batch 488, Loss: 0.7638682126998901\n",
      "Epoch 8, Batch 489, Loss: 0.43152618408203125\n",
      "Epoch 8, Batch 490, Loss: 0.7047544121742249\n",
      "Epoch 8, Batch 491, Loss: 0.4510636627674103\n",
      "Epoch 8, Batch 492, Loss: 0.5846335291862488\n",
      "Epoch 8, Batch 493, Loss: 0.7600046992301941\n",
      "Epoch 8, Batch 494, Loss: 0.586501955986023\n",
      "Epoch 8, Batch 495, Loss: 0.6239127516746521\n",
      "Epoch 8, Batch 496, Loss: 0.4140130579471588\n",
      "Epoch 8, Batch 497, Loss: 0.5587476491928101\n",
      "Epoch 8, Batch 498, Loss: 0.48854702711105347\n",
      "Epoch 8, Batch 499, Loss: 0.8236258029937744\n",
      "Epoch 8, Batch 500, Loss: 0.6067883968353271\n",
      "Epoch 8, Batch 501, Loss: 0.6284257173538208\n",
      "Epoch 8, Batch 502, Loss: 0.6576353311538696\n",
      "Epoch 8, Batch 503, Loss: 0.6796573996543884\n",
      "Epoch 8, Batch 504, Loss: 0.45709824562072754\n",
      "Epoch 8, Batch 505, Loss: 0.5013132095336914\n",
      "Epoch 8, Batch 506, Loss: 0.5857581496238708\n",
      "Epoch 8, Batch 507, Loss: 0.5576958656311035\n",
      "Epoch 8, Batch 508, Loss: 0.4702910780906677\n",
      "Epoch 8, Batch 509, Loss: 0.5181918144226074\n",
      "Epoch 8, Batch 510, Loss: 0.5622250437736511\n",
      "Epoch 8, Batch 511, Loss: 0.5875142812728882\n",
      "Epoch 8, Batch 512, Loss: 0.8063138127326965\n",
      "Epoch 8, Batch 513, Loss: 0.32864707708358765\n",
      "Epoch 8, Batch 514, Loss: 0.3800491988658905\n",
      "Epoch 8, Batch 515, Loss: 0.6895658373832703\n",
      "Epoch 8, Batch 516, Loss: 0.4973406493663788\n",
      "Epoch 8, Batch 517, Loss: 0.497316837310791\n",
      "Epoch 8, Batch 518, Loss: 0.42472776770591736\n",
      "Epoch 8, Batch 519, Loss: 0.4519001543521881\n",
      "Epoch 8, Batch 520, Loss: 0.5472095012664795\n",
      "Epoch 8, Batch 521, Loss: 0.7281634211540222\n",
      "Epoch 8, Batch 522, Loss: 0.5202962756156921\n",
      "Epoch 8, Batch 523, Loss: 0.5458988547325134\n",
      "Epoch 8, Batch 524, Loss: 0.4420773684978485\n",
      "Epoch 8, Batch 525, Loss: 0.436214804649353\n",
      "Epoch 8, Batch 526, Loss: 0.6486640572547913\n",
      "Epoch 8, Batch 527, Loss: 0.33391299843788147\n",
      "Epoch 8, Batch 528, Loss: 0.40207260847091675\n",
      "Epoch 8, Batch 529, Loss: 0.5799840688705444\n",
      "Epoch 8, Batch 530, Loss: 0.5952639579772949\n",
      "Epoch 8, Batch 531, Loss: 0.45205584168434143\n",
      "Epoch 8, Batch 532, Loss: 0.4892948269844055\n",
      "Epoch 8, Batch 533, Loss: 0.5837145447731018\n",
      "Epoch 8, Batch 534, Loss: 0.54913729429245\n",
      "Epoch 8, Batch 535, Loss: 0.499590128660202\n",
      "Epoch 8, Batch 536, Loss: 0.36810266971588135\n",
      "Epoch 8, Batch 537, Loss: 0.40007326006889343\n",
      "Epoch 8, Batch 538, Loss: 0.5638339519500732\n",
      "Epoch 8, Batch 539, Loss: 0.6563022136688232\n",
      "Epoch 8, Batch 540, Loss: 0.6032569408416748\n",
      "Epoch 8, Batch 541, Loss: 0.46493229269981384\n",
      "Epoch 8, Batch 542, Loss: 0.5868659019470215\n",
      "Epoch 8, Batch 543, Loss: 0.506754994392395\n",
      "Epoch 8, Batch 544, Loss: 0.38030576705932617\n",
      "Epoch 8, Batch 545, Loss: 0.5054593086242676\n",
      "Epoch 8, Batch 546, Loss: 0.5293598175048828\n",
      "Epoch 8, Batch 547, Loss: 0.610981285572052\n",
      "Epoch 8, Batch 548, Loss: 0.39878860116004944\n",
      "Epoch 8, Batch 549, Loss: 0.5878756046295166\n",
      "Epoch 8, Batch 550, Loss: 0.6133086085319519\n",
      "Epoch 8, Batch 551, Loss: 0.5595122575759888\n",
      "Epoch 8, Batch 552, Loss: 0.49210986495018005\n",
      "Epoch 8, Batch 553, Loss: 0.5513840913772583\n",
      "Epoch 8, Batch 554, Loss: 0.6381356120109558\n",
      "Epoch 8, Batch 555, Loss: 0.547308623790741\n",
      "Epoch 8, Batch 556, Loss: 0.45293986797332764\n",
      "Epoch 8, Batch 557, Loss: 0.49713507294654846\n",
      "Epoch 8, Batch 558, Loss: 0.6350375413894653\n",
      "Epoch 8, Batch 559, Loss: 0.49956199526786804\n",
      "Epoch 8, Batch 560, Loss: 0.473810613155365\n",
      "Epoch 8, Batch 561, Loss: 0.4105996787548065\n",
      "Epoch 8, Batch 562, Loss: 0.47624608874320984\n",
      "Epoch 8, Batch 563, Loss: 0.7456836700439453\n",
      "Epoch 8, Batch 564, Loss: 0.48419418931007385\n",
      "Epoch 8, Batch 565, Loss: 0.5403403639793396\n",
      "Epoch 8, Batch 566, Loss: 0.48052430152893066\n",
      "Epoch 8, Batch 567, Loss: 0.5905749797821045\n",
      "Epoch 8, Batch 568, Loss: 0.480421245098114\n",
      "Epoch 8, Batch 569, Loss: 0.7280575633049011\n",
      "Epoch 8, Batch 570, Loss: 0.6222609281539917\n",
      "Epoch 8, Batch 571, Loss: 0.5478651523590088\n",
      "Epoch 8, Batch 572, Loss: 0.49144411087036133\n",
      "Epoch 8, Batch 573, Loss: 0.6415951251983643\n",
      "Epoch 8, Batch 574, Loss: 0.5151248574256897\n",
      "Epoch 8, Batch 575, Loss: 0.5686225891113281\n",
      "Epoch 8, Batch 576, Loss: 0.5787109732627869\n",
      "Epoch 8, Batch 577, Loss: 0.5113520622253418\n",
      "Epoch 8, Batch 578, Loss: 0.4261834919452667\n",
      "Epoch 8, Batch 579, Loss: 0.5077086687088013\n",
      "Epoch 8, Batch 580, Loss: 0.5125712752342224\n",
      "Epoch 8, Batch 581, Loss: 0.6781189441680908\n",
      "Epoch 8, Batch 582, Loss: 0.7275825142860413\n",
      "Epoch 8, Batch 583, Loss: 0.5022600889205933\n",
      "Epoch 8, Batch 584, Loss: 0.4096871614456177\n",
      "Epoch 8, Batch 585, Loss: 0.5000965595245361\n",
      "Epoch 8, Batch 586, Loss: 0.5729427337646484\n",
      "Epoch 8, Batch 587, Loss: 0.5765025019645691\n",
      "Epoch 8, Batch 588, Loss: 0.523471474647522\n",
      "Epoch 8, Batch 589, Loss: 0.5376653075218201\n",
      "Epoch 8, Batch 590, Loss: 0.4227522611618042\n",
      "Epoch 8, Batch 591, Loss: 0.5431548357009888\n",
      "Epoch 8, Batch 592, Loss: 0.5563818216323853\n",
      "Epoch 8, Batch 593, Loss: 0.5017975568771362\n",
      "Epoch 8, Batch 594, Loss: 0.4288095533847809\n",
      "Epoch 8, Batch 595, Loss: 0.7181344628334045\n",
      "Epoch 8, Batch 596, Loss: 0.5088229775428772\n",
      "Epoch 8, Batch 597, Loss: 0.7063618302345276\n",
      "Epoch 8, Batch 598, Loss: 0.40140900015830994\n",
      "Epoch 8, Batch 599, Loss: 0.3151334822177887\n",
      "Epoch 8, Batch 600, Loss: 0.5323496460914612\n",
      "Epoch 8, Batch 601, Loss: 0.7312782406806946\n",
      "Epoch 8, Batch 602, Loss: 0.5883898735046387\n",
      "Epoch 8, Batch 603, Loss: 0.7066119909286499\n",
      "Epoch 8, Batch 604, Loss: 0.5102989077568054\n",
      "Epoch 8, Batch 605, Loss: 0.6079146265983582\n",
      "Epoch 8, Batch 606, Loss: 0.5939881801605225\n",
      "Epoch 8, Batch 607, Loss: 0.500424325466156\n",
      "Epoch 8, Batch 608, Loss: 0.7580472826957703\n",
      "Epoch 8, Batch 609, Loss: 0.509406328201294\n",
      "Epoch 8, Batch 610, Loss: 0.724812388420105\n",
      "Epoch 8, Batch 611, Loss: 0.6065372228622437\n",
      "Epoch 8, Batch 612, Loss: 0.5798113346099854\n",
      "Epoch 8, Batch 613, Loss: 0.5311285257339478\n",
      "Epoch 8, Batch 614, Loss: 0.5381803512573242\n",
      "Epoch 8, Batch 615, Loss: 0.4734663963317871\n",
      "Epoch 8, Batch 616, Loss: 0.5317877531051636\n",
      "Epoch 8, Batch 617, Loss: 0.6574621200561523\n",
      "Epoch 8, Batch 618, Loss: 0.4260615110397339\n",
      "Epoch 8, Batch 619, Loss: 0.517592191696167\n",
      "Epoch 8, Batch 620, Loss: 0.3305269479751587\n",
      "Epoch 8, Batch 621, Loss: 0.6467057466506958\n",
      "Epoch 8, Batch 622, Loss: 0.7427405118942261\n",
      "Epoch 8, Batch 623, Loss: 0.4280923008918762\n",
      "Epoch 8, Batch 624, Loss: 0.4852026104927063\n",
      "Epoch 8, Batch 625, Loss: 0.45120319724082947\n",
      "Epoch 8, Batch 626, Loss: 0.31642672419548035\n",
      "Epoch 8, Batch 627, Loss: 0.43520212173461914\n",
      "Epoch 8, Batch 628, Loss: 0.49284523725509644\n",
      "Epoch 8, Batch 629, Loss: 0.8430736064910889\n",
      "Epoch 8, Batch 630, Loss: 0.634029746055603\n",
      "Epoch 8, Batch 631, Loss: 0.5673691034317017\n",
      "Epoch 8, Batch 632, Loss: 0.5658404231071472\n",
      "Epoch 8, Batch 633, Loss: 0.49543696641921997\n",
      "Epoch 8, Batch 634, Loss: 0.6020606756210327\n",
      "Epoch 8, Batch 635, Loss: 0.6133325695991516\n",
      "Epoch 8, Batch 636, Loss: 0.42626088857650757\n",
      "Epoch 8, Batch 637, Loss: 0.5809041261672974\n",
      "Epoch 8, Batch 638, Loss: 0.48953408002853394\n",
      "Epoch 8, Batch 639, Loss: 0.6522966623306274\n",
      "Epoch 8, Batch 640, Loss: 0.5626391172409058\n",
      "Epoch 8, Batch 641, Loss: 0.5202842354774475\n",
      "Epoch 8, Batch 642, Loss: 0.45522770285606384\n",
      "Epoch 8, Batch 643, Loss: 0.7575462460517883\n",
      "Epoch 8, Batch 644, Loss: 0.6247534155845642\n",
      "Epoch 8, Batch 645, Loss: 0.346976637840271\n",
      "Epoch 8, Batch 646, Loss: 0.5447171926498413\n",
      "Epoch 8, Batch 647, Loss: 0.34496599435806274\n",
      "Epoch 8, Batch 648, Loss: 0.5847913026809692\n",
      "Epoch 8, Batch 649, Loss: 0.31199079751968384\n",
      "Epoch 8, Batch 650, Loss: 0.4998760223388672\n",
      "Epoch 8, Batch 651, Loss: 0.5868411064147949\n",
      "Epoch 8, Batch 652, Loss: 0.5763232111930847\n",
      "Epoch 8, Batch 653, Loss: 0.4984630048274994\n",
      "Epoch 8, Batch 654, Loss: 0.4192807674407959\n",
      "Epoch 8, Batch 655, Loss: 0.716439962387085\n",
      "Epoch 8, Batch 656, Loss: 0.5104323029518127\n",
      "Epoch 8, Batch 657, Loss: 0.6396095752716064\n",
      "Epoch 8, Batch 658, Loss: 0.6132254600524902\n",
      "Epoch 8, Batch 659, Loss: 0.6705687046051025\n",
      "Epoch 8, Batch 660, Loss: 0.6235328316688538\n",
      "Epoch 8, Batch 661, Loss: 0.5383045673370361\n",
      "Epoch 8, Batch 662, Loss: 0.5269210934638977\n",
      "Epoch 8, Batch 663, Loss: 0.5192896127700806\n",
      "Epoch 8, Batch 664, Loss: 0.38978293538093567\n",
      "Epoch 8, Batch 665, Loss: 0.5265268683433533\n",
      "Epoch 8, Batch 666, Loss: 0.5948845744132996\n",
      "Epoch 8, Batch 667, Loss: 0.867667555809021\n",
      "Epoch 8, Batch 668, Loss: 0.433609277009964\n",
      "Epoch 8, Batch 669, Loss: 0.7409831285476685\n",
      "Epoch 8, Batch 670, Loss: 0.4329433739185333\n",
      "Epoch 8, Batch 671, Loss: 0.456981360912323\n",
      "Epoch 8, Batch 672, Loss: 0.5891052484512329\n",
      "Epoch 8, Batch 673, Loss: 0.6087305545806885\n",
      "Epoch 8, Batch 674, Loss: 0.5360644459724426\n",
      "Epoch 8, Batch 675, Loss: 0.4193095564842224\n",
      "Epoch 8, Batch 676, Loss: 0.5086621046066284\n",
      "Epoch 8, Batch 677, Loss: 0.46219760179519653\n",
      "Epoch 8, Batch 678, Loss: 0.47418642044067383\n",
      "Epoch 8, Batch 679, Loss: 0.704434871673584\n",
      "Epoch 8, Batch 680, Loss: 0.4865553379058838\n",
      "Epoch 8, Batch 681, Loss: 0.4200560450553894\n",
      "Epoch 8, Batch 682, Loss: 0.38632845878601074\n",
      "Epoch 8, Batch 683, Loss: 0.5229330658912659\n",
      "Epoch 8, Batch 684, Loss: 0.408626914024353\n",
      "Epoch 8, Batch 685, Loss: 0.7625242471694946\n",
      "Epoch 8, Batch 686, Loss: 0.5127009153366089\n",
      "Epoch 8, Batch 687, Loss: 0.7764090895652771\n",
      "Epoch 8, Batch 688, Loss: 0.48234570026397705\n",
      "Epoch 8, Batch 689, Loss: 0.3378124535083771\n",
      "Epoch 8, Batch 690, Loss: 0.43754008412361145\n",
      "Epoch 8, Batch 691, Loss: 0.5116073489189148\n",
      "Epoch 8, Batch 692, Loss: 0.5369862914085388\n",
      "Epoch 8, Batch 693, Loss: 0.46243050694465637\n",
      "Epoch 8, Batch 694, Loss: 0.7120668888092041\n",
      "Epoch 8, Batch 695, Loss: 0.5243126153945923\n",
      "Epoch 8, Batch 696, Loss: 0.5794264078140259\n",
      "Epoch 8, Batch 697, Loss: 0.4556402564048767\n",
      "Epoch 8, Batch 698, Loss: 0.4269840717315674\n",
      "Epoch 8, Batch 699, Loss: 0.5517529249191284\n",
      "Epoch 8, Batch 700, Loss: 0.44124072790145874\n",
      "Epoch 8, Batch 701, Loss: 0.6115570664405823\n",
      "Epoch 8, Batch 702, Loss: 0.4646212160587311\n",
      "Epoch 8, Batch 703, Loss: 0.5774274468421936\n",
      "Epoch 8, Batch 704, Loss: 0.5737261772155762\n",
      "Epoch 8, Batch 705, Loss: 0.6075403094291687\n",
      "Epoch 8, Batch 706, Loss: 0.4522706866264343\n",
      "Epoch 8, Batch 707, Loss: 0.6007570028305054\n",
      "Epoch 8, Batch 708, Loss: 0.5724601149559021\n",
      "Epoch 8, Batch 709, Loss: 0.6244140267372131\n",
      "Epoch 8, Batch 710, Loss: 0.5222812294960022\n",
      "Epoch 8, Batch 711, Loss: 0.44146621227264404\n",
      "Epoch 8, Batch 712, Loss: 0.5298150181770325\n",
      "Epoch 8, Batch 713, Loss: 0.5596292614936829\n",
      "Epoch 8, Batch 714, Loss: 0.828628659248352\n",
      "Epoch 8, Batch 715, Loss: 0.6089286208152771\n",
      "Epoch 8, Batch 716, Loss: 0.4955439269542694\n",
      "Epoch 8, Batch 717, Loss: 0.3633400499820709\n",
      "Epoch 8, Batch 718, Loss: 0.599947452545166\n",
      "Epoch 8, Batch 719, Loss: 0.6121084690093994\n",
      "Epoch 8, Batch 720, Loss: 0.5238022804260254\n",
      "Epoch 8, Batch 721, Loss: 0.716218888759613\n",
      "Epoch 8, Batch 722, Loss: 0.45948728919029236\n",
      "Epoch 8, Batch 723, Loss: 0.5395174026489258\n",
      "Epoch 8, Batch 724, Loss: 0.734093427658081\n",
      "Epoch 8, Batch 725, Loss: 0.39218220114707947\n",
      "Epoch 8, Batch 726, Loss: 0.5514347553253174\n",
      "Epoch 8, Batch 727, Loss: 0.7158251404762268\n",
      "Epoch 8, Batch 728, Loss: 0.4260185658931732\n",
      "Epoch 8, Batch 729, Loss: 0.6160148978233337\n",
      "Epoch 8, Batch 730, Loss: 0.6804710626602173\n",
      "Epoch 8, Batch 731, Loss: 0.4762026071548462\n",
      "Epoch 8, Batch 732, Loss: 0.49999651312828064\n",
      "Epoch 8, Batch 733, Loss: 0.45263591408729553\n",
      "Epoch 8, Batch 734, Loss: 0.5644075274467468\n",
      "Epoch 8, Batch 735, Loss: 0.5465631484985352\n",
      "Epoch 8, Batch 736, Loss: 0.4518725872039795\n",
      "Epoch 8, Batch 737, Loss: 0.2852611243724823\n",
      "Epoch 8, Batch 738, Loss: 0.563025951385498\n",
      "Epoch 8, Batch 739, Loss: 0.44005370140075684\n",
      "Epoch 8, Batch 740, Loss: 0.3109048902988434\n",
      "Epoch 8, Batch 741, Loss: 0.641736626625061\n",
      "Epoch 8, Batch 742, Loss: 0.5073123574256897\n",
      "Epoch 8, Batch 743, Loss: 0.4585161805152893\n",
      "Epoch 8, Batch 744, Loss: 0.3936307728290558\n",
      "Epoch 8, Batch 745, Loss: 0.5558444261550903\n",
      "Epoch 8, Batch 746, Loss: 0.8525984883308411\n",
      "Epoch 8, Batch 747, Loss: 0.7033291459083557\n",
      "Epoch 8, Batch 748, Loss: 0.7133079171180725\n",
      "Epoch 8, Batch 749, Loss: 0.5539253354072571\n",
      "Epoch 8, Batch 750, Loss: 0.6404823064804077\n",
      "Epoch 8, Batch 751, Loss: 0.6575173139572144\n",
      "Epoch 8, Batch 752, Loss: 0.5054530501365662\n",
      "Epoch 8, Batch 753, Loss: 0.4791170656681061\n",
      "Epoch 8, Batch 754, Loss: 0.6501344442367554\n",
      "Epoch 8, Batch 755, Loss: 0.6677206158638\n",
      "Epoch 8, Batch 756, Loss: 0.3817901611328125\n",
      "Epoch 8, Batch 757, Loss: 0.7294938564300537\n",
      "Epoch 8, Batch 758, Loss: 0.5401726961135864\n",
      "Epoch 8, Batch 759, Loss: 0.5852953791618347\n",
      "Epoch 8, Batch 760, Loss: 0.842617392539978\n",
      "Epoch 8, Batch 761, Loss: 0.6378347277641296\n",
      "Epoch 8, Batch 762, Loss: 0.5984290838241577\n",
      "Epoch 8, Batch 763, Loss: 0.4762268662452698\n",
      "Epoch 8, Batch 764, Loss: 0.4142075181007385\n",
      "Epoch 8, Batch 765, Loss: 0.6266664862632751\n",
      "Epoch 8, Batch 766, Loss: 0.4103723466396332\n",
      "Epoch 8, Batch 767, Loss: 0.5015430450439453\n",
      "Epoch 8, Batch 768, Loss: 0.42230018973350525\n",
      "Epoch 8, Batch 769, Loss: 0.4570314884185791\n",
      "Epoch 8, Batch 770, Loss: 0.5141069293022156\n",
      "Epoch 8, Batch 771, Loss: 0.6247655153274536\n",
      "Epoch 8, Batch 772, Loss: 0.5439611077308655\n",
      "Epoch 8, Batch 773, Loss: 0.7050986289978027\n",
      "Epoch 8, Batch 774, Loss: 0.3351038694381714\n",
      "Epoch 8, Batch 775, Loss: 0.48501694202423096\n",
      "Epoch 8, Batch 776, Loss: 0.7553389072418213\n",
      "Epoch 8, Batch 777, Loss: 0.49656033515930176\n",
      "Epoch 8, Batch 778, Loss: 0.5013315677642822\n",
      "Epoch 8, Batch 779, Loss: 0.6909760236740112\n",
      "Epoch 8, Batch 780, Loss: 0.4318121671676636\n",
      "Epoch 8, Batch 781, Loss: 0.39713695645332336\n",
      "Epoch 8, Batch 782, Loss: 0.7330043315887451\n",
      "Epoch 8, Batch 783, Loss: 0.43848973512649536\n",
      "Epoch 8, Batch 784, Loss: 0.43596023321151733\n",
      "Epoch 8, Batch 785, Loss: 0.6774992346763611\n",
      "Epoch 8, Batch 786, Loss: 0.3733486235141754\n",
      "Epoch 8, Batch 787, Loss: 0.3895324170589447\n",
      "Epoch 8, Batch 788, Loss: 0.5543343424797058\n",
      "Epoch 8, Batch 789, Loss: 0.6260035037994385\n",
      "Epoch 8, Batch 790, Loss: 0.6446766257286072\n",
      "Epoch 8, Batch 791, Loss: 0.5178698301315308\n",
      "Epoch 8, Batch 792, Loss: 0.7023812532424927\n",
      "Epoch 8, Batch 793, Loss: 0.5608034133911133\n",
      "Epoch 8, Batch 794, Loss: 0.4558236598968506\n",
      "Epoch 8, Batch 795, Loss: 0.35290852189064026\n",
      "Epoch 8, Batch 796, Loss: 0.7022918462753296\n",
      "Epoch 8, Batch 797, Loss: 0.5869999527931213\n",
      "Epoch 8, Batch 798, Loss: 0.6159698367118835\n",
      "Epoch 8, Batch 799, Loss: 0.7875826358795166\n",
      "Epoch 8, Batch 800, Loss: 0.5002237558364868\n",
      "Epoch 8, Batch 801, Loss: 0.6547878384590149\n",
      "Epoch 8, Batch 802, Loss: 0.48508599400520325\n",
      "Epoch 8, Batch 803, Loss: 0.36505648493766785\n",
      "Epoch 8, Batch 804, Loss: 0.5788488388061523\n",
      "Epoch 8, Batch 805, Loss: 0.46099957823753357\n",
      "Epoch 8, Batch 806, Loss: 0.6436400413513184\n",
      "Epoch 8, Batch 807, Loss: 0.6135395765304565\n",
      "Epoch 8, Batch 808, Loss: 0.38659030199050903\n",
      "Epoch 8, Batch 809, Loss: 0.6241331696510315\n",
      "Epoch 8, Batch 810, Loss: 0.7206674218177795\n",
      "Epoch 8, Batch 811, Loss: 0.6725518703460693\n",
      "Epoch 8, Batch 812, Loss: 0.4411994516849518\n",
      "Epoch 8, Batch 813, Loss: 0.4404459297657013\n",
      "Epoch 8, Batch 814, Loss: 0.3242252767086029\n",
      "Epoch 8, Batch 815, Loss: 0.688538670539856\n",
      "Epoch 8, Batch 816, Loss: 0.6268602013587952\n",
      "Epoch 8, Batch 817, Loss: 0.46723389625549316\n",
      "Epoch 8, Batch 818, Loss: 0.7371141910552979\n",
      "Epoch 8, Batch 819, Loss: 0.5963795185089111\n",
      "Epoch 8, Batch 820, Loss: 0.4758685529232025\n",
      "Epoch 8, Batch 821, Loss: 0.5238045454025269\n",
      "Epoch 8, Batch 822, Loss: 0.42410022020339966\n",
      "Epoch 8, Batch 823, Loss: 0.6234298348426819\n",
      "Epoch 8, Batch 824, Loss: 0.35365030169487\n",
      "Epoch 8, Batch 825, Loss: 0.5123847126960754\n",
      "Epoch 8, Batch 826, Loss: 0.6168787479400635\n",
      "Epoch 8, Batch 827, Loss: 0.43342775106430054\n",
      "Epoch 8, Batch 828, Loss: 0.5992451906204224\n",
      "Epoch 8, Batch 829, Loss: 0.47196152806282043\n",
      "Epoch 8, Batch 830, Loss: 0.4534897208213806\n",
      "Epoch 8, Batch 831, Loss: 0.5170395970344543\n",
      "Epoch 8, Batch 832, Loss: 0.612923800945282\n",
      "Epoch 8, Batch 833, Loss: 0.5921022295951843\n",
      "Epoch 8, Batch 834, Loss: 0.4209648370742798\n",
      "Epoch 8, Batch 835, Loss: 0.5569883584976196\n",
      "Epoch 8, Batch 836, Loss: 0.6336838006973267\n",
      "Epoch 8, Batch 837, Loss: 0.6367335319519043\n",
      "Epoch 8, Batch 838, Loss: 0.6256235241889954\n",
      "Epoch 8, Batch 839, Loss: 0.5244765281677246\n",
      "Epoch 8, Batch 840, Loss: 0.7911885380744934\n",
      "Epoch 8, Batch 841, Loss: 0.6523419618606567\n",
      "Epoch 8, Batch 842, Loss: 0.4720975458621979\n",
      "Epoch 8, Batch 843, Loss: 0.5677017569541931\n",
      "Epoch 8, Batch 844, Loss: 0.5155993700027466\n",
      "Epoch 8, Batch 845, Loss: 0.40055951476097107\n",
      "Epoch 8, Batch 846, Loss: 0.5520703196525574\n",
      "Epoch 8, Batch 847, Loss: 0.5740063786506653\n",
      "Epoch 8, Batch 848, Loss: 0.48873215913772583\n",
      "Epoch 8, Batch 849, Loss: 0.5440239906311035\n",
      "Epoch 8, Batch 850, Loss: 0.4228067696094513\n",
      "Epoch 8, Batch 851, Loss: 0.4853346645832062\n",
      "Epoch 8, Batch 852, Loss: 0.5444527864456177\n",
      "Epoch 8, Batch 853, Loss: 0.6627692580223083\n",
      "Epoch 8, Batch 854, Loss: 0.8728763461112976\n",
      "Epoch 8, Batch 855, Loss: 0.3886038362979889\n",
      "Epoch 8, Batch 856, Loss: 0.4698438048362732\n",
      "Epoch 8, Batch 857, Loss: 0.5894639492034912\n",
      "Epoch 8, Batch 858, Loss: 0.4326767325401306\n",
      "Epoch 8, Batch 859, Loss: 0.46770673990249634\n",
      "Epoch 8, Batch 860, Loss: 0.41690126061439514\n",
      "Epoch 8, Batch 861, Loss: 0.4697266221046448\n",
      "Epoch 8, Batch 862, Loss: 0.578569769859314\n",
      "Epoch 8, Batch 863, Loss: 0.5640363693237305\n",
      "Epoch 8, Batch 864, Loss: 0.5830546021461487\n",
      "Epoch 8, Batch 865, Loss: 0.5473704934120178\n",
      "Epoch 8, Batch 866, Loss: 0.3693805932998657\n",
      "Epoch 8, Batch 867, Loss: 0.43107569217681885\n",
      "Epoch 8, Batch 868, Loss: 0.4478975534439087\n",
      "Epoch 8, Batch 869, Loss: 0.47457683086395264\n",
      "Epoch 8, Batch 870, Loss: 0.6014233827590942\n",
      "Epoch 8, Batch 871, Loss: 0.42809826135635376\n",
      "Epoch 8, Batch 872, Loss: 0.5783576369285583\n",
      "Epoch 8, Batch 873, Loss: 0.49414125084877014\n",
      "Epoch 8, Batch 874, Loss: 0.4554402530193329\n",
      "Epoch 8, Batch 875, Loss: 0.5425846576690674\n",
      "Epoch 8, Batch 876, Loss: 0.48794835805892944\n",
      "Epoch 8, Batch 877, Loss: 0.5068372488021851\n",
      "Epoch 8, Batch 878, Loss: 0.5506703853607178\n",
      "Epoch 8, Batch 879, Loss: 0.6363470554351807\n",
      "Epoch 8, Batch 880, Loss: 0.7990740537643433\n",
      "Epoch 8, Batch 881, Loss: 0.440510094165802\n",
      "Epoch 8, Batch 882, Loss: 0.9336004257202148\n",
      "Epoch 8, Batch 883, Loss: 0.7229774594306946\n",
      "Epoch 8, Batch 884, Loss: 0.5537876486778259\n",
      "Epoch 8, Batch 885, Loss: 0.5670250058174133\n",
      "Epoch 8, Batch 886, Loss: 0.534846842288971\n",
      "Epoch 8, Batch 887, Loss: 0.831669270992279\n",
      "Epoch 8, Batch 888, Loss: 0.5321376323699951\n",
      "Epoch 8, Batch 889, Loss: 0.4979133903980255\n",
      "Epoch 8, Batch 890, Loss: 0.4566595256328583\n",
      "Epoch 8, Batch 891, Loss: 0.5511879920959473\n",
      "Epoch 8, Batch 892, Loss: 0.5146764516830444\n",
      "Epoch 8, Batch 893, Loss: 0.6033597588539124\n",
      "Epoch 8, Batch 894, Loss: 0.5609723329544067\n",
      "Epoch 8, Batch 895, Loss: 0.6337054967880249\n",
      "Epoch 8, Batch 896, Loss: 0.717495322227478\n",
      "Epoch 8, Batch 897, Loss: 0.5796643495559692\n",
      "Epoch 8, Batch 898, Loss: 0.4820275902748108\n",
      "Epoch 8, Batch 899, Loss: 0.5678836703300476\n",
      "Epoch 8, Batch 900, Loss: 0.5283661484718323\n",
      "Epoch 8, Batch 901, Loss: 0.4873408079147339\n",
      "Epoch 8, Batch 902, Loss: 0.5571426153182983\n",
      "Epoch 8, Batch 903, Loss: 0.43229466676712036\n",
      "Epoch 8, Batch 904, Loss: 0.5006387829780579\n",
      "Epoch 8, Batch 905, Loss: 0.6428422331809998\n",
      "Epoch 8, Batch 906, Loss: 0.8077622652053833\n",
      "Epoch 8, Batch 907, Loss: 0.5731147527694702\n",
      "Epoch 8, Batch 908, Loss: 0.3915392756462097\n",
      "Epoch 8, Batch 909, Loss: 0.5899748206138611\n",
      "Epoch 8, Batch 910, Loss: 0.5559533834457397\n",
      "Epoch 8, Batch 911, Loss: 0.5164092183113098\n",
      "Epoch 8, Batch 912, Loss: 0.5741996765136719\n",
      "Epoch 8, Batch 913, Loss: 0.6449579000473022\n",
      "Epoch 8, Batch 914, Loss: 0.6110047698020935\n",
      "Epoch 8, Batch 915, Loss: 0.5260257720947266\n",
      "Epoch 8, Batch 916, Loss: 0.5876834988594055\n",
      "Epoch 8, Batch 917, Loss: 0.4225156903266907\n",
      "Epoch 8, Batch 918, Loss: 0.45172107219696045\n",
      "Epoch 8, Batch 919, Loss: 0.5030159950256348\n",
      "Epoch 8, Batch 920, Loss: 0.5785804986953735\n",
      "Epoch 8, Batch 921, Loss: 0.5589353442192078\n",
      "Epoch 8, Batch 922, Loss: 0.6795397400856018\n",
      "Epoch 8, Batch 923, Loss: 0.6192634701728821\n",
      "Epoch 8, Batch 924, Loss: 0.3997449576854706\n",
      "Epoch 8, Batch 925, Loss: 0.6879783868789673\n",
      "Epoch 8, Batch 926, Loss: 0.5334672927856445\n",
      "Epoch 8, Batch 927, Loss: 0.4601842164993286\n",
      "Epoch 8, Batch 928, Loss: 0.5725337266921997\n",
      "Epoch 8, Batch 929, Loss: 0.3844009339809418\n",
      "Epoch 8, Batch 930, Loss: 0.5287631750106812\n",
      "Epoch 8, Batch 931, Loss: 0.4557507038116455\n",
      "Epoch 8, Batch 932, Loss: 0.7787439227104187\n",
      "Epoch 8, Batch 933, Loss: 0.46595877408981323\n",
      "Epoch 8, Batch 934, Loss: 0.6152042150497437\n",
      "Epoch 8, Batch 935, Loss: 0.4766860008239746\n",
      "Epoch 8, Batch 936, Loss: 0.4679982364177704\n",
      "Epoch 8, Batch 937, Loss: 0.5871562957763672\n",
      "Epoch 8, Batch 938, Loss: 0.7225614190101624\n",
      "Accuracy of train set: 0.8066666666666666\n",
      "Epoch 8, Batch 1, Test Loss: 0.6262761950492859\n",
      "Epoch 8, Batch 2, Test Loss: 0.6685819625854492\n",
      "Epoch 8, Batch 3, Test Loss: 0.7078778147697449\n",
      "Epoch 8, Batch 4, Test Loss: 0.7058661580085754\n",
      "Epoch 8, Batch 5, Test Loss: 0.4423198699951172\n",
      "Epoch 8, Batch 6, Test Loss: 0.45085829496383667\n",
      "Epoch 8, Batch 7, Test Loss: 0.6235108375549316\n",
      "Epoch 8, Batch 8, Test Loss: 0.5236194729804993\n",
      "Epoch 8, Batch 9, Test Loss: 0.6262762546539307\n",
      "Epoch 8, Batch 10, Test Loss: 0.5964472889900208\n",
      "Epoch 8, Batch 11, Test Loss: 0.5441513657569885\n",
      "Epoch 8, Batch 12, Test Loss: 0.6263259649276733\n",
      "Epoch 8, Batch 13, Test Loss: 0.6363347768783569\n",
      "Epoch 8, Batch 14, Test Loss: 0.4126603901386261\n",
      "Epoch 8, Batch 15, Test Loss: 0.5342977046966553\n",
      "Epoch 8, Batch 16, Test Loss: 0.4866761565208435\n",
      "Epoch 8, Batch 17, Test Loss: 0.5476964116096497\n",
      "Epoch 8, Batch 18, Test Loss: 0.7088009715080261\n",
      "Epoch 8, Batch 19, Test Loss: 0.724666178226471\n",
      "Epoch 8, Batch 20, Test Loss: 0.6271644234657288\n",
      "Epoch 8, Batch 21, Test Loss: 0.42921867966651917\n",
      "Epoch 8, Batch 22, Test Loss: 0.49620330333709717\n",
      "Epoch 8, Batch 23, Test Loss: 0.4345685839653015\n",
      "Epoch 8, Batch 24, Test Loss: 0.6145375370979309\n",
      "Epoch 8, Batch 25, Test Loss: 0.5654041767120361\n",
      "Epoch 8, Batch 26, Test Loss: 0.6002240777015686\n",
      "Epoch 8, Batch 27, Test Loss: 0.44139984250068665\n",
      "Epoch 8, Batch 28, Test Loss: 0.5711971521377563\n",
      "Epoch 8, Batch 29, Test Loss: 0.40445518493652344\n",
      "Epoch 8, Batch 30, Test Loss: 0.5493931174278259\n",
      "Epoch 8, Batch 31, Test Loss: 0.7479332089424133\n",
      "Epoch 8, Batch 32, Test Loss: 0.45862871408462524\n",
      "Epoch 8, Batch 33, Test Loss: 0.7550426125526428\n",
      "Epoch 8, Batch 34, Test Loss: 0.4552382230758667\n",
      "Epoch 8, Batch 35, Test Loss: 0.48501691222190857\n",
      "Epoch 8, Batch 36, Test Loss: 0.46249574422836304\n",
      "Epoch 8, Batch 37, Test Loss: 0.903792142868042\n",
      "Epoch 8, Batch 38, Test Loss: 0.5188232660293579\n",
      "Epoch 8, Batch 39, Test Loss: 0.39099833369255066\n",
      "Epoch 8, Batch 40, Test Loss: 0.7996368408203125\n",
      "Epoch 8, Batch 41, Test Loss: 0.7745813727378845\n",
      "Epoch 8, Batch 42, Test Loss: 0.48216626048088074\n",
      "Epoch 8, Batch 43, Test Loss: 0.44984170794487\n",
      "Epoch 8, Batch 44, Test Loss: 0.37731826305389404\n",
      "Epoch 8, Batch 45, Test Loss: 0.5825962424278259\n",
      "Epoch 8, Batch 46, Test Loss: 0.3453938961029053\n",
      "Epoch 8, Batch 47, Test Loss: 0.5340157151222229\n",
      "Epoch 8, Batch 48, Test Loss: 0.6139324903488159\n",
      "Epoch 8, Batch 49, Test Loss: 0.5456059575080872\n",
      "Epoch 8, Batch 50, Test Loss: 0.49782973527908325\n",
      "Epoch 8, Batch 51, Test Loss: 0.5345023274421692\n",
      "Epoch 8, Batch 52, Test Loss: 0.5673398971557617\n",
      "Epoch 8, Batch 53, Test Loss: 0.5193957686424255\n",
      "Epoch 8, Batch 54, Test Loss: 0.5474295020103455\n",
      "Epoch 8, Batch 55, Test Loss: 0.6688371896743774\n",
      "Epoch 8, Batch 56, Test Loss: 0.6739413142204285\n",
      "Epoch 8, Batch 57, Test Loss: 0.645574688911438\n",
      "Epoch 8, Batch 58, Test Loss: 0.48903894424438477\n",
      "Epoch 8, Batch 59, Test Loss: 0.32740095257759094\n",
      "Epoch 8, Batch 60, Test Loss: 0.5627284049987793\n",
      "Epoch 8, Batch 61, Test Loss: 0.5226548910140991\n",
      "Epoch 8, Batch 62, Test Loss: 0.6038615703582764\n",
      "Epoch 8, Batch 63, Test Loss: 0.5108588933944702\n",
      "Epoch 8, Batch 64, Test Loss: 0.7302461862564087\n",
      "Epoch 8, Batch 65, Test Loss: 0.4608970284461975\n",
      "Epoch 8, Batch 66, Test Loss: 0.43700024485588074\n",
      "Epoch 8, Batch 67, Test Loss: 0.7147946357727051\n",
      "Epoch 8, Batch 68, Test Loss: 0.5103998780250549\n",
      "Epoch 8, Batch 69, Test Loss: 0.4280646741390228\n",
      "Epoch 8, Batch 70, Test Loss: 0.5180521607398987\n",
      "Epoch 8, Batch 71, Test Loss: 0.6325677633285522\n",
      "Epoch 8, Batch 72, Test Loss: 0.7093347907066345\n",
      "Epoch 8, Batch 73, Test Loss: 0.5928738117218018\n",
      "Epoch 8, Batch 74, Test Loss: 0.5113628506660461\n",
      "Epoch 8, Batch 75, Test Loss: 0.4921705722808838\n",
      "Epoch 8, Batch 76, Test Loss: 0.7574343681335449\n",
      "Epoch 8, Batch 77, Test Loss: 0.42518723011016846\n",
      "Epoch 8, Batch 78, Test Loss: 0.5118187069892883\n",
      "Epoch 8, Batch 79, Test Loss: 0.6717479228973389\n",
      "Epoch 8, Batch 80, Test Loss: 0.5145189166069031\n",
      "Epoch 8, Batch 81, Test Loss: 0.526661217212677\n",
      "Epoch 8, Batch 82, Test Loss: 0.49387484788894653\n",
      "Epoch 8, Batch 83, Test Loss: 0.6920551657676697\n",
      "Epoch 8, Batch 84, Test Loss: 0.633313000202179\n",
      "Epoch 8, Batch 85, Test Loss: 0.637704074382782\n",
      "Epoch 8, Batch 86, Test Loss: 0.6734475493431091\n",
      "Epoch 8, Batch 87, Test Loss: 0.7276408076286316\n",
      "Epoch 8, Batch 88, Test Loss: 0.43970099091529846\n",
      "Epoch 8, Batch 89, Test Loss: 0.5921275019645691\n",
      "Epoch 8, Batch 90, Test Loss: 0.481440931558609\n",
      "Epoch 8, Batch 91, Test Loss: 0.5065445303916931\n",
      "Epoch 8, Batch 92, Test Loss: 0.6310135722160339\n",
      "Epoch 8, Batch 93, Test Loss: 0.5393626689910889\n",
      "Epoch 8, Batch 94, Test Loss: 0.43308186531066895\n",
      "Epoch 8, Batch 95, Test Loss: 0.556236982345581\n",
      "Epoch 8, Batch 96, Test Loss: 0.4526023268699646\n",
      "Epoch 8, Batch 97, Test Loss: 0.4402617812156677\n",
      "Epoch 8, Batch 98, Test Loss: 0.548123300075531\n",
      "Epoch 8, Batch 99, Test Loss: 0.7367363572120667\n",
      "Epoch 8, Batch 100, Test Loss: 0.6573231816291809\n",
      "Epoch 8, Batch 101, Test Loss: 0.4683144986629486\n",
      "Epoch 8, Batch 102, Test Loss: 0.4309009611606598\n",
      "Epoch 8, Batch 103, Test Loss: 0.7406779527664185\n",
      "Epoch 8, Batch 104, Test Loss: 0.5638145804405212\n",
      "Epoch 8, Batch 105, Test Loss: 0.49632906913757324\n",
      "Epoch 8, Batch 106, Test Loss: 0.7423464059829712\n",
      "Epoch 8, Batch 107, Test Loss: 0.5341846942901611\n",
      "Epoch 8, Batch 108, Test Loss: 0.4655454158782959\n",
      "Epoch 8, Batch 109, Test Loss: 0.49384576082229614\n",
      "Epoch 8, Batch 110, Test Loss: 0.6571629643440247\n",
      "Epoch 8, Batch 111, Test Loss: 0.636406421661377\n",
      "Epoch 8, Batch 112, Test Loss: 0.6496422290802002\n",
      "Epoch 8, Batch 113, Test Loss: 0.41739338636398315\n",
      "Epoch 8, Batch 114, Test Loss: 0.5129615068435669\n",
      "Epoch 8, Batch 115, Test Loss: 0.7744914293289185\n",
      "Epoch 8, Batch 116, Test Loss: 0.47535157203674316\n",
      "Epoch 8, Batch 117, Test Loss: 0.5199799537658691\n",
      "Epoch 8, Batch 118, Test Loss: 0.548861026763916\n",
      "Epoch 8, Batch 119, Test Loss: 0.4679317772388458\n",
      "Epoch 8, Batch 120, Test Loss: 0.49710729718208313\n",
      "Epoch 8, Batch 121, Test Loss: 0.5439598560333252\n",
      "Epoch 8, Batch 122, Test Loss: 0.6701537370681763\n",
      "Epoch 8, Batch 123, Test Loss: 0.4825440049171448\n",
      "Epoch 8, Batch 124, Test Loss: 0.524392306804657\n",
      "Epoch 8, Batch 125, Test Loss: 0.6420761942863464\n",
      "Epoch 8, Batch 126, Test Loss: 0.4532506465911865\n",
      "Epoch 8, Batch 127, Test Loss: 0.7635264992713928\n",
      "Epoch 8, Batch 128, Test Loss: 0.4795450270175934\n",
      "Epoch 8, Batch 129, Test Loss: 0.5895487070083618\n",
      "Epoch 8, Batch 130, Test Loss: 0.5818488597869873\n",
      "Epoch 8, Batch 131, Test Loss: 0.4108591675758362\n",
      "Epoch 8, Batch 132, Test Loss: 0.4663519859313965\n",
      "Epoch 8, Batch 133, Test Loss: 0.3768678307533264\n",
      "Epoch 8, Batch 134, Test Loss: 0.5697973966598511\n",
      "Epoch 8, Batch 135, Test Loss: 0.5490449666976929\n",
      "Epoch 8, Batch 136, Test Loss: 0.6284907460212708\n",
      "Epoch 8, Batch 137, Test Loss: 0.6117218732833862\n",
      "Epoch 8, Batch 138, Test Loss: 0.4146350920200348\n",
      "Epoch 8, Batch 139, Test Loss: 0.42517560720443726\n",
      "Epoch 8, Batch 140, Test Loss: 0.5884619951248169\n",
      "Epoch 8, Batch 141, Test Loss: 0.4610455632209778\n",
      "Epoch 8, Batch 142, Test Loss: 0.6217237114906311\n",
      "Epoch 8, Batch 143, Test Loss: 0.4498341977596283\n",
      "Epoch 8, Batch 144, Test Loss: 0.5926566123962402\n",
      "Epoch 8, Batch 145, Test Loss: 0.4117947816848755\n",
      "Epoch 8, Batch 146, Test Loss: 0.46260586380958557\n",
      "Epoch 8, Batch 147, Test Loss: 0.490517795085907\n",
      "Epoch 8, Batch 148, Test Loss: 0.5565369129180908\n",
      "Epoch 8, Batch 149, Test Loss: 0.4600338041782379\n",
      "Epoch 8, Batch 150, Test Loss: 0.5184699892997742\n",
      "Epoch 8, Batch 151, Test Loss: 0.32226109504699707\n",
      "Epoch 8, Batch 152, Test Loss: 0.6157861351966858\n",
      "Epoch 8, Batch 153, Test Loss: 0.6251651644706726\n",
      "Epoch 8, Batch 154, Test Loss: 0.5139904022216797\n",
      "Epoch 8, Batch 155, Test Loss: 0.466782808303833\n",
      "Epoch 8, Batch 156, Test Loss: 0.5936543345451355\n",
      "Epoch 8, Batch 157, Test Loss: 0.43931740522384644\n",
      "Epoch 8, Batch 158, Test Loss: 0.4911583662033081\n",
      "Epoch 8, Batch 159, Test Loss: 0.7275195121765137\n",
      "Epoch 8, Batch 160, Test Loss: 0.5008609890937805\n",
      "Epoch 8, Batch 161, Test Loss: 0.7297056317329407\n",
      "Epoch 8, Batch 162, Test Loss: 0.4160989820957184\n",
      "Epoch 8, Batch 163, Test Loss: 0.39299917221069336\n",
      "Epoch 8, Batch 164, Test Loss: 0.38828039169311523\n",
      "Epoch 8, Batch 165, Test Loss: 0.6324565410614014\n",
      "Epoch 8, Batch 166, Test Loss: 0.5096771121025085\n",
      "Epoch 8, Batch 167, Test Loss: 0.6393060684204102\n",
      "Epoch 8, Batch 168, Test Loss: 0.4071241617202759\n",
      "Epoch 8, Batch 169, Test Loss: 0.3741287887096405\n",
      "Epoch 8, Batch 170, Test Loss: 0.45206451416015625\n",
      "Epoch 8, Batch 171, Test Loss: 0.5799909234046936\n",
      "Epoch 8, Batch 172, Test Loss: 0.49540001153945923\n",
      "Epoch 8, Batch 173, Test Loss: 0.760993480682373\n",
      "Epoch 8, Batch 174, Test Loss: 0.7297306060791016\n",
      "Epoch 8, Batch 175, Test Loss: 0.5663970112800598\n",
      "Epoch 8, Batch 176, Test Loss: 0.651299238204956\n",
      "Epoch 8, Batch 177, Test Loss: 0.47707054018974304\n",
      "Epoch 8, Batch 178, Test Loss: 0.6301887035369873\n",
      "Epoch 8, Batch 179, Test Loss: 0.549247145652771\n",
      "Epoch 8, Batch 180, Test Loss: 0.7826045155525208\n",
      "Epoch 8, Batch 181, Test Loss: 0.5209879875183105\n",
      "Epoch 8, Batch 182, Test Loss: 0.6516643762588501\n",
      "Epoch 8, Batch 183, Test Loss: 0.7218973636627197\n",
      "Epoch 8, Batch 184, Test Loss: 0.5066751837730408\n",
      "Epoch 8, Batch 185, Test Loss: 0.41733354330062866\n",
      "Epoch 8, Batch 186, Test Loss: 0.5068531036376953\n",
      "Epoch 8, Batch 187, Test Loss: 0.41513746976852417\n",
      "Epoch 8, Batch 188, Test Loss: 0.5890116095542908\n",
      "Epoch 8, Batch 189, Test Loss: 0.5125486850738525\n",
      "Epoch 8, Batch 190, Test Loss: 0.5193237066268921\n",
      "Epoch 8, Batch 191, Test Loss: 0.42092201113700867\n",
      "Epoch 8, Batch 192, Test Loss: 0.6708298325538635\n",
      "Epoch 8, Batch 193, Test Loss: 0.6710482835769653\n",
      "Epoch 8, Batch 194, Test Loss: 0.522670567035675\n",
      "Epoch 8, Batch 195, Test Loss: 0.519470751285553\n",
      "Epoch 8, Batch 196, Test Loss: 0.41997697949409485\n",
      "Epoch 8, Batch 197, Test Loss: 0.7371277213096619\n",
      "Epoch 8, Batch 198, Test Loss: 0.5306240916252136\n",
      "Epoch 8, Batch 199, Test Loss: 0.6697585582733154\n",
      "Epoch 8, Batch 200, Test Loss: 0.6204538345336914\n",
      "Epoch 8, Batch 201, Test Loss: 0.6080487966537476\n",
      "Epoch 8, Batch 202, Test Loss: 0.4870481789112091\n",
      "Epoch 8, Batch 203, Test Loss: 0.4056963324546814\n",
      "Epoch 8, Batch 204, Test Loss: 0.5509921908378601\n",
      "Epoch 8, Batch 205, Test Loss: 0.7104136943817139\n",
      "Epoch 8, Batch 206, Test Loss: 0.4815402925014496\n",
      "Epoch 8, Batch 207, Test Loss: 0.4332715570926666\n",
      "Epoch 8, Batch 208, Test Loss: 0.550422191619873\n",
      "Epoch 8, Batch 209, Test Loss: 0.5048840045928955\n",
      "Epoch 8, Batch 210, Test Loss: 0.5278201699256897\n",
      "Epoch 8, Batch 211, Test Loss: 0.44208186864852905\n",
      "Epoch 8, Batch 212, Test Loss: 0.4505650997161865\n",
      "Epoch 8, Batch 213, Test Loss: 0.5108288526535034\n",
      "Epoch 8, Batch 214, Test Loss: 0.7172104716300964\n",
      "Epoch 8, Batch 215, Test Loss: 0.6501340866088867\n",
      "Epoch 8, Batch 216, Test Loss: 0.49636879563331604\n",
      "Epoch 8, Batch 217, Test Loss: 0.37170910835266113\n",
      "Epoch 8, Batch 218, Test Loss: 0.4320610761642456\n",
      "Epoch 8, Batch 219, Test Loss: 0.6131545901298523\n",
      "Epoch 8, Batch 220, Test Loss: 0.6981786489486694\n",
      "Epoch 8, Batch 221, Test Loss: 0.5413342118263245\n",
      "Epoch 8, Batch 222, Test Loss: 0.5145558714866638\n",
      "Epoch 8, Batch 223, Test Loss: 0.6065603494644165\n",
      "Epoch 8, Batch 224, Test Loss: 0.368379682302475\n",
      "Epoch 8, Batch 225, Test Loss: 0.32660073041915894\n",
      "Epoch 8, Batch 226, Test Loss: 0.631439745426178\n",
      "Epoch 8, Batch 227, Test Loss: 0.6908246278762817\n",
      "Epoch 8, Batch 228, Test Loss: 0.49953198432922363\n",
      "Epoch 8, Batch 229, Test Loss: 0.6998065710067749\n",
      "Epoch 8, Batch 230, Test Loss: 0.4085034728050232\n",
      "Epoch 8, Batch 231, Test Loss: 0.5605840086936951\n",
      "Epoch 8, Batch 232, Test Loss: 0.5310779809951782\n",
      "Epoch 8, Batch 233, Test Loss: 0.5039615035057068\n",
      "Epoch 8, Batch 234, Test Loss: 0.4677002429962158\n",
      "Epoch 8, Batch 235, Test Loss: 0.763893723487854\n",
      "Epoch 8, Batch 236, Test Loss: 0.47385573387145996\n",
      "Epoch 8, Batch 237, Test Loss: 0.6230156421661377\n",
      "Epoch 8, Batch 238, Test Loss: 0.5423198342323303\n",
      "Epoch 8, Batch 239, Test Loss: 0.37533852458000183\n",
      "Epoch 8, Batch 240, Test Loss: 0.5693252682685852\n",
      "Epoch 8, Batch 241, Test Loss: 0.5200613141059875\n",
      "Epoch 8, Batch 242, Test Loss: 0.5658680200576782\n",
      "Epoch 8, Batch 243, Test Loss: 0.5523902773857117\n",
      "Epoch 8, Batch 244, Test Loss: 0.47103577852249146\n",
      "Epoch 8, Batch 245, Test Loss: 0.531174898147583\n",
      "Epoch 8, Batch 246, Test Loss: 0.7120681405067444\n",
      "Epoch 8, Batch 247, Test Loss: 0.8244317770004272\n",
      "Epoch 8, Batch 248, Test Loss: 0.725854218006134\n",
      "Epoch 8, Batch 249, Test Loss: 0.46956226229667664\n",
      "Epoch 8, Batch 250, Test Loss: 0.5793116688728333\n",
      "Epoch 8, Batch 251, Test Loss: 0.44422322511672974\n",
      "Epoch 8, Batch 252, Test Loss: 0.41675734519958496\n",
      "Epoch 8, Batch 253, Test Loss: 0.7144192457199097\n",
      "Epoch 8, Batch 254, Test Loss: 0.47618532180786133\n",
      "Epoch 8, Batch 255, Test Loss: 0.6512413620948792\n",
      "Epoch 8, Batch 256, Test Loss: 0.4852895140647888\n",
      "Epoch 8, Batch 257, Test Loss: 0.5091765522956848\n",
      "Epoch 8, Batch 258, Test Loss: 0.47202375531196594\n",
      "Epoch 8, Batch 259, Test Loss: 0.4666309058666229\n",
      "Epoch 8, Batch 260, Test Loss: 0.6599462628364563\n",
      "Epoch 8, Batch 261, Test Loss: 0.5698063969612122\n",
      "Epoch 8, Batch 262, Test Loss: 0.5418822169303894\n",
      "Epoch 8, Batch 263, Test Loss: 0.5546814799308777\n",
      "Epoch 8, Batch 264, Test Loss: 0.8182068467140198\n",
      "Epoch 8, Batch 265, Test Loss: 0.6953408718109131\n",
      "Epoch 8, Batch 266, Test Loss: 0.45554453134536743\n",
      "Epoch 8, Batch 267, Test Loss: 0.7726542949676514\n",
      "Epoch 8, Batch 268, Test Loss: 0.5730366706848145\n",
      "Epoch 8, Batch 269, Test Loss: 0.5032958984375\n",
      "Epoch 8, Batch 270, Test Loss: 0.6811419725418091\n",
      "Epoch 8, Batch 271, Test Loss: 0.32376620173454285\n",
      "Epoch 8, Batch 272, Test Loss: 0.6613677144050598\n",
      "Epoch 8, Batch 273, Test Loss: 0.4287318289279938\n",
      "Epoch 8, Batch 274, Test Loss: 0.5280418992042542\n",
      "Epoch 8, Batch 275, Test Loss: 0.847252607345581\n",
      "Epoch 8, Batch 276, Test Loss: 0.3838373124599457\n",
      "Epoch 8, Batch 277, Test Loss: 0.623741626739502\n",
      "Epoch 8, Batch 278, Test Loss: 0.6170414090156555\n",
      "Epoch 8, Batch 279, Test Loss: 0.595052182674408\n",
      "Epoch 8, Batch 280, Test Loss: 0.4373059570789337\n",
      "Epoch 8, Batch 281, Test Loss: 0.619682788848877\n",
      "Epoch 8, Batch 282, Test Loss: 0.4742928147315979\n",
      "Epoch 8, Batch 283, Test Loss: 0.39242756366729736\n",
      "Epoch 8, Batch 284, Test Loss: 0.5247154235839844\n",
      "Epoch 8, Batch 285, Test Loss: 0.5688819885253906\n",
      "Epoch 8, Batch 286, Test Loss: 0.6599158048629761\n",
      "Epoch 8, Batch 287, Test Loss: 0.42965996265411377\n",
      "Epoch 8, Batch 288, Test Loss: 0.5744780898094177\n",
      "Epoch 8, Batch 289, Test Loss: 0.6432288289070129\n",
      "Epoch 8, Batch 290, Test Loss: 0.48929542303085327\n",
      "Epoch 8, Batch 291, Test Loss: 0.6255592703819275\n",
      "Epoch 8, Batch 292, Test Loss: 0.49141770601272583\n",
      "Epoch 8, Batch 293, Test Loss: 0.5664713978767395\n",
      "Epoch 8, Batch 294, Test Loss: 0.6591053009033203\n",
      "Epoch 8, Batch 295, Test Loss: 0.6553608775138855\n",
      "Epoch 8, Batch 296, Test Loss: 0.7678496837615967\n",
      "Epoch 8, Batch 297, Test Loss: 0.40396928787231445\n",
      "Epoch 8, Batch 298, Test Loss: 0.5322391986846924\n",
      "Epoch 8, Batch 299, Test Loss: 0.512943685054779\n",
      "Epoch 8, Batch 300, Test Loss: 0.42618459463119507\n",
      "Epoch 8, Batch 301, Test Loss: 0.5217456817626953\n",
      "Epoch 8, Batch 302, Test Loss: 0.7699348330497742\n",
      "Epoch 8, Batch 303, Test Loss: 0.43149006366729736\n",
      "Epoch 8, Batch 304, Test Loss: 0.6426430344581604\n",
      "Epoch 8, Batch 305, Test Loss: 0.5698215961456299\n",
      "Epoch 8, Batch 306, Test Loss: 0.4482635259628296\n",
      "Epoch 8, Batch 307, Test Loss: 0.6642478704452515\n",
      "Epoch 8, Batch 308, Test Loss: 0.5883001089096069\n",
      "Epoch 8, Batch 309, Test Loss: 0.570292055606842\n",
      "Epoch 8, Batch 310, Test Loss: 0.6092057228088379\n",
      "Epoch 8, Batch 311, Test Loss: 0.7155884504318237\n",
      "Epoch 8, Batch 312, Test Loss: 0.5562601685523987\n",
      "Epoch 8, Batch 313, Test Loss: 0.5931555032730103\n",
      "Epoch 8, Batch 314, Test Loss: 0.4607575833797455\n",
      "Epoch 8, Batch 315, Test Loss: 0.5478477478027344\n",
      "Epoch 8, Batch 316, Test Loss: 0.5420252680778503\n",
      "Epoch 8, Batch 317, Test Loss: 0.5794428586959839\n",
      "Epoch 8, Batch 318, Test Loss: 0.4641302227973938\n",
      "Epoch 8, Batch 319, Test Loss: 0.42290791869163513\n",
      "Epoch 8, Batch 320, Test Loss: 0.5351568460464478\n",
      "Epoch 8, Batch 321, Test Loss: 0.8250033259391785\n",
      "Epoch 8, Batch 322, Test Loss: 0.601542592048645\n",
      "Epoch 8, Batch 323, Test Loss: 0.5058358311653137\n",
      "Epoch 8, Batch 324, Test Loss: 0.39840438961982727\n",
      "Epoch 8, Batch 325, Test Loss: 0.6122708916664124\n",
      "Epoch 8, Batch 326, Test Loss: 0.738325297832489\n",
      "Epoch 8, Batch 327, Test Loss: 0.5036787390708923\n",
      "Epoch 8, Batch 328, Test Loss: 0.43445664644241333\n",
      "Epoch 8, Batch 329, Test Loss: 0.6560500860214233\n",
      "Epoch 8, Batch 330, Test Loss: 0.68975430727005\n",
      "Epoch 8, Batch 331, Test Loss: 0.5147368311882019\n",
      "Epoch 8, Batch 332, Test Loss: 0.5860644578933716\n",
      "Epoch 8, Batch 333, Test Loss: 0.5692707300186157\n",
      "Epoch 8, Batch 334, Test Loss: 0.3984779715538025\n",
      "Epoch 8, Batch 335, Test Loss: 0.4652239680290222\n",
      "Epoch 8, Batch 336, Test Loss: 0.6231655478477478\n",
      "Epoch 8, Batch 337, Test Loss: 0.49393367767333984\n",
      "Epoch 8, Batch 338, Test Loss: 0.4915236532688141\n",
      "Epoch 8, Batch 339, Test Loss: 0.5255963206291199\n",
      "Epoch 8, Batch 340, Test Loss: 0.40567705035209656\n",
      "Epoch 8, Batch 341, Test Loss: 0.5282011032104492\n",
      "Epoch 8, Batch 342, Test Loss: 0.5488948822021484\n",
      "Epoch 8, Batch 343, Test Loss: 0.6368699669837952\n",
      "Epoch 8, Batch 344, Test Loss: 0.6481386423110962\n",
      "Epoch 8, Batch 345, Test Loss: 0.4642437696456909\n",
      "Epoch 8, Batch 346, Test Loss: 0.4518047869205475\n",
      "Epoch 8, Batch 347, Test Loss: 0.4990089535713196\n",
      "Epoch 8, Batch 348, Test Loss: 0.4960607886314392\n",
      "Epoch 8, Batch 349, Test Loss: 0.6030532121658325\n",
      "Epoch 8, Batch 350, Test Loss: 0.7083203792572021\n",
      "Epoch 8, Batch 351, Test Loss: 0.4778023660182953\n",
      "Epoch 8, Batch 352, Test Loss: 0.6654748320579529\n",
      "Epoch 8, Batch 353, Test Loss: 0.5105943083763123\n",
      "Epoch 8, Batch 354, Test Loss: 0.5742378830909729\n",
      "Epoch 8, Batch 355, Test Loss: 0.6075499057769775\n",
      "Epoch 8, Batch 356, Test Loss: 0.5514867901802063\n",
      "Epoch 8, Batch 357, Test Loss: 0.8068841695785522\n",
      "Epoch 8, Batch 358, Test Loss: 0.6152749061584473\n",
      "Epoch 8, Batch 359, Test Loss: 0.7178212404251099\n",
      "Epoch 8, Batch 360, Test Loss: 0.6268013715744019\n",
      "Epoch 8, Batch 361, Test Loss: 0.37804627418518066\n",
      "Epoch 8, Batch 362, Test Loss: 0.3630111813545227\n",
      "Epoch 8, Batch 363, Test Loss: 0.535431981086731\n",
      "Epoch 8, Batch 364, Test Loss: 0.5732210874557495\n",
      "Epoch 8, Batch 365, Test Loss: 0.39099279046058655\n",
      "Epoch 8, Batch 366, Test Loss: 0.45593950152397156\n",
      "Epoch 8, Batch 367, Test Loss: 0.5734751224517822\n",
      "Epoch 8, Batch 368, Test Loss: 0.40856367349624634\n",
      "Epoch 8, Batch 369, Test Loss: 0.46015626192092896\n",
      "Epoch 8, Batch 370, Test Loss: 0.5789772272109985\n",
      "Epoch 8, Batch 371, Test Loss: 0.5725200772285461\n",
      "Epoch 8, Batch 372, Test Loss: 0.631141185760498\n",
      "Epoch 8, Batch 373, Test Loss: 0.42634931206703186\n",
      "Epoch 8, Batch 374, Test Loss: 0.6096420288085938\n",
      "Epoch 8, Batch 375, Test Loss: 0.7190899848937988\n",
      "Epoch 8, Batch 376, Test Loss: 0.5506701469421387\n",
      "Epoch 8, Batch 377, Test Loss: 0.5614452362060547\n",
      "Epoch 8, Batch 378, Test Loss: 0.7326599359512329\n",
      "Epoch 8, Batch 379, Test Loss: 0.37740856409072876\n",
      "Epoch 8, Batch 380, Test Loss: 0.6403942704200745\n",
      "Epoch 8, Batch 381, Test Loss: 0.5098981857299805\n",
      "Epoch 8, Batch 382, Test Loss: 0.45401203632354736\n",
      "Epoch 8, Batch 383, Test Loss: 0.4789220094680786\n",
      "Epoch 8, Batch 384, Test Loss: 0.6003576517105103\n",
      "Epoch 8, Batch 385, Test Loss: 0.546662449836731\n",
      "Epoch 8, Batch 386, Test Loss: 0.5460659861564636\n",
      "Epoch 8, Batch 387, Test Loss: 0.5072413086891174\n",
      "Epoch 8, Batch 388, Test Loss: 0.5789816379547119\n",
      "Epoch 8, Batch 389, Test Loss: 0.4219438135623932\n",
      "Epoch 8, Batch 390, Test Loss: 0.5164666175842285\n",
      "Epoch 8, Batch 391, Test Loss: 0.5034569501876831\n",
      "Epoch 8, Batch 392, Test Loss: 0.5108926892280579\n",
      "Epoch 8, Batch 393, Test Loss: 0.4993263781070709\n",
      "Epoch 8, Batch 394, Test Loss: 0.5583875179290771\n",
      "Epoch 8, Batch 395, Test Loss: 0.5580970048904419\n",
      "Epoch 8, Batch 396, Test Loss: 0.5507029294967651\n",
      "Epoch 8, Batch 397, Test Loss: 0.5025343894958496\n",
      "Epoch 8, Batch 398, Test Loss: 0.4766528904438019\n",
      "Epoch 8, Batch 399, Test Loss: 0.6765384078025818\n",
      "Epoch 8, Batch 400, Test Loss: 0.631920337677002\n",
      "Epoch 8, Batch 401, Test Loss: 0.4302201271057129\n",
      "Epoch 8, Batch 402, Test Loss: 0.5961194038391113\n",
      "Epoch 8, Batch 403, Test Loss: 0.5397011041641235\n",
      "Epoch 8, Batch 404, Test Loss: 0.5799051523208618\n",
      "Epoch 8, Batch 405, Test Loss: 0.682155966758728\n",
      "Epoch 8, Batch 406, Test Loss: 0.4790214002132416\n",
      "Epoch 8, Batch 407, Test Loss: 0.9101026058197021\n",
      "Epoch 8, Batch 408, Test Loss: 0.563651978969574\n",
      "Epoch 8, Batch 409, Test Loss: 0.9161828756332397\n",
      "Epoch 8, Batch 410, Test Loss: 0.4842768907546997\n",
      "Epoch 8, Batch 411, Test Loss: 0.48848703503608704\n",
      "Epoch 8, Batch 412, Test Loss: 0.4910280704498291\n",
      "Epoch 8, Batch 413, Test Loss: 0.5174617171287537\n",
      "Epoch 8, Batch 414, Test Loss: 0.525169312953949\n",
      "Epoch 8, Batch 415, Test Loss: 0.619253396987915\n",
      "Epoch 8, Batch 416, Test Loss: 0.4680803716182709\n",
      "Epoch 8, Batch 417, Test Loss: 0.43009835481643677\n",
      "Epoch 8, Batch 418, Test Loss: 0.5929473042488098\n",
      "Epoch 8, Batch 419, Test Loss: 0.4675142168998718\n",
      "Epoch 8, Batch 420, Test Loss: 0.5874292850494385\n",
      "Epoch 8, Batch 421, Test Loss: 0.3668602406978607\n",
      "Epoch 8, Batch 422, Test Loss: 0.6549219489097595\n",
      "Epoch 8, Batch 423, Test Loss: 0.4897267520427704\n",
      "Epoch 8, Batch 424, Test Loss: 0.7590770721435547\n",
      "Epoch 8, Batch 425, Test Loss: 0.4560774266719818\n",
      "Epoch 8, Batch 426, Test Loss: 0.5806490778923035\n",
      "Epoch 8, Batch 427, Test Loss: 0.4691998064517975\n",
      "Epoch 8, Batch 428, Test Loss: 0.5567673444747925\n",
      "Epoch 8, Batch 429, Test Loss: 0.5535274744033813\n",
      "Epoch 8, Batch 430, Test Loss: 0.7074480652809143\n",
      "Epoch 8, Batch 431, Test Loss: 0.4772379398345947\n",
      "Epoch 8, Batch 432, Test Loss: 0.523842990398407\n",
      "Epoch 8, Batch 433, Test Loss: 0.7357841730117798\n",
      "Epoch 8, Batch 434, Test Loss: 0.47401919960975647\n",
      "Epoch 8, Batch 435, Test Loss: 0.4526846408843994\n",
      "Epoch 8, Batch 436, Test Loss: 0.41072309017181396\n",
      "Epoch 8, Batch 437, Test Loss: 0.6427996754646301\n",
      "Epoch 8, Batch 438, Test Loss: 0.45049214363098145\n",
      "Epoch 8, Batch 439, Test Loss: 0.46484869718551636\n",
      "Epoch 8, Batch 440, Test Loss: 0.4279620349407196\n",
      "Epoch 8, Batch 441, Test Loss: 0.5790234208106995\n",
      "Epoch 8, Batch 442, Test Loss: 0.4879800081253052\n",
      "Epoch 8, Batch 443, Test Loss: 0.5482577681541443\n",
      "Epoch 8, Batch 444, Test Loss: 0.6614705920219421\n",
      "Epoch 8, Batch 445, Test Loss: 0.7451329231262207\n",
      "Epoch 8, Batch 446, Test Loss: 0.6209850311279297\n",
      "Epoch 8, Batch 447, Test Loss: 0.4857323467731476\n",
      "Epoch 8, Batch 448, Test Loss: 0.5574803948402405\n",
      "Epoch 8, Batch 449, Test Loss: 0.6217432022094727\n",
      "Epoch 8, Batch 450, Test Loss: 0.5645937919616699\n",
      "Epoch 8, Batch 451, Test Loss: 0.7030346393585205\n",
      "Epoch 8, Batch 452, Test Loss: 0.5148773789405823\n",
      "Epoch 8, Batch 453, Test Loss: 0.4676103889942169\n",
      "Epoch 8, Batch 454, Test Loss: 0.6036491394042969\n",
      "Epoch 8, Batch 455, Test Loss: 0.8313363790512085\n",
      "Epoch 8, Batch 456, Test Loss: 0.49789291620254517\n",
      "Epoch 8, Batch 457, Test Loss: 0.6532959342002869\n",
      "Epoch 8, Batch 458, Test Loss: 0.5982358455657959\n",
      "Epoch 8, Batch 459, Test Loss: 0.5094622373580933\n",
      "Epoch 8, Batch 460, Test Loss: 0.4226490259170532\n",
      "Epoch 8, Batch 461, Test Loss: 0.5718274116516113\n",
      "Epoch 8, Batch 462, Test Loss: 0.7090089321136475\n",
      "Epoch 8, Batch 463, Test Loss: 0.5370655655860901\n",
      "Epoch 8, Batch 464, Test Loss: 0.6956372857093811\n",
      "Epoch 8, Batch 465, Test Loss: 0.6815045475959778\n",
      "Epoch 8, Batch 466, Test Loss: 0.8068524599075317\n",
      "Epoch 8, Batch 467, Test Loss: 0.5392435789108276\n",
      "Epoch 8, Batch 468, Test Loss: 0.480557918548584\n",
      "Epoch 8, Batch 469, Test Loss: 0.4883692264556885\n",
      "Epoch 8, Batch 470, Test Loss: 0.48115450143814087\n",
      "Epoch 8, Batch 471, Test Loss: 0.5184887647628784\n",
      "Epoch 8, Batch 472, Test Loss: 0.5555617213249207\n",
      "Epoch 8, Batch 473, Test Loss: 0.4376802444458008\n",
      "Epoch 8, Batch 474, Test Loss: 0.5218206644058228\n",
      "Epoch 8, Batch 475, Test Loss: 0.5313580632209778\n",
      "Epoch 8, Batch 476, Test Loss: 0.5267534255981445\n",
      "Epoch 8, Batch 477, Test Loss: 0.6127376556396484\n",
      "Epoch 8, Batch 478, Test Loss: 0.694934070110321\n",
      "Epoch 8, Batch 479, Test Loss: 0.5829540491104126\n",
      "Epoch 8, Batch 480, Test Loss: 0.46495991945266724\n",
      "Epoch 8, Batch 481, Test Loss: 0.3613702058792114\n",
      "Epoch 8, Batch 482, Test Loss: 0.44503557682037354\n",
      "Epoch 8, Batch 483, Test Loss: 0.609581708908081\n",
      "Epoch 8, Batch 484, Test Loss: 0.5034730434417725\n",
      "Epoch 8, Batch 485, Test Loss: 0.5495167374610901\n",
      "Epoch 8, Batch 486, Test Loss: 0.5705535411834717\n",
      "Epoch 8, Batch 487, Test Loss: 0.5764390826225281\n",
      "Epoch 8, Batch 488, Test Loss: 0.6031118035316467\n",
      "Epoch 8, Batch 489, Test Loss: 0.4266096353530884\n",
      "Epoch 8, Batch 490, Test Loss: 0.4921436011791229\n",
      "Epoch 8, Batch 491, Test Loss: 0.5199385285377502\n",
      "Epoch 8, Batch 492, Test Loss: 0.41223427653312683\n",
      "Epoch 8, Batch 493, Test Loss: 0.6121225953102112\n",
      "Epoch 8, Batch 494, Test Loss: 0.6348451375961304\n",
      "Epoch 8, Batch 495, Test Loss: 0.5193756818771362\n",
      "Epoch 8, Batch 496, Test Loss: 0.4829259216785431\n",
      "Epoch 8, Batch 497, Test Loss: 0.5999296307563782\n",
      "Epoch 8, Batch 498, Test Loss: 0.42486482858657837\n",
      "Epoch 8, Batch 499, Test Loss: 0.948250949382782\n",
      "Epoch 8, Batch 500, Test Loss: 0.4297346770763397\n",
      "Epoch 8, Batch 501, Test Loss: 0.49888062477111816\n",
      "Epoch 8, Batch 502, Test Loss: 0.8126683831214905\n",
      "Epoch 8, Batch 503, Test Loss: 0.6018080115318298\n",
      "Epoch 8, Batch 504, Test Loss: 0.5498880743980408\n",
      "Epoch 8, Batch 505, Test Loss: 0.5075225830078125\n",
      "Epoch 8, Batch 506, Test Loss: 0.5999319553375244\n",
      "Epoch 8, Batch 507, Test Loss: 0.6123572587966919\n",
      "Epoch 8, Batch 508, Test Loss: 0.5507591366767883\n",
      "Epoch 8, Batch 509, Test Loss: 0.5628162026405334\n",
      "Epoch 8, Batch 510, Test Loss: 0.47907060384750366\n",
      "Epoch 8, Batch 511, Test Loss: 0.5138658285140991\n",
      "Epoch 8, Batch 512, Test Loss: 0.7359486222267151\n",
      "Epoch 8, Batch 513, Test Loss: 0.46271687746047974\n",
      "Epoch 8, Batch 514, Test Loss: 0.7316461205482483\n",
      "Epoch 8, Batch 515, Test Loss: 0.7866705060005188\n",
      "Epoch 8, Batch 516, Test Loss: 0.6179807782173157\n",
      "Epoch 8, Batch 517, Test Loss: 0.5712236166000366\n",
      "Epoch 8, Batch 518, Test Loss: 0.5870786309242249\n",
      "Epoch 8, Batch 519, Test Loss: 0.6006363034248352\n",
      "Epoch 8, Batch 520, Test Loss: 0.43958455324172974\n",
      "Epoch 8, Batch 521, Test Loss: 0.7811530828475952\n",
      "Epoch 8, Batch 522, Test Loss: 0.5167039632797241\n",
      "Epoch 8, Batch 523, Test Loss: 0.5377916693687439\n",
      "Epoch 8, Batch 524, Test Loss: 0.6282622218132019\n",
      "Epoch 8, Batch 525, Test Loss: 0.6087720394134521\n",
      "Epoch 8, Batch 526, Test Loss: 0.40484750270843506\n",
      "Epoch 8, Batch 527, Test Loss: 0.44762593507766724\n",
      "Epoch 8, Batch 528, Test Loss: 0.549026608467102\n",
      "Epoch 8, Batch 529, Test Loss: 0.742489755153656\n",
      "Epoch 8, Batch 530, Test Loss: 0.6297957301139832\n",
      "Epoch 8, Batch 531, Test Loss: 0.614815890789032\n",
      "Epoch 8, Batch 532, Test Loss: 0.804653525352478\n",
      "Epoch 8, Batch 533, Test Loss: 0.36077848076820374\n",
      "Epoch 8, Batch 534, Test Loss: 0.5236748456954956\n",
      "Epoch 8, Batch 535, Test Loss: 0.5074790716171265\n",
      "Epoch 8, Batch 536, Test Loss: 0.5823179483413696\n",
      "Epoch 8, Batch 537, Test Loss: 0.5884289741516113\n",
      "Epoch 8, Batch 538, Test Loss: 0.4963202476501465\n",
      "Epoch 8, Batch 539, Test Loss: 0.43447718024253845\n",
      "Epoch 8, Batch 540, Test Loss: 0.5199800133705139\n",
      "Epoch 8, Batch 541, Test Loss: 0.6085762977600098\n",
      "Epoch 8, Batch 542, Test Loss: 0.7998972535133362\n",
      "Epoch 8, Batch 543, Test Loss: 0.43167468905448914\n",
      "Epoch 8, Batch 544, Test Loss: 0.6842997670173645\n",
      "Epoch 8, Batch 545, Test Loss: 0.6406649351119995\n",
      "Epoch 8, Batch 546, Test Loss: 0.4780733585357666\n",
      "Epoch 8, Batch 547, Test Loss: 0.7425433397293091\n",
      "Epoch 8, Batch 548, Test Loss: 0.6117162108421326\n",
      "Epoch 8, Batch 549, Test Loss: 0.6157659292221069\n",
      "Epoch 8, Batch 550, Test Loss: 0.5329810380935669\n",
      "Epoch 8, Batch 551, Test Loss: 0.5515052080154419\n",
      "Epoch 8, Batch 552, Test Loss: 0.49866652488708496\n",
      "Epoch 8, Batch 553, Test Loss: 0.5465649366378784\n",
      "Epoch 8, Batch 554, Test Loss: 0.8127153515815735\n",
      "Epoch 8, Batch 555, Test Loss: 0.49695008993148804\n",
      "Epoch 8, Batch 556, Test Loss: 0.5953417420387268\n",
      "Epoch 8, Batch 557, Test Loss: 0.4922211170196533\n",
      "Epoch 8, Batch 558, Test Loss: 0.6284624338150024\n",
      "Epoch 8, Batch 559, Test Loss: 0.5970414280891418\n",
      "Epoch 8, Batch 560, Test Loss: 0.8369351029396057\n",
      "Epoch 8, Batch 561, Test Loss: 0.5025362968444824\n",
      "Epoch 8, Batch 562, Test Loss: 0.4515257775783539\n",
      "Epoch 8, Batch 563, Test Loss: 0.7085767388343811\n",
      "Epoch 8, Batch 564, Test Loss: 0.8273602724075317\n",
      "Epoch 8, Batch 565, Test Loss: 0.6704478859901428\n",
      "Epoch 8, Batch 566, Test Loss: 0.3749257028102875\n",
      "Epoch 8, Batch 567, Test Loss: 0.6202601194381714\n",
      "Epoch 8, Batch 568, Test Loss: 0.7271345853805542\n",
      "Epoch 8, Batch 569, Test Loss: 0.5736885666847229\n",
      "Epoch 8, Batch 570, Test Loss: 0.39443475008010864\n",
      "Epoch 8, Batch 571, Test Loss: 0.720626711845398\n",
      "Epoch 8, Batch 572, Test Loss: 0.6870289444923401\n",
      "Epoch 8, Batch 573, Test Loss: 0.6982600092887878\n",
      "Epoch 8, Batch 574, Test Loss: 0.6481819748878479\n",
      "Epoch 8, Batch 575, Test Loss: 0.5836163759231567\n",
      "Epoch 8, Batch 576, Test Loss: 0.5202835202217102\n",
      "Epoch 8, Batch 577, Test Loss: 0.6197956800460815\n",
      "Epoch 8, Batch 578, Test Loss: 0.5113309621810913\n",
      "Epoch 8, Batch 579, Test Loss: 0.5855292677879333\n",
      "Epoch 8, Batch 580, Test Loss: 0.6500442028045654\n",
      "Epoch 8, Batch 581, Test Loss: 0.6312165260314941\n",
      "Epoch 8, Batch 582, Test Loss: 0.5337353348731995\n",
      "Epoch 8, Batch 583, Test Loss: 0.44600555300712585\n",
      "Epoch 8, Batch 584, Test Loss: 0.5463409423828125\n",
      "Epoch 8, Batch 585, Test Loss: 0.8230942487716675\n",
      "Epoch 8, Batch 586, Test Loss: 0.5112600326538086\n",
      "Epoch 8, Batch 587, Test Loss: 0.49459245800971985\n",
      "Epoch 8, Batch 588, Test Loss: 0.6822031736373901\n",
      "Epoch 8, Batch 589, Test Loss: 0.5845308303833008\n",
      "Epoch 8, Batch 590, Test Loss: 0.5060114860534668\n",
      "Epoch 8, Batch 591, Test Loss: 0.6386364102363586\n",
      "Epoch 8, Batch 592, Test Loss: 0.591712474822998\n",
      "Epoch 8, Batch 593, Test Loss: 0.6568477749824524\n",
      "Epoch 8, Batch 594, Test Loss: 0.6227189302444458\n",
      "Epoch 8, Batch 595, Test Loss: 0.8377264738082886\n",
      "Epoch 8, Batch 596, Test Loss: 0.43625107407569885\n",
      "Epoch 8, Batch 597, Test Loss: 0.5838392972946167\n",
      "Epoch 8, Batch 598, Test Loss: 0.48416638374328613\n",
      "Epoch 8, Batch 599, Test Loss: 0.5607924461364746\n",
      "Epoch 8, Batch 600, Test Loss: 0.4585370421409607\n",
      "Epoch 8, Batch 601, Test Loss: 0.39530450105667114\n",
      "Epoch 8, Batch 602, Test Loss: 0.5267913341522217\n",
      "Epoch 8, Batch 603, Test Loss: 0.5814573168754578\n",
      "Epoch 8, Batch 604, Test Loss: 0.6934359073638916\n",
      "Epoch 8, Batch 605, Test Loss: 0.5597997903823853\n",
      "Epoch 8, Batch 606, Test Loss: 0.5884125232696533\n",
      "Epoch 8, Batch 607, Test Loss: 0.5937485098838806\n",
      "Epoch 8, Batch 608, Test Loss: 0.5527349710464478\n",
      "Epoch 8, Batch 609, Test Loss: 0.793697714805603\n",
      "Epoch 8, Batch 610, Test Loss: 0.5135441422462463\n",
      "Epoch 8, Batch 611, Test Loss: 0.4818326532840729\n",
      "Epoch 8, Batch 612, Test Loss: 0.4455089867115021\n",
      "Epoch 8, Batch 613, Test Loss: 0.6869056224822998\n",
      "Epoch 8, Batch 614, Test Loss: 0.7068620920181274\n",
      "Epoch 8, Batch 615, Test Loss: 0.674551248550415\n",
      "Epoch 8, Batch 616, Test Loss: 0.6423535943031311\n",
      "Epoch 8, Batch 617, Test Loss: 0.6226694583892822\n",
      "Epoch 8, Batch 618, Test Loss: 0.6516862511634827\n",
      "Epoch 8, Batch 619, Test Loss: 0.3993849754333496\n",
      "Epoch 8, Batch 620, Test Loss: 0.5632616877555847\n",
      "Epoch 8, Batch 621, Test Loss: 0.5035390853881836\n",
      "Epoch 8, Batch 622, Test Loss: 0.49138402938842773\n",
      "Epoch 8, Batch 623, Test Loss: 0.4270392954349518\n",
      "Epoch 8, Batch 624, Test Loss: 0.5177544951438904\n",
      "Epoch 8, Batch 625, Test Loss: 0.5991688966751099\n",
      "Epoch 8, Batch 626, Test Loss: 0.4461277425289154\n",
      "Epoch 8, Batch 627, Test Loss: 0.4597066640853882\n",
      "Epoch 8, Batch 628, Test Loss: 0.5950644016265869\n",
      "Epoch 8, Batch 629, Test Loss: 0.558350682258606\n",
      "Epoch 8, Batch 630, Test Loss: 0.41530701518058777\n",
      "Epoch 8, Batch 631, Test Loss: 0.38228151202201843\n",
      "Epoch 8, Batch 632, Test Loss: 0.5548288822174072\n",
      "Epoch 8, Batch 633, Test Loss: 0.5932473540306091\n",
      "Epoch 8, Batch 634, Test Loss: 0.42581743001937866\n",
      "Epoch 8, Batch 635, Test Loss: 0.6742245554924011\n",
      "Epoch 8, Batch 636, Test Loss: 0.434124231338501\n",
      "Epoch 8, Batch 637, Test Loss: 0.8476230502128601\n",
      "Epoch 8, Batch 638, Test Loss: 0.6983230710029602\n",
      "Epoch 8, Batch 639, Test Loss: 0.552139163017273\n",
      "Epoch 8, Batch 640, Test Loss: 0.7268746495246887\n",
      "Epoch 8, Batch 641, Test Loss: 0.5116928815841675\n",
      "Epoch 8, Batch 642, Test Loss: 0.6979656219482422\n",
      "Epoch 8, Batch 643, Test Loss: 0.6045350432395935\n",
      "Epoch 8, Batch 644, Test Loss: 0.508543074131012\n",
      "Epoch 8, Batch 645, Test Loss: 0.5397463440895081\n",
      "Epoch 8, Batch 646, Test Loss: 0.6863015294075012\n",
      "Epoch 8, Batch 647, Test Loss: 0.5788353085517883\n",
      "Epoch 8, Batch 648, Test Loss: 0.4988556504249573\n",
      "Epoch 8, Batch 649, Test Loss: 0.39113637804985046\n",
      "Epoch 8, Batch 650, Test Loss: 0.3421908915042877\n",
      "Epoch 8, Batch 651, Test Loss: 0.5432966351509094\n",
      "Epoch 8, Batch 652, Test Loss: 0.5966314673423767\n",
      "Epoch 8, Batch 653, Test Loss: 0.4946170449256897\n",
      "Epoch 8, Batch 654, Test Loss: 0.58598393201828\n",
      "Epoch 8, Batch 655, Test Loss: 0.5090785026550293\n",
      "Epoch 8, Batch 656, Test Loss: 0.4986772835254669\n",
      "Epoch 8, Batch 657, Test Loss: 0.4914039373397827\n",
      "Epoch 8, Batch 658, Test Loss: 0.5431826114654541\n",
      "Epoch 8, Batch 659, Test Loss: 0.7780482172966003\n",
      "Epoch 8, Batch 660, Test Loss: 0.6781960725784302\n",
      "Epoch 8, Batch 661, Test Loss: 0.7340123653411865\n",
      "Epoch 8, Batch 662, Test Loss: 0.3302285969257355\n",
      "Epoch 8, Batch 663, Test Loss: 0.5638229846954346\n",
      "Epoch 8, Batch 664, Test Loss: 0.40689224004745483\n",
      "Epoch 8, Batch 665, Test Loss: 0.4030453860759735\n",
      "Epoch 8, Batch 666, Test Loss: 0.4607483148574829\n",
      "Epoch 8, Batch 667, Test Loss: 0.518596887588501\n",
      "Epoch 8, Batch 668, Test Loss: 0.4624312222003937\n",
      "Epoch 8, Batch 669, Test Loss: 0.386544406414032\n",
      "Epoch 8, Batch 670, Test Loss: 0.5482012033462524\n",
      "Epoch 8, Batch 671, Test Loss: 0.5298020839691162\n",
      "Epoch 8, Batch 672, Test Loss: 0.5888206958770752\n",
      "Epoch 8, Batch 673, Test Loss: 0.3934556245803833\n",
      "Epoch 8, Batch 674, Test Loss: 0.7899032235145569\n",
      "Epoch 8, Batch 675, Test Loss: 0.7375062704086304\n",
      "Epoch 8, Batch 676, Test Loss: 0.4080146253108978\n",
      "Epoch 8, Batch 677, Test Loss: 0.36887699365615845\n",
      "Epoch 8, Batch 678, Test Loss: 0.39885473251342773\n",
      "Epoch 8, Batch 679, Test Loss: 0.6562709808349609\n",
      "Epoch 8, Batch 680, Test Loss: 0.6172134876251221\n",
      "Epoch 8, Batch 681, Test Loss: 0.5879225134849548\n",
      "Epoch 8, Batch 682, Test Loss: 0.5250838994979858\n",
      "Epoch 8, Batch 683, Test Loss: 0.468553751707077\n",
      "Epoch 8, Batch 684, Test Loss: 0.5282535552978516\n",
      "Epoch 8, Batch 685, Test Loss: 0.49368777871131897\n",
      "Epoch 8, Batch 686, Test Loss: 0.7047603130340576\n",
      "Epoch 8, Batch 687, Test Loss: 0.8867772221565247\n",
      "Epoch 8, Batch 688, Test Loss: 0.8833553791046143\n",
      "Epoch 8, Batch 689, Test Loss: 0.4462452232837677\n",
      "Epoch 8, Batch 690, Test Loss: 0.7543352842330933\n",
      "Epoch 8, Batch 691, Test Loss: 0.5246750116348267\n",
      "Epoch 8, Batch 692, Test Loss: 0.46728119254112244\n",
      "Epoch 8, Batch 693, Test Loss: 0.462482750415802\n",
      "Epoch 8, Batch 694, Test Loss: 0.5245292782783508\n",
      "Epoch 8, Batch 695, Test Loss: 0.571630597114563\n",
      "Epoch 8, Batch 696, Test Loss: 0.6690664887428284\n",
      "Epoch 8, Batch 697, Test Loss: 0.320904016494751\n",
      "Epoch 8, Batch 698, Test Loss: 0.5151649117469788\n",
      "Epoch 8, Batch 699, Test Loss: 0.6107512712478638\n",
      "Epoch 8, Batch 700, Test Loss: 0.4806205928325653\n",
      "Epoch 8, Batch 701, Test Loss: 0.4042358994483948\n",
      "Epoch 8, Batch 702, Test Loss: 0.4428345561027527\n",
      "Epoch 8, Batch 703, Test Loss: 0.632620096206665\n",
      "Epoch 8, Batch 704, Test Loss: 0.6115912199020386\n",
      "Epoch 8, Batch 705, Test Loss: 0.49350589513778687\n",
      "Epoch 8, Batch 706, Test Loss: 0.6866127848625183\n",
      "Epoch 8, Batch 707, Test Loss: 0.45205432176589966\n",
      "Epoch 8, Batch 708, Test Loss: 0.7862274050712585\n",
      "Epoch 8, Batch 709, Test Loss: 0.421786367893219\n",
      "Epoch 8, Batch 710, Test Loss: 0.5883132815361023\n",
      "Epoch 8, Batch 711, Test Loss: 0.80743408203125\n",
      "Epoch 8, Batch 712, Test Loss: 0.5819599628448486\n",
      "Epoch 8, Batch 713, Test Loss: 0.5829576849937439\n",
      "Epoch 8, Batch 714, Test Loss: 0.6069152355194092\n",
      "Epoch 8, Batch 715, Test Loss: 0.5654410719871521\n",
      "Epoch 8, Batch 716, Test Loss: 0.8256466388702393\n",
      "Epoch 8, Batch 717, Test Loss: 0.5881005525588989\n",
      "Epoch 8, Batch 718, Test Loss: 0.7000630497932434\n",
      "Epoch 8, Batch 719, Test Loss: 0.5165320634841919\n",
      "Epoch 8, Batch 720, Test Loss: 0.5394617319107056\n",
      "Epoch 8, Batch 721, Test Loss: 0.7376403212547302\n",
      "Epoch 8, Batch 722, Test Loss: 0.5404746532440186\n",
      "Epoch 8, Batch 723, Test Loss: 0.4319173991680145\n",
      "Epoch 8, Batch 724, Test Loss: 0.4982171654701233\n",
      "Epoch 8, Batch 725, Test Loss: 0.49547481536865234\n",
      "Epoch 8, Batch 726, Test Loss: 0.7519338130950928\n",
      "Epoch 8, Batch 727, Test Loss: 0.6944841146469116\n",
      "Epoch 8, Batch 728, Test Loss: 0.6339188814163208\n",
      "Epoch 8, Batch 729, Test Loss: 0.48465195298194885\n",
      "Epoch 8, Batch 730, Test Loss: 0.5490328669548035\n",
      "Epoch 8, Batch 731, Test Loss: 0.5830419659614563\n",
      "Epoch 8, Batch 732, Test Loss: 0.40090054273605347\n",
      "Epoch 8, Batch 733, Test Loss: 0.6212341785430908\n",
      "Epoch 8, Batch 734, Test Loss: 0.6631858348846436\n",
      "Epoch 8, Batch 735, Test Loss: 0.4285321831703186\n",
      "Epoch 8, Batch 736, Test Loss: 0.5274366140365601\n",
      "Epoch 8, Batch 737, Test Loss: 0.4107934832572937\n",
      "Epoch 8, Batch 738, Test Loss: 0.576629102230072\n",
      "Epoch 8, Batch 739, Test Loss: 0.6393119096755981\n",
      "Epoch 8, Batch 740, Test Loss: 0.47941577434539795\n",
      "Epoch 8, Batch 741, Test Loss: 0.658507227897644\n",
      "Epoch 8, Batch 742, Test Loss: 0.587489664554596\n",
      "Epoch 8, Batch 743, Test Loss: 0.5353474617004395\n",
      "Epoch 8, Batch 744, Test Loss: 0.6218758821487427\n",
      "Epoch 8, Batch 745, Test Loss: 0.6094893217086792\n",
      "Epoch 8, Batch 746, Test Loss: 0.6749663949012756\n",
      "Epoch 8, Batch 747, Test Loss: 0.39537879824638367\n",
      "Epoch 8, Batch 748, Test Loss: 0.5005749464035034\n",
      "Epoch 8, Batch 749, Test Loss: 0.6490079164505005\n",
      "Epoch 8, Batch 750, Test Loss: 0.44925329089164734\n",
      "Epoch 8, Batch 751, Test Loss: 0.37653136253356934\n",
      "Epoch 8, Batch 752, Test Loss: 0.6464313864707947\n",
      "Epoch 8, Batch 753, Test Loss: 0.5851755738258362\n",
      "Epoch 8, Batch 754, Test Loss: 0.5572758913040161\n",
      "Epoch 8, Batch 755, Test Loss: 0.9374174475669861\n",
      "Epoch 8, Batch 756, Test Loss: 0.4983239769935608\n",
      "Epoch 8, Batch 757, Test Loss: 0.3879321813583374\n",
      "Epoch 8, Batch 758, Test Loss: 0.682403028011322\n",
      "Epoch 8, Batch 759, Test Loss: 0.6284291744232178\n",
      "Epoch 8, Batch 760, Test Loss: 0.4263913929462433\n",
      "Epoch 8, Batch 761, Test Loss: 0.5137512683868408\n",
      "Epoch 8, Batch 762, Test Loss: 0.621505856513977\n",
      "Epoch 8, Batch 763, Test Loss: 0.5472392439842224\n",
      "Epoch 8, Batch 764, Test Loss: 0.6365874409675598\n",
      "Epoch 8, Batch 765, Test Loss: 0.6147019267082214\n",
      "Epoch 8, Batch 766, Test Loss: 0.5418621897697449\n",
      "Epoch 8, Batch 767, Test Loss: 0.7034251093864441\n",
      "Epoch 8, Batch 768, Test Loss: 0.6728955507278442\n",
      "Epoch 8, Batch 769, Test Loss: 0.6333969831466675\n",
      "Epoch 8, Batch 770, Test Loss: 0.5010181069374084\n",
      "Epoch 8, Batch 771, Test Loss: 0.6129342913627625\n",
      "Epoch 8, Batch 772, Test Loss: 0.5789545178413391\n",
      "Epoch 8, Batch 773, Test Loss: 0.41491928696632385\n",
      "Epoch 8, Batch 774, Test Loss: 0.675955057144165\n",
      "Epoch 8, Batch 775, Test Loss: 0.6470069289207458\n",
      "Epoch 8, Batch 776, Test Loss: 0.6457626223564148\n",
      "Epoch 8, Batch 777, Test Loss: 0.5863853096961975\n",
      "Epoch 8, Batch 778, Test Loss: 0.7087446451187134\n",
      "Epoch 8, Batch 779, Test Loss: 0.6227830648422241\n",
      "Epoch 8, Batch 780, Test Loss: 0.5265511274337769\n",
      "Epoch 8, Batch 781, Test Loss: 0.7189226150512695\n",
      "Epoch 8, Batch 782, Test Loss: 0.5589081645011902\n",
      "Epoch 8, Batch 783, Test Loss: 0.40574896335601807\n",
      "Epoch 8, Batch 784, Test Loss: 0.46873781085014343\n",
      "Epoch 8, Batch 785, Test Loss: 0.5024371147155762\n",
      "Epoch 8, Batch 786, Test Loss: 0.5913920998573303\n",
      "Epoch 8, Batch 787, Test Loss: 0.5947694778442383\n",
      "Epoch 8, Batch 788, Test Loss: 0.5710908770561218\n",
      "Epoch 8, Batch 789, Test Loss: 0.41143569350242615\n",
      "Epoch 8, Batch 790, Test Loss: 0.5459344983100891\n",
      "Epoch 8, Batch 791, Test Loss: 0.6536170840263367\n",
      "Epoch 8, Batch 792, Test Loss: 0.564215898513794\n",
      "Epoch 8, Batch 793, Test Loss: 0.6381669044494629\n",
      "Epoch 8, Batch 794, Test Loss: 0.6446887850761414\n",
      "Epoch 8, Batch 795, Test Loss: 0.46651020646095276\n",
      "Epoch 8, Batch 796, Test Loss: 0.769996702671051\n",
      "Epoch 8, Batch 797, Test Loss: 0.6792726516723633\n",
      "Epoch 8, Batch 798, Test Loss: 0.5488268136978149\n",
      "Epoch 8, Batch 799, Test Loss: 0.5266093015670776\n",
      "Epoch 8, Batch 800, Test Loss: 0.5308271646499634\n",
      "Epoch 8, Batch 801, Test Loss: 0.4897979497909546\n",
      "Epoch 8, Batch 802, Test Loss: 0.6136883497238159\n",
      "Epoch 8, Batch 803, Test Loss: 0.5178692936897278\n",
      "Epoch 8, Batch 804, Test Loss: 0.45625734329223633\n",
      "Epoch 8, Batch 805, Test Loss: 0.4734821319580078\n",
      "Epoch 8, Batch 806, Test Loss: 0.5095951557159424\n",
      "Epoch 8, Batch 807, Test Loss: 0.5747628211975098\n",
      "Epoch 8, Batch 808, Test Loss: 0.43599778413772583\n",
      "Epoch 8, Batch 809, Test Loss: 0.4861563742160797\n",
      "Epoch 8, Batch 810, Test Loss: 0.47395944595336914\n",
      "Epoch 8, Batch 811, Test Loss: 0.6025034189224243\n",
      "Epoch 8, Batch 812, Test Loss: 0.39294669032096863\n",
      "Epoch 8, Batch 813, Test Loss: 0.6154646873474121\n",
      "Epoch 8, Batch 814, Test Loss: 0.416409969329834\n",
      "Epoch 8, Batch 815, Test Loss: 0.6949962973594666\n",
      "Epoch 8, Batch 816, Test Loss: 0.6171799898147583\n",
      "Epoch 8, Batch 817, Test Loss: 0.6884937286376953\n",
      "Epoch 8, Batch 818, Test Loss: 0.4261060655117035\n",
      "Epoch 8, Batch 819, Test Loss: 0.6644628047943115\n",
      "Epoch 8, Batch 820, Test Loss: 0.48911136388778687\n",
      "Epoch 8, Batch 821, Test Loss: 0.5777207612991333\n",
      "Epoch 8, Batch 822, Test Loss: 0.7470548152923584\n",
      "Epoch 8, Batch 823, Test Loss: 0.32589036226272583\n",
      "Epoch 8, Batch 824, Test Loss: 0.5294833183288574\n",
      "Epoch 8, Batch 825, Test Loss: 0.4911707639694214\n",
      "Epoch 8, Batch 826, Test Loss: 0.5697460174560547\n",
      "Epoch 8, Batch 827, Test Loss: 0.7703561782836914\n",
      "Epoch 8, Batch 828, Test Loss: 0.467104971408844\n",
      "Epoch 8, Batch 829, Test Loss: 0.41804295778274536\n",
      "Epoch 8, Batch 830, Test Loss: 0.616783857345581\n",
      "Epoch 8, Batch 831, Test Loss: 0.4418148398399353\n",
      "Epoch 8, Batch 832, Test Loss: 0.41521015763282776\n",
      "Epoch 8, Batch 833, Test Loss: 0.4081878960132599\n",
      "Epoch 8, Batch 834, Test Loss: 0.5094441771507263\n",
      "Epoch 8, Batch 835, Test Loss: 0.6201930046081543\n",
      "Epoch 8, Batch 836, Test Loss: 0.3438904285430908\n",
      "Epoch 8, Batch 837, Test Loss: 0.47537240386009216\n",
      "Epoch 8, Batch 838, Test Loss: 0.7054578065872192\n",
      "Epoch 8, Batch 839, Test Loss: 0.5145004987716675\n",
      "Epoch 8, Batch 840, Test Loss: 0.4363541901111603\n",
      "Epoch 8, Batch 841, Test Loss: 0.5295282602310181\n",
      "Epoch 8, Batch 842, Test Loss: 0.6131036877632141\n",
      "Epoch 8, Batch 843, Test Loss: 0.6071024537086487\n",
      "Epoch 8, Batch 844, Test Loss: 0.4614557921886444\n",
      "Epoch 8, Batch 845, Test Loss: 0.606411874294281\n",
      "Epoch 8, Batch 846, Test Loss: 1.1075482368469238\n",
      "Epoch 8, Batch 847, Test Loss: 0.5906840562820435\n",
      "Epoch 8, Batch 848, Test Loss: 0.48317262530326843\n",
      "Epoch 8, Batch 849, Test Loss: 0.7014272809028625\n",
      "Epoch 8, Batch 850, Test Loss: 0.5424155592918396\n",
      "Epoch 8, Batch 851, Test Loss: 0.5701357126235962\n",
      "Epoch 8, Batch 852, Test Loss: 0.42758405208587646\n",
      "Epoch 8, Batch 853, Test Loss: 0.6690686941146851\n",
      "Epoch 8, Batch 854, Test Loss: 0.49285078048706055\n",
      "Epoch 8, Batch 855, Test Loss: 0.4825607240200043\n",
      "Epoch 8, Batch 856, Test Loss: 0.5572081208229065\n",
      "Epoch 8, Batch 857, Test Loss: 0.562105655670166\n",
      "Epoch 8, Batch 858, Test Loss: 0.4860247075557709\n",
      "Epoch 8, Batch 859, Test Loss: 0.46332740783691406\n",
      "Epoch 8, Batch 860, Test Loss: 0.5439760684967041\n",
      "Epoch 8, Batch 861, Test Loss: 0.6691012382507324\n",
      "Epoch 8, Batch 862, Test Loss: 0.4975847005844116\n",
      "Epoch 8, Batch 863, Test Loss: 0.5066447854042053\n",
      "Epoch 8, Batch 864, Test Loss: 0.6271156072616577\n",
      "Epoch 8, Batch 865, Test Loss: 0.6378931999206543\n",
      "Epoch 8, Batch 866, Test Loss: 0.684863269329071\n",
      "Epoch 8, Batch 867, Test Loss: 0.47640806436538696\n",
      "Epoch 8, Batch 868, Test Loss: 0.5938540697097778\n",
      "Epoch 8, Batch 869, Test Loss: 0.5346091985702515\n",
      "Epoch 8, Batch 870, Test Loss: 0.47630804777145386\n",
      "Epoch 8, Batch 871, Test Loss: 0.5498796105384827\n",
      "Epoch 8, Batch 872, Test Loss: 0.45289885997772217\n",
      "Epoch 8, Batch 873, Test Loss: 0.7203081846237183\n",
      "Epoch 8, Batch 874, Test Loss: 0.5988275408744812\n",
      "Epoch 8, Batch 875, Test Loss: 0.6093098521232605\n",
      "Epoch 8, Batch 876, Test Loss: 0.7626560926437378\n",
      "Epoch 8, Batch 877, Test Loss: 0.514980673789978\n",
      "Epoch 8, Batch 878, Test Loss: 0.5734716057777405\n",
      "Epoch 8, Batch 879, Test Loss: 0.4816277027130127\n",
      "Epoch 8, Batch 880, Test Loss: 0.5733416080474854\n",
      "Epoch 8, Batch 881, Test Loss: 0.615727961063385\n",
      "Epoch 8, Batch 882, Test Loss: 0.5922757983207703\n",
      "Epoch 8, Batch 883, Test Loss: 0.49744266271591187\n",
      "Epoch 8, Batch 884, Test Loss: 0.6398045420646667\n",
      "Epoch 8, Batch 885, Test Loss: 0.4853013753890991\n",
      "Epoch 8, Batch 886, Test Loss: 0.572303056716919\n",
      "Epoch 8, Batch 887, Test Loss: 0.487803190946579\n",
      "Epoch 8, Batch 888, Test Loss: 0.5122578144073486\n",
      "Epoch 8, Batch 889, Test Loss: 0.5704014897346497\n",
      "Epoch 8, Batch 890, Test Loss: 0.49712133407592773\n",
      "Epoch 8, Batch 891, Test Loss: 0.6139257550239563\n",
      "Epoch 8, Batch 892, Test Loss: 0.5743978023529053\n",
      "Epoch 8, Batch 893, Test Loss: 0.5186476111412048\n",
      "Epoch 8, Batch 894, Test Loss: 0.5280616879463196\n",
      "Epoch 8, Batch 895, Test Loss: 0.3304077386856079\n",
      "Epoch 8, Batch 896, Test Loss: 0.5854406952857971\n",
      "Epoch 8, Batch 897, Test Loss: 0.6619853973388672\n",
      "Epoch 8, Batch 898, Test Loss: 0.48769068717956543\n",
      "Epoch 8, Batch 899, Test Loss: 0.4161270558834076\n",
      "Epoch 8, Batch 900, Test Loss: 0.5991445779800415\n",
      "Epoch 8, Batch 901, Test Loss: 0.5928205847740173\n",
      "Epoch 8, Batch 902, Test Loss: 0.3797420859336853\n",
      "Epoch 8, Batch 903, Test Loss: 0.4082638621330261\n",
      "Epoch 8, Batch 904, Test Loss: 0.689240574836731\n",
      "Epoch 8, Batch 905, Test Loss: 0.6108853220939636\n",
      "Epoch 8, Batch 906, Test Loss: 0.954453706741333\n",
      "Epoch 8, Batch 907, Test Loss: 0.606916069984436\n",
      "Epoch 8, Batch 908, Test Loss: 0.3792722523212433\n",
      "Epoch 8, Batch 909, Test Loss: 0.5846679210662842\n",
      "Epoch 8, Batch 910, Test Loss: 0.6217947006225586\n",
      "Epoch 8, Batch 911, Test Loss: 0.7030810117721558\n",
      "Epoch 8, Batch 912, Test Loss: 0.6776906847953796\n",
      "Epoch 8, Batch 913, Test Loss: 0.5711894631385803\n",
      "Epoch 8, Batch 914, Test Loss: 0.4260800778865814\n",
      "Epoch 8, Batch 915, Test Loss: 0.4882785677909851\n",
      "Epoch 8, Batch 916, Test Loss: 0.5286475419998169\n",
      "Epoch 8, Batch 917, Test Loss: 0.8736904859542847\n",
      "Epoch 8, Batch 918, Test Loss: 0.4891664385795593\n",
      "Epoch 8, Batch 919, Test Loss: 0.5040588974952698\n",
      "Epoch 8, Batch 920, Test Loss: 0.5755956172943115\n",
      "Epoch 8, Batch 921, Test Loss: 0.46658754348754883\n",
      "Epoch 8, Batch 922, Test Loss: 0.45931071043014526\n",
      "Epoch 8, Batch 923, Test Loss: 0.6189110279083252\n",
      "Epoch 8, Batch 924, Test Loss: 0.44786494970321655\n",
      "Epoch 8, Batch 925, Test Loss: 0.2874051034450531\n",
      "Epoch 8, Batch 926, Test Loss: 0.599444568157196\n",
      "Epoch 8, Batch 927, Test Loss: 0.6652860641479492\n",
      "Epoch 8, Batch 928, Test Loss: 0.8677307963371277\n",
      "Epoch 8, Batch 929, Test Loss: 0.626602292060852\n",
      "Epoch 8, Batch 930, Test Loss: 0.5553655028343201\n",
      "Epoch 8, Batch 931, Test Loss: 0.39503857493400574\n",
      "Epoch 8, Batch 932, Test Loss: 0.5038203597068787\n",
      "Epoch 8, Batch 933, Test Loss: 0.6001094579696655\n",
      "Epoch 8, Batch 934, Test Loss: 0.6073663234710693\n",
      "Epoch 8, Batch 935, Test Loss: 0.5555892586708069\n",
      "Epoch 8, Batch 936, Test Loss: 0.6774522662162781\n",
      "Epoch 8, Batch 937, Test Loss: 0.4699688255786896\n",
      "Epoch 8, Batch 938, Test Loss: 0.6008228659629822\n",
      "Accuracy of Test set: 0.8008333333333333\n",
      "Epoch 9, Batch 1, Loss: 0.5680405497550964\n",
      "Epoch 9, Batch 2, Loss: 0.6629979014396667\n",
      "Epoch 9, Batch 3, Loss: 0.6005508303642273\n",
      "Epoch 9, Batch 4, Loss: 0.5049206614494324\n",
      "Epoch 9, Batch 5, Loss: 0.6298344135284424\n",
      "Epoch 9, Batch 6, Loss: 0.5406992435455322\n",
      "Epoch 9, Batch 7, Loss: 0.5256538987159729\n",
      "Epoch 9, Batch 8, Loss: 0.43285924196243286\n",
      "Epoch 9, Batch 9, Loss: 0.5081091523170471\n",
      "Epoch 9, Batch 10, Loss: 0.36429283022880554\n",
      "Epoch 9, Batch 11, Loss: 0.4591713547706604\n",
      "Epoch 9, Batch 12, Loss: 0.6251599192619324\n",
      "Epoch 9, Batch 13, Loss: 0.6663427352905273\n",
      "Epoch 9, Batch 14, Loss: 0.4607338309288025\n",
      "Epoch 9, Batch 15, Loss: 0.5290184617042542\n",
      "Epoch 9, Batch 16, Loss: 0.4135294556617737\n",
      "Epoch 9, Batch 17, Loss: 0.503243625164032\n",
      "Epoch 9, Batch 18, Loss: 0.5022892355918884\n",
      "Epoch 9, Batch 19, Loss: 0.5469478964805603\n",
      "Epoch 9, Batch 20, Loss: 0.6406447887420654\n",
      "Epoch 9, Batch 21, Loss: 0.5078471302986145\n",
      "Epoch 9, Batch 22, Loss: 0.5147407650947571\n",
      "Epoch 9, Batch 23, Loss: 0.6853512525558472\n",
      "Epoch 9, Batch 24, Loss: 0.38268420100212097\n",
      "Epoch 9, Batch 25, Loss: 0.5916312336921692\n",
      "Epoch 9, Batch 26, Loss: 0.642177402973175\n",
      "Epoch 9, Batch 27, Loss: 0.49581897258758545\n",
      "Epoch 9, Batch 28, Loss: 0.7124234437942505\n",
      "Epoch 9, Batch 29, Loss: 0.46213823556900024\n",
      "Epoch 9, Batch 30, Loss: 0.7758669853210449\n",
      "Epoch 9, Batch 31, Loss: 0.5694178342819214\n",
      "Epoch 9, Batch 32, Loss: 0.6101835370063782\n",
      "Epoch 9, Batch 33, Loss: 0.5124115347862244\n",
      "Epoch 9, Batch 34, Loss: 0.4721183180809021\n",
      "Epoch 9, Batch 35, Loss: 0.7253808975219727\n",
      "Epoch 9, Batch 36, Loss: 0.5215743780136108\n",
      "Epoch 9, Batch 37, Loss: 0.5414425134658813\n",
      "Epoch 9, Batch 38, Loss: 0.689470648765564\n",
      "Epoch 9, Batch 39, Loss: 0.7047876715660095\n",
      "Epoch 9, Batch 40, Loss: 0.44699031114578247\n",
      "Epoch 9, Batch 41, Loss: 0.5452868938446045\n",
      "Epoch 9, Batch 42, Loss: 0.637948215007782\n",
      "Epoch 9, Batch 43, Loss: 0.7003576755523682\n",
      "Epoch 9, Batch 44, Loss: 0.6114251613616943\n",
      "Epoch 9, Batch 45, Loss: 0.6192600727081299\n",
      "Epoch 9, Batch 46, Loss: 0.47357240319252014\n",
      "Epoch 9, Batch 47, Loss: 0.49505457282066345\n",
      "Epoch 9, Batch 48, Loss: 0.5878609418869019\n",
      "Epoch 9, Batch 49, Loss: 0.43689244985580444\n",
      "Epoch 9, Batch 50, Loss: 0.6027153134346008\n",
      "Epoch 9, Batch 51, Loss: 0.6415132880210876\n",
      "Epoch 9, Batch 52, Loss: 0.4308778941631317\n",
      "Epoch 9, Batch 53, Loss: 0.6385083198547363\n",
      "Epoch 9, Batch 54, Loss: 0.5827991962432861\n",
      "Epoch 9, Batch 55, Loss: 0.5063039660453796\n",
      "Epoch 9, Batch 56, Loss: 0.569303035736084\n",
      "Epoch 9, Batch 57, Loss: 0.5169011950492859\n",
      "Epoch 9, Batch 58, Loss: 0.41832399368286133\n",
      "Epoch 9, Batch 59, Loss: 0.3904113173484802\n",
      "Epoch 9, Batch 60, Loss: 0.4337778389453888\n",
      "Epoch 9, Batch 61, Loss: 0.460178405046463\n",
      "Epoch 9, Batch 62, Loss: 0.562213122844696\n",
      "Epoch 9, Batch 63, Loss: 0.55299973487854\n",
      "Epoch 9, Batch 64, Loss: 0.7187211513519287\n",
      "Epoch 9, Batch 65, Loss: 0.4267263114452362\n",
      "Epoch 9, Batch 66, Loss: 0.6458859443664551\n",
      "Epoch 9, Batch 67, Loss: 0.5475922226905823\n",
      "Epoch 9, Batch 68, Loss: 0.6583572030067444\n",
      "Epoch 9, Batch 69, Loss: 0.47853291034698486\n",
      "Epoch 9, Batch 70, Loss: 0.6131444573402405\n",
      "Epoch 9, Batch 71, Loss: 0.4666162133216858\n",
      "Epoch 9, Batch 72, Loss: 0.578140377998352\n",
      "Epoch 9, Batch 73, Loss: 0.46198177337646484\n",
      "Epoch 9, Batch 74, Loss: 0.6061757802963257\n",
      "Epoch 9, Batch 75, Loss: 0.4500044584274292\n",
      "Epoch 9, Batch 76, Loss: 0.4104873538017273\n",
      "Epoch 9, Batch 77, Loss: 0.41819125413894653\n",
      "Epoch 9, Batch 78, Loss: 0.5339217782020569\n",
      "Epoch 9, Batch 79, Loss: 0.44098541140556335\n",
      "Epoch 9, Batch 80, Loss: 0.5356770157814026\n",
      "Epoch 9, Batch 81, Loss: 0.47787144780158997\n",
      "Epoch 9, Batch 82, Loss: 0.6180224418640137\n",
      "Epoch 9, Batch 83, Loss: 0.6974774599075317\n",
      "Epoch 9, Batch 84, Loss: 0.5053414106369019\n",
      "Epoch 9, Batch 85, Loss: 0.7687559127807617\n",
      "Epoch 9, Batch 86, Loss: 0.3989063799381256\n",
      "Epoch 9, Batch 87, Loss: 0.5771664381027222\n",
      "Epoch 9, Batch 88, Loss: 0.5384839773178101\n",
      "Epoch 9, Batch 89, Loss: 0.48036229610443115\n",
      "Epoch 9, Batch 90, Loss: 0.508625328540802\n",
      "Epoch 9, Batch 91, Loss: 0.6657562851905823\n",
      "Epoch 9, Batch 92, Loss: 0.694316029548645\n",
      "Epoch 9, Batch 93, Loss: 0.4604406952857971\n",
      "Epoch 9, Batch 94, Loss: 0.5900106430053711\n",
      "Epoch 9, Batch 95, Loss: 0.5039337277412415\n",
      "Epoch 9, Batch 96, Loss: 0.5206081867218018\n",
      "Epoch 9, Batch 97, Loss: 0.8356162905693054\n",
      "Epoch 9, Batch 98, Loss: 0.46983760595321655\n",
      "Epoch 9, Batch 99, Loss: 0.5545816421508789\n",
      "Epoch 9, Batch 100, Loss: 0.4387928545475006\n",
      "Epoch 9, Batch 101, Loss: 0.4595257341861725\n",
      "Epoch 9, Batch 102, Loss: 0.4362485706806183\n",
      "Epoch 9, Batch 103, Loss: 0.48684459924697876\n",
      "Epoch 9, Batch 104, Loss: 0.5204272270202637\n",
      "Epoch 9, Batch 105, Loss: 0.611189067363739\n",
      "Epoch 9, Batch 106, Loss: 0.6193480491638184\n",
      "Epoch 9, Batch 107, Loss: 0.4951557219028473\n",
      "Epoch 9, Batch 108, Loss: 0.7301971316337585\n",
      "Epoch 9, Batch 109, Loss: 0.48879721760749817\n",
      "Epoch 9, Batch 110, Loss: 0.7273598313331604\n",
      "Epoch 9, Batch 111, Loss: 0.3806235194206238\n",
      "Epoch 9, Batch 112, Loss: 0.45017245411872864\n",
      "Epoch 9, Batch 113, Loss: 0.4952866733074188\n",
      "Epoch 9, Batch 114, Loss: 0.41680487990379333\n",
      "Epoch 9, Batch 115, Loss: 0.5926218032836914\n",
      "Epoch 9, Batch 116, Loss: 0.6271286606788635\n",
      "Epoch 9, Batch 117, Loss: 0.44771242141723633\n",
      "Epoch 9, Batch 118, Loss: 0.40002575516700745\n",
      "Epoch 9, Batch 119, Loss: 0.6156785488128662\n",
      "Epoch 9, Batch 120, Loss: 0.48911750316619873\n",
      "Epoch 9, Batch 121, Loss: 0.41378045082092285\n",
      "Epoch 9, Batch 122, Loss: 0.5711329579353333\n",
      "Epoch 9, Batch 123, Loss: 0.5375697612762451\n",
      "Epoch 9, Batch 124, Loss: 0.38231104612350464\n",
      "Epoch 9, Batch 125, Loss: 0.5389019250869751\n",
      "Epoch 9, Batch 126, Loss: 0.5606610178947449\n",
      "Epoch 9, Batch 127, Loss: 0.7193658947944641\n",
      "Epoch 9, Batch 128, Loss: 0.5273625254631042\n",
      "Epoch 9, Batch 129, Loss: 0.4296995997428894\n",
      "Epoch 9, Batch 130, Loss: 0.5342698097229004\n",
      "Epoch 9, Batch 131, Loss: 0.6362822651863098\n",
      "Epoch 9, Batch 132, Loss: 0.4470822811126709\n",
      "Epoch 9, Batch 133, Loss: 0.4339028000831604\n",
      "Epoch 9, Batch 134, Loss: 0.6850842833518982\n",
      "Epoch 9, Batch 135, Loss: 0.5219147801399231\n",
      "Epoch 9, Batch 136, Loss: 0.6168440580368042\n",
      "Epoch 9, Batch 137, Loss: 0.6022320985794067\n",
      "Epoch 9, Batch 138, Loss: 0.6559861302375793\n",
      "Epoch 9, Batch 139, Loss: 0.4016692638397217\n",
      "Epoch 9, Batch 140, Loss: 0.46624910831451416\n",
      "Epoch 9, Batch 141, Loss: 0.43005016446113586\n",
      "Epoch 9, Batch 142, Loss: 0.4813956022262573\n",
      "Epoch 9, Batch 143, Loss: 0.6892884373664856\n",
      "Epoch 9, Batch 144, Loss: 0.5877881646156311\n",
      "Epoch 9, Batch 145, Loss: 0.42541787028312683\n",
      "Epoch 9, Batch 146, Loss: 0.44034382700920105\n",
      "Epoch 9, Batch 147, Loss: 0.397339403629303\n",
      "Epoch 9, Batch 148, Loss: 0.6210477948188782\n",
      "Epoch 9, Batch 149, Loss: 0.6665419936180115\n",
      "Epoch 9, Batch 150, Loss: 0.5131219029426575\n",
      "Epoch 9, Batch 151, Loss: 0.4854225516319275\n",
      "Epoch 9, Batch 152, Loss: 0.4549672305583954\n",
      "Epoch 9, Batch 153, Loss: 0.47208619117736816\n",
      "Epoch 9, Batch 154, Loss: 0.5249674916267395\n",
      "Epoch 9, Batch 155, Loss: 0.573426365852356\n",
      "Epoch 9, Batch 156, Loss: 0.43339478969573975\n",
      "Epoch 9, Batch 157, Loss: 0.3779716491699219\n",
      "Epoch 9, Batch 158, Loss: 0.6020468473434448\n",
      "Epoch 9, Batch 159, Loss: 0.5641077160835266\n",
      "Epoch 9, Batch 160, Loss: 0.44714444875717163\n",
      "Epoch 9, Batch 161, Loss: 0.6083892583847046\n",
      "Epoch 9, Batch 162, Loss: 0.5270907282829285\n",
      "Epoch 9, Batch 163, Loss: 0.5429902076721191\n",
      "Epoch 9, Batch 164, Loss: 0.5255576968193054\n",
      "Epoch 9, Batch 165, Loss: 0.46116116642951965\n",
      "Epoch 9, Batch 166, Loss: 0.44253790378570557\n",
      "Epoch 9, Batch 167, Loss: 0.47357261180877686\n",
      "Epoch 9, Batch 168, Loss: 0.5354708433151245\n",
      "Epoch 9, Batch 169, Loss: 0.5979776978492737\n",
      "Epoch 9, Batch 170, Loss: 0.603537380695343\n",
      "Epoch 9, Batch 171, Loss: 0.5449846982955933\n",
      "Epoch 9, Batch 172, Loss: 0.3818832039833069\n",
      "Epoch 9, Batch 173, Loss: 0.6271899342536926\n",
      "Epoch 9, Batch 174, Loss: 0.3839588165283203\n",
      "Epoch 9, Batch 175, Loss: 0.35793337225914\n",
      "Epoch 9, Batch 176, Loss: 0.5183057188987732\n",
      "Epoch 9, Batch 177, Loss: 0.5221799612045288\n",
      "Epoch 9, Batch 178, Loss: 0.5795278549194336\n",
      "Epoch 9, Batch 179, Loss: 0.41494759917259216\n",
      "Epoch 9, Batch 180, Loss: 0.8786810636520386\n",
      "Epoch 9, Batch 181, Loss: 0.5194238424301147\n",
      "Epoch 9, Batch 182, Loss: 0.5692408084869385\n",
      "Epoch 9, Batch 183, Loss: 0.5317211747169495\n",
      "Epoch 9, Batch 184, Loss: 0.37670037150382996\n",
      "Epoch 9, Batch 185, Loss: 0.7096308469772339\n",
      "Epoch 9, Batch 186, Loss: 0.42842817306518555\n",
      "Epoch 9, Batch 187, Loss: 0.4710026681423187\n",
      "Epoch 9, Batch 188, Loss: 0.5977131724357605\n",
      "Epoch 9, Batch 189, Loss: 0.5233168005943298\n",
      "Epoch 9, Batch 190, Loss: 0.40350908041000366\n",
      "Epoch 9, Batch 191, Loss: 0.4930843114852905\n",
      "Epoch 9, Batch 192, Loss: 0.46342045068740845\n",
      "Epoch 9, Batch 193, Loss: 0.5425102710723877\n",
      "Epoch 9, Batch 194, Loss: 0.5636114478111267\n",
      "Epoch 9, Batch 195, Loss: 0.7035970091819763\n",
      "Epoch 9, Batch 196, Loss: 0.5247070789337158\n",
      "Epoch 9, Batch 197, Loss: 0.552713930606842\n",
      "Epoch 9, Batch 198, Loss: 0.5983980894088745\n",
      "Epoch 9, Batch 199, Loss: 0.49036547541618347\n",
      "Epoch 9, Batch 200, Loss: 0.27859920263290405\n",
      "Epoch 9, Batch 201, Loss: 0.47301894426345825\n",
      "Epoch 9, Batch 202, Loss: 0.5113440752029419\n",
      "Epoch 9, Batch 203, Loss: 0.5151902437210083\n",
      "Epoch 9, Batch 204, Loss: 0.6113434433937073\n",
      "Epoch 9, Batch 205, Loss: 0.47952884435653687\n",
      "Epoch 9, Batch 206, Loss: 0.7088146805763245\n",
      "Epoch 9, Batch 207, Loss: 0.3963889181613922\n",
      "Epoch 9, Batch 208, Loss: 0.5232623815536499\n",
      "Epoch 9, Batch 209, Loss: 0.39588800072669983\n",
      "Epoch 9, Batch 210, Loss: 0.6726697683334351\n",
      "Epoch 9, Batch 211, Loss: 0.44473737478256226\n",
      "Epoch 9, Batch 212, Loss: 0.5021436214447021\n",
      "Epoch 9, Batch 213, Loss: 0.7522689700126648\n",
      "Epoch 9, Batch 214, Loss: 0.3621109426021576\n",
      "Epoch 9, Batch 215, Loss: 0.5640681982040405\n",
      "Epoch 9, Batch 216, Loss: 0.5730254650115967\n",
      "Epoch 9, Batch 217, Loss: 0.7491948008537292\n",
      "Epoch 9, Batch 218, Loss: 0.5616559982299805\n",
      "Epoch 9, Batch 219, Loss: 0.4751984179019928\n",
      "Epoch 9, Batch 220, Loss: 0.5945783853530884\n",
      "Epoch 9, Batch 221, Loss: 0.438416451215744\n",
      "Epoch 9, Batch 222, Loss: 0.5163790583610535\n",
      "Epoch 9, Batch 223, Loss: 0.5428228378295898\n",
      "Epoch 9, Batch 224, Loss: 0.5914315581321716\n",
      "Epoch 9, Batch 225, Loss: 0.6187257170677185\n",
      "Epoch 9, Batch 226, Loss: 0.575886607170105\n",
      "Epoch 9, Batch 227, Loss: 0.4468667209148407\n",
      "Epoch 9, Batch 228, Loss: 0.5277712941169739\n",
      "Epoch 9, Batch 229, Loss: 0.5420413613319397\n",
      "Epoch 9, Batch 230, Loss: 0.4433227479457855\n",
      "Epoch 9, Batch 231, Loss: 0.5477935671806335\n",
      "Epoch 9, Batch 232, Loss: 0.9356554746627808\n",
      "Epoch 9, Batch 233, Loss: 0.4062005877494812\n",
      "Epoch 9, Batch 234, Loss: 0.6481835842132568\n",
      "Epoch 9, Batch 235, Loss: 0.45639076828956604\n",
      "Epoch 9, Batch 236, Loss: 0.45057281851768494\n",
      "Epoch 9, Batch 237, Loss: 0.5842775106430054\n",
      "Epoch 9, Batch 238, Loss: 0.44813814759254456\n",
      "Epoch 9, Batch 239, Loss: 0.4453187584877014\n",
      "Epoch 9, Batch 240, Loss: 0.4698179364204407\n",
      "Epoch 9, Batch 241, Loss: 0.3462064862251282\n",
      "Epoch 9, Batch 242, Loss: 0.5197939276695251\n",
      "Epoch 9, Batch 243, Loss: 0.9406477212905884\n",
      "Epoch 9, Batch 244, Loss: 0.43331244587898254\n",
      "Epoch 9, Batch 245, Loss: 0.5197566747665405\n",
      "Epoch 9, Batch 246, Loss: 0.6411961317062378\n",
      "Epoch 9, Batch 247, Loss: 0.8848743438720703\n",
      "Epoch 9, Batch 248, Loss: 0.6979027986526489\n",
      "Epoch 9, Batch 249, Loss: 0.4845888614654541\n",
      "Epoch 9, Batch 250, Loss: 0.5405598282814026\n",
      "Epoch 9, Batch 251, Loss: 0.5268573760986328\n",
      "Epoch 9, Batch 252, Loss: 0.6554701924324036\n",
      "Epoch 9, Batch 253, Loss: 0.27893438935279846\n",
      "Epoch 9, Batch 254, Loss: 0.49664387106895447\n",
      "Epoch 9, Batch 255, Loss: 0.6127084493637085\n",
      "Epoch 9, Batch 256, Loss: 0.4227495491504669\n",
      "Epoch 9, Batch 257, Loss: 0.4974972605705261\n",
      "Epoch 9, Batch 258, Loss: 0.37037473917007446\n",
      "Epoch 9, Batch 259, Loss: 0.4583107829093933\n",
      "Epoch 9, Batch 260, Loss: 0.48019444942474365\n",
      "Epoch 9, Batch 261, Loss: 0.5416940450668335\n",
      "Epoch 9, Batch 262, Loss: 0.7252838611602783\n",
      "Epoch 9, Batch 263, Loss: 0.4012967348098755\n",
      "Epoch 9, Batch 264, Loss: 0.3460483253002167\n",
      "Epoch 9, Batch 265, Loss: 0.653389573097229\n",
      "Epoch 9, Batch 266, Loss: 0.5461281538009644\n",
      "Epoch 9, Batch 267, Loss: 0.5363936424255371\n",
      "Epoch 9, Batch 268, Loss: 0.7245863080024719\n",
      "Epoch 9, Batch 269, Loss: 0.4635281562805176\n",
      "Epoch 9, Batch 270, Loss: 0.7939242124557495\n",
      "Epoch 9, Batch 271, Loss: 0.4575653374195099\n",
      "Epoch 9, Batch 272, Loss: 0.5003879070281982\n",
      "Epoch 9, Batch 273, Loss: 0.6388880610466003\n",
      "Epoch 9, Batch 274, Loss: 0.4793137311935425\n",
      "Epoch 9, Batch 275, Loss: 0.48906946182250977\n",
      "Epoch 9, Batch 276, Loss: 0.6786434650421143\n",
      "Epoch 9, Batch 277, Loss: 0.6694326400756836\n",
      "Epoch 9, Batch 278, Loss: 0.6034618616104126\n",
      "Epoch 9, Batch 279, Loss: 0.7243415713310242\n",
      "Epoch 9, Batch 280, Loss: 0.6221448183059692\n",
      "Epoch 9, Batch 281, Loss: 0.5428333282470703\n",
      "Epoch 9, Batch 282, Loss: 0.9072940349578857\n",
      "Epoch 9, Batch 283, Loss: 0.3683140277862549\n",
      "Epoch 9, Batch 284, Loss: 0.5285248756408691\n",
      "Epoch 9, Batch 285, Loss: 0.47637873888015747\n",
      "Epoch 9, Batch 286, Loss: 0.48607325553894043\n",
      "Epoch 9, Batch 287, Loss: 0.3303776979446411\n",
      "Epoch 9, Batch 288, Loss: 0.484978586435318\n",
      "Epoch 9, Batch 289, Loss: 0.5462976694107056\n",
      "Epoch 9, Batch 290, Loss: 0.6362108588218689\n",
      "Epoch 9, Batch 291, Loss: 0.4016486704349518\n",
      "Epoch 9, Batch 292, Loss: 0.5311188101768494\n",
      "Epoch 9, Batch 293, Loss: 0.5864878296852112\n",
      "Epoch 9, Batch 294, Loss: 0.4720281958580017\n",
      "Epoch 9, Batch 295, Loss: 0.5457772612571716\n",
      "Epoch 9, Batch 296, Loss: 0.5082047581672668\n",
      "Epoch 9, Batch 297, Loss: 0.6849240660667419\n",
      "Epoch 9, Batch 298, Loss: 0.512061357498169\n",
      "Epoch 9, Batch 299, Loss: 0.5615770816802979\n",
      "Epoch 9, Batch 300, Loss: 0.5347305536270142\n",
      "Epoch 9, Batch 301, Loss: 0.42042696475982666\n",
      "Epoch 9, Batch 302, Loss: 0.4215576946735382\n",
      "Epoch 9, Batch 303, Loss: 0.5980423092842102\n",
      "Epoch 9, Batch 304, Loss: 0.40370622277259827\n",
      "Epoch 9, Batch 305, Loss: 0.6078090071678162\n",
      "Epoch 9, Batch 306, Loss: 0.518280029296875\n",
      "Epoch 9, Batch 307, Loss: 0.6128912568092346\n",
      "Epoch 9, Batch 308, Loss: 0.5371484756469727\n",
      "Epoch 9, Batch 309, Loss: 0.561039388179779\n",
      "Epoch 9, Batch 310, Loss: 0.516956627368927\n",
      "Epoch 9, Batch 311, Loss: 0.5836862921714783\n",
      "Epoch 9, Batch 312, Loss: 0.5383592247962952\n",
      "Epoch 9, Batch 313, Loss: 0.720702052116394\n",
      "Epoch 9, Batch 314, Loss: 0.5140966176986694\n",
      "Epoch 9, Batch 315, Loss: 0.5577871203422546\n",
      "Epoch 9, Batch 316, Loss: 0.7053410410881042\n",
      "Epoch 9, Batch 317, Loss: 0.4547393321990967\n",
      "Epoch 9, Batch 318, Loss: 0.7379163503646851\n",
      "Epoch 9, Batch 319, Loss: 0.7035362720489502\n",
      "Epoch 9, Batch 320, Loss: 0.46089911460876465\n",
      "Epoch 9, Batch 321, Loss: 0.47547435760498047\n",
      "Epoch 9, Batch 322, Loss: 0.5340389013290405\n",
      "Epoch 9, Batch 323, Loss: 0.7526783347129822\n",
      "Epoch 9, Batch 324, Loss: 0.5589362978935242\n",
      "Epoch 9, Batch 325, Loss: 0.6387141346931458\n",
      "Epoch 9, Batch 326, Loss: 0.4898359775543213\n",
      "Epoch 9, Batch 327, Loss: 0.4782940745353699\n",
      "Epoch 9, Batch 328, Loss: 0.7919783592224121\n",
      "Epoch 9, Batch 329, Loss: 0.6553816795349121\n",
      "Epoch 9, Batch 330, Loss: 0.36763131618499756\n",
      "Epoch 9, Batch 331, Loss: 0.5669716596603394\n",
      "Epoch 9, Batch 332, Loss: 0.5393324494361877\n",
      "Epoch 9, Batch 333, Loss: 0.5521969795227051\n",
      "Epoch 9, Batch 334, Loss: 0.5997286438941956\n",
      "Epoch 9, Batch 335, Loss: 0.44296711683273315\n",
      "Epoch 9, Batch 336, Loss: 0.48745763301849365\n",
      "Epoch 9, Batch 337, Loss: 0.4395926296710968\n",
      "Epoch 9, Batch 338, Loss: 0.4754165709018707\n",
      "Epoch 9, Batch 339, Loss: 0.578671395778656\n",
      "Epoch 9, Batch 340, Loss: 0.32481929659843445\n",
      "Epoch 9, Batch 341, Loss: 0.4740157723426819\n",
      "Epoch 9, Batch 342, Loss: 0.6532727479934692\n",
      "Epoch 9, Batch 343, Loss: 0.4696214497089386\n",
      "Epoch 9, Batch 344, Loss: 0.5667108297348022\n",
      "Epoch 9, Batch 345, Loss: 0.9412272572517395\n",
      "Epoch 9, Batch 346, Loss: 0.3930022418498993\n",
      "Epoch 9, Batch 347, Loss: 0.545728862285614\n",
      "Epoch 9, Batch 348, Loss: 0.38759487867355347\n",
      "Epoch 9, Batch 349, Loss: 0.3623791038990021\n",
      "Epoch 9, Batch 350, Loss: 0.47037142515182495\n",
      "Epoch 9, Batch 351, Loss: 0.7487938404083252\n",
      "Epoch 9, Batch 352, Loss: 0.6503642797470093\n",
      "Epoch 9, Batch 353, Loss: 0.5167639255523682\n",
      "Epoch 9, Batch 354, Loss: 0.564408004283905\n",
      "Epoch 9, Batch 355, Loss: 0.4027435779571533\n",
      "Epoch 9, Batch 356, Loss: 0.5686640739440918\n",
      "Epoch 9, Batch 357, Loss: 0.32011961936950684\n",
      "Epoch 9, Batch 358, Loss: 0.816924512386322\n",
      "Epoch 9, Batch 359, Loss: 0.532616138458252\n",
      "Epoch 9, Batch 360, Loss: 0.38897624611854553\n",
      "Epoch 9, Batch 361, Loss: 0.6377910375595093\n",
      "Epoch 9, Batch 362, Loss: 0.4137120544910431\n",
      "Epoch 9, Batch 363, Loss: 0.8501047492027283\n",
      "Epoch 9, Batch 364, Loss: 0.5744134187698364\n",
      "Epoch 9, Batch 365, Loss: 0.4025615155696869\n",
      "Epoch 9, Batch 366, Loss: 0.42048075795173645\n",
      "Epoch 9, Batch 367, Loss: 0.3407600522041321\n",
      "Epoch 9, Batch 368, Loss: 0.37287333607673645\n",
      "Epoch 9, Batch 369, Loss: 0.5427224636077881\n",
      "Epoch 9, Batch 370, Loss: 0.493556410074234\n",
      "Epoch 9, Batch 371, Loss: 0.6087498664855957\n",
      "Epoch 9, Batch 372, Loss: 0.39149925112724304\n",
      "Epoch 9, Batch 373, Loss: 0.5672277212142944\n",
      "Epoch 9, Batch 374, Loss: 0.591304361820221\n",
      "Epoch 9, Batch 375, Loss: 0.7110077738761902\n",
      "Epoch 9, Batch 376, Loss: 0.5344105958938599\n",
      "Epoch 9, Batch 377, Loss: 0.5169857740402222\n",
      "Epoch 9, Batch 378, Loss: 0.6805868148803711\n",
      "Epoch 9, Batch 379, Loss: 0.38083797693252563\n",
      "Epoch 9, Batch 380, Loss: 0.5892404317855835\n",
      "Epoch 9, Batch 381, Loss: 0.39003288745880127\n",
      "Epoch 9, Batch 382, Loss: 0.459561288356781\n",
      "Epoch 9, Batch 383, Loss: 0.5506681203842163\n",
      "Epoch 9, Batch 384, Loss: 0.5027571320533752\n",
      "Epoch 9, Batch 385, Loss: 0.7020208835601807\n",
      "Epoch 9, Batch 386, Loss: 0.6727069616317749\n",
      "Epoch 9, Batch 387, Loss: 0.43653297424316406\n",
      "Epoch 9, Batch 388, Loss: 0.48007825016975403\n",
      "Epoch 9, Batch 389, Loss: 0.6050834655761719\n",
      "Epoch 9, Batch 390, Loss: 0.5901751518249512\n",
      "Epoch 9, Batch 391, Loss: 0.6952764987945557\n",
      "Epoch 9, Batch 392, Loss: 0.4805876612663269\n",
      "Epoch 9, Batch 393, Loss: 0.5777826309204102\n",
      "Epoch 9, Batch 394, Loss: 0.5445869565010071\n",
      "Epoch 9, Batch 395, Loss: 0.6331069469451904\n",
      "Epoch 9, Batch 396, Loss: 0.6778793334960938\n",
      "Epoch 9, Batch 397, Loss: 0.720107913017273\n",
      "Epoch 9, Batch 398, Loss: 0.6606497168540955\n",
      "Epoch 9, Batch 399, Loss: 0.5645095705986023\n",
      "Epoch 9, Batch 400, Loss: 0.43313899636268616\n",
      "Epoch 9, Batch 401, Loss: 0.621361255645752\n",
      "Epoch 9, Batch 402, Loss: 0.5596328377723694\n",
      "Epoch 9, Batch 403, Loss: 0.5415864586830139\n",
      "Epoch 9, Batch 404, Loss: 0.4066060781478882\n",
      "Epoch 9, Batch 405, Loss: 0.47627735137939453\n",
      "Epoch 9, Batch 406, Loss: 0.6209147572517395\n",
      "Epoch 9, Batch 407, Loss: 0.6949312686920166\n",
      "Epoch 9, Batch 408, Loss: 0.5250781774520874\n",
      "Epoch 9, Batch 409, Loss: 0.48279768228530884\n",
      "Epoch 9, Batch 410, Loss: 0.3426401615142822\n",
      "Epoch 9, Batch 411, Loss: 0.6962755918502808\n",
      "Epoch 9, Batch 412, Loss: 0.7142907381057739\n",
      "Epoch 9, Batch 413, Loss: 0.5561213493347168\n",
      "Epoch 9, Batch 414, Loss: 0.5846092700958252\n",
      "Epoch 9, Batch 415, Loss: 0.4039214253425598\n",
      "Epoch 9, Batch 416, Loss: 0.3958285450935364\n",
      "Epoch 9, Batch 417, Loss: 0.5841847062110901\n",
      "Epoch 9, Batch 418, Loss: 0.552742600440979\n",
      "Epoch 9, Batch 419, Loss: 0.5276015996932983\n",
      "Epoch 9, Batch 420, Loss: 0.5018914341926575\n",
      "Epoch 9, Batch 421, Loss: 0.5579724907875061\n",
      "Epoch 9, Batch 422, Loss: 0.43442410230636597\n",
      "Epoch 9, Batch 423, Loss: 0.529784619808197\n",
      "Epoch 9, Batch 424, Loss: 0.4633656144142151\n",
      "Epoch 9, Batch 425, Loss: 0.4045123755931854\n",
      "Epoch 9, Batch 426, Loss: 0.4806728661060333\n",
      "Epoch 9, Batch 427, Loss: 0.44240519404411316\n",
      "Epoch 9, Batch 428, Loss: 0.6076752543449402\n",
      "Epoch 9, Batch 429, Loss: 0.7061103582382202\n",
      "Epoch 9, Batch 430, Loss: 0.8303108215332031\n",
      "Epoch 9, Batch 431, Loss: 0.3297558128833771\n",
      "Epoch 9, Batch 432, Loss: 0.27709001302719116\n",
      "Epoch 9, Batch 433, Loss: 0.47691604495048523\n",
      "Epoch 9, Batch 434, Loss: 0.5375733375549316\n",
      "Epoch 9, Batch 435, Loss: 0.48210233449935913\n",
      "Epoch 9, Batch 436, Loss: 0.7345888614654541\n",
      "Epoch 9, Batch 437, Loss: 0.6237422227859497\n",
      "Epoch 9, Batch 438, Loss: 0.7773829698562622\n",
      "Epoch 9, Batch 439, Loss: 0.5404466390609741\n",
      "Epoch 9, Batch 440, Loss: 0.6807190179824829\n",
      "Epoch 9, Batch 441, Loss: 0.5481947064399719\n",
      "Epoch 9, Batch 442, Loss: 0.422437846660614\n",
      "Epoch 9, Batch 443, Loss: 0.5636737942695618\n",
      "Epoch 9, Batch 444, Loss: 0.43859389424324036\n",
      "Epoch 9, Batch 445, Loss: 0.4906242787837982\n",
      "Epoch 9, Batch 446, Loss: 0.40518131852149963\n",
      "Epoch 9, Batch 447, Loss: 0.6334860920906067\n",
      "Epoch 9, Batch 448, Loss: 0.4749275743961334\n",
      "Epoch 9, Batch 449, Loss: 0.6579636335372925\n",
      "Epoch 9, Batch 450, Loss: 0.39767128229141235\n",
      "Epoch 9, Batch 451, Loss: 0.3322523534297943\n",
      "Epoch 9, Batch 452, Loss: 0.5789570212364197\n",
      "Epoch 9, Batch 453, Loss: 0.7350225448608398\n",
      "Epoch 9, Batch 454, Loss: 0.5212646126747131\n",
      "Epoch 9, Batch 455, Loss: 0.944836437702179\n",
      "Epoch 9, Batch 456, Loss: 0.4851779639720917\n",
      "Epoch 9, Batch 457, Loss: 0.6399695873260498\n",
      "Epoch 9, Batch 458, Loss: 0.4281471073627472\n",
      "Epoch 9, Batch 459, Loss: 0.737412691116333\n",
      "Epoch 9, Batch 460, Loss: 0.5725342035293579\n",
      "Epoch 9, Batch 461, Loss: 0.49670901894569397\n",
      "Epoch 9, Batch 462, Loss: 0.46639227867126465\n",
      "Epoch 9, Batch 463, Loss: 0.6790180802345276\n",
      "Epoch 9, Batch 464, Loss: 0.6035398244857788\n",
      "Epoch 9, Batch 465, Loss: 0.6616719365119934\n",
      "Epoch 9, Batch 466, Loss: 0.5415792465209961\n",
      "Epoch 9, Batch 467, Loss: 0.6756775975227356\n",
      "Epoch 9, Batch 468, Loss: 0.6380641460418701\n",
      "Epoch 9, Batch 469, Loss: 0.6067636013031006\n",
      "Epoch 9, Batch 470, Loss: 0.5529917478561401\n",
      "Epoch 9, Batch 471, Loss: 0.4986240863800049\n",
      "Epoch 9, Batch 472, Loss: 0.5516444444656372\n",
      "Epoch 9, Batch 473, Loss: 0.5990150570869446\n",
      "Epoch 9, Batch 474, Loss: 0.6417099833488464\n",
      "Epoch 9, Batch 475, Loss: 0.580599308013916\n",
      "Epoch 9, Batch 476, Loss: 0.477510005235672\n",
      "Epoch 9, Batch 477, Loss: 0.44685497879981995\n",
      "Epoch 9, Batch 478, Loss: 0.36451587080955505\n",
      "Epoch 9, Batch 479, Loss: 0.6082911491394043\n",
      "Epoch 9, Batch 480, Loss: 0.6394844055175781\n",
      "Epoch 9, Batch 481, Loss: 0.5630941390991211\n",
      "Epoch 9, Batch 482, Loss: 0.5513123273849487\n",
      "Epoch 9, Batch 483, Loss: 0.8332310318946838\n",
      "Epoch 9, Batch 484, Loss: 0.44345682859420776\n",
      "Epoch 9, Batch 485, Loss: 0.46887972950935364\n",
      "Epoch 9, Batch 486, Loss: 0.5043253302574158\n",
      "Epoch 9, Batch 487, Loss: 0.3794627785682678\n",
      "Epoch 9, Batch 488, Loss: 0.6329691410064697\n",
      "Epoch 9, Batch 489, Loss: 0.5767228007316589\n",
      "Epoch 9, Batch 490, Loss: 0.4125635623931885\n",
      "Epoch 9, Batch 491, Loss: 0.5791553854942322\n",
      "Epoch 9, Batch 492, Loss: 0.5634205937385559\n",
      "Epoch 9, Batch 493, Loss: 0.6407328248023987\n",
      "Epoch 9, Batch 494, Loss: 0.5227956771850586\n",
      "Epoch 9, Batch 495, Loss: 0.762676477432251\n",
      "Epoch 9, Batch 496, Loss: 0.4514590799808502\n",
      "Epoch 9, Batch 497, Loss: 0.39948570728302\n",
      "Epoch 9, Batch 498, Loss: 0.49832847714424133\n",
      "Epoch 9, Batch 499, Loss: 0.5194093585014343\n",
      "Epoch 9, Batch 500, Loss: 0.4999818205833435\n",
      "Epoch 9, Batch 501, Loss: 0.3741630017757416\n",
      "Epoch 9, Batch 502, Loss: 0.4968046545982361\n",
      "Epoch 9, Batch 503, Loss: 0.5344960689544678\n",
      "Epoch 9, Batch 504, Loss: 0.44167590141296387\n",
      "Epoch 9, Batch 505, Loss: 0.4465213418006897\n",
      "Epoch 9, Batch 506, Loss: 0.3695273995399475\n",
      "Epoch 9, Batch 507, Loss: 0.6274082064628601\n",
      "Epoch 9, Batch 508, Loss: 0.880567729473114\n",
      "Epoch 9, Batch 509, Loss: 0.7197657227516174\n",
      "Epoch 9, Batch 510, Loss: 0.32765403389930725\n",
      "Epoch 9, Batch 511, Loss: 0.4273659586906433\n",
      "Epoch 9, Batch 512, Loss: 0.3939364552497864\n",
      "Epoch 9, Batch 513, Loss: 0.44123971462249756\n",
      "Epoch 9, Batch 514, Loss: 0.5692194104194641\n",
      "Epoch 9, Batch 515, Loss: 0.6179540157318115\n",
      "Epoch 9, Batch 516, Loss: 0.3892535865306854\n",
      "Epoch 9, Batch 517, Loss: 0.5033284425735474\n",
      "Epoch 9, Batch 518, Loss: 0.26155078411102295\n",
      "Epoch 9, Batch 519, Loss: 0.5065657496452332\n",
      "Epoch 9, Batch 520, Loss: 0.5762829184532166\n",
      "Epoch 9, Batch 521, Loss: 0.544451117515564\n",
      "Epoch 9, Batch 522, Loss: 0.6942566633224487\n",
      "Epoch 9, Batch 523, Loss: 0.4062723219394684\n",
      "Epoch 9, Batch 524, Loss: 0.7558405995368958\n",
      "Epoch 9, Batch 525, Loss: 0.41864797472953796\n",
      "Epoch 9, Batch 526, Loss: 0.41082510352134705\n",
      "Epoch 9, Batch 527, Loss: 0.516254723072052\n",
      "Epoch 9, Batch 528, Loss: 0.5897546410560608\n",
      "Epoch 9, Batch 529, Loss: 0.3727697432041168\n",
      "Epoch 9, Batch 530, Loss: 0.4634193778038025\n",
      "Epoch 9, Batch 531, Loss: 0.561102569103241\n",
      "Epoch 9, Batch 532, Loss: 0.35867545008659363\n",
      "Epoch 9, Batch 533, Loss: 0.4977869391441345\n",
      "Epoch 9, Batch 534, Loss: 0.4981822371482849\n",
      "Epoch 9, Batch 535, Loss: 0.6024595499038696\n",
      "Epoch 9, Batch 536, Loss: 0.7466504573822021\n",
      "Epoch 9, Batch 537, Loss: 0.362598180770874\n",
      "Epoch 9, Batch 538, Loss: 0.4953274428844452\n",
      "Epoch 9, Batch 539, Loss: 0.5840685963630676\n",
      "Epoch 9, Batch 540, Loss: 0.40055835247039795\n",
      "Epoch 9, Batch 541, Loss: 0.5245569348335266\n",
      "Epoch 9, Batch 542, Loss: 0.6530664563179016\n",
      "Epoch 9, Batch 543, Loss: 0.5636513829231262\n",
      "Epoch 9, Batch 544, Loss: 0.40755799412727356\n",
      "Epoch 9, Batch 545, Loss: 0.5065993070602417\n",
      "Epoch 9, Batch 546, Loss: 0.3602939546108246\n",
      "Epoch 9, Batch 547, Loss: 0.416242778301239\n",
      "Epoch 9, Batch 548, Loss: 0.3894236981868744\n",
      "Epoch 9, Batch 549, Loss: 0.6332502365112305\n",
      "Epoch 9, Batch 550, Loss: 0.38256192207336426\n",
      "Epoch 9, Batch 551, Loss: 0.6908969283103943\n",
      "Epoch 9, Batch 552, Loss: 0.4813463091850281\n",
      "Epoch 9, Batch 553, Loss: 0.5456574559211731\n",
      "Epoch 9, Batch 554, Loss: 0.6783598065376282\n",
      "Epoch 9, Batch 555, Loss: 0.35610923171043396\n",
      "Epoch 9, Batch 556, Loss: 0.46702104806900024\n",
      "Epoch 9, Batch 557, Loss: 0.6736165285110474\n",
      "Epoch 9, Batch 558, Loss: 0.5540676116943359\n",
      "Epoch 9, Batch 559, Loss: 0.5629923939704895\n",
      "Epoch 9, Batch 560, Loss: 0.6728029251098633\n",
      "Epoch 9, Batch 561, Loss: 0.4382716417312622\n",
      "Epoch 9, Batch 562, Loss: 0.48181742429733276\n",
      "Epoch 9, Batch 563, Loss: 0.5306023359298706\n",
      "Epoch 9, Batch 564, Loss: 0.5325809717178345\n",
      "Epoch 9, Batch 565, Loss: 0.46647143363952637\n",
      "Epoch 9, Batch 566, Loss: 0.5110295414924622\n",
      "Epoch 9, Batch 567, Loss: 0.5484110713005066\n",
      "Epoch 9, Batch 568, Loss: 0.5419315099716187\n",
      "Epoch 9, Batch 569, Loss: 0.5403299331665039\n",
      "Epoch 9, Batch 570, Loss: 0.3196500539779663\n",
      "Epoch 9, Batch 571, Loss: 0.570649266242981\n",
      "Epoch 9, Batch 572, Loss: 0.683782696723938\n",
      "Epoch 9, Batch 573, Loss: 0.46123483777046204\n",
      "Epoch 9, Batch 574, Loss: 0.5464932322502136\n",
      "Epoch 9, Batch 575, Loss: 0.5523493885993958\n",
      "Epoch 9, Batch 576, Loss: 0.5130237340927124\n",
      "Epoch 9, Batch 577, Loss: 0.6642237901687622\n",
      "Epoch 9, Batch 578, Loss: 0.4799557030200958\n",
      "Epoch 9, Batch 579, Loss: 0.5001718997955322\n",
      "Epoch 9, Batch 580, Loss: 0.6263586282730103\n",
      "Epoch 9, Batch 581, Loss: 0.5967475175857544\n",
      "Epoch 9, Batch 582, Loss: 0.4985677897930145\n",
      "Epoch 9, Batch 583, Loss: 0.515942394733429\n",
      "Epoch 9, Batch 584, Loss: 0.5856798887252808\n",
      "Epoch 9, Batch 585, Loss: 0.5985004901885986\n",
      "Epoch 9, Batch 586, Loss: 0.4948723316192627\n",
      "Epoch 9, Batch 587, Loss: 0.7054962515830994\n",
      "Epoch 9, Batch 588, Loss: 0.5388576984405518\n",
      "Epoch 9, Batch 589, Loss: 0.6472289562225342\n",
      "Epoch 9, Batch 590, Loss: 0.49389782547950745\n",
      "Epoch 9, Batch 591, Loss: 0.47920313477516174\n",
      "Epoch 9, Batch 592, Loss: 0.6290493011474609\n",
      "Epoch 9, Batch 593, Loss: 0.6208499073982239\n",
      "Epoch 9, Batch 594, Loss: 0.4717050790786743\n",
      "Epoch 9, Batch 595, Loss: 0.7733303308486938\n",
      "Epoch 9, Batch 596, Loss: 0.5545175671577454\n",
      "Epoch 9, Batch 597, Loss: 0.5225450992584229\n",
      "Epoch 9, Batch 598, Loss: 0.6397063136100769\n",
      "Epoch 9, Batch 599, Loss: 0.5557711124420166\n",
      "Epoch 9, Batch 600, Loss: 0.4793178141117096\n",
      "Epoch 9, Batch 601, Loss: 0.4790322780609131\n",
      "Epoch 9, Batch 602, Loss: 0.4482308626174927\n",
      "Epoch 9, Batch 603, Loss: 0.35682594776153564\n",
      "Epoch 9, Batch 604, Loss: 0.36482420563697815\n",
      "Epoch 9, Batch 605, Loss: 0.5170394778251648\n",
      "Epoch 9, Batch 606, Loss: 0.5345268249511719\n",
      "Epoch 9, Batch 607, Loss: 0.35869112610816956\n",
      "Epoch 9, Batch 608, Loss: 0.5719250440597534\n",
      "Epoch 9, Batch 609, Loss: 0.5586456060409546\n",
      "Epoch 9, Batch 610, Loss: 0.5761021971702576\n",
      "Epoch 9, Batch 611, Loss: 0.4663780927658081\n",
      "Epoch 9, Batch 612, Loss: 0.46481430530548096\n",
      "Epoch 9, Batch 613, Loss: 0.4573231339454651\n",
      "Epoch 9, Batch 614, Loss: 0.3647209107875824\n",
      "Epoch 9, Batch 615, Loss: 0.4176497459411621\n",
      "Epoch 9, Batch 616, Loss: 0.5288316607475281\n",
      "Epoch 9, Batch 617, Loss: 0.42599818110466003\n",
      "Epoch 9, Batch 618, Loss: 0.7977261543273926\n",
      "Epoch 9, Batch 619, Loss: 0.3957577049732208\n",
      "Epoch 9, Batch 620, Loss: 0.37870606780052185\n",
      "Epoch 9, Batch 621, Loss: 0.5969133377075195\n",
      "Epoch 9, Batch 622, Loss: 0.7543512582778931\n",
      "Epoch 9, Batch 623, Loss: 0.6061040163040161\n",
      "Epoch 9, Batch 624, Loss: 0.503217875957489\n",
      "Epoch 9, Batch 625, Loss: 0.27635109424591064\n",
      "Epoch 9, Batch 626, Loss: 0.4787949025630951\n",
      "Epoch 9, Batch 627, Loss: 0.5508543252944946\n",
      "Epoch 9, Batch 628, Loss: 0.47276127338409424\n",
      "Epoch 9, Batch 629, Loss: 0.5132346749305725\n",
      "Epoch 9, Batch 630, Loss: 0.4923100471496582\n",
      "Epoch 9, Batch 631, Loss: 0.49870067834854126\n",
      "Epoch 9, Batch 632, Loss: 0.5477802157402039\n",
      "Epoch 9, Batch 633, Loss: 0.5584900379180908\n",
      "Epoch 9, Batch 634, Loss: 0.4640572667121887\n",
      "Epoch 9, Batch 635, Loss: 0.49823281168937683\n",
      "Epoch 9, Batch 636, Loss: 0.5419154763221741\n",
      "Epoch 9, Batch 637, Loss: 0.5664195418357849\n",
      "Epoch 9, Batch 638, Loss: 0.4351741075515747\n",
      "Epoch 9, Batch 639, Loss: 0.3974897861480713\n",
      "Epoch 9, Batch 640, Loss: 0.566140353679657\n",
      "Epoch 9, Batch 641, Loss: 0.6207292675971985\n",
      "Epoch 9, Batch 642, Loss: 0.6906875371932983\n",
      "Epoch 9, Batch 643, Loss: 0.514492928981781\n",
      "Epoch 9, Batch 644, Loss: 0.6789451837539673\n",
      "Epoch 9, Batch 645, Loss: 0.5406064391136169\n",
      "Epoch 9, Batch 646, Loss: 0.5130053162574768\n",
      "Epoch 9, Batch 647, Loss: 0.3791876435279846\n",
      "Epoch 9, Batch 648, Loss: 0.668391227722168\n",
      "Epoch 9, Batch 649, Loss: 0.6006827354431152\n",
      "Epoch 9, Batch 650, Loss: 0.6312499046325684\n",
      "Epoch 9, Batch 651, Loss: 0.6295934915542603\n",
      "Epoch 9, Batch 652, Loss: 0.5806114077568054\n",
      "Epoch 9, Batch 653, Loss: 0.62566739320755\n",
      "Epoch 9, Batch 654, Loss: 0.507634699344635\n",
      "Epoch 9, Batch 655, Loss: 0.5131275653839111\n",
      "Epoch 9, Batch 656, Loss: 0.47716426849365234\n",
      "Epoch 9, Batch 657, Loss: 0.5404742360115051\n",
      "Epoch 9, Batch 658, Loss: 0.6037476658821106\n",
      "Epoch 9, Batch 659, Loss: 0.5189706087112427\n",
      "Epoch 9, Batch 660, Loss: 0.5219998359680176\n",
      "Epoch 9, Batch 661, Loss: 0.4727362096309662\n",
      "Epoch 9, Batch 662, Loss: 0.5407507419586182\n",
      "Epoch 9, Batch 663, Loss: 0.4591814875602722\n",
      "Epoch 9, Batch 664, Loss: 0.4909529387950897\n",
      "Epoch 9, Batch 665, Loss: 0.6583144664764404\n",
      "Epoch 9, Batch 666, Loss: 0.4159327745437622\n",
      "Epoch 9, Batch 667, Loss: 0.6429991126060486\n",
      "Epoch 9, Batch 668, Loss: 0.4544394314289093\n",
      "Epoch 9, Batch 669, Loss: 0.5162928700447083\n",
      "Epoch 9, Batch 670, Loss: 0.7486927509307861\n",
      "Epoch 9, Batch 671, Loss: 0.7756363153457642\n",
      "Epoch 9, Batch 672, Loss: 0.5475525856018066\n",
      "Epoch 9, Batch 673, Loss: 0.3988373875617981\n",
      "Epoch 9, Batch 674, Loss: 0.7180523872375488\n",
      "Epoch 9, Batch 675, Loss: 0.6826456785202026\n",
      "Epoch 9, Batch 676, Loss: 0.558455765247345\n",
      "Epoch 9, Batch 677, Loss: 0.6042829751968384\n",
      "Epoch 9, Batch 678, Loss: 0.6974021792411804\n",
      "Epoch 9, Batch 679, Loss: 0.6057980060577393\n",
      "Epoch 9, Batch 680, Loss: 0.6055154800415039\n",
      "Epoch 9, Batch 681, Loss: 0.4112248420715332\n",
      "Epoch 9, Batch 682, Loss: 0.35259056091308594\n",
      "Epoch 9, Batch 683, Loss: 0.5287190675735474\n",
      "Epoch 9, Batch 684, Loss: 0.5999215245246887\n",
      "Epoch 9, Batch 685, Loss: 0.5954984426498413\n",
      "Epoch 9, Batch 686, Loss: 0.5558905005455017\n",
      "Epoch 9, Batch 687, Loss: 0.7339790463447571\n",
      "Epoch 9, Batch 688, Loss: 0.5025416612625122\n",
      "Epoch 9, Batch 689, Loss: 0.7003028988838196\n",
      "Epoch 9, Batch 690, Loss: 0.5889303684234619\n",
      "Epoch 9, Batch 691, Loss: 0.4136562645435333\n",
      "Epoch 9, Batch 692, Loss: 0.48573988676071167\n",
      "Epoch 9, Batch 693, Loss: 0.560103714466095\n",
      "Epoch 9, Batch 694, Loss: 0.6232745051383972\n",
      "Epoch 9, Batch 695, Loss: 0.4753454029560089\n",
      "Epoch 9, Batch 696, Loss: 0.6446370482444763\n",
      "Epoch 9, Batch 697, Loss: 0.3748437464237213\n",
      "Epoch 9, Batch 698, Loss: 0.6012678146362305\n",
      "Epoch 9, Batch 699, Loss: 0.4201413094997406\n",
      "Epoch 9, Batch 700, Loss: 0.4149937629699707\n",
      "Epoch 9, Batch 701, Loss: 0.6172754168510437\n",
      "Epoch 9, Batch 702, Loss: 0.5707364678382874\n",
      "Epoch 9, Batch 703, Loss: 0.4390306770801544\n",
      "Epoch 9, Batch 704, Loss: 0.37232476472854614\n",
      "Epoch 9, Batch 705, Loss: 0.5046442151069641\n",
      "Epoch 9, Batch 706, Loss: 0.5049437284469604\n",
      "Epoch 9, Batch 707, Loss: 0.5015096664428711\n",
      "Epoch 9, Batch 708, Loss: 0.4221549928188324\n",
      "Epoch 9, Batch 709, Loss: 0.4975088834762573\n",
      "Epoch 9, Batch 710, Loss: 0.7035606503486633\n",
      "Epoch 9, Batch 711, Loss: 0.5787917971611023\n",
      "Epoch 9, Batch 712, Loss: 0.35719993710517883\n",
      "Epoch 9, Batch 713, Loss: 0.5016400218009949\n",
      "Epoch 9, Batch 714, Loss: 0.5244799256324768\n",
      "Epoch 9, Batch 715, Loss: 0.6174416542053223\n",
      "Epoch 9, Batch 716, Loss: 0.41289016604423523\n",
      "Epoch 9, Batch 717, Loss: 0.6849156022071838\n",
      "Epoch 9, Batch 718, Loss: 0.6257551908493042\n",
      "Epoch 9, Batch 719, Loss: 0.6956662535667419\n",
      "Epoch 9, Batch 720, Loss: 0.46873411536216736\n",
      "Epoch 9, Batch 721, Loss: 0.6191416382789612\n",
      "Epoch 9, Batch 722, Loss: 0.6455668210983276\n",
      "Epoch 9, Batch 723, Loss: 0.7199543714523315\n",
      "Epoch 9, Batch 724, Loss: 0.5297637581825256\n",
      "Epoch 9, Batch 725, Loss: 0.5297724604606628\n",
      "Epoch 9, Batch 726, Loss: 0.6537770628929138\n",
      "Epoch 9, Batch 727, Loss: 0.4399774670600891\n",
      "Epoch 9, Batch 728, Loss: 0.5320147275924683\n",
      "Epoch 9, Batch 729, Loss: 0.42329132556915283\n",
      "Epoch 9, Batch 730, Loss: 0.3207249641418457\n",
      "Epoch 9, Batch 731, Loss: 0.48031026124954224\n",
      "Epoch 9, Batch 732, Loss: 0.5134375691413879\n",
      "Epoch 9, Batch 733, Loss: 0.6111929416656494\n",
      "Epoch 9, Batch 734, Loss: 0.7246379256248474\n",
      "Epoch 9, Batch 735, Loss: 0.5690790414810181\n",
      "Epoch 9, Batch 736, Loss: 0.5197528004646301\n",
      "Epoch 9, Batch 737, Loss: 0.6005093455314636\n",
      "Epoch 9, Batch 738, Loss: 0.440718412399292\n",
      "Epoch 9, Batch 739, Loss: 0.4527159333229065\n",
      "Epoch 9, Batch 740, Loss: 0.5099263191223145\n",
      "Epoch 9, Batch 741, Loss: 0.6185061931610107\n",
      "Epoch 9, Batch 742, Loss: 0.5479337573051453\n",
      "Epoch 9, Batch 743, Loss: 0.5157390832901001\n",
      "Epoch 9, Batch 744, Loss: 0.48530465364456177\n",
      "Epoch 9, Batch 745, Loss: 0.6218095421791077\n",
      "Epoch 9, Batch 746, Loss: 0.5532521605491638\n",
      "Epoch 9, Batch 747, Loss: 0.5420434474945068\n",
      "Epoch 9, Batch 748, Loss: 0.5217690467834473\n",
      "Epoch 9, Batch 749, Loss: 0.4274060130119324\n",
      "Epoch 9, Batch 750, Loss: 0.5557770133018494\n",
      "Epoch 9, Batch 751, Loss: 0.7533231973648071\n",
      "Epoch 9, Batch 752, Loss: 0.6941304802894592\n",
      "Epoch 9, Batch 753, Loss: 0.43550044298171997\n",
      "Epoch 9, Batch 754, Loss: 0.3180127739906311\n",
      "Epoch 9, Batch 755, Loss: 0.5382709503173828\n",
      "Epoch 9, Batch 756, Loss: 0.5162492990493774\n",
      "Epoch 9, Batch 757, Loss: 0.43324241042137146\n",
      "Epoch 9, Batch 758, Loss: 0.3852081298828125\n",
      "Epoch 9, Batch 759, Loss: 0.5052506327629089\n",
      "Epoch 9, Batch 760, Loss: 0.543174684047699\n",
      "Epoch 9, Batch 761, Loss: 0.4969998300075531\n",
      "Epoch 9, Batch 762, Loss: 0.9806748032569885\n",
      "Epoch 9, Batch 763, Loss: 0.6641393303871155\n",
      "Epoch 9, Batch 764, Loss: 0.623656153678894\n",
      "Epoch 9, Batch 765, Loss: 0.465103417634964\n",
      "Epoch 9, Batch 766, Loss: 0.4795970916748047\n",
      "Epoch 9, Batch 767, Loss: 0.39079105854034424\n",
      "Epoch 9, Batch 768, Loss: 0.36154425144195557\n",
      "Epoch 9, Batch 769, Loss: 0.6678947806358337\n",
      "Epoch 9, Batch 770, Loss: 0.4361596703529358\n",
      "Epoch 9, Batch 771, Loss: 0.4319441318511963\n",
      "Epoch 9, Batch 772, Loss: 0.5938941836357117\n",
      "Epoch 9, Batch 773, Loss: 0.7412182092666626\n",
      "Epoch 9, Batch 774, Loss: 0.4523426592350006\n",
      "Epoch 9, Batch 775, Loss: 0.5354104042053223\n",
      "Epoch 9, Batch 776, Loss: 0.5516558289527893\n",
      "Epoch 9, Batch 777, Loss: 0.6236315369606018\n",
      "Epoch 9, Batch 778, Loss: 0.6153549551963806\n",
      "Epoch 9, Batch 779, Loss: 0.5624053478240967\n",
      "Epoch 9, Batch 780, Loss: 0.4397348463535309\n",
      "Epoch 9, Batch 781, Loss: 0.4444306790828705\n",
      "Epoch 9, Batch 782, Loss: 0.5429987907409668\n",
      "Epoch 9, Batch 783, Loss: 0.5783209800720215\n",
      "Epoch 9, Batch 784, Loss: 0.5025116205215454\n",
      "Epoch 9, Batch 785, Loss: 0.3967950940132141\n",
      "Epoch 9, Batch 786, Loss: 0.42199981212615967\n",
      "Epoch 9, Batch 787, Loss: 0.6905316114425659\n",
      "Epoch 9, Batch 788, Loss: 0.639066755771637\n",
      "Epoch 9, Batch 789, Loss: 0.396221786737442\n",
      "Epoch 9, Batch 790, Loss: 0.5004485845565796\n",
      "Epoch 9, Batch 791, Loss: 0.4163839519023895\n",
      "Epoch 9, Batch 792, Loss: 0.5446138381958008\n",
      "Epoch 9, Batch 793, Loss: 0.5474328398704529\n",
      "Epoch 9, Batch 794, Loss: 0.5083016753196716\n",
      "Epoch 9, Batch 795, Loss: 0.5285387635231018\n",
      "Epoch 9, Batch 796, Loss: 0.499358594417572\n",
      "Epoch 9, Batch 797, Loss: 0.669330894947052\n",
      "Epoch 9, Batch 798, Loss: 0.5479257702827454\n",
      "Epoch 9, Batch 799, Loss: 0.5496236681938171\n",
      "Epoch 9, Batch 800, Loss: 0.39557352662086487\n",
      "Epoch 9, Batch 801, Loss: 0.5320515632629395\n",
      "Epoch 9, Batch 802, Loss: 0.5000219345092773\n",
      "Epoch 9, Batch 803, Loss: 0.4858596920967102\n",
      "Epoch 9, Batch 804, Loss: 0.5770246982574463\n",
      "Epoch 9, Batch 805, Loss: 0.4706707000732422\n",
      "Epoch 9, Batch 806, Loss: 0.5425853729248047\n",
      "Epoch 9, Batch 807, Loss: 0.5204327702522278\n",
      "Epoch 9, Batch 808, Loss: 0.6285409927368164\n",
      "Epoch 9, Batch 809, Loss: 0.5594574213027954\n",
      "Epoch 9, Batch 810, Loss: 0.43552061915397644\n",
      "Epoch 9, Batch 811, Loss: 0.4554503560066223\n",
      "Epoch 9, Batch 812, Loss: 0.43558362126350403\n",
      "Epoch 9, Batch 813, Loss: 0.5523008108139038\n",
      "Epoch 9, Batch 814, Loss: 0.47097092866897583\n",
      "Epoch 9, Batch 815, Loss: 0.6695747971534729\n",
      "Epoch 9, Batch 816, Loss: 0.6199726462364197\n",
      "Epoch 9, Batch 817, Loss: 0.6981426477432251\n",
      "Epoch 9, Batch 818, Loss: 0.484112411737442\n",
      "Epoch 9, Batch 819, Loss: 0.36445796489715576\n",
      "Epoch 9, Batch 820, Loss: 0.5937311053276062\n",
      "Epoch 9, Batch 821, Loss: 0.3917081356048584\n",
      "Epoch 9, Batch 822, Loss: 0.8793968558311462\n",
      "Epoch 9, Batch 823, Loss: 0.3939881920814514\n",
      "Epoch 9, Batch 824, Loss: 0.6661337614059448\n",
      "Epoch 9, Batch 825, Loss: 0.48012781143188477\n",
      "Epoch 9, Batch 826, Loss: 0.5614863634109497\n",
      "Epoch 9, Batch 827, Loss: 0.4193898141384125\n",
      "Epoch 9, Batch 828, Loss: 0.514173686504364\n",
      "Epoch 9, Batch 829, Loss: 0.3778155744075775\n",
      "Epoch 9, Batch 830, Loss: 0.5093239545822144\n",
      "Epoch 9, Batch 831, Loss: 0.6985130310058594\n",
      "Epoch 9, Batch 832, Loss: 0.6550796031951904\n",
      "Epoch 9, Batch 833, Loss: 0.5481449365615845\n",
      "Epoch 9, Batch 834, Loss: 0.48827964067459106\n",
      "Epoch 9, Batch 835, Loss: 0.36635956168174744\n",
      "Epoch 9, Batch 836, Loss: 0.6325395107269287\n",
      "Epoch 9, Batch 837, Loss: 0.6740953326225281\n",
      "Epoch 9, Batch 838, Loss: 0.42313042283058167\n",
      "Epoch 9, Batch 839, Loss: 0.44644495844841003\n",
      "Epoch 9, Batch 840, Loss: 0.6247468590736389\n",
      "Epoch 9, Batch 841, Loss: 0.4853227436542511\n",
      "Epoch 9, Batch 842, Loss: 0.5020973682403564\n",
      "Epoch 9, Batch 843, Loss: 0.4705674350261688\n",
      "Epoch 9, Batch 844, Loss: 0.45141127705574036\n",
      "Epoch 9, Batch 845, Loss: 0.527835488319397\n",
      "Epoch 9, Batch 846, Loss: 0.6005541682243347\n",
      "Epoch 9, Batch 847, Loss: 0.4696613848209381\n",
      "Epoch 9, Batch 848, Loss: 0.7169502377510071\n",
      "Epoch 9, Batch 849, Loss: 0.5118727087974548\n",
      "Epoch 9, Batch 850, Loss: 0.39331215620040894\n",
      "Epoch 9, Batch 851, Loss: 0.49440011382102966\n",
      "Epoch 9, Batch 852, Loss: 0.49057766795158386\n",
      "Epoch 9, Batch 853, Loss: 0.6717117428779602\n",
      "Epoch 9, Batch 854, Loss: 0.6011033058166504\n",
      "Epoch 9, Batch 855, Loss: 0.40326809883117676\n",
      "Epoch 9, Batch 856, Loss: 0.35953494906425476\n",
      "Epoch 9, Batch 857, Loss: 0.4834069013595581\n",
      "Epoch 9, Batch 858, Loss: 0.5082416534423828\n",
      "Epoch 9, Batch 859, Loss: 0.6147392392158508\n",
      "Epoch 9, Batch 860, Loss: 0.4401080012321472\n",
      "Epoch 9, Batch 861, Loss: 0.40846970677375793\n",
      "Epoch 9, Batch 862, Loss: 0.4790526032447815\n",
      "Epoch 9, Batch 863, Loss: 0.4703022539615631\n",
      "Epoch 9, Batch 864, Loss: 0.48702430725097656\n",
      "Epoch 9, Batch 865, Loss: 0.46177616715431213\n",
      "Epoch 9, Batch 866, Loss: 0.41317492723464966\n",
      "Epoch 9, Batch 867, Loss: 0.6090944409370422\n",
      "Epoch 9, Batch 868, Loss: 0.675521969795227\n",
      "Epoch 9, Batch 869, Loss: 0.46404147148132324\n",
      "Epoch 9, Batch 870, Loss: 0.5181412696838379\n",
      "Epoch 9, Batch 871, Loss: 0.5138840079307556\n",
      "Epoch 9, Batch 872, Loss: 0.6123050451278687\n",
      "Epoch 9, Batch 873, Loss: 0.5191452503204346\n",
      "Epoch 9, Batch 874, Loss: 0.645844578742981\n",
      "Epoch 9, Batch 875, Loss: 0.6867656111717224\n",
      "Epoch 9, Batch 876, Loss: 0.693626344203949\n",
      "Epoch 9, Batch 877, Loss: 0.5706923007965088\n",
      "Epoch 9, Batch 878, Loss: 0.4571104049682617\n",
      "Epoch 9, Batch 879, Loss: 0.5908228158950806\n",
      "Epoch 9, Batch 880, Loss: 0.5047043561935425\n",
      "Epoch 9, Batch 881, Loss: 0.5008515119552612\n",
      "Epoch 9, Batch 882, Loss: 0.6349897980690002\n",
      "Epoch 9, Batch 883, Loss: 0.39756742119789124\n",
      "Epoch 9, Batch 884, Loss: 0.4755854308605194\n",
      "Epoch 9, Batch 885, Loss: 0.5681807994842529\n",
      "Epoch 9, Batch 886, Loss: 0.2956354022026062\n",
      "Epoch 9, Batch 887, Loss: 0.3612901568412781\n",
      "Epoch 9, Batch 888, Loss: 0.61871737241745\n",
      "Epoch 9, Batch 889, Loss: 0.43576693534851074\n",
      "Epoch 9, Batch 890, Loss: 0.41352933645248413\n",
      "Epoch 9, Batch 891, Loss: 0.5245801210403442\n",
      "Epoch 9, Batch 892, Loss: 0.4921363294124603\n",
      "Epoch 9, Batch 893, Loss: 0.4445245862007141\n",
      "Epoch 9, Batch 894, Loss: 0.6480911374092102\n",
      "Epoch 9, Batch 895, Loss: 0.5526030659675598\n",
      "Epoch 9, Batch 896, Loss: 0.6074245572090149\n",
      "Epoch 9, Batch 897, Loss: 0.35251155495643616\n",
      "Epoch 9, Batch 898, Loss: 0.6245273351669312\n",
      "Epoch 9, Batch 899, Loss: 0.5390071868896484\n",
      "Epoch 9, Batch 900, Loss: 0.529143750667572\n",
      "Epoch 9, Batch 901, Loss: 0.4347569942474365\n",
      "Epoch 9, Batch 902, Loss: 0.628115177154541\n",
      "Epoch 9, Batch 903, Loss: 0.5915758609771729\n",
      "Epoch 9, Batch 904, Loss: 0.5056830644607544\n",
      "Epoch 9, Batch 905, Loss: 0.49547815322875977\n",
      "Epoch 9, Batch 906, Loss: 0.39554670453071594\n",
      "Epoch 9, Batch 907, Loss: 0.4776051938533783\n",
      "Epoch 9, Batch 908, Loss: 0.6871636509895325\n",
      "Epoch 9, Batch 909, Loss: 0.5049989223480225\n",
      "Epoch 9, Batch 910, Loss: 0.46263083815574646\n",
      "Epoch 9, Batch 911, Loss: 0.4277036786079407\n",
      "Epoch 9, Batch 912, Loss: 0.5046165585517883\n",
      "Epoch 9, Batch 913, Loss: 0.6559067368507385\n",
      "Epoch 9, Batch 914, Loss: 0.49137407541275024\n",
      "Epoch 9, Batch 915, Loss: 0.5275777578353882\n",
      "Epoch 9, Batch 916, Loss: 0.3317025899887085\n",
      "Epoch 9, Batch 917, Loss: 0.6504671573638916\n",
      "Epoch 9, Batch 918, Loss: 0.6025347113609314\n",
      "Epoch 9, Batch 919, Loss: 0.6390891075134277\n",
      "Epoch 9, Batch 920, Loss: 0.4544019103050232\n",
      "Epoch 9, Batch 921, Loss: 0.5969275236129761\n",
      "Epoch 9, Batch 922, Loss: 0.5606963634490967\n",
      "Epoch 9, Batch 923, Loss: 0.40003764629364014\n",
      "Epoch 9, Batch 924, Loss: 0.6939566135406494\n",
      "Epoch 9, Batch 925, Loss: 0.47248294949531555\n",
      "Epoch 9, Batch 926, Loss: 0.7546478509902954\n",
      "Epoch 9, Batch 927, Loss: 0.7659557461738586\n",
      "Epoch 9, Batch 928, Loss: 0.6264098286628723\n",
      "Epoch 9, Batch 929, Loss: 0.4582826495170593\n",
      "Epoch 9, Batch 930, Loss: 0.43447449803352356\n",
      "Epoch 9, Batch 931, Loss: 0.4196223020553589\n",
      "Epoch 9, Batch 932, Loss: 0.5654664039611816\n",
      "Epoch 9, Batch 933, Loss: 0.4416211247444153\n",
      "Epoch 9, Batch 934, Loss: 0.4141659736633301\n",
      "Epoch 9, Batch 935, Loss: 0.5575318932533264\n",
      "Epoch 9, Batch 936, Loss: 0.5534013509750366\n",
      "Epoch 9, Batch 937, Loss: 0.3791232109069824\n",
      "Epoch 9, Batch 938, Loss: 0.4852345883846283\n",
      "Accuracy of train set: 0.8134333333333333\n",
      "Epoch 9, Batch 1, Test Loss: 0.7139148712158203\n",
      "Epoch 9, Batch 2, Test Loss: 0.31492602825164795\n",
      "Epoch 9, Batch 3, Test Loss: 1.0440726280212402\n",
      "Epoch 9, Batch 4, Test Loss: 0.7722344398498535\n",
      "Epoch 9, Batch 5, Test Loss: 0.7206912636756897\n",
      "Epoch 9, Batch 6, Test Loss: 0.5901339054107666\n",
      "Epoch 9, Batch 7, Test Loss: 0.60901939868927\n",
      "Epoch 9, Batch 8, Test Loss: 0.5830260515213013\n",
      "Epoch 9, Batch 9, Test Loss: 0.4986613392829895\n",
      "Epoch 9, Batch 10, Test Loss: 0.518502950668335\n",
      "Epoch 9, Batch 11, Test Loss: 0.5514180660247803\n",
      "Epoch 9, Batch 12, Test Loss: 0.8302006721496582\n",
      "Epoch 9, Batch 13, Test Loss: 0.4678839445114136\n",
      "Epoch 9, Batch 14, Test Loss: 0.5210720896720886\n",
      "Epoch 9, Batch 15, Test Loss: 0.7138667702674866\n",
      "Epoch 9, Batch 16, Test Loss: 0.6143479347229004\n",
      "Epoch 9, Batch 17, Test Loss: 0.7852393984794617\n",
      "Epoch 9, Batch 18, Test Loss: 0.6243425607681274\n",
      "Epoch 9, Batch 19, Test Loss: 0.31485962867736816\n",
      "Epoch 9, Batch 20, Test Loss: 0.5000333786010742\n",
      "Epoch 9, Batch 21, Test Loss: 0.6049167513847351\n",
      "Epoch 9, Batch 22, Test Loss: 0.45566824078559875\n",
      "Epoch 9, Batch 23, Test Loss: 0.6139479875564575\n",
      "Epoch 9, Batch 24, Test Loss: 0.6896703243255615\n",
      "Epoch 9, Batch 25, Test Loss: 0.6555296778678894\n",
      "Epoch 9, Batch 26, Test Loss: 0.505952775478363\n",
      "Epoch 9, Batch 27, Test Loss: 0.6218698024749756\n",
      "Epoch 9, Batch 28, Test Loss: 0.4570884108543396\n",
      "Epoch 9, Batch 29, Test Loss: 0.6493718028068542\n",
      "Epoch 9, Batch 30, Test Loss: 0.5237864255905151\n",
      "Epoch 9, Batch 31, Test Loss: 0.9649819135665894\n",
      "Epoch 9, Batch 32, Test Loss: 0.6070340275764465\n",
      "Epoch 9, Batch 33, Test Loss: 0.4358418881893158\n",
      "Epoch 9, Batch 34, Test Loss: 0.39181768894195557\n",
      "Epoch 9, Batch 35, Test Loss: 0.616519570350647\n",
      "Epoch 9, Batch 36, Test Loss: 0.41190698742866516\n",
      "Epoch 9, Batch 37, Test Loss: 0.6697475910186768\n",
      "Epoch 9, Batch 38, Test Loss: 0.48555848002433777\n",
      "Epoch 9, Batch 39, Test Loss: 0.5455204248428345\n",
      "Epoch 9, Batch 40, Test Loss: 0.60787433385849\n",
      "Epoch 9, Batch 41, Test Loss: 0.5094046592712402\n",
      "Epoch 9, Batch 42, Test Loss: 0.6472190618515015\n",
      "Epoch 9, Batch 43, Test Loss: 0.4970596134662628\n",
      "Epoch 9, Batch 44, Test Loss: 0.5785254240036011\n",
      "Epoch 9, Batch 45, Test Loss: 0.6248034238815308\n",
      "Epoch 9, Batch 46, Test Loss: 0.4660867750644684\n",
      "Epoch 9, Batch 47, Test Loss: 0.44718828797340393\n",
      "Epoch 9, Batch 48, Test Loss: 0.6473958492279053\n",
      "Epoch 9, Batch 49, Test Loss: 0.8038491606712341\n",
      "Epoch 9, Batch 50, Test Loss: 0.5445414781570435\n",
      "Epoch 9, Batch 51, Test Loss: 0.6339104175567627\n",
      "Epoch 9, Batch 52, Test Loss: 0.4780195355415344\n",
      "Epoch 9, Batch 53, Test Loss: 0.4140888452529907\n",
      "Epoch 9, Batch 54, Test Loss: 0.5847910642623901\n",
      "Epoch 9, Batch 55, Test Loss: 0.45063596963882446\n",
      "Epoch 9, Batch 56, Test Loss: 0.6137923002243042\n",
      "Epoch 9, Batch 57, Test Loss: 0.7453826069831848\n",
      "Epoch 9, Batch 58, Test Loss: 0.36967891454696655\n",
      "Epoch 9, Batch 59, Test Loss: 0.6737682223320007\n",
      "Epoch 9, Batch 60, Test Loss: 0.40019872784614563\n",
      "Epoch 9, Batch 61, Test Loss: 0.6720554828643799\n",
      "Epoch 9, Batch 62, Test Loss: 0.5819433331489563\n",
      "Epoch 9, Batch 63, Test Loss: 0.3761911690235138\n",
      "Epoch 9, Batch 64, Test Loss: 0.5444936156272888\n",
      "Epoch 9, Batch 65, Test Loss: 0.543817937374115\n",
      "Epoch 9, Batch 66, Test Loss: 0.6772024035453796\n",
      "Epoch 9, Batch 67, Test Loss: 0.5549393892288208\n",
      "Epoch 9, Batch 68, Test Loss: 0.7114640474319458\n",
      "Epoch 9, Batch 69, Test Loss: 0.5188582539558411\n",
      "Epoch 9, Batch 70, Test Loss: 0.4692295491695404\n",
      "Epoch 9, Batch 71, Test Loss: 0.5546032190322876\n",
      "Epoch 9, Batch 72, Test Loss: 0.5227954983711243\n",
      "Epoch 9, Batch 73, Test Loss: 0.7121351957321167\n",
      "Epoch 9, Batch 74, Test Loss: 0.45094823837280273\n",
      "Epoch 9, Batch 75, Test Loss: 0.7332274913787842\n",
      "Epoch 9, Batch 76, Test Loss: 0.408873975276947\n",
      "Epoch 9, Batch 77, Test Loss: 0.39612483978271484\n",
      "Epoch 9, Batch 78, Test Loss: 0.7322498559951782\n",
      "Epoch 9, Batch 79, Test Loss: 0.7357238531112671\n",
      "Epoch 9, Batch 80, Test Loss: 0.8570016026496887\n",
      "Epoch 9, Batch 81, Test Loss: 0.3506253957748413\n",
      "Epoch 9, Batch 82, Test Loss: 0.5778734683990479\n",
      "Epoch 9, Batch 83, Test Loss: 0.5359265804290771\n",
      "Epoch 9, Batch 84, Test Loss: 0.5855423212051392\n",
      "Epoch 9, Batch 85, Test Loss: 0.3741878867149353\n",
      "Epoch 9, Batch 86, Test Loss: 0.3992030620574951\n",
      "Epoch 9, Batch 87, Test Loss: 0.4371500313282013\n",
      "Epoch 9, Batch 88, Test Loss: 0.7150044441223145\n",
      "Epoch 9, Batch 89, Test Loss: 0.6140895485877991\n",
      "Epoch 9, Batch 90, Test Loss: 0.4584018588066101\n",
      "Epoch 9, Batch 91, Test Loss: 0.5783942937850952\n",
      "Epoch 9, Batch 92, Test Loss: 0.7435445189476013\n",
      "Epoch 9, Batch 93, Test Loss: 0.4784718453884125\n",
      "Epoch 9, Batch 94, Test Loss: 0.5825976133346558\n",
      "Epoch 9, Batch 95, Test Loss: 0.5302070379257202\n",
      "Epoch 9, Batch 96, Test Loss: 0.5585776567459106\n",
      "Epoch 9, Batch 97, Test Loss: 0.7547436952590942\n",
      "Epoch 9, Batch 98, Test Loss: 0.42973265051841736\n",
      "Epoch 9, Batch 99, Test Loss: 0.5023860931396484\n",
      "Epoch 9, Batch 100, Test Loss: 0.45751288533210754\n",
      "Epoch 9, Batch 101, Test Loss: 0.6327330470085144\n",
      "Epoch 9, Batch 102, Test Loss: 0.469982773065567\n",
      "Epoch 9, Batch 103, Test Loss: 0.6216492056846619\n",
      "Epoch 9, Batch 104, Test Loss: 0.6330693960189819\n",
      "Epoch 9, Batch 105, Test Loss: 0.4952751696109772\n",
      "Epoch 9, Batch 106, Test Loss: 0.43597373366355896\n",
      "Epoch 9, Batch 107, Test Loss: 0.5228114128112793\n",
      "Epoch 9, Batch 108, Test Loss: 0.4925099015235901\n",
      "Epoch 9, Batch 109, Test Loss: 0.6982260346412659\n",
      "Epoch 9, Batch 110, Test Loss: 0.4823242425918579\n",
      "Epoch 9, Batch 111, Test Loss: 0.42541447281837463\n",
      "Epoch 9, Batch 112, Test Loss: 0.5303700566291809\n",
      "Epoch 9, Batch 113, Test Loss: 0.4258503317832947\n",
      "Epoch 9, Batch 114, Test Loss: 0.6091418862342834\n",
      "Epoch 9, Batch 115, Test Loss: 0.38987064361572266\n",
      "Epoch 9, Batch 116, Test Loss: 0.4925386309623718\n",
      "Epoch 9, Batch 117, Test Loss: 0.6593595743179321\n",
      "Epoch 9, Batch 118, Test Loss: 0.5245068073272705\n",
      "Epoch 9, Batch 119, Test Loss: 0.3672116994857788\n",
      "Epoch 9, Batch 120, Test Loss: 0.6857845187187195\n",
      "Epoch 9, Batch 121, Test Loss: 0.6229145526885986\n",
      "Epoch 9, Batch 122, Test Loss: 0.5856375694274902\n",
      "Epoch 9, Batch 123, Test Loss: 0.46375197172164917\n",
      "Epoch 9, Batch 124, Test Loss: 0.6003074049949646\n",
      "Epoch 9, Batch 125, Test Loss: 0.4798959195613861\n",
      "Epoch 9, Batch 126, Test Loss: 0.5903686881065369\n",
      "Epoch 9, Batch 127, Test Loss: 0.7521270513534546\n",
      "Epoch 9, Batch 128, Test Loss: 0.7552307844161987\n",
      "Epoch 9, Batch 129, Test Loss: 0.29701894521713257\n",
      "Epoch 9, Batch 130, Test Loss: 0.3442867696285248\n",
      "Epoch 9, Batch 131, Test Loss: 0.41674357652664185\n",
      "Epoch 9, Batch 132, Test Loss: 0.5332327485084534\n",
      "Epoch 9, Batch 133, Test Loss: 0.4889434576034546\n",
      "Epoch 9, Batch 134, Test Loss: 0.5983351469039917\n",
      "Epoch 9, Batch 135, Test Loss: 0.4528955817222595\n",
      "Epoch 9, Batch 136, Test Loss: 0.5935633182525635\n",
      "Epoch 9, Batch 137, Test Loss: 0.8055399656295776\n",
      "Epoch 9, Batch 138, Test Loss: 0.7195630073547363\n",
      "Epoch 9, Batch 139, Test Loss: 0.521639883518219\n",
      "Epoch 9, Batch 140, Test Loss: 0.5650086402893066\n",
      "Epoch 9, Batch 141, Test Loss: 0.7334250211715698\n",
      "Epoch 9, Batch 142, Test Loss: 0.744677722454071\n",
      "Epoch 9, Batch 143, Test Loss: 0.4617787301540375\n",
      "Epoch 9, Batch 144, Test Loss: 0.6521813869476318\n",
      "Epoch 9, Batch 145, Test Loss: 0.3762732148170471\n",
      "Epoch 9, Batch 146, Test Loss: 0.3674204647541046\n",
      "Epoch 9, Batch 147, Test Loss: 0.6520987749099731\n",
      "Epoch 9, Batch 148, Test Loss: 0.42106398940086365\n",
      "Epoch 9, Batch 149, Test Loss: 0.420632004737854\n",
      "Epoch 9, Batch 150, Test Loss: 0.4245310127735138\n",
      "Epoch 9, Batch 151, Test Loss: 0.5420185327529907\n",
      "Epoch 9, Batch 152, Test Loss: 0.6300909519195557\n",
      "Epoch 9, Batch 153, Test Loss: 0.26018884778022766\n",
      "Epoch 9, Batch 154, Test Loss: 0.4659847617149353\n",
      "Epoch 9, Batch 155, Test Loss: 0.610942542552948\n",
      "Epoch 9, Batch 156, Test Loss: 0.47848203778266907\n",
      "Epoch 9, Batch 157, Test Loss: 0.47197893261909485\n",
      "Epoch 9, Batch 158, Test Loss: 0.6179074645042419\n",
      "Epoch 9, Batch 159, Test Loss: 0.7735227346420288\n",
      "Epoch 9, Batch 160, Test Loss: 0.5717593431472778\n",
      "Epoch 9, Batch 161, Test Loss: 0.5133481621742249\n",
      "Epoch 9, Batch 162, Test Loss: 0.42497724294662476\n",
      "Epoch 9, Batch 163, Test Loss: 0.3626291751861572\n",
      "Epoch 9, Batch 164, Test Loss: 0.514884889125824\n",
      "Epoch 9, Batch 165, Test Loss: 0.5981889963150024\n",
      "Epoch 9, Batch 166, Test Loss: 0.5161119699478149\n",
      "Epoch 9, Batch 167, Test Loss: 0.3667893409729004\n",
      "Epoch 9, Batch 168, Test Loss: 0.7238065004348755\n",
      "Epoch 9, Batch 169, Test Loss: 0.6292547583580017\n",
      "Epoch 9, Batch 170, Test Loss: 0.3804391622543335\n",
      "Epoch 9, Batch 171, Test Loss: 0.38515692949295044\n",
      "Epoch 9, Batch 172, Test Loss: 0.4550265669822693\n",
      "Epoch 9, Batch 173, Test Loss: 0.5484273433685303\n",
      "Epoch 9, Batch 174, Test Loss: 0.5768030285835266\n",
      "Epoch 9, Batch 175, Test Loss: 0.6130790710449219\n",
      "Epoch 9, Batch 176, Test Loss: 0.4422003924846649\n",
      "Epoch 9, Batch 177, Test Loss: 0.3644396662712097\n",
      "Epoch 9, Batch 178, Test Loss: 0.42359909415245056\n",
      "Epoch 9, Batch 179, Test Loss: 0.5045552253723145\n",
      "Epoch 9, Batch 180, Test Loss: 0.42204853892326355\n",
      "Epoch 9, Batch 181, Test Loss: 0.4571034908294678\n",
      "Epoch 9, Batch 182, Test Loss: 0.5041014552116394\n",
      "Epoch 9, Batch 183, Test Loss: 0.4281923472881317\n",
      "Epoch 9, Batch 184, Test Loss: 0.47968775033950806\n",
      "Epoch 9, Batch 185, Test Loss: 0.4131574332714081\n",
      "Epoch 9, Batch 186, Test Loss: 0.7106178402900696\n",
      "Epoch 9, Batch 187, Test Loss: 0.5236342549324036\n",
      "Epoch 9, Batch 188, Test Loss: 0.5089722871780396\n",
      "Epoch 9, Batch 189, Test Loss: 0.5175254940986633\n",
      "Epoch 9, Batch 190, Test Loss: 0.42877182364463806\n",
      "Epoch 9, Batch 191, Test Loss: 0.6766552329063416\n",
      "Epoch 9, Batch 192, Test Loss: 0.6157122850418091\n",
      "Epoch 9, Batch 193, Test Loss: 0.9542847871780396\n",
      "Epoch 9, Batch 194, Test Loss: 0.5625261664390564\n",
      "Epoch 9, Batch 195, Test Loss: 0.47848406434059143\n",
      "Epoch 9, Batch 196, Test Loss: 0.5254723429679871\n",
      "Epoch 9, Batch 197, Test Loss: 0.44436216354370117\n",
      "Epoch 9, Batch 198, Test Loss: 0.6016875505447388\n",
      "Epoch 9, Batch 199, Test Loss: 0.6167923212051392\n",
      "Epoch 9, Batch 200, Test Loss: 0.5074684023857117\n",
      "Epoch 9, Batch 201, Test Loss: 0.4373602271080017\n",
      "Epoch 9, Batch 202, Test Loss: 0.5637666583061218\n",
      "Epoch 9, Batch 203, Test Loss: 0.5908017754554749\n",
      "Epoch 9, Batch 204, Test Loss: 0.7789232134819031\n",
      "Epoch 9, Batch 205, Test Loss: 0.5100176930427551\n",
      "Epoch 9, Batch 206, Test Loss: 0.5122271180152893\n",
      "Epoch 9, Batch 207, Test Loss: 0.35790228843688965\n",
      "Epoch 9, Batch 208, Test Loss: 0.6349407434463501\n",
      "Epoch 9, Batch 209, Test Loss: 0.500136137008667\n",
      "Epoch 9, Batch 210, Test Loss: 0.6955586671829224\n",
      "Epoch 9, Batch 211, Test Loss: 0.5171173214912415\n",
      "Epoch 9, Batch 212, Test Loss: 0.7097485065460205\n",
      "Epoch 9, Batch 213, Test Loss: 0.4684368073940277\n",
      "Epoch 9, Batch 214, Test Loss: 0.4354340434074402\n",
      "Epoch 9, Batch 215, Test Loss: 0.4758586883544922\n",
      "Epoch 9, Batch 216, Test Loss: 0.6001689434051514\n",
      "Epoch 9, Batch 217, Test Loss: 0.4527239203453064\n",
      "Epoch 9, Batch 218, Test Loss: 0.4639964997768402\n",
      "Epoch 9, Batch 219, Test Loss: 0.6335015296936035\n",
      "Epoch 9, Batch 220, Test Loss: 0.4952240586280823\n",
      "Epoch 9, Batch 221, Test Loss: 0.5775326490402222\n",
      "Epoch 9, Batch 222, Test Loss: 0.49324071407318115\n",
      "Epoch 9, Batch 223, Test Loss: 0.5329887866973877\n",
      "Epoch 9, Batch 224, Test Loss: 0.546688437461853\n",
      "Epoch 9, Batch 225, Test Loss: 0.8293382525444031\n",
      "Epoch 9, Batch 226, Test Loss: 0.4730214476585388\n",
      "Epoch 9, Batch 227, Test Loss: 0.5419670939445496\n",
      "Epoch 9, Batch 228, Test Loss: 0.5865249037742615\n",
      "Epoch 9, Batch 229, Test Loss: 0.6059948205947876\n",
      "Epoch 9, Batch 230, Test Loss: 0.5657064914703369\n",
      "Epoch 9, Batch 231, Test Loss: 0.4553133249282837\n",
      "Epoch 9, Batch 232, Test Loss: 0.4548225700855255\n",
      "Epoch 9, Batch 233, Test Loss: 0.5387553572654724\n",
      "Epoch 9, Batch 234, Test Loss: 0.5224751830101013\n",
      "Epoch 9, Batch 235, Test Loss: 0.519192099571228\n",
      "Epoch 9, Batch 236, Test Loss: 0.4479249119758606\n",
      "Epoch 9, Batch 237, Test Loss: 0.7084370255470276\n",
      "Epoch 9, Batch 238, Test Loss: 0.6396325826644897\n",
      "Epoch 9, Batch 239, Test Loss: 0.5725579261779785\n",
      "Epoch 9, Batch 240, Test Loss: 0.3462918698787689\n",
      "Epoch 9, Batch 241, Test Loss: 0.6394010186195374\n",
      "Epoch 9, Batch 242, Test Loss: 0.3574696481227875\n",
      "Epoch 9, Batch 243, Test Loss: 0.7170448899269104\n",
      "Epoch 9, Batch 244, Test Loss: 0.6220107078552246\n",
      "Epoch 9, Batch 245, Test Loss: 0.3817156255245209\n",
      "Epoch 9, Batch 246, Test Loss: 0.3941732943058014\n",
      "Epoch 9, Batch 247, Test Loss: 0.500851571559906\n",
      "Epoch 9, Batch 248, Test Loss: 0.5087757706642151\n",
      "Epoch 9, Batch 249, Test Loss: 0.32878196239471436\n",
      "Epoch 9, Batch 250, Test Loss: 0.740386962890625\n",
      "Epoch 9, Batch 251, Test Loss: 0.564614474773407\n",
      "Epoch 9, Batch 252, Test Loss: 0.586834192276001\n",
      "Epoch 9, Batch 253, Test Loss: 0.5267574191093445\n",
      "Epoch 9, Batch 254, Test Loss: 0.506689190864563\n",
      "Epoch 9, Batch 255, Test Loss: 0.7702270746231079\n",
      "Epoch 9, Batch 256, Test Loss: 0.5454995036125183\n",
      "Epoch 9, Batch 257, Test Loss: 0.3947804570198059\n",
      "Epoch 9, Batch 258, Test Loss: 0.4685514569282532\n",
      "Epoch 9, Batch 259, Test Loss: 0.7016096711158752\n",
      "Epoch 9, Batch 260, Test Loss: 0.7316334843635559\n",
      "Epoch 9, Batch 261, Test Loss: 0.5856287479400635\n",
      "Epoch 9, Batch 262, Test Loss: 0.6653224229812622\n",
      "Epoch 9, Batch 263, Test Loss: 0.5939855575561523\n",
      "Epoch 9, Batch 264, Test Loss: 0.35697001218795776\n",
      "Epoch 9, Batch 265, Test Loss: 0.4367060661315918\n",
      "Epoch 9, Batch 266, Test Loss: 0.5455865263938904\n",
      "Epoch 9, Batch 267, Test Loss: 0.583782970905304\n",
      "Epoch 9, Batch 268, Test Loss: 0.5163529515266418\n",
      "Epoch 9, Batch 269, Test Loss: 0.5111806392669678\n",
      "Epoch 9, Batch 270, Test Loss: 0.527491569519043\n",
      "Epoch 9, Batch 271, Test Loss: 0.6366235017776489\n",
      "Epoch 9, Batch 272, Test Loss: 0.46499425172805786\n",
      "Epoch 9, Batch 273, Test Loss: 0.382500022649765\n",
      "Epoch 9, Batch 274, Test Loss: 0.5702029466629028\n",
      "Epoch 9, Batch 275, Test Loss: 0.5834226608276367\n",
      "Epoch 9, Batch 276, Test Loss: 0.5961141586303711\n",
      "Epoch 9, Batch 277, Test Loss: 0.48617643117904663\n",
      "Epoch 9, Batch 278, Test Loss: 0.517025887966156\n",
      "Epoch 9, Batch 279, Test Loss: 0.4913427233695984\n",
      "Epoch 9, Batch 280, Test Loss: 0.5662757158279419\n",
      "Epoch 9, Batch 281, Test Loss: 0.9933598637580872\n",
      "Epoch 9, Batch 282, Test Loss: 0.48947909474372864\n",
      "Epoch 9, Batch 283, Test Loss: 0.3845866620540619\n",
      "Epoch 9, Batch 284, Test Loss: 0.7055487632751465\n",
      "Epoch 9, Batch 285, Test Loss: 0.5248281359672546\n",
      "Epoch 9, Batch 286, Test Loss: 0.6519869565963745\n",
      "Epoch 9, Batch 287, Test Loss: 0.37467724084854126\n",
      "Epoch 9, Batch 288, Test Loss: 0.5489826798439026\n",
      "Epoch 9, Batch 289, Test Loss: 0.5240957736968994\n",
      "Epoch 9, Batch 290, Test Loss: 0.5210002064704895\n",
      "Epoch 9, Batch 291, Test Loss: 0.5628243684768677\n",
      "Epoch 9, Batch 292, Test Loss: 0.5850921273231506\n",
      "Epoch 9, Batch 293, Test Loss: 0.49397704005241394\n",
      "Epoch 9, Batch 294, Test Loss: 0.6918269991874695\n",
      "Epoch 9, Batch 295, Test Loss: 0.7091459631919861\n",
      "Epoch 9, Batch 296, Test Loss: 0.5632802248001099\n",
      "Epoch 9, Batch 297, Test Loss: 0.4830966591835022\n",
      "Epoch 9, Batch 298, Test Loss: 0.3855728209018707\n",
      "Epoch 9, Batch 299, Test Loss: 0.5947614312171936\n",
      "Epoch 9, Batch 300, Test Loss: 0.5735540390014648\n",
      "Epoch 9, Batch 301, Test Loss: 0.557159423828125\n",
      "Epoch 9, Batch 302, Test Loss: 0.5931254625320435\n",
      "Epoch 9, Batch 303, Test Loss: 0.4886314868927002\n",
      "Epoch 9, Batch 304, Test Loss: 0.5249543786048889\n",
      "Epoch 9, Batch 305, Test Loss: 0.6284518241882324\n",
      "Epoch 9, Batch 306, Test Loss: 0.5060064792633057\n",
      "Epoch 9, Batch 307, Test Loss: 0.5423390865325928\n",
      "Epoch 9, Batch 308, Test Loss: 0.5551896095275879\n",
      "Epoch 9, Batch 309, Test Loss: 0.4620021879673004\n",
      "Epoch 9, Batch 310, Test Loss: 0.5152596831321716\n",
      "Epoch 9, Batch 311, Test Loss: 0.5827261209487915\n",
      "Epoch 9, Batch 312, Test Loss: 0.6655288338661194\n",
      "Epoch 9, Batch 313, Test Loss: 0.6576629877090454\n",
      "Epoch 9, Batch 314, Test Loss: 0.5204511880874634\n",
      "Epoch 9, Batch 315, Test Loss: 0.5335503220558167\n",
      "Epoch 9, Batch 316, Test Loss: 0.6438305377960205\n",
      "Epoch 9, Batch 317, Test Loss: 0.7301238775253296\n",
      "Epoch 9, Batch 318, Test Loss: 0.45329052209854126\n",
      "Epoch 9, Batch 319, Test Loss: 0.8624502420425415\n",
      "Epoch 9, Batch 320, Test Loss: 0.4851890206336975\n",
      "Epoch 9, Batch 321, Test Loss: 0.5223478674888611\n",
      "Epoch 9, Batch 322, Test Loss: 0.4046379327774048\n",
      "Epoch 9, Batch 323, Test Loss: 0.5404290556907654\n",
      "Epoch 9, Batch 324, Test Loss: 0.3710020184516907\n",
      "Epoch 9, Batch 325, Test Loss: 0.5707764625549316\n",
      "Epoch 9, Batch 326, Test Loss: 0.5555471181869507\n",
      "Epoch 9, Batch 327, Test Loss: 0.6329111456871033\n",
      "Epoch 9, Batch 328, Test Loss: 0.5782543420791626\n",
      "Epoch 9, Batch 329, Test Loss: 0.7113581299781799\n",
      "Epoch 9, Batch 330, Test Loss: 0.3327683210372925\n",
      "Epoch 9, Batch 331, Test Loss: 0.5447372198104858\n",
      "Epoch 9, Batch 332, Test Loss: 0.649672269821167\n",
      "Epoch 9, Batch 333, Test Loss: 0.5973575711250305\n",
      "Epoch 9, Batch 334, Test Loss: 0.7071212530136108\n",
      "Epoch 9, Batch 335, Test Loss: 0.8395732641220093\n",
      "Epoch 9, Batch 336, Test Loss: 0.47576841711997986\n",
      "Epoch 9, Batch 337, Test Loss: 0.6674005389213562\n",
      "Epoch 9, Batch 338, Test Loss: 0.43518656492233276\n",
      "Epoch 9, Batch 339, Test Loss: 0.2765589952468872\n",
      "Epoch 9, Batch 340, Test Loss: 0.3952848017215729\n",
      "Epoch 9, Batch 341, Test Loss: 0.6197571158409119\n",
      "Epoch 9, Batch 342, Test Loss: 0.489934504032135\n",
      "Epoch 9, Batch 343, Test Loss: 0.48309096693992615\n",
      "Epoch 9, Batch 344, Test Loss: 0.426194429397583\n",
      "Epoch 9, Batch 345, Test Loss: 0.6437863111495972\n",
      "Epoch 9, Batch 346, Test Loss: 0.5041092038154602\n",
      "Epoch 9, Batch 347, Test Loss: 0.3364422917366028\n",
      "Epoch 9, Batch 348, Test Loss: 0.6117952466011047\n",
      "Epoch 9, Batch 349, Test Loss: 0.42872580885887146\n",
      "Epoch 9, Batch 350, Test Loss: 0.44946742057800293\n",
      "Epoch 9, Batch 351, Test Loss: 0.582833468914032\n",
      "Epoch 9, Batch 352, Test Loss: 0.44288378953933716\n",
      "Epoch 9, Batch 353, Test Loss: 0.5419421195983887\n",
      "Epoch 9, Batch 354, Test Loss: 0.4432010054588318\n",
      "Epoch 9, Batch 355, Test Loss: 0.468441903591156\n",
      "Epoch 9, Batch 356, Test Loss: 0.32691848278045654\n",
      "Epoch 9, Batch 357, Test Loss: 0.41394051909446716\n",
      "Epoch 9, Batch 358, Test Loss: 0.5465448498725891\n",
      "Epoch 9, Batch 359, Test Loss: 0.5637460947036743\n",
      "Epoch 9, Batch 360, Test Loss: 0.5007166266441345\n",
      "Epoch 9, Batch 361, Test Loss: 0.6070770025253296\n",
      "Epoch 9, Batch 362, Test Loss: 0.6901589632034302\n",
      "Epoch 9, Batch 363, Test Loss: 0.6358805298805237\n",
      "Epoch 9, Batch 364, Test Loss: 0.5758113265037537\n",
      "Epoch 9, Batch 365, Test Loss: 0.5359246730804443\n",
      "Epoch 9, Batch 366, Test Loss: 0.40430837869644165\n",
      "Epoch 9, Batch 367, Test Loss: 0.7661653757095337\n",
      "Epoch 9, Batch 368, Test Loss: 0.4633477032184601\n",
      "Epoch 9, Batch 369, Test Loss: 0.5821250677108765\n",
      "Epoch 9, Batch 370, Test Loss: 0.6547963619232178\n",
      "Epoch 9, Batch 371, Test Loss: 0.5359452962875366\n",
      "Epoch 9, Batch 372, Test Loss: 0.6083341240882874\n",
      "Epoch 9, Batch 373, Test Loss: 0.6004477739334106\n",
      "Epoch 9, Batch 374, Test Loss: 0.6237988471984863\n",
      "Epoch 9, Batch 375, Test Loss: 1.1056015491485596\n",
      "Epoch 9, Batch 376, Test Loss: 0.7308377623558044\n",
      "Epoch 9, Batch 377, Test Loss: 0.541053831577301\n",
      "Epoch 9, Batch 378, Test Loss: 0.6961577534675598\n",
      "Epoch 9, Batch 379, Test Loss: 0.5521941781044006\n",
      "Epoch 9, Batch 380, Test Loss: 0.41292282938957214\n",
      "Epoch 9, Batch 381, Test Loss: 0.44621556997299194\n",
      "Epoch 9, Batch 382, Test Loss: 0.5188866853713989\n",
      "Epoch 9, Batch 383, Test Loss: 0.5362011194229126\n",
      "Epoch 9, Batch 384, Test Loss: 0.511528491973877\n",
      "Epoch 9, Batch 385, Test Loss: 0.4621303379535675\n",
      "Epoch 9, Batch 386, Test Loss: 0.3899722695350647\n",
      "Epoch 9, Batch 387, Test Loss: 0.6220632791519165\n",
      "Epoch 9, Batch 388, Test Loss: 0.4543861448764801\n",
      "Epoch 9, Batch 389, Test Loss: 0.5596432089805603\n",
      "Epoch 9, Batch 390, Test Loss: 0.4999102056026459\n",
      "Epoch 9, Batch 391, Test Loss: 0.6150209903717041\n",
      "Epoch 9, Batch 392, Test Loss: 0.2957962453365326\n",
      "Epoch 9, Batch 393, Test Loss: 0.6287491321563721\n",
      "Epoch 9, Batch 394, Test Loss: 0.3892722725868225\n",
      "Epoch 9, Batch 395, Test Loss: 0.4367136061191559\n",
      "Epoch 9, Batch 396, Test Loss: 0.7624498009681702\n",
      "Epoch 9, Batch 397, Test Loss: 0.6274498105049133\n",
      "Epoch 9, Batch 398, Test Loss: 0.6046156287193298\n",
      "Epoch 9, Batch 399, Test Loss: 0.6746529340744019\n",
      "Epoch 9, Batch 400, Test Loss: 0.3901121914386749\n",
      "Epoch 9, Batch 401, Test Loss: 0.41723179817199707\n",
      "Epoch 9, Batch 402, Test Loss: 0.6083941459655762\n",
      "Epoch 9, Batch 403, Test Loss: 0.5759319067001343\n",
      "Epoch 9, Batch 404, Test Loss: 0.5769294500350952\n",
      "Epoch 9, Batch 405, Test Loss: 0.5239779949188232\n",
      "Epoch 9, Batch 406, Test Loss: 0.6464089751243591\n",
      "Epoch 9, Batch 407, Test Loss: 0.47119593620300293\n",
      "Epoch 9, Batch 408, Test Loss: 0.5197957754135132\n",
      "Epoch 9, Batch 409, Test Loss: 0.5763618350028992\n",
      "Epoch 9, Batch 410, Test Loss: 0.5049123167991638\n",
      "Epoch 9, Batch 411, Test Loss: 0.6301015019416809\n",
      "Epoch 9, Batch 412, Test Loss: 0.6071295738220215\n",
      "Epoch 9, Batch 413, Test Loss: 0.5628167986869812\n",
      "Epoch 9, Batch 414, Test Loss: 0.5790761709213257\n",
      "Epoch 9, Batch 415, Test Loss: 0.45476019382476807\n",
      "Epoch 9, Batch 416, Test Loss: 0.6839969754219055\n",
      "Epoch 9, Batch 417, Test Loss: 0.5627472400665283\n",
      "Epoch 9, Batch 418, Test Loss: 0.424550861120224\n",
      "Epoch 9, Batch 419, Test Loss: 0.5284789204597473\n",
      "Epoch 9, Batch 420, Test Loss: 0.405443400144577\n",
      "Epoch 9, Batch 421, Test Loss: 0.49080130457878113\n",
      "Epoch 9, Batch 422, Test Loss: 0.5133427977561951\n",
      "Epoch 9, Batch 423, Test Loss: 0.6648017764091492\n",
      "Epoch 9, Batch 424, Test Loss: 0.5384950637817383\n",
      "Epoch 9, Batch 425, Test Loss: 0.5724385976791382\n",
      "Epoch 9, Batch 426, Test Loss: 0.6972799897193909\n",
      "Epoch 9, Batch 427, Test Loss: 0.5310367345809937\n",
      "Epoch 9, Batch 428, Test Loss: 0.654915452003479\n",
      "Epoch 9, Batch 429, Test Loss: 0.4747377038002014\n",
      "Epoch 9, Batch 430, Test Loss: 0.5308306217193604\n",
      "Epoch 9, Batch 431, Test Loss: 0.5461429953575134\n",
      "Epoch 9, Batch 432, Test Loss: 0.4977211058139801\n",
      "Epoch 9, Batch 433, Test Loss: 0.7129788398742676\n",
      "Epoch 9, Batch 434, Test Loss: 0.6672666072845459\n",
      "Epoch 9, Batch 435, Test Loss: 0.4645485281944275\n",
      "Epoch 9, Batch 436, Test Loss: 0.8687215447425842\n",
      "Epoch 9, Batch 437, Test Loss: 0.37147286534309387\n",
      "Epoch 9, Batch 438, Test Loss: 0.4396391212940216\n",
      "Epoch 9, Batch 439, Test Loss: 0.6403566598892212\n",
      "Epoch 9, Batch 440, Test Loss: 0.545291006565094\n",
      "Epoch 9, Batch 441, Test Loss: 0.54776930809021\n",
      "Epoch 9, Batch 442, Test Loss: 0.4920756220817566\n",
      "Epoch 9, Batch 443, Test Loss: 0.49222439527511597\n",
      "Epoch 9, Batch 444, Test Loss: 0.47338759899139404\n",
      "Epoch 9, Batch 445, Test Loss: 0.5083853006362915\n",
      "Epoch 9, Batch 446, Test Loss: 0.6039024591445923\n",
      "Epoch 9, Batch 447, Test Loss: 0.3788704574108124\n",
      "Epoch 9, Batch 448, Test Loss: 0.7972983717918396\n",
      "Epoch 9, Batch 449, Test Loss: 0.6274416446685791\n",
      "Epoch 9, Batch 450, Test Loss: 0.5941461324691772\n",
      "Epoch 9, Batch 451, Test Loss: 0.5799002647399902\n",
      "Epoch 9, Batch 452, Test Loss: 0.6450056433677673\n",
      "Epoch 9, Batch 453, Test Loss: 0.45378628373146057\n",
      "Epoch 9, Batch 454, Test Loss: 0.6901556253433228\n",
      "Epoch 9, Batch 455, Test Loss: 0.5979011058807373\n",
      "Epoch 9, Batch 456, Test Loss: 0.5624228119850159\n",
      "Epoch 9, Batch 457, Test Loss: 0.4996490180492401\n",
      "Epoch 9, Batch 458, Test Loss: 0.5541263222694397\n",
      "Epoch 9, Batch 459, Test Loss: 0.4081934988498688\n",
      "Epoch 9, Batch 460, Test Loss: 0.37733933329582214\n",
      "Epoch 9, Batch 461, Test Loss: 0.5670503377914429\n",
      "Epoch 9, Batch 462, Test Loss: 0.5605871081352234\n",
      "Epoch 9, Batch 463, Test Loss: 0.7088354229927063\n",
      "Epoch 9, Batch 464, Test Loss: 0.3578127920627594\n",
      "Epoch 9, Batch 465, Test Loss: 0.6334410905838013\n",
      "Epoch 9, Batch 466, Test Loss: 0.4877905249595642\n",
      "Epoch 9, Batch 467, Test Loss: 0.6340157389640808\n",
      "Epoch 9, Batch 468, Test Loss: 0.41783446073532104\n",
      "Epoch 9, Batch 469, Test Loss: 0.35138848423957825\n",
      "Epoch 9, Batch 470, Test Loss: 0.6183687448501587\n",
      "Epoch 9, Batch 471, Test Loss: 0.684705913066864\n",
      "Epoch 9, Batch 472, Test Loss: 0.5759982466697693\n",
      "Epoch 9, Batch 473, Test Loss: 0.4951430559158325\n",
      "Epoch 9, Batch 474, Test Loss: 0.2958112955093384\n",
      "Epoch 9, Batch 475, Test Loss: 0.5125355124473572\n",
      "Epoch 9, Batch 476, Test Loss: 0.5178799629211426\n",
      "Epoch 9, Batch 477, Test Loss: 0.6504950523376465\n",
      "Epoch 9, Batch 478, Test Loss: 0.5623102188110352\n",
      "Epoch 9, Batch 479, Test Loss: 0.5855404138565063\n",
      "Epoch 9, Batch 480, Test Loss: 0.5361966490745544\n",
      "Epoch 9, Batch 481, Test Loss: 0.3580342233181\n",
      "Epoch 9, Batch 482, Test Loss: 0.652009904384613\n",
      "Epoch 9, Batch 483, Test Loss: 0.5596705079078674\n",
      "Epoch 9, Batch 484, Test Loss: 0.5592950582504272\n",
      "Epoch 9, Batch 485, Test Loss: 0.3934287130832672\n",
      "Epoch 9, Batch 486, Test Loss: 0.633307695388794\n",
      "Epoch 9, Batch 487, Test Loss: 0.5665897130966187\n",
      "Epoch 9, Batch 488, Test Loss: 0.6561779379844666\n",
      "Epoch 9, Batch 489, Test Loss: 0.6243212819099426\n",
      "Epoch 9, Batch 490, Test Loss: 0.5820847153663635\n",
      "Epoch 9, Batch 491, Test Loss: 0.48425090312957764\n",
      "Epoch 9, Batch 492, Test Loss: 0.5777749419212341\n",
      "Epoch 9, Batch 493, Test Loss: 0.4962567090988159\n",
      "Epoch 9, Batch 494, Test Loss: 0.4555818438529968\n",
      "Epoch 9, Batch 495, Test Loss: 0.5221664905548096\n",
      "Epoch 9, Batch 496, Test Loss: 0.6498226523399353\n",
      "Epoch 9, Batch 497, Test Loss: 0.602491021156311\n",
      "Epoch 9, Batch 498, Test Loss: 0.624405026435852\n",
      "Epoch 9, Batch 499, Test Loss: 0.5534125566482544\n",
      "Epoch 9, Batch 500, Test Loss: 0.5368844270706177\n",
      "Epoch 9, Batch 501, Test Loss: 0.5709599256515503\n",
      "Epoch 9, Batch 502, Test Loss: 0.4391433894634247\n",
      "Epoch 9, Batch 503, Test Loss: 0.8259826302528381\n",
      "Epoch 9, Batch 504, Test Loss: 0.6152563095092773\n",
      "Epoch 9, Batch 505, Test Loss: 0.44128116965293884\n",
      "Epoch 9, Batch 506, Test Loss: 0.769855797290802\n",
      "Epoch 9, Batch 507, Test Loss: 0.7688697576522827\n",
      "Epoch 9, Batch 508, Test Loss: 0.8992513418197632\n",
      "Epoch 9, Batch 509, Test Loss: 0.3524150848388672\n",
      "Epoch 9, Batch 510, Test Loss: 0.28167763352394104\n",
      "Epoch 9, Batch 511, Test Loss: 0.4711378216743469\n",
      "Epoch 9, Batch 512, Test Loss: 0.48552489280700684\n",
      "Epoch 9, Batch 513, Test Loss: 0.6510910987854004\n",
      "Epoch 9, Batch 514, Test Loss: 0.7064225077629089\n",
      "Epoch 9, Batch 515, Test Loss: 0.3508427143096924\n",
      "Epoch 9, Batch 516, Test Loss: 0.5176591873168945\n",
      "Epoch 9, Batch 517, Test Loss: 0.3770008385181427\n",
      "Epoch 9, Batch 518, Test Loss: 0.4891450107097626\n",
      "Epoch 9, Batch 519, Test Loss: 0.45394402742385864\n",
      "Epoch 9, Batch 520, Test Loss: 0.5354049205780029\n",
      "Epoch 9, Batch 521, Test Loss: 0.38974419236183167\n",
      "Epoch 9, Batch 522, Test Loss: 0.42829906940460205\n",
      "Epoch 9, Batch 523, Test Loss: 0.648878276348114\n",
      "Epoch 9, Batch 524, Test Loss: 0.6323591470718384\n",
      "Epoch 9, Batch 525, Test Loss: 0.3862856328487396\n",
      "Epoch 9, Batch 526, Test Loss: 0.42315417528152466\n",
      "Epoch 9, Batch 527, Test Loss: 0.47764357924461365\n",
      "Epoch 9, Batch 528, Test Loss: 0.570958137512207\n",
      "Epoch 9, Batch 529, Test Loss: 0.4617862105369568\n",
      "Epoch 9, Batch 530, Test Loss: 0.49776095151901245\n",
      "Epoch 9, Batch 531, Test Loss: 0.5932601690292358\n",
      "Epoch 9, Batch 532, Test Loss: 0.6526598334312439\n",
      "Epoch 9, Batch 533, Test Loss: 0.592140257358551\n",
      "Epoch 9, Batch 534, Test Loss: 0.48688194155693054\n",
      "Epoch 9, Batch 535, Test Loss: 0.5344517230987549\n",
      "Epoch 9, Batch 536, Test Loss: 0.695856511592865\n",
      "Epoch 9, Batch 537, Test Loss: 0.4072347581386566\n",
      "Epoch 9, Batch 538, Test Loss: 0.6007113456726074\n",
      "Epoch 9, Batch 539, Test Loss: 0.45069923996925354\n",
      "Epoch 9, Batch 540, Test Loss: 0.4643799960613251\n",
      "Epoch 9, Batch 541, Test Loss: 0.6566702723503113\n",
      "Epoch 9, Batch 542, Test Loss: 0.42410317063331604\n",
      "Epoch 9, Batch 543, Test Loss: 0.642535388469696\n",
      "Epoch 9, Batch 544, Test Loss: 0.464769572019577\n",
      "Epoch 9, Batch 545, Test Loss: 0.5873321890830994\n",
      "Epoch 9, Batch 546, Test Loss: 0.7760598659515381\n",
      "Epoch 9, Batch 547, Test Loss: 0.6692596673965454\n",
      "Epoch 9, Batch 548, Test Loss: 0.4922844171524048\n",
      "Epoch 9, Batch 549, Test Loss: 0.7291716933250427\n",
      "Epoch 9, Batch 550, Test Loss: 0.8736003041267395\n",
      "Epoch 9, Batch 551, Test Loss: 0.5404670238494873\n",
      "Epoch 9, Batch 552, Test Loss: 0.7743650078773499\n",
      "Epoch 9, Batch 553, Test Loss: 0.4323766827583313\n",
      "Epoch 9, Batch 554, Test Loss: 0.46011343598365784\n",
      "Epoch 9, Batch 555, Test Loss: 0.49325641989707947\n",
      "Epoch 9, Batch 556, Test Loss: 0.5332459211349487\n",
      "Epoch 9, Batch 557, Test Loss: 0.5008435845375061\n",
      "Epoch 9, Batch 558, Test Loss: 0.6196743249893188\n",
      "Epoch 9, Batch 559, Test Loss: 0.6541850566864014\n",
      "Epoch 9, Batch 560, Test Loss: 0.6702771782875061\n",
      "Epoch 9, Batch 561, Test Loss: 0.5196266174316406\n",
      "Epoch 9, Batch 562, Test Loss: 0.4987843632698059\n",
      "Epoch 9, Batch 563, Test Loss: 0.4591962397098541\n",
      "Epoch 9, Batch 564, Test Loss: 0.7119563817977905\n",
      "Epoch 9, Batch 565, Test Loss: 0.5223403573036194\n",
      "Epoch 9, Batch 566, Test Loss: 0.502095103263855\n",
      "Epoch 9, Batch 567, Test Loss: 0.5158889889717102\n",
      "Epoch 9, Batch 568, Test Loss: 0.4513600170612335\n",
      "Epoch 9, Batch 569, Test Loss: 0.38312339782714844\n",
      "Epoch 9, Batch 570, Test Loss: 0.49385711550712585\n",
      "Epoch 9, Batch 571, Test Loss: 0.6740037798881531\n",
      "Epoch 9, Batch 572, Test Loss: 0.817089855670929\n",
      "Epoch 9, Batch 573, Test Loss: 0.7705923318862915\n",
      "Epoch 9, Batch 574, Test Loss: 0.5186862349510193\n",
      "Epoch 9, Batch 575, Test Loss: 0.5285583138465881\n",
      "Epoch 9, Batch 576, Test Loss: 0.6306357979774475\n",
      "Epoch 9, Batch 577, Test Loss: 0.3081587553024292\n",
      "Epoch 9, Batch 578, Test Loss: 0.6525425314903259\n",
      "Epoch 9, Batch 579, Test Loss: 0.46446436643600464\n",
      "Epoch 9, Batch 580, Test Loss: 0.49549365043640137\n",
      "Epoch 9, Batch 581, Test Loss: 0.6685693860054016\n",
      "Epoch 9, Batch 582, Test Loss: 0.6170667409896851\n",
      "Epoch 9, Batch 583, Test Loss: 0.5860512256622314\n",
      "Epoch 9, Batch 584, Test Loss: 0.46411600708961487\n",
      "Epoch 9, Batch 585, Test Loss: 0.41411662101745605\n",
      "Epoch 9, Batch 586, Test Loss: 0.5436298847198486\n",
      "Epoch 9, Batch 587, Test Loss: 0.5492050647735596\n",
      "Epoch 9, Batch 588, Test Loss: 0.5173547267913818\n",
      "Epoch 9, Batch 589, Test Loss: 0.5361337661743164\n",
      "Epoch 9, Batch 590, Test Loss: 0.3439766764640808\n",
      "Epoch 9, Batch 591, Test Loss: 0.5722554922103882\n",
      "Epoch 9, Batch 592, Test Loss: 0.6265963315963745\n",
      "Epoch 9, Batch 593, Test Loss: 0.513605535030365\n",
      "Epoch 9, Batch 594, Test Loss: 0.556831419467926\n",
      "Epoch 9, Batch 595, Test Loss: 0.6214954853057861\n",
      "Epoch 9, Batch 596, Test Loss: 0.40841639041900635\n",
      "Epoch 9, Batch 597, Test Loss: 0.4636741280555725\n",
      "Epoch 9, Batch 598, Test Loss: 0.4574560523033142\n",
      "Epoch 9, Batch 599, Test Loss: 0.45541805028915405\n",
      "Epoch 9, Batch 600, Test Loss: 0.5714042782783508\n",
      "Epoch 9, Batch 601, Test Loss: 0.7572368383407593\n",
      "Epoch 9, Batch 602, Test Loss: 0.5255923867225647\n",
      "Epoch 9, Batch 603, Test Loss: 0.8852418661117554\n",
      "Epoch 9, Batch 604, Test Loss: 0.4089773893356323\n",
      "Epoch 9, Batch 605, Test Loss: 0.5807015299797058\n",
      "Epoch 9, Batch 606, Test Loss: 0.43666401505470276\n",
      "Epoch 9, Batch 607, Test Loss: 0.5502359867095947\n",
      "Epoch 9, Batch 608, Test Loss: 0.6967053413391113\n",
      "Epoch 9, Batch 609, Test Loss: 0.5270749926567078\n",
      "Epoch 9, Batch 610, Test Loss: 0.7052860260009766\n",
      "Epoch 9, Batch 611, Test Loss: 0.5605930685997009\n",
      "Epoch 9, Batch 612, Test Loss: 0.6273630261421204\n",
      "Epoch 9, Batch 613, Test Loss: 0.6382567882537842\n",
      "Epoch 9, Batch 614, Test Loss: 0.5432655811309814\n",
      "Epoch 9, Batch 615, Test Loss: 0.42900240421295166\n",
      "Epoch 9, Batch 616, Test Loss: 0.6207384467124939\n",
      "Epoch 9, Batch 617, Test Loss: 0.8592574000358582\n",
      "Epoch 9, Batch 618, Test Loss: 0.40400898456573486\n",
      "Epoch 9, Batch 619, Test Loss: 0.36630120873451233\n",
      "Epoch 9, Batch 620, Test Loss: 0.6493242979049683\n",
      "Epoch 9, Batch 621, Test Loss: 0.6766291856765747\n",
      "Epoch 9, Batch 622, Test Loss: 0.5004129409790039\n",
      "Epoch 9, Batch 623, Test Loss: 0.62907874584198\n",
      "Epoch 9, Batch 624, Test Loss: 0.4547840356826782\n",
      "Epoch 9, Batch 625, Test Loss: 0.4743368327617645\n",
      "Epoch 9, Batch 626, Test Loss: 0.5449140667915344\n",
      "Epoch 9, Batch 627, Test Loss: 0.7703198790550232\n",
      "Epoch 9, Batch 628, Test Loss: 0.47352054715156555\n",
      "Epoch 9, Batch 629, Test Loss: 0.5697163343429565\n",
      "Epoch 9, Batch 630, Test Loss: 0.6577762961387634\n",
      "Epoch 9, Batch 631, Test Loss: 0.6848123073577881\n",
      "Epoch 9, Batch 632, Test Loss: 0.5422669053077698\n",
      "Epoch 9, Batch 633, Test Loss: 0.69322669506073\n",
      "Epoch 9, Batch 634, Test Loss: 0.7690044641494751\n",
      "Epoch 9, Batch 635, Test Loss: 0.6840004324913025\n",
      "Epoch 9, Batch 636, Test Loss: 0.7126524448394775\n",
      "Epoch 9, Batch 637, Test Loss: 0.5745159387588501\n",
      "Epoch 9, Batch 638, Test Loss: 0.489715039730072\n",
      "Epoch 9, Batch 639, Test Loss: 0.3951466977596283\n",
      "Epoch 9, Batch 640, Test Loss: 0.7270685434341431\n",
      "Epoch 9, Batch 641, Test Loss: 0.5000775456428528\n",
      "Epoch 9, Batch 642, Test Loss: 0.3112579584121704\n",
      "Epoch 9, Batch 643, Test Loss: 0.48972728848457336\n",
      "Epoch 9, Batch 644, Test Loss: 0.32584941387176514\n",
      "Epoch 9, Batch 645, Test Loss: 0.583500325679779\n",
      "Epoch 9, Batch 646, Test Loss: 0.42901408672332764\n",
      "Epoch 9, Batch 647, Test Loss: 0.5784071683883667\n",
      "Epoch 9, Batch 648, Test Loss: 0.5326722264289856\n",
      "Epoch 9, Batch 649, Test Loss: 0.6368237137794495\n",
      "Epoch 9, Batch 650, Test Loss: 0.5194868445396423\n",
      "Epoch 9, Batch 651, Test Loss: 0.573562741279602\n",
      "Epoch 9, Batch 652, Test Loss: 0.5137045979499817\n",
      "Epoch 9, Batch 653, Test Loss: 0.5150148272514343\n",
      "Epoch 9, Batch 654, Test Loss: 0.5658614635467529\n",
      "Epoch 9, Batch 655, Test Loss: 0.4430638551712036\n",
      "Epoch 9, Batch 656, Test Loss: 0.5741873383522034\n",
      "Epoch 9, Batch 657, Test Loss: 0.5999621152877808\n",
      "Epoch 9, Batch 658, Test Loss: 0.575246274471283\n",
      "Epoch 9, Batch 659, Test Loss: 0.6058129072189331\n",
      "Epoch 9, Batch 660, Test Loss: 0.46714261174201965\n",
      "Epoch 9, Batch 661, Test Loss: 0.49029847979545593\n",
      "Epoch 9, Batch 662, Test Loss: 0.4978201985359192\n",
      "Epoch 9, Batch 663, Test Loss: 0.5462377071380615\n",
      "Epoch 9, Batch 664, Test Loss: 0.49018165469169617\n",
      "Epoch 9, Batch 665, Test Loss: 0.5670411586761475\n",
      "Epoch 9, Batch 666, Test Loss: 0.534214198589325\n",
      "Epoch 9, Batch 667, Test Loss: 0.35160940885543823\n",
      "Epoch 9, Batch 668, Test Loss: 0.5674432516098022\n",
      "Epoch 9, Batch 669, Test Loss: 0.5430284738540649\n",
      "Epoch 9, Batch 670, Test Loss: 0.45962828397750854\n",
      "Epoch 9, Batch 671, Test Loss: 0.6681392788887024\n",
      "Epoch 9, Batch 672, Test Loss: 0.6239436268806458\n",
      "Epoch 9, Batch 673, Test Loss: 0.28797653317451477\n",
      "Epoch 9, Batch 674, Test Loss: 0.6610592603683472\n",
      "Epoch 9, Batch 675, Test Loss: 0.5674105882644653\n",
      "Epoch 9, Batch 676, Test Loss: 0.40341103076934814\n",
      "Epoch 9, Batch 677, Test Loss: 0.5819239616394043\n",
      "Epoch 9, Batch 678, Test Loss: 0.5207723379135132\n",
      "Epoch 9, Batch 679, Test Loss: 0.6411129832267761\n",
      "Epoch 9, Batch 680, Test Loss: 0.7467730641365051\n",
      "Epoch 9, Batch 681, Test Loss: 0.5384271740913391\n",
      "Epoch 9, Batch 682, Test Loss: 0.5376491546630859\n",
      "Epoch 9, Batch 683, Test Loss: 0.550538957118988\n",
      "Epoch 9, Batch 684, Test Loss: 0.8044631481170654\n",
      "Epoch 9, Batch 685, Test Loss: 0.3467966914176941\n",
      "Epoch 9, Batch 686, Test Loss: 0.6547040939331055\n",
      "Epoch 9, Batch 687, Test Loss: 0.5673708319664001\n",
      "Epoch 9, Batch 688, Test Loss: 0.43296417593955994\n",
      "Epoch 9, Batch 689, Test Loss: 0.46564286947250366\n",
      "Epoch 9, Batch 690, Test Loss: 0.5662222504615784\n",
      "Epoch 9, Batch 691, Test Loss: 0.5644328594207764\n",
      "Epoch 9, Batch 692, Test Loss: 0.6650747060775757\n",
      "Epoch 9, Batch 693, Test Loss: 0.6154516339302063\n",
      "Epoch 9, Batch 694, Test Loss: 0.5487104654312134\n",
      "Epoch 9, Batch 695, Test Loss: 0.435241162776947\n",
      "Epoch 9, Batch 696, Test Loss: 0.45175108313560486\n",
      "Epoch 9, Batch 697, Test Loss: 0.6117319464683533\n",
      "Epoch 9, Batch 698, Test Loss: 0.4230218827724457\n",
      "Epoch 9, Batch 699, Test Loss: 0.49203425645828247\n",
      "Epoch 9, Batch 700, Test Loss: 0.6890139579772949\n",
      "Epoch 9, Batch 701, Test Loss: 0.49287188053131104\n",
      "Epoch 9, Batch 702, Test Loss: 0.4516298770904541\n",
      "Epoch 9, Batch 703, Test Loss: 0.3898772597312927\n",
      "Epoch 9, Batch 704, Test Loss: 0.5232925415039062\n",
      "Epoch 9, Batch 705, Test Loss: 0.5948972105979919\n",
      "Epoch 9, Batch 706, Test Loss: 0.46592646837234497\n",
      "Epoch 9, Batch 707, Test Loss: 0.5624406933784485\n",
      "Epoch 9, Batch 708, Test Loss: 0.42595362663269043\n",
      "Epoch 9, Batch 709, Test Loss: 0.5897039771080017\n",
      "Epoch 9, Batch 710, Test Loss: 0.500454843044281\n",
      "Epoch 9, Batch 711, Test Loss: 0.7088527679443359\n",
      "Epoch 9, Batch 712, Test Loss: 0.5726189613342285\n",
      "Epoch 9, Batch 713, Test Loss: 0.6023156642913818\n",
      "Epoch 9, Batch 714, Test Loss: 0.32229864597320557\n",
      "Epoch 9, Batch 715, Test Loss: 0.6413952112197876\n",
      "Epoch 9, Batch 716, Test Loss: 0.684024453163147\n",
      "Epoch 9, Batch 717, Test Loss: 0.6694835424423218\n",
      "Epoch 9, Batch 718, Test Loss: 0.6109553575515747\n",
      "Epoch 9, Batch 719, Test Loss: 0.4126878082752228\n",
      "Epoch 9, Batch 720, Test Loss: 0.6037977933883667\n",
      "Epoch 9, Batch 721, Test Loss: 0.35753530263900757\n",
      "Epoch 9, Batch 722, Test Loss: 0.4466180205345154\n",
      "Epoch 9, Batch 723, Test Loss: 0.5632858276367188\n",
      "Epoch 9, Batch 724, Test Loss: 0.39485397934913635\n",
      "Epoch 9, Batch 725, Test Loss: 0.5151787996292114\n",
      "Epoch 9, Batch 726, Test Loss: 0.6066961288452148\n",
      "Epoch 9, Batch 727, Test Loss: 0.6447931528091431\n",
      "Epoch 9, Batch 728, Test Loss: 0.7464313507080078\n",
      "Epoch 9, Batch 729, Test Loss: 0.5724351406097412\n",
      "Epoch 9, Batch 730, Test Loss: 0.475795179605484\n",
      "Epoch 9, Batch 731, Test Loss: 0.8648459911346436\n",
      "Epoch 9, Batch 732, Test Loss: 0.5514475703239441\n",
      "Epoch 9, Batch 733, Test Loss: 0.40096500515937805\n",
      "Epoch 9, Batch 734, Test Loss: 0.6054353713989258\n",
      "Epoch 9, Batch 735, Test Loss: 0.3082011342048645\n",
      "Epoch 9, Batch 736, Test Loss: 0.4696069061756134\n",
      "Epoch 9, Batch 737, Test Loss: 0.6411553621292114\n",
      "Epoch 9, Batch 738, Test Loss: 0.6622223258018494\n",
      "Epoch 9, Batch 739, Test Loss: 0.6560112237930298\n",
      "Epoch 9, Batch 740, Test Loss: 0.4807852506637573\n",
      "Epoch 9, Batch 741, Test Loss: 0.5017356872558594\n",
      "Epoch 9, Batch 742, Test Loss: 0.6465016007423401\n",
      "Epoch 9, Batch 743, Test Loss: 0.35785746574401855\n",
      "Epoch 9, Batch 744, Test Loss: 0.39892569184303284\n",
      "Epoch 9, Batch 745, Test Loss: 0.5200992226600647\n",
      "Epoch 9, Batch 746, Test Loss: 0.6261463165283203\n",
      "Epoch 9, Batch 747, Test Loss: 0.5919660925865173\n",
      "Epoch 9, Batch 748, Test Loss: 0.42781952023506165\n",
      "Epoch 9, Batch 749, Test Loss: 0.5805943608283997\n",
      "Epoch 9, Batch 750, Test Loss: 0.5087760090827942\n",
      "Epoch 9, Batch 751, Test Loss: 0.511772632598877\n",
      "Epoch 9, Batch 752, Test Loss: 0.7335270643234253\n",
      "Epoch 9, Batch 753, Test Loss: 0.5432600975036621\n",
      "Epoch 9, Batch 754, Test Loss: 0.6276712417602539\n",
      "Epoch 9, Batch 755, Test Loss: 0.4346822500228882\n",
      "Epoch 9, Batch 756, Test Loss: 0.7035666108131409\n",
      "Epoch 9, Batch 757, Test Loss: 0.4442824423313141\n",
      "Epoch 9, Batch 758, Test Loss: 0.5300869345664978\n",
      "Epoch 9, Batch 759, Test Loss: 0.4713762402534485\n",
      "Epoch 9, Batch 760, Test Loss: 0.5781856775283813\n",
      "Epoch 9, Batch 761, Test Loss: 0.2966718375682831\n",
      "Epoch 9, Batch 762, Test Loss: 0.8166658878326416\n",
      "Epoch 9, Batch 763, Test Loss: 0.5188236236572266\n",
      "Epoch 9, Batch 764, Test Loss: 0.4706971347332001\n",
      "Epoch 9, Batch 765, Test Loss: 0.4107629358768463\n",
      "Epoch 9, Batch 766, Test Loss: 0.5837118029594421\n",
      "Epoch 9, Batch 767, Test Loss: 0.5250241756439209\n",
      "Epoch 9, Batch 768, Test Loss: 0.5054062604904175\n",
      "Epoch 9, Batch 769, Test Loss: 0.5736732482910156\n",
      "Epoch 9, Batch 770, Test Loss: 0.49286913871765137\n",
      "Epoch 9, Batch 771, Test Loss: 0.5663944482803345\n",
      "Epoch 9, Batch 772, Test Loss: 0.39331746101379395\n",
      "Epoch 9, Batch 773, Test Loss: 0.6357848644256592\n",
      "Epoch 9, Batch 774, Test Loss: 0.4425274729728699\n",
      "Epoch 9, Batch 775, Test Loss: 0.6162054538726807\n",
      "Epoch 9, Batch 776, Test Loss: 0.4872722327709198\n",
      "Epoch 9, Batch 777, Test Loss: 0.6062808036804199\n",
      "Epoch 9, Batch 778, Test Loss: 0.5482445955276489\n",
      "Epoch 9, Batch 779, Test Loss: 0.7589371800422668\n",
      "Epoch 9, Batch 780, Test Loss: 0.6100718379020691\n",
      "Epoch 9, Batch 781, Test Loss: 0.6201716661453247\n",
      "Epoch 9, Batch 782, Test Loss: 0.45883405208587646\n",
      "Epoch 9, Batch 783, Test Loss: 0.6385396718978882\n",
      "Epoch 9, Batch 784, Test Loss: 0.4030908942222595\n",
      "Epoch 9, Batch 785, Test Loss: 0.514428436756134\n",
      "Epoch 9, Batch 786, Test Loss: 0.5260661244392395\n",
      "Epoch 9, Batch 787, Test Loss: 0.5164867639541626\n",
      "Epoch 9, Batch 788, Test Loss: 0.46813416481018066\n",
      "Epoch 9, Batch 789, Test Loss: 0.5340752005577087\n",
      "Epoch 9, Batch 790, Test Loss: 0.41631269454956055\n",
      "Epoch 9, Batch 791, Test Loss: 0.5504357218742371\n",
      "Epoch 9, Batch 792, Test Loss: 0.5483569502830505\n",
      "Epoch 9, Batch 793, Test Loss: 0.5865366458892822\n",
      "Epoch 9, Batch 794, Test Loss: 0.5257316827774048\n",
      "Epoch 9, Batch 795, Test Loss: 0.5150846242904663\n",
      "Epoch 9, Batch 796, Test Loss: 0.83949214220047\n",
      "Epoch 9, Batch 797, Test Loss: 0.5614829659461975\n",
      "Epoch 9, Batch 798, Test Loss: 0.6285185217857361\n",
      "Epoch 9, Batch 799, Test Loss: 0.44457560777664185\n",
      "Epoch 9, Batch 800, Test Loss: 0.37001916766166687\n",
      "Epoch 9, Batch 801, Test Loss: 0.6217427849769592\n",
      "Epoch 9, Batch 802, Test Loss: 0.6146129369735718\n",
      "Epoch 9, Batch 803, Test Loss: 0.5539608597755432\n",
      "Epoch 9, Batch 804, Test Loss: 0.9107707738876343\n",
      "Epoch 9, Batch 805, Test Loss: 0.5154126882553101\n",
      "Epoch 9, Batch 806, Test Loss: 0.3974328637123108\n",
      "Epoch 9, Batch 807, Test Loss: 0.39090999960899353\n",
      "Epoch 9, Batch 808, Test Loss: 0.4922753572463989\n",
      "Epoch 9, Batch 809, Test Loss: 0.4617159366607666\n",
      "Epoch 9, Batch 810, Test Loss: 0.5714130997657776\n",
      "Epoch 9, Batch 811, Test Loss: 0.5423590540885925\n",
      "Epoch 9, Batch 812, Test Loss: 0.4776920676231384\n",
      "Epoch 9, Batch 813, Test Loss: 0.5700181126594543\n",
      "Epoch 9, Batch 814, Test Loss: 0.42391636967658997\n",
      "Epoch 9, Batch 815, Test Loss: 0.6031951904296875\n",
      "Epoch 9, Batch 816, Test Loss: 0.5956820249557495\n",
      "Epoch 9, Batch 817, Test Loss: 0.6563957333564758\n",
      "Epoch 9, Batch 818, Test Loss: 0.4378562271595001\n",
      "Epoch 9, Batch 819, Test Loss: 0.4064939618110657\n",
      "Epoch 9, Batch 820, Test Loss: 0.5724846124649048\n",
      "Epoch 9, Batch 821, Test Loss: 0.40141192078590393\n",
      "Epoch 9, Batch 822, Test Loss: 0.5974109172821045\n",
      "Epoch 9, Batch 823, Test Loss: 0.573705792427063\n",
      "Epoch 9, Batch 824, Test Loss: 0.4782888889312744\n",
      "Epoch 9, Batch 825, Test Loss: 0.425612211227417\n",
      "Epoch 9, Batch 826, Test Loss: 0.5092740058898926\n",
      "Epoch 9, Batch 827, Test Loss: 0.509480893611908\n",
      "Epoch 9, Batch 828, Test Loss: 0.5078484416007996\n",
      "Epoch 9, Batch 829, Test Loss: 0.32349714636802673\n",
      "Epoch 9, Batch 830, Test Loss: 0.5057413578033447\n",
      "Epoch 9, Batch 831, Test Loss: 0.4240606129169464\n",
      "Epoch 9, Batch 832, Test Loss: 0.36466413736343384\n",
      "Epoch 9, Batch 833, Test Loss: 0.5720852613449097\n",
      "Epoch 9, Batch 834, Test Loss: 0.7214385271072388\n",
      "Epoch 9, Batch 835, Test Loss: 0.5167035460472107\n",
      "Epoch 9, Batch 836, Test Loss: 0.5823363661766052\n",
      "Epoch 9, Batch 837, Test Loss: 0.4576336443424225\n",
      "Epoch 9, Batch 838, Test Loss: 0.6410847306251526\n",
      "Epoch 9, Batch 839, Test Loss: 0.6077744960784912\n",
      "Epoch 9, Batch 840, Test Loss: 0.6259294152259827\n",
      "Epoch 9, Batch 841, Test Loss: 0.5868791937828064\n",
      "Epoch 9, Batch 842, Test Loss: 0.5663631558418274\n",
      "Epoch 9, Batch 843, Test Loss: 0.5363873243331909\n",
      "Epoch 9, Batch 844, Test Loss: 0.7326818704605103\n",
      "Epoch 9, Batch 845, Test Loss: 0.5247783064842224\n",
      "Epoch 9, Batch 846, Test Loss: 0.6513751745223999\n",
      "Epoch 9, Batch 847, Test Loss: 0.5708875060081482\n",
      "Epoch 9, Batch 848, Test Loss: 0.5531922578811646\n",
      "Epoch 9, Batch 849, Test Loss: 0.4612438976764679\n",
      "Epoch 9, Batch 850, Test Loss: 0.49599626660346985\n",
      "Epoch 9, Batch 851, Test Loss: 0.5247761011123657\n",
      "Epoch 9, Batch 852, Test Loss: 0.6223580241203308\n",
      "Epoch 9, Batch 853, Test Loss: 0.46358078718185425\n",
      "Epoch 9, Batch 854, Test Loss: 0.48089858889579773\n",
      "Epoch 9, Batch 855, Test Loss: 0.5352728366851807\n",
      "Epoch 9, Batch 856, Test Loss: 0.3669683635234833\n",
      "Epoch 9, Batch 857, Test Loss: 0.41746068000793457\n",
      "Epoch 9, Batch 858, Test Loss: 0.5805130004882812\n",
      "Epoch 9, Batch 859, Test Loss: 0.5083629488945007\n",
      "Epoch 9, Batch 860, Test Loss: 0.43281471729278564\n",
      "Epoch 9, Batch 861, Test Loss: 0.38601478934288025\n",
      "Epoch 9, Batch 862, Test Loss: 0.8070009350776672\n",
      "Epoch 9, Batch 863, Test Loss: 0.4735000431537628\n",
      "Epoch 9, Batch 864, Test Loss: 0.734840452671051\n",
      "Epoch 9, Batch 865, Test Loss: 0.4982220232486725\n",
      "Epoch 9, Batch 866, Test Loss: 0.4433172047138214\n",
      "Epoch 9, Batch 867, Test Loss: 0.9263363480567932\n",
      "Epoch 9, Batch 868, Test Loss: 0.6119164228439331\n",
      "Epoch 9, Batch 869, Test Loss: 0.5584802627563477\n",
      "Epoch 9, Batch 870, Test Loss: 0.7024102807044983\n",
      "Epoch 9, Batch 871, Test Loss: 0.5533446073532104\n",
      "Epoch 9, Batch 872, Test Loss: 0.5163572430610657\n",
      "Epoch 9, Batch 873, Test Loss: 0.39182764291763306\n",
      "Epoch 9, Batch 874, Test Loss: 0.5011115670204163\n",
      "Epoch 9, Batch 875, Test Loss: 0.40502676367759705\n",
      "Epoch 9, Batch 876, Test Loss: 0.3848732113838196\n",
      "Epoch 9, Batch 877, Test Loss: 0.3568030595779419\n",
      "Epoch 9, Batch 878, Test Loss: 0.5282279849052429\n",
      "Epoch 9, Batch 879, Test Loss: 0.5054503083229065\n",
      "Epoch 9, Batch 880, Test Loss: 0.46513989567756653\n",
      "Epoch 9, Batch 881, Test Loss: 0.5376534461975098\n",
      "Epoch 9, Batch 882, Test Loss: 0.5866655111312866\n",
      "Epoch 9, Batch 883, Test Loss: 0.48230499029159546\n",
      "Epoch 9, Batch 884, Test Loss: 0.5112363696098328\n",
      "Epoch 9, Batch 885, Test Loss: 0.7550886869430542\n",
      "Epoch 9, Batch 886, Test Loss: 0.6437017321586609\n",
      "Epoch 9, Batch 887, Test Loss: 0.6193278431892395\n",
      "Epoch 9, Batch 888, Test Loss: 0.504501223564148\n",
      "Epoch 9, Batch 889, Test Loss: 0.4763963520526886\n",
      "Epoch 9, Batch 890, Test Loss: 0.6776469945907593\n",
      "Epoch 9, Batch 891, Test Loss: 0.581519365310669\n",
      "Epoch 9, Batch 892, Test Loss: 0.7489480376243591\n",
      "Epoch 9, Batch 893, Test Loss: 0.5810112953186035\n",
      "Epoch 9, Batch 894, Test Loss: 0.46540361642837524\n",
      "Epoch 9, Batch 895, Test Loss: 0.4944847524166107\n",
      "Epoch 9, Batch 896, Test Loss: 0.3667263090610504\n",
      "Epoch 9, Batch 897, Test Loss: 0.4583137631416321\n",
      "Epoch 9, Batch 898, Test Loss: 0.5310571789741516\n",
      "Epoch 9, Batch 899, Test Loss: 0.5201819539070129\n",
      "Epoch 9, Batch 900, Test Loss: 0.4091281294822693\n",
      "Epoch 9, Batch 901, Test Loss: 0.4379512071609497\n",
      "Epoch 9, Batch 902, Test Loss: 0.5017118453979492\n",
      "Epoch 9, Batch 903, Test Loss: 0.5820956826210022\n",
      "Epoch 9, Batch 904, Test Loss: 0.5943311452865601\n",
      "Epoch 9, Batch 905, Test Loss: 0.5334945917129517\n",
      "Epoch 9, Batch 906, Test Loss: 0.44700103998184204\n",
      "Epoch 9, Batch 907, Test Loss: 0.5679042935371399\n",
      "Epoch 9, Batch 908, Test Loss: 0.5404314398765564\n",
      "Epoch 9, Batch 909, Test Loss: 0.42315682768821716\n",
      "Epoch 9, Batch 910, Test Loss: 0.4284224212169647\n",
      "Epoch 9, Batch 911, Test Loss: 0.5304087400436401\n",
      "Epoch 9, Batch 912, Test Loss: 0.6150245666503906\n",
      "Epoch 9, Batch 913, Test Loss: 0.42433756589889526\n",
      "Epoch 9, Batch 914, Test Loss: 0.3930608332157135\n",
      "Epoch 9, Batch 915, Test Loss: 0.6339710354804993\n",
      "Epoch 9, Batch 916, Test Loss: 0.6443403959274292\n",
      "Epoch 9, Batch 917, Test Loss: 0.5016067028045654\n",
      "Epoch 9, Batch 918, Test Loss: 0.717101514339447\n",
      "Epoch 9, Batch 919, Test Loss: 0.5282206535339355\n",
      "Epoch 9, Batch 920, Test Loss: 0.5760675668716431\n",
      "Epoch 9, Batch 921, Test Loss: 0.6615850925445557\n",
      "Epoch 9, Batch 922, Test Loss: 0.4073992967605591\n",
      "Epoch 9, Batch 923, Test Loss: 0.5971300601959229\n",
      "Epoch 9, Batch 924, Test Loss: 0.7382735013961792\n",
      "Epoch 9, Batch 925, Test Loss: 0.6125320792198181\n",
      "Epoch 9, Batch 926, Test Loss: 0.5362405776977539\n",
      "Epoch 9, Batch 927, Test Loss: 0.4215388000011444\n",
      "Epoch 9, Batch 928, Test Loss: 0.6731272339820862\n",
      "Epoch 9, Batch 929, Test Loss: 0.6526219248771667\n",
      "Epoch 9, Batch 930, Test Loss: 0.392856240272522\n",
      "Epoch 9, Batch 931, Test Loss: 0.5346121788024902\n",
      "Epoch 9, Batch 932, Test Loss: 0.5646426677703857\n",
      "Epoch 9, Batch 933, Test Loss: 0.9172722101211548\n",
      "Epoch 9, Batch 934, Test Loss: 0.4145379960536957\n",
      "Epoch 9, Batch 935, Test Loss: 0.5024405717849731\n",
      "Epoch 9, Batch 936, Test Loss: 0.603836715221405\n",
      "Epoch 9, Batch 937, Test Loss: 0.5177533626556396\n",
      "Epoch 9, Batch 938, Test Loss: 0.44542884826660156\n",
      "Accuracy of Test set: 0.80535\n",
      "Epoch 10, Batch 1, Loss: 0.521091103553772\n",
      "Epoch 10, Batch 2, Loss: 0.5657842755317688\n",
      "Epoch 10, Batch 3, Loss: 0.33269110321998596\n",
      "Epoch 10, Batch 4, Loss: 0.4423765242099762\n",
      "Epoch 10, Batch 5, Loss: 0.5115551948547363\n",
      "Epoch 10, Batch 6, Loss: 0.5997161269187927\n",
      "Epoch 10, Batch 7, Loss: 0.4591951072216034\n",
      "Epoch 10, Batch 8, Loss: 0.7003865242004395\n",
      "Epoch 10, Batch 9, Loss: 0.5152920484542847\n",
      "Epoch 10, Batch 10, Loss: 0.4522858262062073\n",
      "Epoch 10, Batch 11, Loss: 0.7642314434051514\n",
      "Epoch 10, Batch 12, Loss: 0.4979866147041321\n",
      "Epoch 10, Batch 13, Loss: 0.621907114982605\n",
      "Epoch 10, Batch 14, Loss: 0.378145694732666\n",
      "Epoch 10, Batch 15, Loss: 0.3483443260192871\n",
      "Epoch 10, Batch 16, Loss: 0.6695592403411865\n",
      "Epoch 10, Batch 17, Loss: 0.5242999196052551\n",
      "Epoch 10, Batch 18, Loss: 0.6346176266670227\n",
      "Epoch 10, Batch 19, Loss: 0.44148844480514526\n",
      "Epoch 10, Batch 20, Loss: 0.6255786418914795\n",
      "Epoch 10, Batch 21, Loss: 0.4678603410720825\n",
      "Epoch 10, Batch 22, Loss: 0.6107476353645325\n",
      "Epoch 10, Batch 23, Loss: 0.48884934186935425\n",
      "Epoch 10, Batch 24, Loss: 0.47292423248291016\n",
      "Epoch 10, Batch 25, Loss: 0.805223822593689\n",
      "Epoch 10, Batch 26, Loss: 0.43021079897880554\n",
      "Epoch 10, Batch 27, Loss: 0.5238913893699646\n",
      "Epoch 10, Batch 28, Loss: 0.4786732792854309\n",
      "Epoch 10, Batch 29, Loss: 0.8487958908081055\n",
      "Epoch 10, Batch 30, Loss: 0.35173845291137695\n",
      "Epoch 10, Batch 31, Loss: 0.5464766621589661\n",
      "Epoch 10, Batch 32, Loss: 0.7589990496635437\n",
      "Epoch 10, Batch 33, Loss: 0.5188227891921997\n",
      "Epoch 10, Batch 34, Loss: 0.5082706212997437\n",
      "Epoch 10, Batch 35, Loss: 0.5120910406112671\n",
      "Epoch 10, Batch 36, Loss: 0.4231230914592743\n",
      "Epoch 10, Batch 37, Loss: 0.5033842921257019\n",
      "Epoch 10, Batch 38, Loss: 0.5308431386947632\n",
      "Epoch 10, Batch 39, Loss: 0.4309701919555664\n",
      "Epoch 10, Batch 40, Loss: 0.5211695432662964\n",
      "Epoch 10, Batch 41, Loss: 0.45123764872550964\n",
      "Epoch 10, Batch 42, Loss: 0.30511289834976196\n",
      "Epoch 10, Batch 43, Loss: 0.5042677521705627\n",
      "Epoch 10, Batch 44, Loss: 0.7159336805343628\n",
      "Epoch 10, Batch 45, Loss: 0.6105281114578247\n",
      "Epoch 10, Batch 46, Loss: 0.6805089712142944\n",
      "Epoch 10, Batch 47, Loss: 0.6241204738616943\n",
      "Epoch 10, Batch 48, Loss: 0.4258197546005249\n",
      "Epoch 10, Batch 49, Loss: 0.6519782543182373\n",
      "Epoch 10, Batch 50, Loss: 0.46715718507766724\n",
      "Epoch 10, Batch 51, Loss: 0.5688334703445435\n",
      "Epoch 10, Batch 52, Loss: 0.4801805019378662\n",
      "Epoch 10, Batch 53, Loss: 0.5414121747016907\n",
      "Epoch 10, Batch 54, Loss: 0.4334595501422882\n",
      "Epoch 10, Batch 55, Loss: 0.5996066927909851\n",
      "Epoch 10, Batch 56, Loss: 0.40470388531684875\n",
      "Epoch 10, Batch 57, Loss: 0.5812848210334778\n",
      "Epoch 10, Batch 58, Loss: 0.3525134325027466\n",
      "Epoch 10, Batch 59, Loss: 0.29991382360458374\n",
      "Epoch 10, Batch 60, Loss: 0.6576335430145264\n",
      "Epoch 10, Batch 61, Loss: 0.3974628746509552\n",
      "Epoch 10, Batch 62, Loss: 0.5189081430435181\n",
      "Epoch 10, Batch 63, Loss: 0.5910657048225403\n",
      "Epoch 10, Batch 64, Loss: 0.5856282711029053\n",
      "Epoch 10, Batch 65, Loss: 0.6200328469276428\n",
      "Epoch 10, Batch 66, Loss: 0.6518869400024414\n",
      "Epoch 10, Batch 67, Loss: 0.6238968372344971\n",
      "Epoch 10, Batch 68, Loss: 0.5928988456726074\n",
      "Epoch 10, Batch 69, Loss: 0.6086525917053223\n",
      "Epoch 10, Batch 70, Loss: 0.8370436429977417\n",
      "Epoch 10, Batch 71, Loss: 0.6105446219444275\n",
      "Epoch 10, Batch 72, Loss: 0.6432400345802307\n",
      "Epoch 10, Batch 73, Loss: 0.5670892000198364\n",
      "Epoch 10, Batch 74, Loss: 0.5670486688613892\n",
      "Epoch 10, Batch 75, Loss: 0.6758805513381958\n",
      "Epoch 10, Batch 76, Loss: 0.5674596428871155\n",
      "Epoch 10, Batch 77, Loss: 0.6062444448471069\n",
      "Epoch 10, Batch 78, Loss: 0.4195575714111328\n",
      "Epoch 10, Batch 79, Loss: 0.5526118874549866\n",
      "Epoch 10, Batch 80, Loss: 0.35644295811653137\n",
      "Epoch 10, Batch 81, Loss: 0.6168186068534851\n",
      "Epoch 10, Batch 82, Loss: 0.3619539737701416\n",
      "Epoch 10, Batch 83, Loss: 0.38206061720848083\n",
      "Epoch 10, Batch 84, Loss: 0.6511871814727783\n",
      "Epoch 10, Batch 85, Loss: 0.5680070519447327\n",
      "Epoch 10, Batch 86, Loss: 0.4646087884902954\n",
      "Epoch 10, Batch 87, Loss: 0.5436533689498901\n",
      "Epoch 10, Batch 88, Loss: 0.5199900269508362\n",
      "Epoch 10, Batch 89, Loss: 0.49313774704933167\n",
      "Epoch 10, Batch 90, Loss: 0.6694668531417847\n",
      "Epoch 10, Batch 91, Loss: 0.6091989874839783\n",
      "Epoch 10, Batch 92, Loss: 0.4647754430770874\n",
      "Epoch 10, Batch 93, Loss: 0.5031934976577759\n",
      "Epoch 10, Batch 94, Loss: 0.48065268993377686\n",
      "Epoch 10, Batch 95, Loss: 0.4407321512699127\n",
      "Epoch 10, Batch 96, Loss: 0.5224571228027344\n",
      "Epoch 10, Batch 97, Loss: 0.5824031233787537\n",
      "Epoch 10, Batch 98, Loss: 0.46203747391700745\n",
      "Epoch 10, Batch 99, Loss: 0.5056183338165283\n",
      "Epoch 10, Batch 100, Loss: 0.5053849816322327\n",
      "Epoch 10, Batch 101, Loss: 0.6170271635055542\n",
      "Epoch 10, Batch 102, Loss: 0.3763104975223541\n",
      "Epoch 10, Batch 103, Loss: 0.5659043192863464\n",
      "Epoch 10, Batch 104, Loss: 0.3672240972518921\n",
      "Epoch 10, Batch 105, Loss: 0.7075952291488647\n",
      "Epoch 10, Batch 106, Loss: 0.6884743571281433\n",
      "Epoch 10, Batch 107, Loss: 0.5828280448913574\n",
      "Epoch 10, Batch 108, Loss: 0.6391820311546326\n",
      "Epoch 10, Batch 109, Loss: 0.5891966223716736\n",
      "Epoch 10, Batch 110, Loss: 0.5137218832969666\n",
      "Epoch 10, Batch 111, Loss: 0.5984271764755249\n",
      "Epoch 10, Batch 112, Loss: 0.5994017124176025\n",
      "Epoch 10, Batch 113, Loss: 0.29588550329208374\n",
      "Epoch 10, Batch 114, Loss: 0.5158204436302185\n",
      "Epoch 10, Batch 115, Loss: 0.6794613599777222\n",
      "Epoch 10, Batch 116, Loss: 0.650017499923706\n",
      "Epoch 10, Batch 117, Loss: 0.5343268513679504\n",
      "Epoch 10, Batch 118, Loss: 0.5253114104270935\n",
      "Epoch 10, Batch 119, Loss: 0.5062092542648315\n",
      "Epoch 10, Batch 120, Loss: 0.44491565227508545\n",
      "Epoch 10, Batch 121, Loss: 0.5089199542999268\n",
      "Epoch 10, Batch 122, Loss: 0.5289183855056763\n",
      "Epoch 10, Batch 123, Loss: 0.6122744679450989\n",
      "Epoch 10, Batch 124, Loss: 0.5044102668762207\n",
      "Epoch 10, Batch 125, Loss: 0.4375705420970917\n",
      "Epoch 10, Batch 126, Loss: 0.47579818964004517\n",
      "Epoch 10, Batch 127, Loss: 0.5033146142959595\n",
      "Epoch 10, Batch 128, Loss: 0.47534799575805664\n",
      "Epoch 10, Batch 129, Loss: 0.45835644006729126\n",
      "Epoch 10, Batch 130, Loss: 0.5664262175559998\n",
      "Epoch 10, Batch 131, Loss: 0.7173166275024414\n",
      "Epoch 10, Batch 132, Loss: 0.7949094176292419\n",
      "Epoch 10, Batch 133, Loss: 0.45718032121658325\n",
      "Epoch 10, Batch 134, Loss: 0.5611028671264648\n",
      "Epoch 10, Batch 135, Loss: 0.35290202498435974\n",
      "Epoch 10, Batch 136, Loss: 0.6644190549850464\n",
      "Epoch 10, Batch 137, Loss: 0.3740987181663513\n",
      "Epoch 10, Batch 138, Loss: 0.3514559864997864\n",
      "Epoch 10, Batch 139, Loss: 0.4967879354953766\n",
      "Epoch 10, Batch 140, Loss: 0.6333653330802917\n",
      "Epoch 10, Batch 141, Loss: 0.6588836312294006\n",
      "Epoch 10, Batch 142, Loss: 0.4620908796787262\n",
      "Epoch 10, Batch 143, Loss: 0.6476287245750427\n",
      "Epoch 10, Batch 144, Loss: 0.5169054269790649\n",
      "Epoch 10, Batch 145, Loss: 0.6948107481002808\n",
      "Epoch 10, Batch 146, Loss: 0.4068530201911926\n",
      "Epoch 10, Batch 147, Loss: 0.5900283455848694\n",
      "Epoch 10, Batch 148, Loss: 0.6177733540534973\n",
      "Epoch 10, Batch 149, Loss: 0.5888596773147583\n",
      "Epoch 10, Batch 150, Loss: 0.6146081686019897\n",
      "Epoch 10, Batch 151, Loss: 0.4226072430610657\n",
      "Epoch 10, Batch 152, Loss: 0.4190899133682251\n",
      "Epoch 10, Batch 153, Loss: 0.5632936358451843\n",
      "Epoch 10, Batch 154, Loss: 0.7457513809204102\n",
      "Epoch 10, Batch 155, Loss: 0.5410071611404419\n",
      "Epoch 10, Batch 156, Loss: 0.5161043405532837\n",
      "Epoch 10, Batch 157, Loss: 0.4798319935798645\n",
      "Epoch 10, Batch 158, Loss: 0.5780985355377197\n",
      "Epoch 10, Batch 159, Loss: 0.47544941306114197\n",
      "Epoch 10, Batch 160, Loss: 0.6042144894599915\n",
      "Epoch 10, Batch 161, Loss: 0.6525066494941711\n",
      "Epoch 10, Batch 162, Loss: 0.6463854312896729\n",
      "Epoch 10, Batch 163, Loss: 0.5358642935752869\n",
      "Epoch 10, Batch 164, Loss: 0.4686495065689087\n",
      "Epoch 10, Batch 165, Loss: 0.5706717371940613\n",
      "Epoch 10, Batch 166, Loss: 0.5130767226219177\n",
      "Epoch 10, Batch 167, Loss: 0.5227402448654175\n",
      "Epoch 10, Batch 168, Loss: 0.39258474111557007\n",
      "Epoch 10, Batch 169, Loss: 0.42398935556411743\n",
      "Epoch 10, Batch 170, Loss: 0.7602474689483643\n",
      "Epoch 10, Batch 171, Loss: 0.3508230149745941\n",
      "Epoch 10, Batch 172, Loss: 0.30965739488601685\n",
      "Epoch 10, Batch 173, Loss: 0.5023254752159119\n",
      "Epoch 10, Batch 174, Loss: 0.5963493585586548\n",
      "Epoch 10, Batch 175, Loss: 0.3690422475337982\n",
      "Epoch 10, Batch 176, Loss: 0.49160343408584595\n",
      "Epoch 10, Batch 177, Loss: 0.5353472828865051\n",
      "Epoch 10, Batch 178, Loss: 0.44708243012428284\n",
      "Epoch 10, Batch 179, Loss: 0.5349188446998596\n",
      "Epoch 10, Batch 180, Loss: 0.35898810625076294\n",
      "Epoch 10, Batch 181, Loss: 0.6323002576828003\n",
      "Epoch 10, Batch 182, Loss: 0.4171634316444397\n",
      "Epoch 10, Batch 183, Loss: 0.41084814071655273\n",
      "Epoch 10, Batch 184, Loss: 0.36698055267333984\n",
      "Epoch 10, Batch 185, Loss: 0.812816858291626\n",
      "Epoch 10, Batch 186, Loss: 0.6475704312324524\n",
      "Epoch 10, Batch 187, Loss: 0.382665753364563\n",
      "Epoch 10, Batch 188, Loss: 0.3871384859085083\n",
      "Epoch 10, Batch 189, Loss: 0.4127359092235565\n",
      "Epoch 10, Batch 190, Loss: 0.4919412136077881\n",
      "Epoch 10, Batch 191, Loss: 0.4936128854751587\n",
      "Epoch 10, Batch 192, Loss: 0.3700353503227234\n",
      "Epoch 10, Batch 193, Loss: 0.39583274722099304\n",
      "Epoch 10, Batch 194, Loss: 0.47690317034721375\n",
      "Epoch 10, Batch 195, Loss: 0.5871012806892395\n",
      "Epoch 10, Batch 196, Loss: 0.443435937166214\n",
      "Epoch 10, Batch 197, Loss: 0.538992166519165\n",
      "Epoch 10, Batch 198, Loss: 0.5181151032447815\n",
      "Epoch 10, Batch 199, Loss: 0.537902295589447\n",
      "Epoch 10, Batch 200, Loss: 0.5315576195716858\n",
      "Epoch 10, Batch 201, Loss: 0.6501822471618652\n",
      "Epoch 10, Batch 202, Loss: 0.5444823503494263\n",
      "Epoch 10, Batch 203, Loss: 0.5950926542282104\n",
      "Epoch 10, Batch 204, Loss: 0.5203243494033813\n",
      "Epoch 10, Batch 205, Loss: 0.5727430582046509\n",
      "Epoch 10, Batch 206, Loss: 0.5771778225898743\n",
      "Epoch 10, Batch 207, Loss: 0.7955305576324463\n",
      "Epoch 10, Batch 208, Loss: 0.6443014740943909\n",
      "Epoch 10, Batch 209, Loss: 0.47017428278923035\n",
      "Epoch 10, Batch 210, Loss: 0.418414443731308\n",
      "Epoch 10, Batch 211, Loss: 0.650018572807312\n",
      "Epoch 10, Batch 212, Loss: 0.5002955198287964\n",
      "Epoch 10, Batch 213, Loss: 0.5226212739944458\n",
      "Epoch 10, Batch 214, Loss: 0.46140483021736145\n",
      "Epoch 10, Batch 215, Loss: 0.6359529495239258\n",
      "Epoch 10, Batch 216, Loss: 0.5642173290252686\n",
      "Epoch 10, Batch 217, Loss: 0.4669109582901001\n",
      "Epoch 10, Batch 218, Loss: 0.512637197971344\n",
      "Epoch 10, Batch 219, Loss: 0.5482721924781799\n",
      "Epoch 10, Batch 220, Loss: 0.5962806940078735\n",
      "Epoch 10, Batch 221, Loss: 0.3541668653488159\n",
      "Epoch 10, Batch 222, Loss: 0.4753109812736511\n",
      "Epoch 10, Batch 223, Loss: 0.6017252206802368\n",
      "Epoch 10, Batch 224, Loss: 0.3852967619895935\n",
      "Epoch 10, Batch 225, Loss: 0.6144677400588989\n",
      "Epoch 10, Batch 226, Loss: 0.46569615602493286\n",
      "Epoch 10, Batch 227, Loss: 0.41031327843666077\n",
      "Epoch 10, Batch 228, Loss: 0.5102705359458923\n",
      "Epoch 10, Batch 229, Loss: 0.5573832988739014\n",
      "Epoch 10, Batch 230, Loss: 0.5767711400985718\n",
      "Epoch 10, Batch 231, Loss: 0.31621435284614563\n",
      "Epoch 10, Batch 232, Loss: 0.5328267812728882\n",
      "Epoch 10, Batch 233, Loss: 0.46270552277565\n",
      "Epoch 10, Batch 234, Loss: 0.5025225281715393\n",
      "Epoch 10, Batch 235, Loss: 0.6515820622444153\n",
      "Epoch 10, Batch 236, Loss: 0.5037891864776611\n",
      "Epoch 10, Batch 237, Loss: 0.6722999215126038\n",
      "Epoch 10, Batch 238, Loss: 0.629901647567749\n",
      "Epoch 10, Batch 239, Loss: 0.47363096475601196\n",
      "Epoch 10, Batch 240, Loss: 0.47036027908325195\n",
      "Epoch 10, Batch 241, Loss: 0.5485565066337585\n",
      "Epoch 10, Batch 242, Loss: 0.46373897790908813\n",
      "Epoch 10, Batch 243, Loss: 0.49725139141082764\n",
      "Epoch 10, Batch 244, Loss: 0.39473292231559753\n",
      "Epoch 10, Batch 245, Loss: 0.49605435132980347\n",
      "Epoch 10, Batch 246, Loss: 0.6245572566986084\n",
      "Epoch 10, Batch 247, Loss: 0.4916675388813019\n",
      "Epoch 10, Batch 248, Loss: 0.3547039031982422\n",
      "Epoch 10, Batch 249, Loss: 0.9464297294616699\n",
      "Epoch 10, Batch 250, Loss: 0.46917739510536194\n",
      "Epoch 10, Batch 251, Loss: 0.5184974670410156\n",
      "Epoch 10, Batch 252, Loss: 0.5592886805534363\n",
      "Epoch 10, Batch 253, Loss: 0.4989495277404785\n",
      "Epoch 10, Batch 254, Loss: 0.47745761275291443\n",
      "Epoch 10, Batch 255, Loss: 0.44550004601478577\n",
      "Epoch 10, Batch 256, Loss: 0.6666285991668701\n",
      "Epoch 10, Batch 257, Loss: 0.5177924633026123\n",
      "Epoch 10, Batch 258, Loss: 0.4606681168079376\n",
      "Epoch 10, Batch 259, Loss: 0.7941591739654541\n",
      "Epoch 10, Batch 260, Loss: 0.32656174898147583\n",
      "Epoch 10, Batch 261, Loss: 0.768621027469635\n",
      "Epoch 10, Batch 262, Loss: 0.45216813683509827\n",
      "Epoch 10, Batch 263, Loss: 0.7428589463233948\n",
      "Epoch 10, Batch 264, Loss: 0.4211605191230774\n",
      "Epoch 10, Batch 265, Loss: 0.4235551655292511\n",
      "Epoch 10, Batch 266, Loss: 0.3960191309452057\n",
      "Epoch 10, Batch 267, Loss: 0.32320165634155273\n",
      "Epoch 10, Batch 268, Loss: 0.5391605496406555\n",
      "Epoch 10, Batch 269, Loss: 0.5573249459266663\n",
      "Epoch 10, Batch 270, Loss: 0.4945007860660553\n",
      "Epoch 10, Batch 271, Loss: 0.558353066444397\n",
      "Epoch 10, Batch 272, Loss: 0.5094910860061646\n",
      "Epoch 10, Batch 273, Loss: 0.7802389860153198\n",
      "Epoch 10, Batch 274, Loss: 0.34824883937835693\n",
      "Epoch 10, Batch 275, Loss: 0.3922908902168274\n",
      "Epoch 10, Batch 276, Loss: 0.5711359977722168\n",
      "Epoch 10, Batch 277, Loss: 0.33830997347831726\n",
      "Epoch 10, Batch 278, Loss: 0.4215480387210846\n",
      "Epoch 10, Batch 279, Loss: 0.5806344151496887\n",
      "Epoch 10, Batch 280, Loss: 0.4610389471054077\n",
      "Epoch 10, Batch 281, Loss: 0.4874761402606964\n",
      "Epoch 10, Batch 282, Loss: 0.3982938230037689\n",
      "Epoch 10, Batch 283, Loss: 0.3581489324569702\n",
      "Epoch 10, Batch 284, Loss: 0.5514295697212219\n",
      "Epoch 10, Batch 285, Loss: 0.4772205650806427\n",
      "Epoch 10, Batch 286, Loss: 0.4661440849304199\n",
      "Epoch 10, Batch 287, Loss: 0.6018349528312683\n",
      "Epoch 10, Batch 288, Loss: 0.6258732676506042\n",
      "Epoch 10, Batch 289, Loss: 0.4718940556049347\n",
      "Epoch 10, Batch 290, Loss: 0.5054912567138672\n",
      "Epoch 10, Batch 291, Loss: 0.5114361643791199\n",
      "Epoch 10, Batch 292, Loss: 0.3118528723716736\n",
      "Epoch 10, Batch 293, Loss: 0.5057224035263062\n",
      "Epoch 10, Batch 294, Loss: 0.40130260586738586\n",
      "Epoch 10, Batch 295, Loss: 0.6264276504516602\n",
      "Epoch 10, Batch 296, Loss: 0.5349955558776855\n",
      "Epoch 10, Batch 297, Loss: 0.6312990784645081\n",
      "Epoch 10, Batch 298, Loss: 0.5125587582588196\n",
      "Epoch 10, Batch 299, Loss: 0.2640193700790405\n",
      "Epoch 10, Batch 300, Loss: 0.6106866598129272\n",
      "Epoch 10, Batch 301, Loss: 0.44691818952560425\n",
      "Epoch 10, Batch 302, Loss: 0.2911994159221649\n",
      "Epoch 10, Batch 303, Loss: 0.4064279794692993\n",
      "Epoch 10, Batch 304, Loss: 0.6189219355583191\n",
      "Epoch 10, Batch 305, Loss: 0.4823514521121979\n",
      "Epoch 10, Batch 306, Loss: 0.5489201545715332\n",
      "Epoch 10, Batch 307, Loss: 0.7245170474052429\n",
      "Epoch 10, Batch 308, Loss: 0.7967522144317627\n",
      "Epoch 10, Batch 309, Loss: 0.5031434297561646\n",
      "Epoch 10, Batch 310, Loss: 0.5656936168670654\n",
      "Epoch 10, Batch 311, Loss: 0.47817081212997437\n",
      "Epoch 10, Batch 312, Loss: 0.6796045303344727\n",
      "Epoch 10, Batch 313, Loss: 0.5263930559158325\n",
      "Epoch 10, Batch 314, Loss: 0.42051196098327637\n",
      "Epoch 10, Batch 315, Loss: 0.6127657294273376\n",
      "Epoch 10, Batch 316, Loss: 0.6719225645065308\n",
      "Epoch 10, Batch 317, Loss: 0.5399109721183777\n",
      "Epoch 10, Batch 318, Loss: 0.35254958271980286\n",
      "Epoch 10, Batch 319, Loss: 0.4229127764701843\n",
      "Epoch 10, Batch 320, Loss: 0.7575156092643738\n",
      "Epoch 10, Batch 321, Loss: 0.6098713278770447\n",
      "Epoch 10, Batch 322, Loss: 0.5277988314628601\n",
      "Epoch 10, Batch 323, Loss: 0.4575254023075104\n",
      "Epoch 10, Batch 324, Loss: 0.3454393148422241\n",
      "Epoch 10, Batch 325, Loss: 0.4262599050998688\n",
      "Epoch 10, Batch 326, Loss: 0.5525509119033813\n",
      "Epoch 10, Batch 327, Loss: 0.6478102803230286\n",
      "Epoch 10, Batch 328, Loss: 0.45509323477745056\n",
      "Epoch 10, Batch 329, Loss: 0.47857049107551575\n",
      "Epoch 10, Batch 330, Loss: 0.34489184617996216\n",
      "Epoch 10, Batch 331, Loss: 0.5280846953392029\n",
      "Epoch 10, Batch 332, Loss: 0.501911461353302\n",
      "Epoch 10, Batch 333, Loss: 0.45320624113082886\n",
      "Epoch 10, Batch 334, Loss: 0.5226940512657166\n",
      "Epoch 10, Batch 335, Loss: 0.6251251101493835\n",
      "Epoch 10, Batch 336, Loss: 0.7424709796905518\n",
      "Epoch 10, Batch 337, Loss: 0.5452049970626831\n",
      "Epoch 10, Batch 338, Loss: 0.5127594470977783\n",
      "Epoch 10, Batch 339, Loss: 0.41017675399780273\n",
      "Epoch 10, Batch 340, Loss: 0.47523099184036255\n",
      "Epoch 10, Batch 341, Loss: 0.5059219598770142\n",
      "Epoch 10, Batch 342, Loss: 0.4515168070793152\n",
      "Epoch 10, Batch 343, Loss: 0.480783075094223\n",
      "Epoch 10, Batch 344, Loss: 0.6505007147789001\n",
      "Epoch 10, Batch 345, Loss: 0.538277268409729\n",
      "Epoch 10, Batch 346, Loss: 0.31973984837532043\n",
      "Epoch 10, Batch 347, Loss: 0.41772696375846863\n",
      "Epoch 10, Batch 348, Loss: 0.43758776783943176\n",
      "Epoch 10, Batch 349, Loss: 0.419962078332901\n",
      "Epoch 10, Batch 350, Loss: 0.4392642080783844\n",
      "Epoch 10, Batch 351, Loss: 0.5310714244842529\n",
      "Epoch 10, Batch 352, Loss: 0.6524181962013245\n",
      "Epoch 10, Batch 353, Loss: 0.39220523834228516\n",
      "Epoch 10, Batch 354, Loss: 0.6199446320533752\n",
      "Epoch 10, Batch 355, Loss: 0.5269437432289124\n",
      "Epoch 10, Batch 356, Loss: 0.5068782567977905\n",
      "Epoch 10, Batch 357, Loss: 0.47397443652153015\n",
      "Epoch 10, Batch 358, Loss: 0.5613016486167908\n",
      "Epoch 10, Batch 359, Loss: 0.5198373198509216\n",
      "Epoch 10, Batch 360, Loss: 0.718059778213501\n",
      "Epoch 10, Batch 361, Loss: 0.5384601950645447\n",
      "Epoch 10, Batch 362, Loss: 0.358968049287796\n",
      "Epoch 10, Batch 363, Loss: 0.34093815088272095\n",
      "Epoch 10, Batch 364, Loss: 0.8397778868675232\n",
      "Epoch 10, Batch 365, Loss: 0.4377884864807129\n",
      "Epoch 10, Batch 366, Loss: 0.5492262244224548\n",
      "Epoch 10, Batch 367, Loss: 0.4109143018722534\n",
      "Epoch 10, Batch 368, Loss: 0.4101945459842682\n",
      "Epoch 10, Batch 369, Loss: 0.5888862609863281\n",
      "Epoch 10, Batch 370, Loss: 0.7336452007293701\n",
      "Epoch 10, Batch 371, Loss: 0.6446871161460876\n",
      "Epoch 10, Batch 372, Loss: 0.6993688344955444\n",
      "Epoch 10, Batch 373, Loss: 0.4703461825847626\n",
      "Epoch 10, Batch 374, Loss: 0.4333938956260681\n",
      "Epoch 10, Batch 375, Loss: 0.41495928168296814\n",
      "Epoch 10, Batch 376, Loss: 0.4017115831375122\n",
      "Epoch 10, Batch 377, Loss: 0.48367932438850403\n",
      "Epoch 10, Batch 378, Loss: 0.6234095692634583\n",
      "Epoch 10, Batch 379, Loss: 0.6969294548034668\n",
      "Epoch 10, Batch 380, Loss: 0.4498309791088104\n",
      "Epoch 10, Batch 381, Loss: 0.5614520311355591\n",
      "Epoch 10, Batch 382, Loss: 0.4962468445301056\n",
      "Epoch 10, Batch 383, Loss: 0.4625491797924042\n",
      "Epoch 10, Batch 384, Loss: 0.4952077269554138\n",
      "Epoch 10, Batch 385, Loss: 0.5782564878463745\n",
      "Epoch 10, Batch 386, Loss: 0.4312891960144043\n",
      "Epoch 10, Batch 387, Loss: 0.35726675391197205\n",
      "Epoch 10, Batch 388, Loss: 0.48415735363960266\n",
      "Epoch 10, Batch 389, Loss: 0.5728327631950378\n",
      "Epoch 10, Batch 390, Loss: 0.5210949778556824\n",
      "Epoch 10, Batch 391, Loss: 0.5530628561973572\n",
      "Epoch 10, Batch 392, Loss: 0.34335193037986755\n",
      "Epoch 10, Batch 393, Loss: 0.712486207485199\n",
      "Epoch 10, Batch 394, Loss: 0.7274303436279297\n",
      "Epoch 10, Batch 395, Loss: 0.5633309483528137\n",
      "Epoch 10, Batch 396, Loss: 0.4559904932975769\n",
      "Epoch 10, Batch 397, Loss: 0.5253981947898865\n",
      "Epoch 10, Batch 398, Loss: 0.4430527985095978\n",
      "Epoch 10, Batch 399, Loss: 0.5427824854850769\n",
      "Epoch 10, Batch 400, Loss: 0.5970195531845093\n",
      "Epoch 10, Batch 401, Loss: 0.5805565118789673\n",
      "Epoch 10, Batch 402, Loss: 0.5926397442817688\n",
      "Epoch 10, Batch 403, Loss: 0.5127564072608948\n",
      "Epoch 10, Batch 404, Loss: 0.5800890922546387\n",
      "Epoch 10, Batch 405, Loss: 0.5865157842636108\n",
      "Epoch 10, Batch 406, Loss: 0.4676000475883484\n",
      "Epoch 10, Batch 407, Loss: 0.41051408648490906\n",
      "Epoch 10, Batch 408, Loss: 0.4118577539920807\n",
      "Epoch 10, Batch 409, Loss: 0.6952740550041199\n",
      "Epoch 10, Batch 410, Loss: 0.5200037956237793\n",
      "Epoch 10, Batch 411, Loss: 0.4849250316619873\n",
      "Epoch 10, Batch 412, Loss: 0.27520209550857544\n",
      "Epoch 10, Batch 413, Loss: 0.4015447199344635\n",
      "Epoch 10, Batch 414, Loss: 0.6159087419509888\n",
      "Epoch 10, Batch 415, Loss: 0.3603895604610443\n",
      "Epoch 10, Batch 416, Loss: 0.6172741055488586\n",
      "Epoch 10, Batch 417, Loss: 0.38645458221435547\n",
      "Epoch 10, Batch 418, Loss: 0.577210545539856\n",
      "Epoch 10, Batch 419, Loss: 0.6281002759933472\n",
      "Epoch 10, Batch 420, Loss: 0.34051617980003357\n",
      "Epoch 10, Batch 421, Loss: 0.7079106569290161\n",
      "Epoch 10, Batch 422, Loss: 0.582311749458313\n",
      "Epoch 10, Batch 423, Loss: 0.7196414470672607\n",
      "Epoch 10, Batch 424, Loss: 0.5381360650062561\n",
      "Epoch 10, Batch 425, Loss: 0.5168730616569519\n",
      "Epoch 10, Batch 426, Loss: 0.573095977306366\n",
      "Epoch 10, Batch 427, Loss: 0.4751818776130676\n",
      "Epoch 10, Batch 428, Loss: 0.39204975962638855\n",
      "Epoch 10, Batch 429, Loss: 0.6176350116729736\n",
      "Epoch 10, Batch 430, Loss: 0.37529709935188293\n",
      "Epoch 10, Batch 431, Loss: 0.3697322607040405\n",
      "Epoch 10, Batch 432, Loss: 0.6028165221214294\n",
      "Epoch 10, Batch 433, Loss: 0.6203463673591614\n",
      "Epoch 10, Batch 434, Loss: 0.33862078189849854\n",
      "Epoch 10, Batch 435, Loss: 0.45389285683631897\n",
      "Epoch 10, Batch 436, Loss: 0.3438388705253601\n",
      "Epoch 10, Batch 437, Loss: 0.6122577786445618\n",
      "Epoch 10, Batch 438, Loss: 0.4771476089954376\n",
      "Epoch 10, Batch 439, Loss: 0.41149598360061646\n",
      "Epoch 10, Batch 440, Loss: 0.45822983980178833\n",
      "Epoch 10, Batch 441, Loss: 0.5263980627059937\n",
      "Epoch 10, Batch 442, Loss: 0.3978067934513092\n",
      "Epoch 10, Batch 443, Loss: 0.4717848300933838\n",
      "Epoch 10, Batch 444, Loss: 0.3722842335700989\n",
      "Epoch 10, Batch 445, Loss: 0.42689844965934753\n",
      "Epoch 10, Batch 446, Loss: 0.5775635838508606\n",
      "Epoch 10, Batch 447, Loss: 0.45863059163093567\n",
      "Epoch 10, Batch 448, Loss: 0.7004048824310303\n",
      "Epoch 10, Batch 449, Loss: 0.4107523262500763\n",
      "Epoch 10, Batch 450, Loss: 0.6537216901779175\n",
      "Epoch 10, Batch 451, Loss: 0.5031101107597351\n",
      "Epoch 10, Batch 452, Loss: 0.37270328402519226\n",
      "Epoch 10, Batch 453, Loss: 0.30273738503456116\n",
      "Epoch 10, Batch 454, Loss: 0.49366387724876404\n",
      "Epoch 10, Batch 455, Loss: 0.4574330747127533\n",
      "Epoch 10, Batch 456, Loss: 0.49146366119384766\n",
      "Epoch 10, Batch 457, Loss: 0.4908943176269531\n",
      "Epoch 10, Batch 458, Loss: 0.34279200434684753\n",
      "Epoch 10, Batch 459, Loss: 0.4391598701477051\n",
      "Epoch 10, Batch 460, Loss: 0.5798910856246948\n",
      "Epoch 10, Batch 461, Loss: 0.6495791077613831\n",
      "Epoch 10, Batch 462, Loss: 0.369992733001709\n",
      "Epoch 10, Batch 463, Loss: 0.35238584876060486\n",
      "Epoch 10, Batch 464, Loss: 0.5921153426170349\n",
      "Epoch 10, Batch 465, Loss: 0.26573044061660767\n",
      "Epoch 10, Batch 466, Loss: 0.7079397439956665\n",
      "Epoch 10, Batch 467, Loss: 0.6762070655822754\n",
      "Epoch 10, Batch 468, Loss: 0.3749088644981384\n",
      "Epoch 10, Batch 469, Loss: 0.5373975038528442\n",
      "Epoch 10, Batch 470, Loss: 0.4768723249435425\n",
      "Epoch 10, Batch 471, Loss: 0.40603891015052795\n",
      "Epoch 10, Batch 472, Loss: 0.6522529721260071\n",
      "Epoch 10, Batch 473, Loss: 0.6187583208084106\n",
      "Epoch 10, Batch 474, Loss: 0.5685226917266846\n",
      "Epoch 10, Batch 475, Loss: 0.6190704107284546\n",
      "Epoch 10, Batch 476, Loss: 0.3203880786895752\n",
      "Epoch 10, Batch 477, Loss: 0.8706498742103577\n",
      "Epoch 10, Batch 478, Loss: 0.5517998933792114\n",
      "Epoch 10, Batch 479, Loss: 0.5469487905502319\n",
      "Epoch 10, Batch 480, Loss: 0.3942406177520752\n",
      "Epoch 10, Batch 481, Loss: 0.3580346703529358\n",
      "Epoch 10, Batch 482, Loss: 0.47644907236099243\n",
      "Epoch 10, Batch 483, Loss: 0.5075380802154541\n",
      "Epoch 10, Batch 484, Loss: 0.4219035506248474\n",
      "Epoch 10, Batch 485, Loss: 0.5290579199790955\n",
      "Epoch 10, Batch 486, Loss: 0.5534501075744629\n",
      "Epoch 10, Batch 487, Loss: 0.7921148538589478\n",
      "Epoch 10, Batch 488, Loss: 0.5269107222557068\n",
      "Epoch 10, Batch 489, Loss: 0.24697932600975037\n",
      "Epoch 10, Batch 490, Loss: 0.28052765130996704\n",
      "Epoch 10, Batch 491, Loss: 0.37616264820098877\n",
      "Epoch 10, Batch 492, Loss: 0.3677034378051758\n",
      "Epoch 10, Batch 493, Loss: 0.45267075300216675\n",
      "Epoch 10, Batch 494, Loss: 0.388523131608963\n",
      "Epoch 10, Batch 495, Loss: 0.39462101459503174\n",
      "Epoch 10, Batch 496, Loss: 0.6870719194412231\n",
      "Epoch 10, Batch 497, Loss: 0.4464808404445648\n",
      "Epoch 10, Batch 498, Loss: 0.6559579372406006\n",
      "Epoch 10, Batch 499, Loss: 0.4570637047290802\n",
      "Epoch 10, Batch 500, Loss: 0.37299293279647827\n",
      "Epoch 10, Batch 501, Loss: 0.39914172887802124\n",
      "Epoch 10, Batch 502, Loss: 0.6329779028892517\n",
      "Epoch 10, Batch 503, Loss: 0.42491966485977173\n",
      "Epoch 10, Batch 504, Loss: 0.6599966287612915\n",
      "Epoch 10, Batch 505, Loss: 0.5953172445297241\n",
      "Epoch 10, Batch 506, Loss: 0.6533502340316772\n",
      "Epoch 10, Batch 507, Loss: 0.5263462662696838\n",
      "Epoch 10, Batch 508, Loss: 0.4988434910774231\n",
      "Epoch 10, Batch 509, Loss: 0.7032034993171692\n",
      "Epoch 10, Batch 510, Loss: 0.4759686291217804\n",
      "Epoch 10, Batch 511, Loss: 0.6281132102012634\n",
      "Epoch 10, Batch 512, Loss: 0.5002065896987915\n",
      "Epoch 10, Batch 513, Loss: 0.5379765629768372\n",
      "Epoch 10, Batch 514, Loss: 0.7097365856170654\n",
      "Epoch 10, Batch 515, Loss: 0.45228299498558044\n",
      "Epoch 10, Batch 516, Loss: 0.5649900436401367\n",
      "Epoch 10, Batch 517, Loss: 0.511849045753479\n",
      "Epoch 10, Batch 518, Loss: 0.5370897054672241\n",
      "Epoch 10, Batch 519, Loss: 0.5623559951782227\n",
      "Epoch 10, Batch 520, Loss: 0.3262903690338135\n",
      "Epoch 10, Batch 521, Loss: 0.5681096315383911\n",
      "Epoch 10, Batch 522, Loss: 0.5410144329071045\n",
      "Epoch 10, Batch 523, Loss: 0.359894335269928\n",
      "Epoch 10, Batch 524, Loss: 0.7355796098709106\n",
      "Epoch 10, Batch 525, Loss: 0.4816538393497467\n",
      "Epoch 10, Batch 526, Loss: 0.3756377100944519\n",
      "Epoch 10, Batch 527, Loss: 0.43078184127807617\n",
      "Epoch 10, Batch 528, Loss: 0.3649449944496155\n",
      "Epoch 10, Batch 529, Loss: 0.647577166557312\n",
      "Epoch 10, Batch 530, Loss: 0.43264541029930115\n",
      "Epoch 10, Batch 531, Loss: 0.7802505493164062\n",
      "Epoch 10, Batch 532, Loss: 0.7983390688896179\n",
      "Epoch 10, Batch 533, Loss: 0.8499870300292969\n",
      "Epoch 10, Batch 534, Loss: 0.45625999569892883\n",
      "Epoch 10, Batch 535, Loss: 0.4303120970726013\n",
      "Epoch 10, Batch 536, Loss: 0.5575563907623291\n",
      "Epoch 10, Batch 537, Loss: 0.4300241470336914\n",
      "Epoch 10, Batch 538, Loss: 0.6835837364196777\n",
      "Epoch 10, Batch 539, Loss: 0.5733034610748291\n",
      "Epoch 10, Batch 540, Loss: 0.6645902991294861\n",
      "Epoch 10, Batch 541, Loss: 0.7961891293525696\n",
      "Epoch 10, Batch 542, Loss: 0.32354116439819336\n",
      "Epoch 10, Batch 543, Loss: 0.5852652788162231\n",
      "Epoch 10, Batch 544, Loss: 0.8014702796936035\n",
      "Epoch 10, Batch 545, Loss: 0.4287028908729553\n",
      "Epoch 10, Batch 546, Loss: 0.515310525894165\n",
      "Epoch 10, Batch 547, Loss: 0.43507418036460876\n",
      "Epoch 10, Batch 548, Loss: 0.5986586213111877\n",
      "Epoch 10, Batch 549, Loss: 0.5364399552345276\n",
      "Epoch 10, Batch 550, Loss: 0.35064560174942017\n",
      "Epoch 10, Batch 551, Loss: 0.846013069152832\n",
      "Epoch 10, Batch 552, Loss: 0.4617493450641632\n",
      "Epoch 10, Batch 553, Loss: 0.7339879870414734\n",
      "Epoch 10, Batch 554, Loss: 0.45431971549987793\n",
      "Epoch 10, Batch 555, Loss: 0.6802704334259033\n",
      "Epoch 10, Batch 556, Loss: 0.5317518711090088\n",
      "Epoch 10, Batch 557, Loss: 0.3275308609008789\n",
      "Epoch 10, Batch 558, Loss: 0.4611658453941345\n",
      "Epoch 10, Batch 559, Loss: 0.5029098987579346\n",
      "Epoch 10, Batch 560, Loss: 0.5504734516143799\n",
      "Epoch 10, Batch 561, Loss: 0.7060950994491577\n",
      "Epoch 10, Batch 562, Loss: 0.5847394466400146\n",
      "Epoch 10, Batch 563, Loss: 0.5049611330032349\n",
      "Epoch 10, Batch 564, Loss: 0.5658192038536072\n",
      "Epoch 10, Batch 565, Loss: 0.46023690700531006\n",
      "Epoch 10, Batch 566, Loss: 0.39373618364334106\n",
      "Epoch 10, Batch 567, Loss: 0.4542430341243744\n",
      "Epoch 10, Batch 568, Loss: 0.45100638270378113\n",
      "Epoch 10, Batch 569, Loss: 0.5543965101242065\n",
      "Epoch 10, Batch 570, Loss: 0.3615782856941223\n",
      "Epoch 10, Batch 571, Loss: 0.34551873803138733\n",
      "Epoch 10, Batch 572, Loss: 0.46808063983917236\n",
      "Epoch 10, Batch 573, Loss: 0.5028349757194519\n",
      "Epoch 10, Batch 574, Loss: 0.4383086562156677\n",
      "Epoch 10, Batch 575, Loss: 0.6700080037117004\n",
      "Epoch 10, Batch 576, Loss: 0.5632708072662354\n",
      "Epoch 10, Batch 577, Loss: 0.7143168449401855\n",
      "Epoch 10, Batch 578, Loss: 0.3434091806411743\n",
      "Epoch 10, Batch 579, Loss: 0.6933203339576721\n",
      "Epoch 10, Batch 580, Loss: 0.4996558427810669\n",
      "Epoch 10, Batch 581, Loss: 0.5476294755935669\n",
      "Epoch 10, Batch 582, Loss: 0.5234473347663879\n",
      "Epoch 10, Batch 583, Loss: 0.6391875147819519\n",
      "Epoch 10, Batch 584, Loss: 0.4727665185928345\n",
      "Epoch 10, Batch 585, Loss: 0.3503585457801819\n",
      "Epoch 10, Batch 586, Loss: 0.465718150138855\n",
      "Epoch 10, Batch 587, Loss: 0.47055238485336304\n",
      "Epoch 10, Batch 588, Loss: 0.563367486000061\n",
      "Epoch 10, Batch 589, Loss: 0.38616248965263367\n",
      "Epoch 10, Batch 590, Loss: 0.44237738847732544\n",
      "Epoch 10, Batch 591, Loss: 0.43692731857299805\n",
      "Epoch 10, Batch 592, Loss: 0.31401538848876953\n",
      "Epoch 10, Batch 593, Loss: 0.6060066223144531\n",
      "Epoch 10, Batch 594, Loss: 0.591940701007843\n",
      "Epoch 10, Batch 595, Loss: 0.4497400224208832\n",
      "Epoch 10, Batch 596, Loss: 0.44310057163238525\n",
      "Epoch 10, Batch 597, Loss: 0.5180052518844604\n",
      "Epoch 10, Batch 598, Loss: 0.46847400069236755\n",
      "Epoch 10, Batch 599, Loss: 0.31024640798568726\n",
      "Epoch 10, Batch 600, Loss: 0.46422621607780457\n",
      "Epoch 10, Batch 601, Loss: 0.40219825506210327\n",
      "Epoch 10, Batch 602, Loss: 0.403164803981781\n",
      "Epoch 10, Batch 603, Loss: 0.7110728025436401\n",
      "Epoch 10, Batch 604, Loss: 0.6437415480613708\n",
      "Epoch 10, Batch 605, Loss: 0.49375057220458984\n",
      "Epoch 10, Batch 606, Loss: 0.7230319976806641\n",
      "Epoch 10, Batch 607, Loss: 0.4742732644081116\n",
      "Epoch 10, Batch 608, Loss: 0.5568824410438538\n",
      "Epoch 10, Batch 609, Loss: 0.5976114869117737\n",
      "Epoch 10, Batch 610, Loss: 0.5336382389068604\n",
      "Epoch 10, Batch 611, Loss: 0.4915243983268738\n",
      "Epoch 10, Batch 612, Loss: 0.5885105133056641\n",
      "Epoch 10, Batch 613, Loss: 0.4444844126701355\n",
      "Epoch 10, Batch 614, Loss: 0.5603407621383667\n",
      "Epoch 10, Batch 615, Loss: 0.7780836224555969\n",
      "Epoch 10, Batch 616, Loss: 0.4973103106021881\n",
      "Epoch 10, Batch 617, Loss: 0.41820430755615234\n",
      "Epoch 10, Batch 618, Loss: 0.5081167221069336\n",
      "Epoch 10, Batch 619, Loss: 0.40208953619003296\n",
      "Epoch 10, Batch 620, Loss: 0.8008067607879639\n",
      "Epoch 10, Batch 621, Loss: 0.4775404632091522\n",
      "Epoch 10, Batch 622, Loss: 0.4411025941371918\n",
      "Epoch 10, Batch 623, Loss: 0.4481738209724426\n",
      "Epoch 10, Batch 624, Loss: 0.63240647315979\n",
      "Epoch 10, Batch 625, Loss: 0.29070594906806946\n",
      "Epoch 10, Batch 626, Loss: 0.5021781325340271\n",
      "Epoch 10, Batch 627, Loss: 0.5745943784713745\n",
      "Epoch 10, Batch 628, Loss: 0.7066987752914429\n",
      "Epoch 10, Batch 629, Loss: 0.4631631672382355\n",
      "Epoch 10, Batch 630, Loss: 0.7217383980751038\n",
      "Epoch 10, Batch 631, Loss: 0.489828884601593\n",
      "Epoch 10, Batch 632, Loss: 0.6152892112731934\n",
      "Epoch 10, Batch 633, Loss: 0.5347760915756226\n",
      "Epoch 10, Batch 634, Loss: 0.4757734537124634\n",
      "Epoch 10, Batch 635, Loss: 0.3430595397949219\n",
      "Epoch 10, Batch 636, Loss: 0.5622808337211609\n",
      "Epoch 10, Batch 637, Loss: 0.5855033993721008\n",
      "Epoch 10, Batch 638, Loss: 0.6387569308280945\n",
      "Epoch 10, Batch 639, Loss: 0.42644011974334717\n",
      "Epoch 10, Batch 640, Loss: 0.5731825232505798\n",
      "Epoch 10, Batch 641, Loss: 0.49566298723220825\n",
      "Epoch 10, Batch 642, Loss: 0.5987550616264343\n",
      "Epoch 10, Batch 643, Loss: 0.598991334438324\n",
      "Epoch 10, Batch 644, Loss: 0.45721203088760376\n",
      "Epoch 10, Batch 645, Loss: 0.4804789125919342\n",
      "Epoch 10, Batch 646, Loss: 0.4271595776081085\n",
      "Epoch 10, Batch 647, Loss: 0.6609663367271423\n",
      "Epoch 10, Batch 648, Loss: 0.5620378851890564\n",
      "Epoch 10, Batch 649, Loss: 0.46660780906677246\n",
      "Epoch 10, Batch 650, Loss: 0.5700564384460449\n",
      "Epoch 10, Batch 651, Loss: 0.4932078421115875\n",
      "Epoch 10, Batch 652, Loss: 0.46608567237854004\n",
      "Epoch 10, Batch 653, Loss: 0.529168963432312\n",
      "Epoch 10, Batch 654, Loss: 0.5580376386642456\n",
      "Epoch 10, Batch 655, Loss: 0.5662803053855896\n",
      "Epoch 10, Batch 656, Loss: 0.3876800835132599\n",
      "Epoch 10, Batch 657, Loss: 0.5031919479370117\n",
      "Epoch 10, Batch 658, Loss: 0.38028010725975037\n",
      "Epoch 10, Batch 659, Loss: 0.3717689514160156\n",
      "Epoch 10, Batch 660, Loss: 0.5881447792053223\n",
      "Epoch 10, Batch 661, Loss: 0.6531825661659241\n",
      "Epoch 10, Batch 662, Loss: 0.5917571187019348\n",
      "Epoch 10, Batch 663, Loss: 0.4421460032463074\n",
      "Epoch 10, Batch 664, Loss: 0.4570730924606323\n",
      "Epoch 10, Batch 665, Loss: 0.6680775880813599\n",
      "Epoch 10, Batch 666, Loss: 0.5345844030380249\n",
      "Epoch 10, Batch 667, Loss: 0.42001086473464966\n",
      "Epoch 10, Batch 668, Loss: 0.43098053336143494\n",
      "Epoch 10, Batch 669, Loss: 0.3913319706916809\n",
      "Epoch 10, Batch 670, Loss: 0.47010791301727295\n",
      "Epoch 10, Batch 671, Loss: 0.5017913579940796\n",
      "Epoch 10, Batch 672, Loss: 0.510097324848175\n",
      "Epoch 10, Batch 673, Loss: 0.8206867575645447\n",
      "Epoch 10, Batch 674, Loss: 0.3580658435821533\n",
      "Epoch 10, Batch 675, Loss: 0.5944400429725647\n",
      "Epoch 10, Batch 676, Loss: 0.6150818467140198\n",
      "Epoch 10, Batch 677, Loss: 0.5867575407028198\n",
      "Epoch 10, Batch 678, Loss: 0.7275785207748413\n",
      "Epoch 10, Batch 679, Loss: 0.5311079621315002\n",
      "Epoch 10, Batch 680, Loss: 0.4923464059829712\n",
      "Epoch 10, Batch 681, Loss: 0.4914165437221527\n",
      "Epoch 10, Batch 682, Loss: 0.4623749256134033\n",
      "Epoch 10, Batch 683, Loss: 0.41288912296295166\n",
      "Epoch 10, Batch 684, Loss: 0.6020228266716003\n",
      "Epoch 10, Batch 685, Loss: 0.5554764270782471\n",
      "Epoch 10, Batch 686, Loss: 0.6382886171340942\n",
      "Epoch 10, Batch 687, Loss: 0.554720938205719\n",
      "Epoch 10, Batch 688, Loss: 0.3878841996192932\n",
      "Epoch 10, Batch 689, Loss: 0.5918194055557251\n",
      "Epoch 10, Batch 690, Loss: 0.6839649081230164\n",
      "Epoch 10, Batch 691, Loss: 0.4903092086315155\n",
      "Epoch 10, Batch 692, Loss: 0.4426220655441284\n",
      "Epoch 10, Batch 693, Loss: 0.5263161659240723\n",
      "Epoch 10, Batch 694, Loss: 0.46764957904815674\n",
      "Epoch 10, Batch 695, Loss: 0.44138479232788086\n",
      "Epoch 10, Batch 696, Loss: 0.47256022691726685\n",
      "Epoch 10, Batch 697, Loss: 0.6662591099739075\n",
      "Epoch 10, Batch 698, Loss: 0.3395686745643616\n",
      "Epoch 10, Batch 699, Loss: 0.486438125371933\n",
      "Epoch 10, Batch 700, Loss: 0.5412120223045349\n",
      "Epoch 10, Batch 701, Loss: 0.6521348357200623\n",
      "Epoch 10, Batch 702, Loss: 0.6196649670600891\n",
      "Epoch 10, Batch 703, Loss: 0.4955228865146637\n",
      "Epoch 10, Batch 704, Loss: 0.4674747586250305\n",
      "Epoch 10, Batch 705, Loss: 0.6070989966392517\n",
      "Epoch 10, Batch 706, Loss: 0.6264851689338684\n",
      "Epoch 10, Batch 707, Loss: 0.564606249332428\n",
      "Epoch 10, Batch 708, Loss: 0.5146422386169434\n",
      "Epoch 10, Batch 709, Loss: 0.5427934527397156\n",
      "Epoch 10, Batch 710, Loss: 0.45941734313964844\n",
      "Epoch 10, Batch 711, Loss: 0.43189412355422974\n",
      "Epoch 10, Batch 712, Loss: 0.6868016719818115\n",
      "Epoch 10, Batch 713, Loss: 0.4592097997665405\n",
      "Epoch 10, Batch 714, Loss: 0.43257570266723633\n",
      "Epoch 10, Batch 715, Loss: 0.43008869886398315\n",
      "Epoch 10, Batch 716, Loss: 0.3358178734779358\n",
      "Epoch 10, Batch 717, Loss: 0.6776297092437744\n",
      "Epoch 10, Batch 718, Loss: 0.4063512682914734\n",
      "Epoch 10, Batch 719, Loss: 0.6430134773254395\n",
      "Epoch 10, Batch 720, Loss: 0.4571634829044342\n",
      "Epoch 10, Batch 721, Loss: 0.459258496761322\n",
      "Epoch 10, Batch 722, Loss: 0.4317276179790497\n",
      "Epoch 10, Batch 723, Loss: 0.5379883050918579\n",
      "Epoch 10, Batch 724, Loss: 0.39796122908592224\n",
      "Epoch 10, Batch 725, Loss: 0.6878345012664795\n",
      "Epoch 10, Batch 726, Loss: 0.7089323401451111\n",
      "Epoch 10, Batch 727, Loss: 0.7932726144790649\n",
      "Epoch 10, Batch 728, Loss: 0.46193599700927734\n",
      "Epoch 10, Batch 729, Loss: 0.6853073835372925\n",
      "Epoch 10, Batch 730, Loss: 0.5775562524795532\n",
      "Epoch 10, Batch 731, Loss: 0.417138934135437\n",
      "Epoch 10, Batch 732, Loss: 0.5443334579467773\n",
      "Epoch 10, Batch 733, Loss: 0.5129141807556152\n",
      "Epoch 10, Batch 734, Loss: 0.46701478958129883\n",
      "Epoch 10, Batch 735, Loss: 0.4702751338481903\n",
      "Epoch 10, Batch 736, Loss: 0.5287701487541199\n",
      "Epoch 10, Batch 737, Loss: 0.7241023182868958\n",
      "Epoch 10, Batch 738, Loss: 0.41005149483680725\n",
      "Epoch 10, Batch 739, Loss: 0.5967727899551392\n",
      "Epoch 10, Batch 740, Loss: 0.6316595673561096\n",
      "Epoch 10, Batch 741, Loss: 0.6334311366081238\n",
      "Epoch 10, Batch 742, Loss: 0.822483241558075\n",
      "Epoch 10, Batch 743, Loss: 0.5490516424179077\n",
      "Epoch 10, Batch 744, Loss: 0.43222761154174805\n",
      "Epoch 10, Batch 745, Loss: 0.7370421886444092\n",
      "Epoch 10, Batch 746, Loss: 0.4534733295440674\n",
      "Epoch 10, Batch 747, Loss: 0.5495983362197876\n",
      "Epoch 10, Batch 748, Loss: 0.8245740532875061\n",
      "Epoch 10, Batch 749, Loss: 0.5666254162788391\n",
      "Epoch 10, Batch 750, Loss: 0.5554277896881104\n",
      "Epoch 10, Batch 751, Loss: 0.3761087656021118\n",
      "Epoch 10, Batch 752, Loss: 0.6285743117332458\n",
      "Epoch 10, Batch 753, Loss: 0.6240387558937073\n",
      "Epoch 10, Batch 754, Loss: 0.48439526557922363\n",
      "Epoch 10, Batch 755, Loss: 0.6454078555107117\n",
      "Epoch 10, Batch 756, Loss: 0.5262715816497803\n",
      "Epoch 10, Batch 757, Loss: 0.4711732268333435\n",
      "Epoch 10, Batch 758, Loss: 0.5990158319473267\n",
      "Epoch 10, Batch 759, Loss: 0.39819541573524475\n",
      "Epoch 10, Batch 760, Loss: 0.5054721236228943\n",
      "Epoch 10, Batch 761, Loss: 0.4986646771430969\n",
      "Epoch 10, Batch 762, Loss: 0.3825743794441223\n",
      "Epoch 10, Batch 763, Loss: 0.3897715210914612\n",
      "Epoch 10, Batch 764, Loss: 0.5393639206886292\n",
      "Epoch 10, Batch 765, Loss: 0.393650084733963\n",
      "Epoch 10, Batch 766, Loss: 0.676146388053894\n",
      "Epoch 10, Batch 767, Loss: 0.6271457076072693\n",
      "Epoch 10, Batch 768, Loss: 0.6518101096153259\n",
      "Epoch 10, Batch 769, Loss: 0.4334210157394409\n",
      "Epoch 10, Batch 770, Loss: 0.5033445358276367\n",
      "Epoch 10, Batch 771, Loss: 0.6159749031066895\n",
      "Epoch 10, Batch 772, Loss: 0.5600840449333191\n",
      "Epoch 10, Batch 773, Loss: 0.6552584767341614\n",
      "Epoch 10, Batch 774, Loss: 0.46466493606567383\n",
      "Epoch 10, Batch 775, Loss: 0.3549647033214569\n",
      "Epoch 10, Batch 776, Loss: 0.7431854605674744\n",
      "Epoch 10, Batch 777, Loss: 0.5594756603240967\n",
      "Epoch 10, Batch 778, Loss: 0.6189174056053162\n",
      "Epoch 10, Batch 779, Loss: 0.52358478307724\n",
      "Epoch 10, Batch 780, Loss: 0.6503698229789734\n",
      "Epoch 10, Batch 781, Loss: 0.5294671058654785\n",
      "Epoch 10, Batch 782, Loss: 0.4771653711795807\n",
      "Epoch 10, Batch 783, Loss: 0.5147762894630432\n",
      "Epoch 10, Batch 784, Loss: 0.3582797348499298\n",
      "Epoch 10, Batch 785, Loss: 0.7389810085296631\n",
      "Epoch 10, Batch 786, Loss: 0.39609837532043457\n",
      "Epoch 10, Batch 787, Loss: 0.45660507678985596\n",
      "Epoch 10, Batch 788, Loss: 0.5838430523872375\n",
      "Epoch 10, Batch 789, Loss: 0.3686228394508362\n",
      "Epoch 10, Batch 790, Loss: 0.5588982701301575\n",
      "Epoch 10, Batch 791, Loss: 0.4341161549091339\n",
      "Epoch 10, Batch 792, Loss: 0.3475550711154938\n",
      "Epoch 10, Batch 793, Loss: 0.4137631356716156\n",
      "Epoch 10, Batch 794, Loss: 0.37151792645454407\n",
      "Epoch 10, Batch 795, Loss: 0.4309832453727722\n",
      "Epoch 10, Batch 796, Loss: 0.33590489625930786\n",
      "Epoch 10, Batch 797, Loss: 0.5524395108222961\n",
      "Epoch 10, Batch 798, Loss: 0.5403067469596863\n",
      "Epoch 10, Batch 799, Loss: 0.4928513169288635\n",
      "Epoch 10, Batch 800, Loss: 0.46187514066696167\n",
      "Epoch 10, Batch 801, Loss: 0.7787148952484131\n",
      "Epoch 10, Batch 802, Loss: 0.4468037486076355\n",
      "Epoch 10, Batch 803, Loss: 0.4144110381603241\n",
      "Epoch 10, Batch 804, Loss: 0.42971619963645935\n",
      "Epoch 10, Batch 805, Loss: 0.5629968643188477\n",
      "Epoch 10, Batch 806, Loss: 0.631001889705658\n",
      "Epoch 10, Batch 807, Loss: 0.5487632155418396\n",
      "Epoch 10, Batch 808, Loss: 0.4155443608760834\n",
      "Epoch 10, Batch 809, Loss: 0.5405633449554443\n",
      "Epoch 10, Batch 810, Loss: 0.5637127161026001\n",
      "Epoch 10, Batch 811, Loss: 0.5992344617843628\n",
      "Epoch 10, Batch 812, Loss: 0.5792513489723206\n",
      "Epoch 10, Batch 813, Loss: 0.5469589829444885\n",
      "Epoch 10, Batch 814, Loss: 0.4438459873199463\n",
      "Epoch 10, Batch 815, Loss: 0.576397716999054\n",
      "Epoch 10, Batch 816, Loss: 0.7505820989608765\n",
      "Epoch 10, Batch 817, Loss: 0.60416579246521\n",
      "Epoch 10, Batch 818, Loss: 0.5548642873764038\n",
      "Epoch 10, Batch 819, Loss: 0.43462100625038147\n",
      "Epoch 10, Batch 820, Loss: 0.5736488103866577\n",
      "Epoch 10, Batch 821, Loss: 0.6569366455078125\n",
      "Epoch 10, Batch 822, Loss: 0.70258629322052\n",
      "Epoch 10, Batch 823, Loss: 0.40558192133903503\n",
      "Epoch 10, Batch 824, Loss: 0.5374122858047485\n",
      "Epoch 10, Batch 825, Loss: 0.4501152038574219\n",
      "Epoch 10, Batch 826, Loss: 0.6068927049636841\n",
      "Epoch 10, Batch 827, Loss: 0.6010004878044128\n",
      "Epoch 10, Batch 828, Loss: 0.5140880942344666\n",
      "Epoch 10, Batch 829, Loss: 0.5277000069618225\n",
      "Epoch 10, Batch 830, Loss: 0.42154762148857117\n",
      "Epoch 10, Batch 831, Loss: 0.41203975677490234\n",
      "Epoch 10, Batch 832, Loss: 0.6030120849609375\n",
      "Epoch 10, Batch 833, Loss: 0.37814196944236755\n",
      "Epoch 10, Batch 834, Loss: 0.5140195488929749\n",
      "Epoch 10, Batch 835, Loss: 0.6644936800003052\n",
      "Epoch 10, Batch 836, Loss: 0.4697680175304413\n",
      "Epoch 10, Batch 837, Loss: 0.6434142589569092\n",
      "Epoch 10, Batch 838, Loss: 0.3647558391094208\n",
      "Epoch 10, Batch 839, Loss: 0.5703210830688477\n",
      "Epoch 10, Batch 840, Loss: 0.5427860617637634\n",
      "Epoch 10, Batch 841, Loss: 0.369181752204895\n",
      "Epoch 10, Batch 842, Loss: 0.41326016187667847\n",
      "Epoch 10, Batch 843, Loss: 0.4553927481174469\n",
      "Epoch 10, Batch 844, Loss: 0.44646772742271423\n",
      "Epoch 10, Batch 845, Loss: 0.41798049211502075\n",
      "Epoch 10, Batch 846, Loss: 0.43031787872314453\n",
      "Epoch 10, Batch 847, Loss: 0.6095257997512817\n",
      "Epoch 10, Batch 848, Loss: 0.5844475030899048\n",
      "Epoch 10, Batch 849, Loss: 0.2721717059612274\n",
      "Epoch 10, Batch 850, Loss: 0.4953904449939728\n",
      "Epoch 10, Batch 851, Loss: 0.48423752188682556\n",
      "Epoch 10, Batch 852, Loss: 0.49561604857444763\n",
      "Epoch 10, Batch 853, Loss: 0.41613247990608215\n",
      "Epoch 10, Batch 854, Loss: 0.6287075877189636\n",
      "Epoch 10, Batch 855, Loss: 0.3954680562019348\n",
      "Epoch 10, Batch 856, Loss: 0.3057197034358978\n",
      "Epoch 10, Batch 857, Loss: 0.6049165725708008\n",
      "Epoch 10, Batch 858, Loss: 0.7125386595726013\n",
      "Epoch 10, Batch 859, Loss: 0.8800802230834961\n",
      "Epoch 10, Batch 860, Loss: 0.6459726095199585\n",
      "Epoch 10, Batch 861, Loss: 0.7594751119613647\n",
      "Epoch 10, Batch 862, Loss: 0.3788537383079529\n",
      "Epoch 10, Batch 863, Loss: 0.5162567496299744\n",
      "Epoch 10, Batch 864, Loss: 0.42942139506340027\n",
      "Epoch 10, Batch 865, Loss: 0.45734354853630066\n",
      "Epoch 10, Batch 866, Loss: 0.5934971570968628\n",
      "Epoch 10, Batch 867, Loss: 0.5440399050712585\n",
      "Epoch 10, Batch 868, Loss: 0.6416301727294922\n",
      "Epoch 10, Batch 869, Loss: 0.5879095792770386\n",
      "Epoch 10, Batch 870, Loss: 0.6448546648025513\n",
      "Epoch 10, Batch 871, Loss: 0.5239180326461792\n",
      "Epoch 10, Batch 872, Loss: 0.5939309597015381\n",
      "Epoch 10, Batch 873, Loss: 0.7253233194351196\n",
      "Epoch 10, Batch 874, Loss: 0.338327556848526\n",
      "Epoch 10, Batch 875, Loss: 0.3237907290458679\n",
      "Epoch 10, Batch 876, Loss: 0.7233255505561829\n",
      "Epoch 10, Batch 877, Loss: 0.5130580067634583\n",
      "Epoch 10, Batch 878, Loss: 0.4098817706108093\n",
      "Epoch 10, Batch 879, Loss: 0.558188796043396\n",
      "Epoch 10, Batch 880, Loss: 0.5458940267562866\n",
      "Epoch 10, Batch 881, Loss: 0.6615858674049377\n",
      "Epoch 10, Batch 882, Loss: 0.7114222049713135\n",
      "Epoch 10, Batch 883, Loss: 0.5891713500022888\n",
      "Epoch 10, Batch 884, Loss: 0.4365273416042328\n",
      "Epoch 10, Batch 885, Loss: 0.48191162943840027\n",
      "Epoch 10, Batch 886, Loss: 0.4369913935661316\n",
      "Epoch 10, Batch 887, Loss: 0.6908445358276367\n",
      "Epoch 10, Batch 888, Loss: 0.5661779642105103\n",
      "Epoch 10, Batch 889, Loss: 0.4446398615837097\n",
      "Epoch 10, Batch 890, Loss: 0.37843751907348633\n",
      "Epoch 10, Batch 891, Loss: 0.34058842062950134\n",
      "Epoch 10, Batch 892, Loss: 0.5270330905914307\n",
      "Epoch 10, Batch 893, Loss: 0.5511615872383118\n",
      "Epoch 10, Batch 894, Loss: 0.3688274025917053\n",
      "Epoch 10, Batch 895, Loss: 0.6149332523345947\n",
      "Epoch 10, Batch 896, Loss: 0.3748791217803955\n",
      "Epoch 10, Batch 897, Loss: 0.5756927728652954\n",
      "Epoch 10, Batch 898, Loss: 0.4429526925086975\n",
      "Epoch 10, Batch 899, Loss: 0.49568888545036316\n",
      "Epoch 10, Batch 900, Loss: 0.45373988151550293\n",
      "Epoch 10, Batch 901, Loss: 0.4793192148208618\n",
      "Epoch 10, Batch 902, Loss: 0.5791947245597839\n",
      "Epoch 10, Batch 903, Loss: 0.500265896320343\n",
      "Epoch 10, Batch 904, Loss: 0.6005852222442627\n",
      "Epoch 10, Batch 905, Loss: 0.671334445476532\n",
      "Epoch 10, Batch 906, Loss: 0.48251402378082275\n",
      "Epoch 10, Batch 907, Loss: 0.4398294985294342\n",
      "Epoch 10, Batch 908, Loss: 0.7072762846946716\n",
      "Epoch 10, Batch 909, Loss: 0.4664019048213959\n",
      "Epoch 10, Batch 910, Loss: 0.7012370824813843\n",
      "Epoch 10, Batch 911, Loss: 0.7738855481147766\n",
      "Epoch 10, Batch 912, Loss: 0.37897637486457825\n",
      "Epoch 10, Batch 913, Loss: 0.42133042216300964\n",
      "Epoch 10, Batch 914, Loss: 0.47830691933631897\n",
      "Epoch 10, Batch 915, Loss: 0.44568246603012085\n",
      "Epoch 10, Batch 916, Loss: 0.43494001030921936\n",
      "Epoch 10, Batch 917, Loss: 0.5886601209640503\n",
      "Epoch 10, Batch 918, Loss: 0.5434675812721252\n",
      "Epoch 10, Batch 919, Loss: 0.4486137628555298\n",
      "Epoch 10, Batch 920, Loss: 0.6008251309394836\n",
      "Epoch 10, Batch 921, Loss: 0.3792895972728729\n",
      "Epoch 10, Batch 922, Loss: 0.4104430675506592\n",
      "Epoch 10, Batch 923, Loss: 0.4972984194755554\n",
      "Epoch 10, Batch 924, Loss: 0.3201773464679718\n",
      "Epoch 10, Batch 925, Loss: 0.6500986814498901\n",
      "Epoch 10, Batch 926, Loss: 0.34875553846359253\n",
      "Epoch 10, Batch 927, Loss: 0.6699301600456238\n",
      "Epoch 10, Batch 928, Loss: 0.4428080916404724\n",
      "Epoch 10, Batch 929, Loss: 0.6751492619514465\n",
      "Epoch 10, Batch 930, Loss: 0.6892834305763245\n",
      "Epoch 10, Batch 931, Loss: 0.47108957171440125\n",
      "Epoch 10, Batch 932, Loss: 0.6590139269828796\n",
      "Epoch 10, Batch 933, Loss: 0.539096474647522\n",
      "Epoch 10, Batch 934, Loss: 0.5673478245735168\n",
      "Epoch 10, Batch 935, Loss: 0.364571750164032\n",
      "Epoch 10, Batch 936, Loss: 0.4767582416534424\n",
      "Epoch 10, Batch 937, Loss: 0.47449368238449097\n",
      "Epoch 10, Batch 938, Loss: 0.2642579972743988\n",
      "Accuracy of train set: 0.81785\n",
      "Epoch 10, Batch 1, Test Loss: 0.5174641609191895\n",
      "Epoch 10, Batch 2, Test Loss: 0.4969066381454468\n",
      "Epoch 10, Batch 3, Test Loss: 0.5039123892784119\n",
      "Epoch 10, Batch 4, Test Loss: 0.6487645506858826\n",
      "Epoch 10, Batch 5, Test Loss: 0.6568804383277893\n",
      "Epoch 10, Batch 6, Test Loss: 0.44319283962249756\n",
      "Epoch 10, Batch 7, Test Loss: 0.6964849829673767\n",
      "Epoch 10, Batch 8, Test Loss: 0.39943644404411316\n",
      "Epoch 10, Batch 9, Test Loss: 0.5393427610397339\n",
      "Epoch 10, Batch 10, Test Loss: 0.45859870314598083\n",
      "Epoch 10, Batch 11, Test Loss: 0.5110259652137756\n",
      "Epoch 10, Batch 12, Test Loss: 0.5527788400650024\n",
      "Epoch 10, Batch 13, Test Loss: 0.592728316783905\n",
      "Epoch 10, Batch 14, Test Loss: 0.39787665009498596\n",
      "Epoch 10, Batch 15, Test Loss: 0.4282401204109192\n",
      "Epoch 10, Batch 16, Test Loss: 0.5608968138694763\n",
      "Epoch 10, Batch 17, Test Loss: 0.6036169528961182\n",
      "Epoch 10, Batch 18, Test Loss: 0.6345319151878357\n",
      "Epoch 10, Batch 19, Test Loss: 0.6370173096656799\n",
      "Epoch 10, Batch 20, Test Loss: 0.5361955761909485\n",
      "Epoch 10, Batch 21, Test Loss: 0.7847239971160889\n",
      "Epoch 10, Batch 22, Test Loss: 0.344075471162796\n",
      "Epoch 10, Batch 23, Test Loss: 0.519364595413208\n",
      "Epoch 10, Batch 24, Test Loss: 0.3506050109863281\n",
      "Epoch 10, Batch 25, Test Loss: 0.48324036598205566\n",
      "Epoch 10, Batch 26, Test Loss: 0.5642479658126831\n",
      "Epoch 10, Batch 27, Test Loss: 0.38919639587402344\n",
      "Epoch 10, Batch 28, Test Loss: 0.5088457465171814\n",
      "Epoch 10, Batch 29, Test Loss: 0.4811941981315613\n",
      "Epoch 10, Batch 30, Test Loss: 0.3963364362716675\n",
      "Epoch 10, Batch 31, Test Loss: 0.5307126045227051\n",
      "Epoch 10, Batch 32, Test Loss: 0.4694301187992096\n",
      "Epoch 10, Batch 33, Test Loss: 0.5459927320480347\n",
      "Epoch 10, Batch 34, Test Loss: 0.6260924339294434\n",
      "Epoch 10, Batch 35, Test Loss: 0.5052230954170227\n",
      "Epoch 10, Batch 36, Test Loss: 0.4339313507080078\n",
      "Epoch 10, Batch 37, Test Loss: 0.48009756207466125\n",
      "Epoch 10, Batch 38, Test Loss: 0.4844929277896881\n",
      "Epoch 10, Batch 39, Test Loss: 0.5045977830886841\n",
      "Epoch 10, Batch 40, Test Loss: 0.317716509103775\n",
      "Epoch 10, Batch 41, Test Loss: 0.6568272113800049\n",
      "Epoch 10, Batch 42, Test Loss: 0.42817407846450806\n",
      "Epoch 10, Batch 43, Test Loss: 0.46891823410987854\n",
      "Epoch 10, Batch 44, Test Loss: 0.47821617126464844\n",
      "Epoch 10, Batch 45, Test Loss: 0.5615166425704956\n",
      "Epoch 10, Batch 46, Test Loss: 0.5206986665725708\n",
      "Epoch 10, Batch 47, Test Loss: 0.5993413925170898\n",
      "Epoch 10, Batch 48, Test Loss: 0.42049527168273926\n",
      "Epoch 10, Batch 49, Test Loss: 0.3812371790409088\n",
      "Epoch 10, Batch 50, Test Loss: 0.34750497341156006\n",
      "Epoch 10, Batch 51, Test Loss: 0.47627151012420654\n",
      "Epoch 10, Batch 52, Test Loss: 0.5141211748123169\n",
      "Epoch 10, Batch 53, Test Loss: 0.433379203081131\n",
      "Epoch 10, Batch 54, Test Loss: 0.33769485354423523\n",
      "Epoch 10, Batch 55, Test Loss: 0.41490328311920166\n",
      "Epoch 10, Batch 56, Test Loss: 0.601176381111145\n",
      "Epoch 10, Batch 57, Test Loss: 0.3629400134086609\n",
      "Epoch 10, Batch 58, Test Loss: 0.3925617039203644\n",
      "Epoch 10, Batch 59, Test Loss: 0.35669782757759094\n",
      "Epoch 10, Batch 60, Test Loss: 0.6010456681251526\n",
      "Epoch 10, Batch 61, Test Loss: 0.5038889050483704\n",
      "Epoch 10, Batch 62, Test Loss: 0.29049593210220337\n",
      "Epoch 10, Batch 63, Test Loss: 0.5224010348320007\n",
      "Epoch 10, Batch 64, Test Loss: 0.5755960941314697\n",
      "Epoch 10, Batch 65, Test Loss: 0.4697186350822449\n",
      "Epoch 10, Batch 66, Test Loss: 0.3068690896034241\n",
      "Epoch 10, Batch 67, Test Loss: 0.4816059470176697\n",
      "Epoch 10, Batch 68, Test Loss: 0.371553510427475\n",
      "Epoch 10, Batch 69, Test Loss: 0.603350043296814\n",
      "Epoch 10, Batch 70, Test Loss: 0.5636715888977051\n",
      "Epoch 10, Batch 71, Test Loss: 0.6804556846618652\n",
      "Epoch 10, Batch 72, Test Loss: 0.5485660433769226\n",
      "Epoch 10, Batch 73, Test Loss: 0.5383114814758301\n",
      "Epoch 10, Batch 74, Test Loss: 0.5320459604263306\n",
      "Epoch 10, Batch 75, Test Loss: 0.28715434670448303\n",
      "Epoch 10, Batch 76, Test Loss: 0.553205668926239\n",
      "Epoch 10, Batch 77, Test Loss: 0.42819899320602417\n",
      "Epoch 10, Batch 78, Test Loss: 0.4605904221534729\n",
      "Epoch 10, Batch 79, Test Loss: 0.43958580493927\n",
      "Epoch 10, Batch 80, Test Loss: 0.6722953915596008\n",
      "Epoch 10, Batch 81, Test Loss: 0.6043837666511536\n",
      "Epoch 10, Batch 82, Test Loss: 0.32282954454421997\n",
      "Epoch 10, Batch 83, Test Loss: 0.6403989791870117\n",
      "Epoch 10, Batch 84, Test Loss: 0.43461373448371887\n",
      "Epoch 10, Batch 85, Test Loss: 0.6027646660804749\n",
      "Epoch 10, Batch 86, Test Loss: 0.4725475013256073\n",
      "Epoch 10, Batch 87, Test Loss: 0.6830654144287109\n",
      "Epoch 10, Batch 88, Test Loss: 0.508949875831604\n",
      "Epoch 10, Batch 89, Test Loss: 0.4462140202522278\n",
      "Epoch 10, Batch 90, Test Loss: 0.6016510725021362\n",
      "Epoch 10, Batch 91, Test Loss: 0.7046175599098206\n",
      "Epoch 10, Batch 92, Test Loss: 0.5815320014953613\n",
      "Epoch 10, Batch 93, Test Loss: 0.5279029607772827\n",
      "Epoch 10, Batch 94, Test Loss: 0.3553340435028076\n",
      "Epoch 10, Batch 95, Test Loss: 0.4953542649745941\n",
      "Epoch 10, Batch 96, Test Loss: 0.47962039709091187\n",
      "Epoch 10, Batch 97, Test Loss: 0.6374548077583313\n",
      "Epoch 10, Batch 98, Test Loss: 0.6601731777191162\n",
      "Epoch 10, Batch 99, Test Loss: 0.3611365556716919\n",
      "Epoch 10, Batch 100, Test Loss: 0.477294385433197\n",
      "Epoch 10, Batch 101, Test Loss: 0.4692549407482147\n",
      "Epoch 10, Batch 102, Test Loss: 0.4377390742301941\n",
      "Epoch 10, Batch 103, Test Loss: 0.5243575572967529\n",
      "Epoch 10, Batch 104, Test Loss: 0.6202824711799622\n",
      "Epoch 10, Batch 105, Test Loss: 0.4817899167537689\n",
      "Epoch 10, Batch 106, Test Loss: 0.5816507339477539\n",
      "Epoch 10, Batch 107, Test Loss: 0.4452900290489197\n",
      "Epoch 10, Batch 108, Test Loss: 0.6951322555541992\n",
      "Epoch 10, Batch 109, Test Loss: 0.4514888525009155\n",
      "Epoch 10, Batch 110, Test Loss: 0.47914230823516846\n",
      "Epoch 10, Batch 111, Test Loss: 0.49148324131965637\n",
      "Epoch 10, Batch 112, Test Loss: 0.2962840795516968\n",
      "Epoch 10, Batch 113, Test Loss: 0.46594804525375366\n",
      "Epoch 10, Batch 114, Test Loss: 0.651962161064148\n",
      "Epoch 10, Batch 115, Test Loss: 0.6298043131828308\n",
      "Epoch 10, Batch 116, Test Loss: 0.5404994487762451\n",
      "Epoch 10, Batch 117, Test Loss: 0.43157848715782166\n",
      "Epoch 10, Batch 118, Test Loss: 0.4082106649875641\n",
      "Epoch 10, Batch 119, Test Loss: 0.31254637241363525\n",
      "Epoch 10, Batch 120, Test Loss: 0.5092282891273499\n",
      "Epoch 10, Batch 121, Test Loss: 0.57375168800354\n",
      "Epoch 10, Batch 122, Test Loss: 0.48848849534988403\n",
      "Epoch 10, Batch 123, Test Loss: 0.3713558316230774\n",
      "Epoch 10, Batch 124, Test Loss: 0.5002884268760681\n",
      "Epoch 10, Batch 125, Test Loss: 0.6373056173324585\n",
      "Epoch 10, Batch 126, Test Loss: 0.5810778141021729\n",
      "Epoch 10, Batch 127, Test Loss: 0.5496050119400024\n",
      "Epoch 10, Batch 128, Test Loss: 0.558007001876831\n",
      "Epoch 10, Batch 129, Test Loss: 0.6168974041938782\n",
      "Epoch 10, Batch 130, Test Loss: 0.3208235800266266\n",
      "Epoch 10, Batch 131, Test Loss: 0.7421548962593079\n",
      "Epoch 10, Batch 132, Test Loss: 0.4949384033679962\n",
      "Epoch 10, Batch 133, Test Loss: 0.43881529569625854\n",
      "Epoch 10, Batch 134, Test Loss: 0.498184472322464\n",
      "Epoch 10, Batch 135, Test Loss: 0.7668852806091309\n",
      "Epoch 10, Batch 136, Test Loss: 0.49296507239341736\n",
      "Epoch 10, Batch 137, Test Loss: 0.6935316324234009\n",
      "Epoch 10, Batch 138, Test Loss: 0.5175151824951172\n",
      "Epoch 10, Batch 139, Test Loss: 0.3743523359298706\n",
      "Epoch 10, Batch 140, Test Loss: 0.5998450517654419\n",
      "Epoch 10, Batch 141, Test Loss: 0.4417983591556549\n",
      "Epoch 10, Batch 142, Test Loss: 0.4620569944381714\n",
      "Epoch 10, Batch 143, Test Loss: 0.42126742005348206\n",
      "Epoch 10, Batch 144, Test Loss: 0.5566945672035217\n",
      "Epoch 10, Batch 145, Test Loss: 0.43840715289115906\n",
      "Epoch 10, Batch 146, Test Loss: 0.4417722821235657\n",
      "Epoch 10, Batch 147, Test Loss: 0.49431028962135315\n",
      "Epoch 10, Batch 148, Test Loss: 0.5832068920135498\n",
      "Epoch 10, Batch 149, Test Loss: 0.45990049839019775\n",
      "Epoch 10, Batch 150, Test Loss: 0.4984511733055115\n",
      "Epoch 10, Batch 151, Test Loss: 0.4363859295845032\n",
      "Epoch 10, Batch 152, Test Loss: 0.5138899087905884\n",
      "Epoch 10, Batch 153, Test Loss: 0.5304768085479736\n",
      "Epoch 10, Batch 154, Test Loss: 0.48309895396232605\n",
      "Epoch 10, Batch 155, Test Loss: 0.5876153111457825\n",
      "Epoch 10, Batch 156, Test Loss: 0.36838170886039734\n",
      "Epoch 10, Batch 157, Test Loss: 0.4786038398742676\n",
      "Epoch 10, Batch 158, Test Loss: 0.5149795413017273\n",
      "Epoch 10, Batch 159, Test Loss: 0.6421942710876465\n",
      "Epoch 10, Batch 160, Test Loss: 0.7828031182289124\n",
      "Epoch 10, Batch 161, Test Loss: 0.5537904500961304\n",
      "Epoch 10, Batch 162, Test Loss: 0.3442482650279999\n",
      "Epoch 10, Batch 163, Test Loss: 0.47271281480789185\n",
      "Epoch 10, Batch 164, Test Loss: 0.47936564683914185\n",
      "Epoch 10, Batch 165, Test Loss: 0.43007728457450867\n",
      "Epoch 10, Batch 166, Test Loss: 0.47386887669563293\n",
      "Epoch 10, Batch 167, Test Loss: 0.7746772766113281\n",
      "Epoch 10, Batch 168, Test Loss: 0.23545005917549133\n",
      "Epoch 10, Batch 169, Test Loss: 0.7646980881690979\n",
      "Epoch 10, Batch 170, Test Loss: 0.5130147337913513\n",
      "Epoch 10, Batch 171, Test Loss: 0.5520328283309937\n",
      "Epoch 10, Batch 172, Test Loss: 0.3873777389526367\n",
      "Epoch 10, Batch 173, Test Loss: 0.46840623021125793\n",
      "Epoch 10, Batch 174, Test Loss: 0.6221929788589478\n",
      "Epoch 10, Batch 175, Test Loss: 0.5430312752723694\n",
      "Epoch 10, Batch 176, Test Loss: 0.4568455219268799\n",
      "Epoch 10, Batch 177, Test Loss: 0.43739253282546997\n",
      "Epoch 10, Batch 178, Test Loss: 0.5954533219337463\n",
      "Epoch 10, Batch 179, Test Loss: 0.38271793723106384\n",
      "Epoch 10, Batch 180, Test Loss: 0.38701263070106506\n",
      "Epoch 10, Batch 181, Test Loss: 0.42580169439315796\n",
      "Epoch 10, Batch 182, Test Loss: 0.2861081659793854\n",
      "Epoch 10, Batch 183, Test Loss: 0.3000625967979431\n",
      "Epoch 10, Batch 184, Test Loss: 0.506275475025177\n",
      "Epoch 10, Batch 185, Test Loss: 0.5014035105705261\n",
      "Epoch 10, Batch 186, Test Loss: 0.5956470370292664\n",
      "Epoch 10, Batch 187, Test Loss: 0.5701286196708679\n",
      "Epoch 10, Batch 188, Test Loss: 0.4865283966064453\n",
      "Epoch 10, Batch 189, Test Loss: 0.6379555463790894\n",
      "Epoch 10, Batch 190, Test Loss: 0.3794121742248535\n",
      "Epoch 10, Batch 191, Test Loss: 0.3214573562145233\n",
      "Epoch 10, Batch 192, Test Loss: 0.30575108528137207\n",
      "Epoch 10, Batch 193, Test Loss: 0.6088056564331055\n",
      "Epoch 10, Batch 194, Test Loss: 0.4048313498497009\n",
      "Epoch 10, Batch 195, Test Loss: 0.5373122096061707\n",
      "Epoch 10, Batch 196, Test Loss: 0.46658027172088623\n",
      "Epoch 10, Batch 197, Test Loss: 0.3543678820133209\n",
      "Epoch 10, Batch 198, Test Loss: 0.5865401029586792\n",
      "Epoch 10, Batch 199, Test Loss: 0.28663888573646545\n",
      "Epoch 10, Batch 200, Test Loss: 0.3647940158843994\n",
      "Epoch 10, Batch 201, Test Loss: 0.3989536166191101\n",
      "Epoch 10, Batch 202, Test Loss: 0.45006245374679565\n",
      "Epoch 10, Batch 203, Test Loss: 0.9172239303588867\n",
      "Epoch 10, Batch 204, Test Loss: 0.5291422605514526\n",
      "Epoch 10, Batch 205, Test Loss: 0.5301359295845032\n",
      "Epoch 10, Batch 206, Test Loss: 0.6295005679130554\n",
      "Epoch 10, Batch 207, Test Loss: 0.4097469449043274\n",
      "Epoch 10, Batch 208, Test Loss: 0.5745607018470764\n",
      "Epoch 10, Batch 209, Test Loss: 0.6003856658935547\n",
      "Epoch 10, Batch 210, Test Loss: 0.3505234718322754\n",
      "Epoch 10, Batch 211, Test Loss: 0.4421156644821167\n",
      "Epoch 10, Batch 212, Test Loss: 0.6766838431358337\n",
      "Epoch 10, Batch 213, Test Loss: 0.3587093949317932\n",
      "Epoch 10, Batch 214, Test Loss: 0.4945535659790039\n",
      "Epoch 10, Batch 215, Test Loss: 0.4165734648704529\n",
      "Epoch 10, Batch 216, Test Loss: 0.5360572934150696\n",
      "Epoch 10, Batch 217, Test Loss: 0.319521427154541\n",
      "Epoch 10, Batch 218, Test Loss: 0.7089190483093262\n",
      "Epoch 10, Batch 219, Test Loss: 0.5168861150741577\n",
      "Epoch 10, Batch 220, Test Loss: 0.42562222480773926\n",
      "Epoch 10, Batch 221, Test Loss: 0.4009464979171753\n",
      "Epoch 10, Batch 222, Test Loss: 0.594241201877594\n",
      "Epoch 10, Batch 223, Test Loss: 0.5149489045143127\n",
      "Epoch 10, Batch 224, Test Loss: 0.5201737880706787\n",
      "Epoch 10, Batch 225, Test Loss: 0.45791947841644287\n",
      "Epoch 10, Batch 226, Test Loss: 0.4719838798046112\n",
      "Epoch 10, Batch 227, Test Loss: 0.40583091974258423\n",
      "Epoch 10, Batch 228, Test Loss: 0.586133599281311\n",
      "Epoch 10, Batch 229, Test Loss: 0.4208082854747772\n",
      "Epoch 10, Batch 230, Test Loss: 0.4407269358634949\n",
      "Epoch 10, Batch 231, Test Loss: 0.3309713900089264\n",
      "Epoch 10, Batch 232, Test Loss: 0.3297255337238312\n",
      "Epoch 10, Batch 233, Test Loss: 0.7204578518867493\n",
      "Epoch 10, Batch 234, Test Loss: 0.38184842467308044\n",
      "Epoch 10, Batch 235, Test Loss: 0.5770686864852905\n",
      "Epoch 10, Batch 236, Test Loss: 0.544827401638031\n",
      "Epoch 10, Batch 237, Test Loss: 0.4982191324234009\n",
      "Epoch 10, Batch 238, Test Loss: 0.42170682549476624\n",
      "Epoch 10, Batch 239, Test Loss: 0.5531001687049866\n",
      "Epoch 10, Batch 240, Test Loss: 0.5984771251678467\n",
      "Epoch 10, Batch 241, Test Loss: 0.5076302289962769\n",
      "Epoch 10, Batch 242, Test Loss: 0.4562346339225769\n",
      "Epoch 10, Batch 243, Test Loss: 0.45925673842430115\n",
      "Epoch 10, Batch 244, Test Loss: 0.6863278150558472\n",
      "Epoch 10, Batch 245, Test Loss: 0.3885883688926697\n",
      "Epoch 10, Batch 246, Test Loss: 0.5126164555549622\n",
      "Epoch 10, Batch 247, Test Loss: 0.5676326751708984\n",
      "Epoch 10, Batch 248, Test Loss: 0.516136884689331\n",
      "Epoch 10, Batch 249, Test Loss: 0.5286253690719604\n",
      "Epoch 10, Batch 250, Test Loss: 0.6411130428314209\n",
      "Epoch 10, Batch 251, Test Loss: 0.5343034267425537\n",
      "Epoch 10, Batch 252, Test Loss: 0.5361162424087524\n",
      "Epoch 10, Batch 253, Test Loss: 0.5193150639533997\n",
      "Epoch 10, Batch 254, Test Loss: 0.4287082254886627\n",
      "Epoch 10, Batch 255, Test Loss: 0.4747503399848938\n",
      "Epoch 10, Batch 256, Test Loss: 0.35918954014778137\n",
      "Epoch 10, Batch 257, Test Loss: 0.44291430711746216\n",
      "Epoch 10, Batch 258, Test Loss: 0.49270516633987427\n",
      "Epoch 10, Batch 259, Test Loss: 0.5538452863693237\n",
      "Epoch 10, Batch 260, Test Loss: 0.5438025593757629\n",
      "Epoch 10, Batch 261, Test Loss: 0.5284116268157959\n",
      "Epoch 10, Batch 262, Test Loss: 0.8585902452468872\n",
      "Epoch 10, Batch 263, Test Loss: 0.5203563570976257\n",
      "Epoch 10, Batch 264, Test Loss: 0.6181825995445251\n",
      "Epoch 10, Batch 265, Test Loss: 0.5091778039932251\n",
      "Epoch 10, Batch 266, Test Loss: 0.3768197298049927\n",
      "Epoch 10, Batch 267, Test Loss: 0.36138463020324707\n",
      "Epoch 10, Batch 268, Test Loss: 0.48970529437065125\n",
      "Epoch 10, Batch 269, Test Loss: 0.603532075881958\n",
      "Epoch 10, Batch 270, Test Loss: 0.3902900815010071\n",
      "Epoch 10, Batch 271, Test Loss: 0.6687392592430115\n",
      "Epoch 10, Batch 272, Test Loss: 0.6527274250984192\n",
      "Epoch 10, Batch 273, Test Loss: 0.5022884607315063\n",
      "Epoch 10, Batch 274, Test Loss: 0.49202027916908264\n",
      "Epoch 10, Batch 275, Test Loss: 0.4858928918838501\n",
      "Epoch 10, Batch 276, Test Loss: 0.521528422832489\n",
      "Epoch 10, Batch 277, Test Loss: 0.6360121965408325\n",
      "Epoch 10, Batch 278, Test Loss: 0.5914602875709534\n",
      "Epoch 10, Batch 279, Test Loss: 0.50505131483078\n",
      "Epoch 10, Batch 280, Test Loss: 0.5234242081642151\n",
      "Epoch 10, Batch 281, Test Loss: 0.530510425567627\n",
      "Epoch 10, Batch 282, Test Loss: 0.3572375476360321\n",
      "Epoch 10, Batch 283, Test Loss: 0.46358349919319153\n",
      "Epoch 10, Batch 284, Test Loss: 0.44614434242248535\n",
      "Epoch 10, Batch 285, Test Loss: 0.45509466528892517\n",
      "Epoch 10, Batch 286, Test Loss: 0.593935489654541\n",
      "Epoch 10, Batch 287, Test Loss: 0.36301448941230774\n",
      "Epoch 10, Batch 288, Test Loss: 0.7112205028533936\n",
      "Epoch 10, Batch 289, Test Loss: 0.560391366481781\n",
      "Epoch 10, Batch 290, Test Loss: 0.3398273289203644\n",
      "Epoch 10, Batch 291, Test Loss: 0.5929741859436035\n",
      "Epoch 10, Batch 292, Test Loss: 0.505386471748352\n",
      "Epoch 10, Batch 293, Test Loss: 0.5824716091156006\n",
      "Epoch 10, Batch 294, Test Loss: 0.3972280025482178\n",
      "Epoch 10, Batch 295, Test Loss: 0.5597079992294312\n",
      "Epoch 10, Batch 296, Test Loss: 0.5299355983734131\n",
      "Epoch 10, Batch 297, Test Loss: 0.5363724827766418\n",
      "Epoch 10, Batch 298, Test Loss: 0.6664472818374634\n",
      "Epoch 10, Batch 299, Test Loss: 0.4822680354118347\n",
      "Epoch 10, Batch 300, Test Loss: 0.4718371033668518\n",
      "Epoch 10, Batch 301, Test Loss: 0.4391797184944153\n",
      "Epoch 10, Batch 302, Test Loss: 0.5621548891067505\n",
      "Epoch 10, Batch 303, Test Loss: 0.3685736060142517\n",
      "Epoch 10, Batch 304, Test Loss: 0.432420015335083\n",
      "Epoch 10, Batch 305, Test Loss: 0.7176376581192017\n",
      "Epoch 10, Batch 306, Test Loss: 0.7676312923431396\n",
      "Epoch 10, Batch 307, Test Loss: 0.42762207984924316\n",
      "Epoch 10, Batch 308, Test Loss: 0.6166689991950989\n",
      "Epoch 10, Batch 309, Test Loss: 0.6040167808532715\n",
      "Epoch 10, Batch 310, Test Loss: 0.5360614061355591\n",
      "Epoch 10, Batch 311, Test Loss: 0.28483712673187256\n",
      "Epoch 10, Batch 312, Test Loss: 0.4182444214820862\n",
      "Epoch 10, Batch 313, Test Loss: 0.6311808824539185\n",
      "Epoch 10, Batch 314, Test Loss: 0.568528950214386\n",
      "Epoch 10, Batch 315, Test Loss: 0.3654301166534424\n",
      "Epoch 10, Batch 316, Test Loss: 0.49940982460975647\n",
      "Epoch 10, Batch 317, Test Loss: 0.38396114110946655\n",
      "Epoch 10, Batch 318, Test Loss: 0.3897910416126251\n",
      "Epoch 10, Batch 319, Test Loss: 0.48311513662338257\n",
      "Epoch 10, Batch 320, Test Loss: 0.30077698826789856\n",
      "Epoch 10, Batch 321, Test Loss: 0.5196640491485596\n",
      "Epoch 10, Batch 322, Test Loss: 0.6847078800201416\n",
      "Epoch 10, Batch 323, Test Loss: 0.6141781210899353\n",
      "Epoch 10, Batch 324, Test Loss: 0.4908822178840637\n",
      "Epoch 10, Batch 325, Test Loss: 0.664072573184967\n",
      "Epoch 10, Batch 326, Test Loss: 0.45186546444892883\n",
      "Epoch 10, Batch 327, Test Loss: 0.341291606426239\n",
      "Epoch 10, Batch 328, Test Loss: 0.3716178834438324\n",
      "Epoch 10, Batch 329, Test Loss: 0.424716055393219\n",
      "Epoch 10, Batch 330, Test Loss: 0.43489760160446167\n",
      "Epoch 10, Batch 331, Test Loss: 0.493112713098526\n",
      "Epoch 10, Batch 332, Test Loss: 0.5944358706474304\n",
      "Epoch 10, Batch 333, Test Loss: 0.5186874270439148\n",
      "Epoch 10, Batch 334, Test Loss: 0.7143085598945618\n",
      "Epoch 10, Batch 335, Test Loss: 0.5290679931640625\n",
      "Epoch 10, Batch 336, Test Loss: 0.45381659269332886\n",
      "Epoch 10, Batch 337, Test Loss: 0.5897963047027588\n",
      "Epoch 10, Batch 338, Test Loss: 0.5676107406616211\n",
      "Epoch 10, Batch 339, Test Loss: 0.3696177899837494\n",
      "Epoch 10, Batch 340, Test Loss: 0.6401115655899048\n",
      "Epoch 10, Batch 341, Test Loss: 0.40563467144966125\n",
      "Epoch 10, Batch 342, Test Loss: 0.6195600628852844\n",
      "Epoch 10, Batch 343, Test Loss: 0.4950844645500183\n",
      "Epoch 10, Batch 344, Test Loss: 0.3772987723350525\n",
      "Epoch 10, Batch 345, Test Loss: 0.5068488717079163\n",
      "Epoch 10, Batch 346, Test Loss: 0.4250973165035248\n",
      "Epoch 10, Batch 347, Test Loss: 0.6209894418716431\n",
      "Epoch 10, Batch 348, Test Loss: 0.5910727977752686\n",
      "Epoch 10, Batch 349, Test Loss: 0.574784517288208\n",
      "Epoch 10, Batch 350, Test Loss: 0.7657229900360107\n",
      "Epoch 10, Batch 351, Test Loss: 0.38522475957870483\n",
      "Epoch 10, Batch 352, Test Loss: 0.5985891819000244\n",
      "Epoch 10, Batch 353, Test Loss: 0.521763026714325\n",
      "Epoch 10, Batch 354, Test Loss: 0.5315549969673157\n",
      "Epoch 10, Batch 355, Test Loss: 0.599858283996582\n",
      "Epoch 10, Batch 356, Test Loss: 0.40487784147262573\n",
      "Epoch 10, Batch 357, Test Loss: 0.3972119092941284\n",
      "Epoch 10, Batch 358, Test Loss: 0.7374060750007629\n",
      "Epoch 10, Batch 359, Test Loss: 0.4690858721733093\n",
      "Epoch 10, Batch 360, Test Loss: 0.6122428178787231\n",
      "Epoch 10, Batch 361, Test Loss: 0.5134925842285156\n",
      "Epoch 10, Batch 362, Test Loss: 0.621517539024353\n",
      "Epoch 10, Batch 363, Test Loss: 0.5038396716117859\n",
      "Epoch 10, Batch 364, Test Loss: 0.5216574668884277\n",
      "Epoch 10, Batch 365, Test Loss: 0.6726738214492798\n",
      "Epoch 10, Batch 366, Test Loss: 0.3976086974143982\n",
      "Epoch 10, Batch 367, Test Loss: 0.47516533732414246\n",
      "Epoch 10, Batch 368, Test Loss: 0.35369929671287537\n",
      "Epoch 10, Batch 369, Test Loss: 0.6392441987991333\n",
      "Epoch 10, Batch 370, Test Loss: 0.3044230043888092\n",
      "Epoch 10, Batch 371, Test Loss: 0.5698365569114685\n",
      "Epoch 10, Batch 372, Test Loss: 0.47072461247444153\n",
      "Epoch 10, Batch 373, Test Loss: 0.5040819048881531\n",
      "Epoch 10, Batch 374, Test Loss: 0.2563868463039398\n",
      "Epoch 10, Batch 375, Test Loss: 0.41747602820396423\n",
      "Epoch 10, Batch 376, Test Loss: 0.6610127687454224\n",
      "Epoch 10, Batch 377, Test Loss: 0.6292338371276855\n",
      "Epoch 10, Batch 378, Test Loss: 0.4140086770057678\n",
      "Epoch 10, Batch 379, Test Loss: 0.5477095246315002\n",
      "Epoch 10, Batch 380, Test Loss: 0.5488085746765137\n",
      "Epoch 10, Batch 381, Test Loss: 0.41575947403907776\n",
      "Epoch 10, Batch 382, Test Loss: 0.5857526063919067\n",
      "Epoch 10, Batch 383, Test Loss: 0.4018869698047638\n",
      "Epoch 10, Batch 384, Test Loss: 0.5990384817123413\n",
      "Epoch 10, Batch 385, Test Loss: 0.555472731590271\n",
      "Epoch 10, Batch 386, Test Loss: 0.533680260181427\n",
      "Epoch 10, Batch 387, Test Loss: 0.27784547209739685\n",
      "Epoch 10, Batch 388, Test Loss: 0.4738110899925232\n",
      "Epoch 10, Batch 389, Test Loss: 0.37149080634117126\n",
      "Epoch 10, Batch 390, Test Loss: 0.7463486790657043\n",
      "Epoch 10, Batch 391, Test Loss: 0.5311416387557983\n",
      "Epoch 10, Batch 392, Test Loss: 0.623190701007843\n",
      "Epoch 10, Batch 393, Test Loss: 0.3536122441291809\n",
      "Epoch 10, Batch 394, Test Loss: 0.5115277171134949\n",
      "Epoch 10, Batch 395, Test Loss: 0.6255797147750854\n",
      "Epoch 10, Batch 396, Test Loss: 0.48952752351760864\n",
      "Epoch 10, Batch 397, Test Loss: 0.5317829847335815\n",
      "Epoch 10, Batch 398, Test Loss: 0.40523335337638855\n",
      "Epoch 10, Batch 399, Test Loss: 0.5713276267051697\n",
      "Epoch 10, Batch 400, Test Loss: 0.4197467565536499\n",
      "Epoch 10, Batch 401, Test Loss: 0.6078041195869446\n",
      "Epoch 10, Batch 402, Test Loss: 0.3826996982097626\n",
      "Epoch 10, Batch 403, Test Loss: 0.5575909614562988\n",
      "Epoch 10, Batch 404, Test Loss: 0.6532046794891357\n",
      "Epoch 10, Batch 405, Test Loss: 0.3261997103691101\n",
      "Epoch 10, Batch 406, Test Loss: 0.5915625095367432\n",
      "Epoch 10, Batch 407, Test Loss: 0.5100833773612976\n",
      "Epoch 10, Batch 408, Test Loss: 0.41159623861312866\n",
      "Epoch 10, Batch 409, Test Loss: 0.5648900866508484\n",
      "Epoch 10, Batch 410, Test Loss: 0.4175078570842743\n",
      "Epoch 10, Batch 411, Test Loss: 0.5295723676681519\n",
      "Epoch 10, Batch 412, Test Loss: 0.48658809065818787\n",
      "Epoch 10, Batch 413, Test Loss: 0.36075207591056824\n",
      "Epoch 10, Batch 414, Test Loss: 0.5010219216346741\n",
      "Epoch 10, Batch 415, Test Loss: 0.5251234173774719\n",
      "Epoch 10, Batch 416, Test Loss: 0.7911199331283569\n",
      "Epoch 10, Batch 417, Test Loss: 0.4710407257080078\n",
      "Epoch 10, Batch 418, Test Loss: 0.5920693278312683\n",
      "Epoch 10, Batch 419, Test Loss: 0.40227407217025757\n",
      "Epoch 10, Batch 420, Test Loss: 0.5044822096824646\n",
      "Epoch 10, Batch 421, Test Loss: 0.4231097400188446\n",
      "Epoch 10, Batch 422, Test Loss: 0.5930160880088806\n",
      "Epoch 10, Batch 423, Test Loss: 0.5121396780014038\n",
      "Epoch 10, Batch 424, Test Loss: 0.40881457924842834\n",
      "Epoch 10, Batch 425, Test Loss: 0.3891827464103699\n",
      "Epoch 10, Batch 426, Test Loss: 0.4604918360710144\n",
      "Epoch 10, Batch 427, Test Loss: 0.6100730299949646\n",
      "Epoch 10, Batch 428, Test Loss: 0.29887136816978455\n",
      "Epoch 10, Batch 429, Test Loss: 0.44406187534332275\n",
      "Epoch 10, Batch 430, Test Loss: 0.6379053592681885\n",
      "Epoch 10, Batch 431, Test Loss: 0.5417959094047546\n",
      "Epoch 10, Batch 432, Test Loss: 0.725672721862793\n",
      "Epoch 10, Batch 433, Test Loss: 0.3712598979473114\n",
      "Epoch 10, Batch 434, Test Loss: 0.43289417028427124\n",
      "Epoch 10, Batch 435, Test Loss: 0.4459998607635498\n",
      "Epoch 10, Batch 436, Test Loss: 0.566210150718689\n",
      "Epoch 10, Batch 437, Test Loss: 0.4814743995666504\n",
      "Epoch 10, Batch 438, Test Loss: 0.5986144542694092\n",
      "Epoch 10, Batch 439, Test Loss: 0.5620896816253662\n",
      "Epoch 10, Batch 440, Test Loss: 0.563493013381958\n",
      "Epoch 10, Batch 441, Test Loss: 0.5296914577484131\n",
      "Epoch 10, Batch 442, Test Loss: 0.6004679203033447\n",
      "Epoch 10, Batch 443, Test Loss: 0.5445454120635986\n",
      "Epoch 10, Batch 444, Test Loss: 0.5867663025856018\n",
      "Epoch 10, Batch 445, Test Loss: 0.46486401557922363\n",
      "Epoch 10, Batch 446, Test Loss: 0.39023566246032715\n",
      "Epoch 10, Batch 447, Test Loss: 0.3543429672718048\n",
      "Epoch 10, Batch 448, Test Loss: 0.37966516613960266\n",
      "Epoch 10, Batch 449, Test Loss: 0.6352789402008057\n",
      "Epoch 10, Batch 450, Test Loss: 0.47579604387283325\n",
      "Epoch 10, Batch 451, Test Loss: 0.43668875098228455\n",
      "Epoch 10, Batch 452, Test Loss: 0.4557020962238312\n",
      "Epoch 10, Batch 453, Test Loss: 0.6604932546615601\n",
      "Epoch 10, Batch 454, Test Loss: 0.4812440872192383\n",
      "Epoch 10, Batch 455, Test Loss: 0.4269507825374603\n",
      "Epoch 10, Batch 456, Test Loss: 0.3524854779243469\n",
      "Epoch 10, Batch 457, Test Loss: 0.4049834907054901\n",
      "Epoch 10, Batch 458, Test Loss: 0.4354811906814575\n",
      "Epoch 10, Batch 459, Test Loss: 0.7113078832626343\n",
      "Epoch 10, Batch 460, Test Loss: 0.6279618144035339\n",
      "Epoch 10, Batch 461, Test Loss: 0.5476713180541992\n",
      "Epoch 10, Batch 462, Test Loss: 0.3340994715690613\n",
      "Epoch 10, Batch 463, Test Loss: 0.4925766885280609\n",
      "Epoch 10, Batch 464, Test Loss: 0.35099899768829346\n",
      "Epoch 10, Batch 465, Test Loss: 0.43314284086227417\n",
      "Epoch 10, Batch 466, Test Loss: 0.7292505502700806\n",
      "Epoch 10, Batch 467, Test Loss: 0.5437648892402649\n",
      "Epoch 10, Batch 468, Test Loss: 0.4371160566806793\n",
      "Epoch 10, Batch 469, Test Loss: 0.6200111508369446\n",
      "Epoch 10, Batch 470, Test Loss: 0.48262694478034973\n",
      "Epoch 10, Batch 471, Test Loss: 0.4873453378677368\n",
      "Epoch 10, Batch 472, Test Loss: 0.5617680549621582\n",
      "Epoch 10, Batch 473, Test Loss: 0.46902763843536377\n",
      "Epoch 10, Batch 474, Test Loss: 0.3881048858165741\n",
      "Epoch 10, Batch 475, Test Loss: 0.38185974955558777\n",
      "Epoch 10, Batch 476, Test Loss: 0.35414430499076843\n",
      "Epoch 10, Batch 477, Test Loss: 0.32933297753334045\n",
      "Epoch 10, Batch 478, Test Loss: 0.5816143751144409\n",
      "Epoch 10, Batch 479, Test Loss: 0.38365375995635986\n",
      "Epoch 10, Batch 480, Test Loss: 0.6049017310142517\n",
      "Epoch 10, Batch 481, Test Loss: 0.5815176963806152\n",
      "Epoch 10, Batch 482, Test Loss: 0.46189358830451965\n",
      "Epoch 10, Batch 483, Test Loss: 0.5880999565124512\n",
      "Epoch 10, Batch 484, Test Loss: 0.38665059208869934\n",
      "Epoch 10, Batch 485, Test Loss: 0.5172842741012573\n",
      "Epoch 10, Batch 486, Test Loss: 0.39634063839912415\n",
      "Epoch 10, Batch 487, Test Loss: 0.47880178689956665\n",
      "Epoch 10, Batch 488, Test Loss: 0.6252508759498596\n",
      "Epoch 10, Batch 489, Test Loss: 0.6553670167922974\n",
      "Epoch 10, Batch 490, Test Loss: 0.6321887969970703\n",
      "Epoch 10, Batch 491, Test Loss: 0.3762081563472748\n",
      "Epoch 10, Batch 492, Test Loss: 0.6630793809890747\n",
      "Epoch 10, Batch 493, Test Loss: 0.48015815019607544\n",
      "Epoch 10, Batch 494, Test Loss: 0.6413465142250061\n",
      "Epoch 10, Batch 495, Test Loss: 0.46791204810142517\n",
      "Epoch 10, Batch 496, Test Loss: 0.44348907470703125\n",
      "Epoch 10, Batch 497, Test Loss: 0.4551112949848175\n",
      "Epoch 10, Batch 498, Test Loss: 0.5710368752479553\n",
      "Epoch 10, Batch 499, Test Loss: 0.4530138373374939\n",
      "Epoch 10, Batch 500, Test Loss: 0.47039270401000977\n",
      "Epoch 10, Batch 501, Test Loss: 0.6432029008865356\n",
      "Epoch 10, Batch 502, Test Loss: 0.35063183307647705\n",
      "Epoch 10, Batch 503, Test Loss: 0.7632570266723633\n",
      "Epoch 10, Batch 504, Test Loss: 0.4185071289539337\n",
      "Epoch 10, Batch 505, Test Loss: 0.5251533389091492\n",
      "Epoch 10, Batch 506, Test Loss: 0.5149540305137634\n",
      "Epoch 10, Batch 507, Test Loss: 0.6065995097160339\n",
      "Epoch 10, Batch 508, Test Loss: 0.44582781195640564\n",
      "Epoch 10, Batch 509, Test Loss: 0.38058826327323914\n",
      "Epoch 10, Batch 510, Test Loss: 0.5735586285591125\n",
      "Epoch 10, Batch 511, Test Loss: 0.34791961312294006\n",
      "Epoch 10, Batch 512, Test Loss: 0.5140193700790405\n",
      "Epoch 10, Batch 513, Test Loss: 0.5500813126564026\n",
      "Epoch 10, Batch 514, Test Loss: 0.7281336784362793\n",
      "Epoch 10, Batch 515, Test Loss: 0.6860496401786804\n",
      "Epoch 10, Batch 516, Test Loss: 0.681644082069397\n",
      "Epoch 10, Batch 517, Test Loss: 0.4926954507827759\n",
      "Epoch 10, Batch 518, Test Loss: 0.36900073289871216\n",
      "Epoch 10, Batch 519, Test Loss: 0.5807819962501526\n",
      "Epoch 10, Batch 520, Test Loss: 0.47790348529815674\n",
      "Epoch 10, Batch 521, Test Loss: 0.27728110551834106\n",
      "Epoch 10, Batch 522, Test Loss: 0.5669103860855103\n",
      "Epoch 10, Batch 523, Test Loss: 0.5123226642608643\n",
      "Epoch 10, Batch 524, Test Loss: 0.4503138065338135\n",
      "Epoch 10, Batch 525, Test Loss: 0.5556941628456116\n",
      "Epoch 10, Batch 526, Test Loss: 0.5503228902816772\n",
      "Epoch 10, Batch 527, Test Loss: 0.41691020131111145\n",
      "Epoch 10, Batch 528, Test Loss: 0.47620469331741333\n",
      "Epoch 10, Batch 529, Test Loss: 0.423030287027359\n",
      "Epoch 10, Batch 530, Test Loss: 0.5605251789093018\n",
      "Epoch 10, Batch 531, Test Loss: 0.7330461740493774\n",
      "Epoch 10, Batch 532, Test Loss: 0.6368584036827087\n",
      "Epoch 10, Batch 533, Test Loss: 0.3790101408958435\n",
      "Epoch 10, Batch 534, Test Loss: 0.583770215511322\n",
      "Epoch 10, Batch 535, Test Loss: 0.5648790597915649\n",
      "Epoch 10, Batch 536, Test Loss: 0.713300883769989\n",
      "Epoch 10, Batch 537, Test Loss: 0.4518386423587799\n",
      "Epoch 10, Batch 538, Test Loss: 0.550025463104248\n",
      "Epoch 10, Batch 539, Test Loss: 0.3671127259731293\n",
      "Epoch 10, Batch 540, Test Loss: 0.6376485824584961\n",
      "Epoch 10, Batch 541, Test Loss: 0.5469120144844055\n",
      "Epoch 10, Batch 542, Test Loss: 0.5006530284881592\n",
      "Epoch 10, Batch 543, Test Loss: 0.5899355411529541\n",
      "Epoch 10, Batch 544, Test Loss: 0.382865846157074\n",
      "Epoch 10, Batch 545, Test Loss: 0.4678189754486084\n",
      "Epoch 10, Batch 546, Test Loss: 0.5395089387893677\n",
      "Epoch 10, Batch 547, Test Loss: 0.4617534279823303\n",
      "Epoch 10, Batch 548, Test Loss: 0.4857720732688904\n",
      "Epoch 10, Batch 549, Test Loss: 0.6335774064064026\n",
      "Epoch 10, Batch 550, Test Loss: 0.41064947843551636\n",
      "Epoch 10, Batch 551, Test Loss: 0.5223902463912964\n",
      "Epoch 10, Batch 552, Test Loss: 0.47384774684906006\n",
      "Epoch 10, Batch 553, Test Loss: 0.4885099232196808\n",
      "Epoch 10, Batch 554, Test Loss: 0.38159194588661194\n",
      "Epoch 10, Batch 555, Test Loss: 0.6486262083053589\n",
      "Epoch 10, Batch 556, Test Loss: 0.3104647397994995\n",
      "Epoch 10, Batch 557, Test Loss: 0.652839720249176\n",
      "Epoch 10, Batch 558, Test Loss: 0.5541374683380127\n",
      "Epoch 10, Batch 559, Test Loss: 0.41480186581611633\n",
      "Epoch 10, Batch 560, Test Loss: 0.5368010401725769\n",
      "Epoch 10, Batch 561, Test Loss: 0.5555387139320374\n",
      "Epoch 10, Batch 562, Test Loss: 0.5189703702926636\n",
      "Epoch 10, Batch 563, Test Loss: 0.40407612919807434\n",
      "Epoch 10, Batch 564, Test Loss: 0.5525214076042175\n",
      "Epoch 10, Batch 565, Test Loss: 0.6079604029655457\n",
      "Epoch 10, Batch 566, Test Loss: 0.48363280296325684\n",
      "Epoch 10, Batch 567, Test Loss: 0.4291011691093445\n",
      "Epoch 10, Batch 568, Test Loss: 0.48755306005477905\n",
      "Epoch 10, Batch 569, Test Loss: 0.4484427869319916\n",
      "Epoch 10, Batch 570, Test Loss: 0.5101271867752075\n",
      "Epoch 10, Batch 571, Test Loss: 0.6190305948257446\n",
      "Epoch 10, Batch 572, Test Loss: 0.4195665419101715\n",
      "Epoch 10, Batch 573, Test Loss: 0.3716847896575928\n",
      "Epoch 10, Batch 574, Test Loss: 0.349248468875885\n",
      "Epoch 10, Batch 575, Test Loss: 0.4733404815196991\n",
      "Epoch 10, Batch 576, Test Loss: 0.4231114983558655\n",
      "Epoch 10, Batch 577, Test Loss: 0.522185742855072\n",
      "Epoch 10, Batch 578, Test Loss: 0.7468898892402649\n",
      "Epoch 10, Batch 579, Test Loss: 0.5633863210678101\n",
      "Epoch 10, Batch 580, Test Loss: 0.7733514308929443\n",
      "Epoch 10, Batch 581, Test Loss: 0.5090065598487854\n",
      "Epoch 10, Batch 582, Test Loss: 0.3642019033432007\n",
      "Epoch 10, Batch 583, Test Loss: 0.6457788944244385\n",
      "Epoch 10, Batch 584, Test Loss: 0.5013337135314941\n",
      "Epoch 10, Batch 585, Test Loss: 0.3712243437767029\n",
      "Epoch 10, Batch 586, Test Loss: 0.3907132148742676\n",
      "Epoch 10, Batch 587, Test Loss: 0.48575717210769653\n",
      "Epoch 10, Batch 588, Test Loss: 0.7827256917953491\n",
      "Epoch 10, Batch 589, Test Loss: 0.4082755446434021\n",
      "Epoch 10, Batch 590, Test Loss: 0.5285418033599854\n",
      "Epoch 10, Batch 591, Test Loss: 0.5123938322067261\n",
      "Epoch 10, Batch 592, Test Loss: 0.3253628611564636\n",
      "Epoch 10, Batch 593, Test Loss: 0.47370755672454834\n",
      "Epoch 10, Batch 594, Test Loss: 0.5657511353492737\n",
      "Epoch 10, Batch 595, Test Loss: 0.5996406078338623\n",
      "Epoch 10, Batch 596, Test Loss: 0.5559780597686768\n",
      "Epoch 10, Batch 597, Test Loss: 0.36028242111206055\n",
      "Epoch 10, Batch 598, Test Loss: 0.5190838575363159\n",
      "Epoch 10, Batch 599, Test Loss: 0.6084280610084534\n",
      "Epoch 10, Batch 600, Test Loss: 0.5295695662498474\n",
      "Epoch 10, Batch 601, Test Loss: 0.5607114434242249\n",
      "Epoch 10, Batch 602, Test Loss: 0.4874350428581238\n",
      "Epoch 10, Batch 603, Test Loss: 0.47561049461364746\n",
      "Epoch 10, Batch 604, Test Loss: 0.43333831429481506\n",
      "Epoch 10, Batch 605, Test Loss: 0.45149263739585876\n",
      "Epoch 10, Batch 606, Test Loss: 0.5373910665512085\n",
      "Epoch 10, Batch 607, Test Loss: 0.5166116952896118\n",
      "Epoch 10, Batch 608, Test Loss: 0.6523913145065308\n",
      "Epoch 10, Batch 609, Test Loss: 0.5966699123382568\n",
      "Epoch 10, Batch 610, Test Loss: 0.5663623809814453\n",
      "Epoch 10, Batch 611, Test Loss: 0.6470548510551453\n",
      "Epoch 10, Batch 612, Test Loss: 0.5680974125862122\n",
      "Epoch 10, Batch 613, Test Loss: 0.4823048710823059\n",
      "Epoch 10, Batch 614, Test Loss: 0.334332138299942\n",
      "Epoch 10, Batch 615, Test Loss: 0.5881190299987793\n",
      "Epoch 10, Batch 616, Test Loss: 0.4925611913204193\n",
      "Epoch 10, Batch 617, Test Loss: 0.41712701320648193\n",
      "Epoch 10, Batch 618, Test Loss: 0.6815013289451599\n",
      "Epoch 10, Batch 619, Test Loss: 0.5691581964492798\n",
      "Epoch 10, Batch 620, Test Loss: 0.4245893061161041\n",
      "Epoch 10, Batch 621, Test Loss: 0.40136638283729553\n",
      "Epoch 10, Batch 622, Test Loss: 0.49125412106513977\n",
      "Epoch 10, Batch 623, Test Loss: 0.48796576261520386\n",
      "Epoch 10, Batch 624, Test Loss: 0.5314283967018127\n",
      "Epoch 10, Batch 625, Test Loss: 0.4941176176071167\n",
      "Epoch 10, Batch 626, Test Loss: 0.5085048079490662\n",
      "Epoch 10, Batch 627, Test Loss: 0.6040282845497131\n",
      "Epoch 10, Batch 628, Test Loss: 0.5876903533935547\n",
      "Epoch 10, Batch 629, Test Loss: 0.7261239290237427\n",
      "Epoch 10, Batch 630, Test Loss: 0.534467875957489\n",
      "Epoch 10, Batch 631, Test Loss: 0.5868910551071167\n",
      "Epoch 10, Batch 632, Test Loss: 0.6112610697746277\n",
      "Epoch 10, Batch 633, Test Loss: 0.4577343165874481\n",
      "Epoch 10, Batch 634, Test Loss: 0.49660488963127136\n",
      "Epoch 10, Batch 635, Test Loss: 0.6819328665733337\n",
      "Epoch 10, Batch 636, Test Loss: 0.6305701732635498\n",
      "Epoch 10, Batch 637, Test Loss: 0.3652353286743164\n",
      "Epoch 10, Batch 638, Test Loss: 0.49996066093444824\n",
      "Epoch 10, Batch 639, Test Loss: 0.37136879563331604\n",
      "Epoch 10, Batch 640, Test Loss: 0.5058284997940063\n",
      "Epoch 10, Batch 641, Test Loss: 0.47617924213409424\n",
      "Epoch 10, Batch 642, Test Loss: 0.6317874193191528\n",
      "Epoch 10, Batch 643, Test Loss: 0.4572925865650177\n",
      "Epoch 10, Batch 644, Test Loss: 0.462129145860672\n",
      "Epoch 10, Batch 645, Test Loss: 0.44070231914520264\n",
      "Epoch 10, Batch 646, Test Loss: 0.47642189264297485\n",
      "Epoch 10, Batch 647, Test Loss: 0.4360743463039398\n",
      "Epoch 10, Batch 648, Test Loss: 0.6324723362922668\n",
      "Epoch 10, Batch 649, Test Loss: 0.5102384090423584\n",
      "Epoch 10, Batch 650, Test Loss: 0.3795773386955261\n",
      "Epoch 10, Batch 651, Test Loss: 0.40115779638290405\n",
      "Epoch 10, Batch 652, Test Loss: 0.4907185435295105\n",
      "Epoch 10, Batch 653, Test Loss: 0.5156764388084412\n",
      "Epoch 10, Batch 654, Test Loss: 0.40909069776535034\n",
      "Epoch 10, Batch 655, Test Loss: 0.5731050372123718\n",
      "Epoch 10, Batch 656, Test Loss: 0.490256667137146\n",
      "Epoch 10, Batch 657, Test Loss: 0.40817734599113464\n",
      "Epoch 10, Batch 658, Test Loss: 0.3472692370414734\n",
      "Epoch 10, Batch 659, Test Loss: 0.6012187004089355\n",
      "Epoch 10, Batch 660, Test Loss: 0.566653311252594\n",
      "Epoch 10, Batch 661, Test Loss: 0.4745296239852905\n",
      "Epoch 10, Batch 662, Test Loss: 0.32632067799568176\n",
      "Epoch 10, Batch 663, Test Loss: 0.4885666072368622\n",
      "Epoch 10, Batch 664, Test Loss: 0.4328523278236389\n",
      "Epoch 10, Batch 665, Test Loss: 0.6002634763717651\n",
      "Epoch 10, Batch 666, Test Loss: 0.4676539897918701\n",
      "Epoch 10, Batch 667, Test Loss: 0.6317886710166931\n",
      "Epoch 10, Batch 668, Test Loss: 0.3497980535030365\n",
      "Epoch 10, Batch 669, Test Loss: 0.5033076405525208\n",
      "Epoch 10, Batch 670, Test Loss: 0.36177676916122437\n",
      "Epoch 10, Batch 671, Test Loss: 0.5454111695289612\n",
      "Epoch 10, Batch 672, Test Loss: 0.4886181652545929\n",
      "Epoch 10, Batch 673, Test Loss: 0.4101254343986511\n",
      "Epoch 10, Batch 674, Test Loss: 0.5108080506324768\n",
      "Epoch 10, Batch 675, Test Loss: 0.5773639678955078\n",
      "Epoch 10, Batch 676, Test Loss: 0.4724401533603668\n",
      "Epoch 10, Batch 677, Test Loss: 0.38436251878738403\n",
      "Epoch 10, Batch 678, Test Loss: 0.3178238272666931\n",
      "Epoch 10, Batch 679, Test Loss: 0.7099108695983887\n",
      "Epoch 10, Batch 680, Test Loss: 0.4407016634941101\n",
      "Epoch 10, Batch 681, Test Loss: 0.46893978118896484\n",
      "Epoch 10, Batch 682, Test Loss: 0.3028475344181061\n",
      "Epoch 10, Batch 683, Test Loss: 0.34035977721214294\n",
      "Epoch 10, Batch 684, Test Loss: 0.508263111114502\n",
      "Epoch 10, Batch 685, Test Loss: 0.38139474391937256\n",
      "Epoch 10, Batch 686, Test Loss: 0.6216902732849121\n",
      "Epoch 10, Batch 687, Test Loss: 0.7494033575057983\n",
      "Epoch 10, Batch 688, Test Loss: 0.42352432012557983\n",
      "Epoch 10, Batch 689, Test Loss: 0.4790710210800171\n",
      "Epoch 10, Batch 690, Test Loss: 0.6340232491493225\n",
      "Epoch 10, Batch 691, Test Loss: 0.3625643253326416\n",
      "Epoch 10, Batch 692, Test Loss: 0.7189807295799255\n",
      "Epoch 10, Batch 693, Test Loss: 0.3101889193058014\n",
      "Epoch 10, Batch 694, Test Loss: 0.5342077612876892\n",
      "Epoch 10, Batch 695, Test Loss: 0.40675801038742065\n",
      "Epoch 10, Batch 696, Test Loss: 0.5612152814865112\n",
      "Epoch 10, Batch 697, Test Loss: 0.42954176664352417\n",
      "Epoch 10, Batch 698, Test Loss: 0.4396705627441406\n",
      "Epoch 10, Batch 699, Test Loss: 0.5144545435905457\n",
      "Epoch 10, Batch 700, Test Loss: 0.4741496443748474\n",
      "Epoch 10, Batch 701, Test Loss: 0.48477137088775635\n",
      "Epoch 10, Batch 702, Test Loss: 0.5006304979324341\n",
      "Epoch 10, Batch 703, Test Loss: 0.3223664462566376\n",
      "Epoch 10, Batch 704, Test Loss: 0.5223485231399536\n",
      "Epoch 10, Batch 705, Test Loss: 0.3642604649066925\n",
      "Epoch 10, Batch 706, Test Loss: 0.5806399583816528\n",
      "Epoch 10, Batch 707, Test Loss: 0.4403218626976013\n",
      "Epoch 10, Batch 708, Test Loss: 0.4507669508457184\n",
      "Epoch 10, Batch 709, Test Loss: 0.4668002128601074\n",
      "Epoch 10, Batch 710, Test Loss: 0.46195489168167114\n",
      "Epoch 10, Batch 711, Test Loss: 0.5169239044189453\n",
      "Epoch 10, Batch 712, Test Loss: 0.43002843856811523\n",
      "Epoch 10, Batch 713, Test Loss: 0.4753386974334717\n",
      "Epoch 10, Batch 714, Test Loss: 0.7824735045433044\n",
      "Epoch 10, Batch 715, Test Loss: 0.6260972023010254\n",
      "Epoch 10, Batch 716, Test Loss: 0.5018060803413391\n",
      "Epoch 10, Batch 717, Test Loss: 0.46989476680755615\n",
      "Epoch 10, Batch 718, Test Loss: 0.4008867144584656\n",
      "Epoch 10, Batch 719, Test Loss: 0.47676923871040344\n",
      "Epoch 10, Batch 720, Test Loss: 0.3680461645126343\n",
      "Epoch 10, Batch 721, Test Loss: 0.44175460934638977\n",
      "Epoch 10, Batch 722, Test Loss: 0.5886267423629761\n",
      "Epoch 10, Batch 723, Test Loss: 0.518260657787323\n",
      "Epoch 10, Batch 724, Test Loss: 0.46585118770599365\n",
      "Epoch 10, Batch 725, Test Loss: 0.39119845628738403\n",
      "Epoch 10, Batch 726, Test Loss: 0.5634083151817322\n",
      "Epoch 10, Batch 727, Test Loss: 0.5745719075202942\n",
      "Epoch 10, Batch 728, Test Loss: 0.5342552661895752\n",
      "Epoch 10, Batch 729, Test Loss: 0.410617858171463\n",
      "Epoch 10, Batch 730, Test Loss: 0.5619005560874939\n",
      "Epoch 10, Batch 731, Test Loss: 0.5445402264595032\n",
      "Epoch 10, Batch 732, Test Loss: 0.3397812843322754\n",
      "Epoch 10, Batch 733, Test Loss: 0.5946856737136841\n",
      "Epoch 10, Batch 734, Test Loss: 0.7720288634300232\n",
      "Epoch 10, Batch 735, Test Loss: 0.5622554421424866\n",
      "Epoch 10, Batch 736, Test Loss: 0.4708873927593231\n",
      "Epoch 10, Batch 737, Test Loss: 0.48073166608810425\n",
      "Epoch 10, Batch 738, Test Loss: 0.589464545249939\n",
      "Epoch 10, Batch 739, Test Loss: 0.4148494005203247\n",
      "Epoch 10, Batch 740, Test Loss: 0.4732091426849365\n",
      "Epoch 10, Batch 741, Test Loss: 0.4830394387245178\n",
      "Epoch 10, Batch 742, Test Loss: 0.5358912348747253\n",
      "Epoch 10, Batch 743, Test Loss: 0.5293074250221252\n",
      "Epoch 10, Batch 744, Test Loss: 0.48890453577041626\n",
      "Epoch 10, Batch 745, Test Loss: 0.22677874565124512\n",
      "Epoch 10, Batch 746, Test Loss: 0.5974595546722412\n",
      "Epoch 10, Batch 747, Test Loss: 0.6713523268699646\n",
      "Epoch 10, Batch 748, Test Loss: 0.3931652009487152\n",
      "Epoch 10, Batch 749, Test Loss: 0.6929762959480286\n",
      "Epoch 10, Batch 750, Test Loss: 0.5503870844841003\n",
      "Epoch 10, Batch 751, Test Loss: 0.5052872896194458\n",
      "Epoch 10, Batch 752, Test Loss: 0.3565044701099396\n",
      "Epoch 10, Batch 753, Test Loss: 0.6981006860733032\n",
      "Epoch 10, Batch 754, Test Loss: 0.7628759145736694\n",
      "Epoch 10, Batch 755, Test Loss: 0.40485680103302\n",
      "Epoch 10, Batch 756, Test Loss: 0.5047456622123718\n",
      "Epoch 10, Batch 757, Test Loss: 0.7531872391700745\n",
      "Epoch 10, Batch 758, Test Loss: 0.5773178935050964\n",
      "Epoch 10, Batch 759, Test Loss: 0.5348625183105469\n",
      "Epoch 10, Batch 760, Test Loss: 0.49391353130340576\n",
      "Epoch 10, Batch 761, Test Loss: 0.5659917593002319\n",
      "Epoch 10, Batch 762, Test Loss: 0.5350041389465332\n",
      "Epoch 10, Batch 763, Test Loss: 0.48761704564094543\n",
      "Epoch 10, Batch 764, Test Loss: 0.3386194705963135\n",
      "Epoch 10, Batch 765, Test Loss: 0.4899511933326721\n",
      "Epoch 10, Batch 766, Test Loss: 0.4770127236843109\n",
      "Epoch 10, Batch 767, Test Loss: 0.5183400511741638\n",
      "Epoch 10, Batch 768, Test Loss: 0.6098193526268005\n",
      "Epoch 10, Batch 769, Test Loss: 0.4741634428501129\n",
      "Epoch 10, Batch 770, Test Loss: 0.5160435438156128\n",
      "Epoch 10, Batch 771, Test Loss: 0.417519211769104\n",
      "Epoch 10, Batch 772, Test Loss: 0.386258065700531\n",
      "Epoch 10, Batch 773, Test Loss: 0.5079435110092163\n",
      "Epoch 10, Batch 774, Test Loss: 0.4062027335166931\n",
      "Epoch 10, Batch 775, Test Loss: 0.42566072940826416\n",
      "Epoch 10, Batch 776, Test Loss: 0.42317667603492737\n",
      "Epoch 10, Batch 777, Test Loss: 0.7230845093727112\n",
      "Epoch 10, Batch 778, Test Loss: 0.619385302066803\n",
      "Epoch 10, Batch 779, Test Loss: 0.6480609774589539\n",
      "Epoch 10, Batch 780, Test Loss: 0.4979526400566101\n",
      "Epoch 10, Batch 781, Test Loss: 0.6520934104919434\n",
      "Epoch 10, Batch 782, Test Loss: 0.6516368985176086\n",
      "Epoch 10, Batch 783, Test Loss: 0.6699098944664001\n",
      "Epoch 10, Batch 784, Test Loss: 0.5630572438240051\n",
      "Epoch 10, Batch 785, Test Loss: 0.39890149235725403\n",
      "Epoch 10, Batch 786, Test Loss: 0.5137858986854553\n",
      "Epoch 10, Batch 787, Test Loss: 0.6166573762893677\n",
      "Epoch 10, Batch 788, Test Loss: 0.4397733807563782\n",
      "Epoch 10, Batch 789, Test Loss: 0.620868444442749\n",
      "Epoch 10, Batch 790, Test Loss: 0.6433014273643494\n",
      "Epoch 10, Batch 791, Test Loss: 0.4243338108062744\n",
      "Epoch 10, Batch 792, Test Loss: 0.5105432271957397\n",
      "Epoch 10, Batch 793, Test Loss: 0.608927845954895\n",
      "Epoch 10, Batch 794, Test Loss: 0.6575577855110168\n",
      "Epoch 10, Batch 795, Test Loss: 0.4546761214733124\n",
      "Epoch 10, Batch 796, Test Loss: 0.6133894324302673\n",
      "Epoch 10, Batch 797, Test Loss: 0.4298262596130371\n",
      "Epoch 10, Batch 798, Test Loss: 0.5984910726547241\n",
      "Epoch 10, Batch 799, Test Loss: 0.49210119247436523\n",
      "Epoch 10, Batch 800, Test Loss: 0.32002025842666626\n",
      "Epoch 10, Batch 801, Test Loss: 0.5361199975013733\n",
      "Epoch 10, Batch 802, Test Loss: 0.35896918177604675\n",
      "Epoch 10, Batch 803, Test Loss: 0.5666843056678772\n",
      "Epoch 10, Batch 804, Test Loss: 0.451971173286438\n",
      "Epoch 10, Batch 805, Test Loss: 0.4375239610671997\n",
      "Epoch 10, Batch 806, Test Loss: 0.683735728263855\n",
      "Epoch 10, Batch 807, Test Loss: 0.5089699625968933\n",
      "Epoch 10, Batch 808, Test Loss: 0.48211047053337097\n",
      "Epoch 10, Batch 809, Test Loss: 0.4503229558467865\n",
      "Epoch 10, Batch 810, Test Loss: 0.47053462266921997\n",
      "Epoch 10, Batch 811, Test Loss: 0.575334906578064\n",
      "Epoch 10, Batch 812, Test Loss: 0.37486395239830017\n",
      "Epoch 10, Batch 813, Test Loss: 0.6252184510231018\n",
      "Epoch 10, Batch 814, Test Loss: 0.5931433439254761\n",
      "Epoch 10, Batch 815, Test Loss: 0.49613043665885925\n",
      "Epoch 10, Batch 816, Test Loss: 0.476614385843277\n",
      "Epoch 10, Batch 817, Test Loss: 0.42584699392318726\n",
      "Epoch 10, Batch 818, Test Loss: 0.6200704574584961\n",
      "Epoch 10, Batch 819, Test Loss: 0.4628647565841675\n",
      "Epoch 10, Batch 820, Test Loss: 0.43137630820274353\n",
      "Epoch 10, Batch 821, Test Loss: 0.5782278180122375\n",
      "Epoch 10, Batch 822, Test Loss: 0.3495769500732422\n",
      "Epoch 10, Batch 823, Test Loss: 0.6364219784736633\n",
      "Epoch 10, Batch 824, Test Loss: 0.4602668285369873\n",
      "Epoch 10, Batch 825, Test Loss: 0.5612607598304749\n",
      "Epoch 10, Batch 826, Test Loss: 0.4870655834674835\n",
      "Epoch 10, Batch 827, Test Loss: 0.4926210045814514\n",
      "Epoch 10, Batch 828, Test Loss: 0.8269203901290894\n",
      "Epoch 10, Batch 829, Test Loss: 0.43455755710601807\n",
      "Epoch 10, Batch 830, Test Loss: 0.5297360420227051\n",
      "Epoch 10, Batch 831, Test Loss: 0.6458595991134644\n",
      "Epoch 10, Batch 832, Test Loss: 0.4874671399593353\n",
      "Epoch 10, Batch 833, Test Loss: 0.3194325864315033\n",
      "Epoch 10, Batch 834, Test Loss: 0.34649819135665894\n",
      "Epoch 10, Batch 835, Test Loss: 0.4997932016849518\n",
      "Epoch 10, Batch 836, Test Loss: 0.670512855052948\n",
      "Epoch 10, Batch 837, Test Loss: 0.5277509093284607\n",
      "Epoch 10, Batch 838, Test Loss: 0.4537857174873352\n",
      "Epoch 10, Batch 839, Test Loss: 0.3371758759021759\n",
      "Epoch 10, Batch 840, Test Loss: 0.5709656476974487\n",
      "Epoch 10, Batch 841, Test Loss: 0.6526235342025757\n",
      "Epoch 10, Batch 842, Test Loss: 0.4340939223766327\n",
      "Epoch 10, Batch 843, Test Loss: 0.26731833815574646\n",
      "Epoch 10, Batch 844, Test Loss: 0.43904706835746765\n",
      "Epoch 10, Batch 845, Test Loss: 0.5875270366668701\n",
      "Epoch 10, Batch 846, Test Loss: 0.43914973735809326\n",
      "Epoch 10, Batch 847, Test Loss: 0.777900218963623\n",
      "Epoch 10, Batch 848, Test Loss: 0.4782516062259674\n",
      "Epoch 10, Batch 849, Test Loss: 0.34413617849349976\n",
      "Epoch 10, Batch 850, Test Loss: 0.6263285279273987\n",
      "Epoch 10, Batch 851, Test Loss: 0.4884487986564636\n",
      "Epoch 10, Batch 852, Test Loss: 0.6378853917121887\n",
      "Epoch 10, Batch 853, Test Loss: 0.5610864162445068\n",
      "Epoch 10, Batch 854, Test Loss: 0.3037245571613312\n",
      "Epoch 10, Batch 855, Test Loss: 0.4758501648902893\n",
      "Epoch 10, Batch 856, Test Loss: 0.576421856880188\n",
      "Epoch 10, Batch 857, Test Loss: 0.6332842111587524\n",
      "Epoch 10, Batch 858, Test Loss: 0.523958683013916\n",
      "Epoch 10, Batch 859, Test Loss: 0.5091066360473633\n",
      "Epoch 10, Batch 860, Test Loss: 0.41687241196632385\n",
      "Epoch 10, Batch 861, Test Loss: 0.621550440788269\n",
      "Epoch 10, Batch 862, Test Loss: 0.7375209927558899\n",
      "Epoch 10, Batch 863, Test Loss: 0.5548037886619568\n",
      "Epoch 10, Batch 864, Test Loss: 0.6551511287689209\n",
      "Epoch 10, Batch 865, Test Loss: 0.5584998726844788\n",
      "Epoch 10, Batch 866, Test Loss: 0.5610584020614624\n",
      "Epoch 10, Batch 867, Test Loss: 0.45239031314849854\n",
      "Epoch 10, Batch 868, Test Loss: 0.47703608870506287\n",
      "Epoch 10, Batch 869, Test Loss: 0.5449929237365723\n",
      "Epoch 10, Batch 870, Test Loss: 0.5646947622299194\n",
      "Epoch 10, Batch 871, Test Loss: 0.6986196637153625\n",
      "Epoch 10, Batch 872, Test Loss: 0.6277568340301514\n",
      "Epoch 10, Batch 873, Test Loss: 0.580741286277771\n",
      "Epoch 10, Batch 874, Test Loss: 0.5095877647399902\n",
      "Epoch 10, Batch 875, Test Loss: 0.37885403633117676\n",
      "Epoch 10, Batch 876, Test Loss: 0.7062953114509583\n",
      "Epoch 10, Batch 877, Test Loss: 0.6065744161605835\n",
      "Epoch 10, Batch 878, Test Loss: 0.38433197140693665\n",
      "Epoch 10, Batch 879, Test Loss: 0.6156787276268005\n",
      "Epoch 10, Batch 880, Test Loss: 0.528799831867218\n",
      "Epoch 10, Batch 881, Test Loss: 0.5064999461174011\n",
      "Epoch 10, Batch 882, Test Loss: 0.48539263010025024\n",
      "Epoch 10, Batch 883, Test Loss: 0.5004156231880188\n",
      "Epoch 10, Batch 884, Test Loss: 0.4142318665981293\n",
      "Epoch 10, Batch 885, Test Loss: 0.4351992905139923\n",
      "Epoch 10, Batch 886, Test Loss: 0.7649948596954346\n",
      "Epoch 10, Batch 887, Test Loss: 0.5575379133224487\n",
      "Epoch 10, Batch 888, Test Loss: 0.4664378762245178\n",
      "Epoch 10, Batch 889, Test Loss: 0.41436490416526794\n",
      "Epoch 10, Batch 890, Test Loss: 0.49449241161346436\n",
      "Epoch 10, Batch 891, Test Loss: 0.5053169131278992\n",
      "Epoch 10, Batch 892, Test Loss: 0.5657374858856201\n",
      "Epoch 10, Batch 893, Test Loss: 0.45993420481681824\n",
      "Epoch 10, Batch 894, Test Loss: 0.5744137763977051\n",
      "Epoch 10, Batch 895, Test Loss: 0.4921654760837555\n",
      "Epoch 10, Batch 896, Test Loss: 0.47591909766197205\n",
      "Epoch 10, Batch 897, Test Loss: 0.4676836133003235\n",
      "Epoch 10, Batch 898, Test Loss: 0.4968098998069763\n",
      "Epoch 10, Batch 899, Test Loss: 0.479621946811676\n",
      "Epoch 10, Batch 900, Test Loss: 0.6659888625144958\n",
      "Epoch 10, Batch 901, Test Loss: 0.6325412392616272\n",
      "Epoch 10, Batch 902, Test Loss: 0.49276500940322876\n",
      "Epoch 10, Batch 903, Test Loss: 0.4826952815055847\n",
      "Epoch 10, Batch 904, Test Loss: 0.5119353532791138\n",
      "Epoch 10, Batch 905, Test Loss: 0.4693015217781067\n",
      "Epoch 10, Batch 906, Test Loss: 0.43828946352005005\n",
      "Epoch 10, Batch 907, Test Loss: 0.42990005016326904\n",
      "Epoch 10, Batch 908, Test Loss: 0.3252021074295044\n",
      "Epoch 10, Batch 909, Test Loss: 0.5706428289413452\n",
      "Epoch 10, Batch 910, Test Loss: 0.464632123708725\n",
      "Epoch 10, Batch 911, Test Loss: 0.5014786720275879\n",
      "Epoch 10, Batch 912, Test Loss: 0.3882654011249542\n",
      "Epoch 10, Batch 913, Test Loss: 0.4630974531173706\n",
      "Epoch 10, Batch 914, Test Loss: 0.3910576105117798\n",
      "Epoch 10, Batch 915, Test Loss: 0.5940098762512207\n",
      "Epoch 10, Batch 916, Test Loss: 0.6431432366371155\n",
      "Epoch 10, Batch 917, Test Loss: 0.3432404100894928\n",
      "Epoch 10, Batch 918, Test Loss: 0.3964137136936188\n",
      "Epoch 10, Batch 919, Test Loss: 0.3908237814903259\n",
      "Epoch 10, Batch 920, Test Loss: 0.5396468639373779\n",
      "Epoch 10, Batch 921, Test Loss: 0.6148564219474792\n",
      "Epoch 10, Batch 922, Test Loss: 0.42722585797309875\n",
      "Epoch 10, Batch 923, Test Loss: 0.3473607003688812\n",
      "Epoch 10, Batch 924, Test Loss: 0.4474957585334778\n",
      "Epoch 10, Batch 925, Test Loss: 0.4389624297618866\n",
      "Epoch 10, Batch 926, Test Loss: 0.4684736430644989\n",
      "Epoch 10, Batch 927, Test Loss: 0.547396719455719\n",
      "Epoch 10, Batch 928, Test Loss: 0.6521430611610413\n",
      "Epoch 10, Batch 929, Test Loss: 0.46372151374816895\n",
      "Epoch 10, Batch 930, Test Loss: 0.45426908135414124\n",
      "Epoch 10, Batch 931, Test Loss: 0.5833413600921631\n",
      "Epoch 10, Batch 932, Test Loss: 0.3732525110244751\n",
      "Epoch 10, Batch 933, Test Loss: 0.38569509983062744\n",
      "Epoch 10, Batch 934, Test Loss: 0.4073215126991272\n",
      "Epoch 10, Batch 935, Test Loss: 0.41771310567855835\n",
      "Epoch 10, Batch 936, Test Loss: 0.4362380802631378\n",
      "Epoch 10, Batch 937, Test Loss: 0.553091287612915\n",
      "Epoch 10, Batch 938, Test Loss: 0.37002554535865784\n",
      "Accuracy of Test set: 0.8241666666666667\n",
      "Epoch 11, Batch 1, Loss: 0.528634250164032\n",
      "Epoch 11, Batch 2, Loss: 0.5562741756439209\n",
      "Epoch 11, Batch 3, Loss: 0.457365483045578\n",
      "Epoch 11, Batch 4, Loss: 0.398274302482605\n",
      "Epoch 11, Batch 5, Loss: 0.5545579195022583\n",
      "Epoch 11, Batch 6, Loss: 0.49424558877944946\n",
      "Epoch 11, Batch 7, Loss: 0.4589814245700836\n",
      "Epoch 11, Batch 8, Loss: 0.5214613676071167\n",
      "Epoch 11, Batch 9, Loss: 0.4726831614971161\n",
      "Epoch 11, Batch 10, Loss: 0.49818187952041626\n",
      "Epoch 11, Batch 11, Loss: 0.3862318992614746\n",
      "Epoch 11, Batch 12, Loss: 0.599395751953125\n",
      "Epoch 11, Batch 13, Loss: 0.6608977913856506\n",
      "Epoch 11, Batch 14, Loss: 0.48487621545791626\n",
      "Epoch 11, Batch 15, Loss: 0.4978978633880615\n",
      "Epoch 11, Batch 16, Loss: 0.6585489511489868\n",
      "Epoch 11, Batch 17, Loss: 0.6543209552764893\n",
      "Epoch 11, Batch 18, Loss: 0.4659537374973297\n",
      "Epoch 11, Batch 19, Loss: 0.5764213800430298\n",
      "Epoch 11, Batch 20, Loss: 0.5444566011428833\n",
      "Epoch 11, Batch 21, Loss: 0.5439488291740417\n",
      "Epoch 11, Batch 22, Loss: 0.4436466693878174\n",
      "Epoch 11, Batch 23, Loss: 0.4584275484085083\n",
      "Epoch 11, Batch 24, Loss: 0.5827104449272156\n",
      "Epoch 11, Batch 25, Loss: 0.4404849112033844\n",
      "Epoch 11, Batch 26, Loss: 0.578640341758728\n",
      "Epoch 11, Batch 27, Loss: 0.442471444606781\n",
      "Epoch 11, Batch 28, Loss: 0.4625372886657715\n",
      "Epoch 11, Batch 29, Loss: 0.33309364318847656\n",
      "Epoch 11, Batch 30, Loss: 0.5912638902664185\n",
      "Epoch 11, Batch 31, Loss: 0.49572306871414185\n",
      "Epoch 11, Batch 32, Loss: 0.5837267637252808\n",
      "Epoch 11, Batch 33, Loss: 0.8469291925430298\n",
      "Epoch 11, Batch 34, Loss: 0.7198901176452637\n",
      "Epoch 11, Batch 35, Loss: 0.5724372863769531\n",
      "Epoch 11, Batch 36, Loss: 0.4358668327331543\n",
      "Epoch 11, Batch 37, Loss: 0.44647255539894104\n",
      "Epoch 11, Batch 38, Loss: 0.4761982560157776\n",
      "Epoch 11, Batch 39, Loss: 0.4821404814720154\n",
      "Epoch 11, Batch 40, Loss: 0.5627895593643188\n",
      "Epoch 11, Batch 41, Loss: 0.4796028733253479\n",
      "Epoch 11, Batch 42, Loss: 0.4122234582901001\n",
      "Epoch 11, Batch 43, Loss: 0.4939767122268677\n",
      "Epoch 11, Batch 44, Loss: 0.6151912212371826\n",
      "Epoch 11, Batch 45, Loss: 0.5263069272041321\n",
      "Epoch 11, Batch 46, Loss: 0.4739638566970825\n",
      "Epoch 11, Batch 47, Loss: 0.5388759970664978\n",
      "Epoch 11, Batch 48, Loss: 0.5388268232345581\n",
      "Epoch 11, Batch 49, Loss: 0.6281011700630188\n",
      "Epoch 11, Batch 50, Loss: 0.5680071115493774\n",
      "Epoch 11, Batch 51, Loss: 0.6412352919578552\n",
      "Epoch 11, Batch 52, Loss: 0.454600989818573\n",
      "Epoch 11, Batch 53, Loss: 0.5562889575958252\n",
      "Epoch 11, Batch 54, Loss: 0.4319689869880676\n",
      "Epoch 11, Batch 55, Loss: 0.5556609034538269\n",
      "Epoch 11, Batch 56, Loss: 0.4475843608379364\n",
      "Epoch 11, Batch 57, Loss: 0.4379270672798157\n",
      "Epoch 11, Batch 58, Loss: 0.32512617111206055\n",
      "Epoch 11, Batch 59, Loss: 0.5533721446990967\n",
      "Epoch 11, Batch 60, Loss: 0.7199303507804871\n",
      "Epoch 11, Batch 61, Loss: 0.6583855748176575\n",
      "Epoch 11, Batch 62, Loss: 0.6730726957321167\n",
      "Epoch 11, Batch 63, Loss: 0.536712110042572\n",
      "Epoch 11, Batch 64, Loss: 0.49257659912109375\n",
      "Epoch 11, Batch 65, Loss: 0.39946165680885315\n",
      "Epoch 11, Batch 66, Loss: 0.7958006262779236\n",
      "Epoch 11, Batch 67, Loss: 0.535922110080719\n",
      "Epoch 11, Batch 68, Loss: 0.5135732889175415\n",
      "Epoch 11, Batch 69, Loss: 0.4020553231239319\n",
      "Epoch 11, Batch 70, Loss: 0.711983859539032\n",
      "Epoch 11, Batch 71, Loss: 0.5744411945343018\n",
      "Epoch 11, Batch 72, Loss: 0.5459745526313782\n",
      "Epoch 11, Batch 73, Loss: 0.49111485481262207\n",
      "Epoch 11, Batch 74, Loss: 0.403846800327301\n",
      "Epoch 11, Batch 75, Loss: 0.45532092452049255\n",
      "Epoch 11, Batch 76, Loss: 0.5217552185058594\n",
      "Epoch 11, Batch 77, Loss: 0.5135163068771362\n",
      "Epoch 11, Batch 78, Loss: 0.5301700830459595\n",
      "Epoch 11, Batch 79, Loss: 0.41436630487442017\n",
      "Epoch 11, Batch 80, Loss: 0.37494608759880066\n",
      "Epoch 11, Batch 81, Loss: 0.39411529898643494\n",
      "Epoch 11, Batch 82, Loss: 0.41997310519218445\n",
      "Epoch 11, Batch 83, Loss: 0.5250373482704163\n",
      "Epoch 11, Batch 84, Loss: 0.668883204460144\n",
      "Epoch 11, Batch 85, Loss: 0.6511671543121338\n",
      "Epoch 11, Batch 86, Loss: 0.4218050539493561\n",
      "Epoch 11, Batch 87, Loss: 0.33952829241752625\n",
      "Epoch 11, Batch 88, Loss: 0.5210948586463928\n",
      "Epoch 11, Batch 89, Loss: 0.43086183071136475\n",
      "Epoch 11, Batch 90, Loss: 0.537155270576477\n",
      "Epoch 11, Batch 91, Loss: 0.4995211064815521\n",
      "Epoch 11, Batch 92, Loss: 0.2868447005748749\n",
      "Epoch 11, Batch 93, Loss: 0.3361540734767914\n",
      "Epoch 11, Batch 94, Loss: 0.3748033344745636\n",
      "Epoch 11, Batch 95, Loss: 0.4217877984046936\n",
      "Epoch 11, Batch 96, Loss: 0.5356231331825256\n",
      "Epoch 11, Batch 97, Loss: 0.40242087841033936\n",
      "Epoch 11, Batch 98, Loss: 0.6252234578132629\n",
      "Epoch 11, Batch 99, Loss: 0.5242550373077393\n",
      "Epoch 11, Batch 100, Loss: 0.3475826680660248\n",
      "Epoch 11, Batch 101, Loss: 0.3844054937362671\n",
      "Epoch 11, Batch 102, Loss: 0.3505057394504547\n",
      "Epoch 11, Batch 103, Loss: 0.5658795237541199\n",
      "Epoch 11, Batch 104, Loss: 0.6035177111625671\n",
      "Epoch 11, Batch 105, Loss: 0.6287082433700562\n",
      "Epoch 11, Batch 106, Loss: 0.4535474479198456\n",
      "Epoch 11, Batch 107, Loss: 0.4500570297241211\n",
      "Epoch 11, Batch 108, Loss: 0.7252871990203857\n",
      "Epoch 11, Batch 109, Loss: 0.46252796053886414\n",
      "Epoch 11, Batch 110, Loss: 0.6069457530975342\n",
      "Epoch 11, Batch 111, Loss: 0.5956813097000122\n",
      "Epoch 11, Batch 112, Loss: 0.4179507791996002\n",
      "Epoch 11, Batch 113, Loss: 0.6913285851478577\n",
      "Epoch 11, Batch 114, Loss: 0.3954676687717438\n",
      "Epoch 11, Batch 115, Loss: 0.38369232416152954\n",
      "Epoch 11, Batch 116, Loss: 0.46398845314979553\n",
      "Epoch 11, Batch 117, Loss: 0.4448864161968231\n",
      "Epoch 11, Batch 118, Loss: 0.3826589286327362\n",
      "Epoch 11, Batch 119, Loss: 0.3588100075721741\n",
      "Epoch 11, Batch 120, Loss: 0.6861934065818787\n",
      "Epoch 11, Batch 121, Loss: 0.4587603509426117\n",
      "Epoch 11, Batch 122, Loss: 0.9175772666931152\n",
      "Epoch 11, Batch 123, Loss: 0.6262373328208923\n",
      "Epoch 11, Batch 124, Loss: 0.30910512804985046\n",
      "Epoch 11, Batch 125, Loss: 0.652623176574707\n",
      "Epoch 11, Batch 126, Loss: 0.7089593410491943\n",
      "Epoch 11, Batch 127, Loss: 0.5766646265983582\n",
      "Epoch 11, Batch 128, Loss: 0.3216667175292969\n",
      "Epoch 11, Batch 129, Loss: 0.4920834004878998\n",
      "Epoch 11, Batch 130, Loss: 0.6213212013244629\n",
      "Epoch 11, Batch 131, Loss: 0.7977755069732666\n",
      "Epoch 11, Batch 132, Loss: 0.4111589789390564\n",
      "Epoch 11, Batch 133, Loss: 0.42779695987701416\n",
      "Epoch 11, Batch 134, Loss: 0.5072395205497742\n",
      "Epoch 11, Batch 135, Loss: 0.5834292769432068\n",
      "Epoch 11, Batch 136, Loss: 0.6225805282592773\n",
      "Epoch 11, Batch 137, Loss: 0.6103270053863525\n",
      "Epoch 11, Batch 138, Loss: 0.5389570593833923\n",
      "Epoch 11, Batch 139, Loss: 0.5427692532539368\n",
      "Epoch 11, Batch 140, Loss: 0.5726556181907654\n",
      "Epoch 11, Batch 141, Loss: 0.4924553632736206\n",
      "Epoch 11, Batch 142, Loss: 0.39549916982650757\n",
      "Epoch 11, Batch 143, Loss: 0.360419899225235\n",
      "Epoch 11, Batch 144, Loss: 0.5574707984924316\n",
      "Epoch 11, Batch 145, Loss: 0.35794082283973694\n",
      "Epoch 11, Batch 146, Loss: 0.5110419988632202\n",
      "Epoch 11, Batch 147, Loss: 0.5887500047683716\n",
      "Epoch 11, Batch 148, Loss: 0.4952141046524048\n",
      "Epoch 11, Batch 149, Loss: 0.3143453001976013\n",
      "Epoch 11, Batch 150, Loss: 0.30439668893814087\n",
      "Epoch 11, Batch 151, Loss: 0.32278355956077576\n",
      "Epoch 11, Batch 152, Loss: 0.5305535793304443\n",
      "Epoch 11, Batch 153, Loss: 0.33969515562057495\n",
      "Epoch 11, Batch 154, Loss: 0.5001172423362732\n",
      "Epoch 11, Batch 155, Loss: 0.5149394273757935\n",
      "Epoch 11, Batch 156, Loss: 0.44689518213272095\n",
      "Epoch 11, Batch 157, Loss: 0.65240877866745\n",
      "Epoch 11, Batch 158, Loss: 0.5287261009216309\n",
      "Epoch 11, Batch 159, Loss: 0.3310289680957794\n",
      "Epoch 11, Batch 160, Loss: 0.5409608483314514\n",
      "Epoch 11, Batch 161, Loss: 0.3733383119106293\n",
      "Epoch 11, Batch 162, Loss: 0.34655287861824036\n",
      "Epoch 11, Batch 163, Loss: 0.5638406872749329\n",
      "Epoch 11, Batch 164, Loss: 0.5123296976089478\n",
      "Epoch 11, Batch 165, Loss: 0.502210795879364\n",
      "Epoch 11, Batch 166, Loss: 0.43108126521110535\n",
      "Epoch 11, Batch 167, Loss: 0.674091637134552\n",
      "Epoch 11, Batch 168, Loss: 0.6039682030677795\n",
      "Epoch 11, Batch 169, Loss: 0.5560839772224426\n",
      "Epoch 11, Batch 170, Loss: 0.45555636286735535\n",
      "Epoch 11, Batch 171, Loss: 0.34505975246429443\n",
      "Epoch 11, Batch 172, Loss: 0.45248904824256897\n",
      "Epoch 11, Batch 173, Loss: 0.6007489562034607\n",
      "Epoch 11, Batch 174, Loss: 0.5197639465332031\n",
      "Epoch 11, Batch 175, Loss: 0.4445422291755676\n",
      "Epoch 11, Batch 176, Loss: 0.5464488863945007\n",
      "Epoch 11, Batch 177, Loss: 0.5919326543807983\n",
      "Epoch 11, Batch 178, Loss: 0.6933302879333496\n",
      "Epoch 11, Batch 179, Loss: 0.4140816032886505\n",
      "Epoch 11, Batch 180, Loss: 0.6251854300498962\n",
      "Epoch 11, Batch 181, Loss: 0.7345176339149475\n",
      "Epoch 11, Batch 182, Loss: 0.6250431537628174\n",
      "Epoch 11, Batch 183, Loss: 0.5375620126724243\n",
      "Epoch 11, Batch 184, Loss: 0.5313145518302917\n",
      "Epoch 11, Batch 185, Loss: 0.41910451650619507\n",
      "Epoch 11, Batch 186, Loss: 0.4426814913749695\n",
      "Epoch 11, Batch 187, Loss: 0.5183408260345459\n",
      "Epoch 11, Batch 188, Loss: 0.566290020942688\n",
      "Epoch 11, Batch 189, Loss: 0.5343770980834961\n",
      "Epoch 11, Batch 190, Loss: 0.5739284753799438\n",
      "Epoch 11, Batch 191, Loss: 0.4965517222881317\n",
      "Epoch 11, Batch 192, Loss: 0.6517210006713867\n",
      "Epoch 11, Batch 193, Loss: 0.4131740927696228\n",
      "Epoch 11, Batch 194, Loss: 0.3582078814506531\n",
      "Epoch 11, Batch 195, Loss: 0.5037413835525513\n",
      "Epoch 11, Batch 196, Loss: 0.46265584230422974\n",
      "Epoch 11, Batch 197, Loss: 0.8051629662513733\n",
      "Epoch 11, Batch 198, Loss: 0.6259053945541382\n",
      "Epoch 11, Batch 199, Loss: 0.37195977568626404\n",
      "Epoch 11, Batch 200, Loss: 0.5896905660629272\n",
      "Epoch 11, Batch 201, Loss: 0.5260423421859741\n",
      "Epoch 11, Batch 202, Loss: 0.46397408843040466\n",
      "Epoch 11, Batch 203, Loss: 0.5397366881370544\n",
      "Epoch 11, Batch 204, Loss: 0.6499090790748596\n",
      "Epoch 11, Batch 205, Loss: 0.5425101518630981\n",
      "Epoch 11, Batch 206, Loss: 0.47229111194610596\n",
      "Epoch 11, Batch 207, Loss: 0.35619181394577026\n",
      "Epoch 11, Batch 208, Loss: 0.6662642359733582\n",
      "Epoch 11, Batch 209, Loss: 0.78239905834198\n",
      "Epoch 11, Batch 210, Loss: 0.4395834505558014\n",
      "Epoch 11, Batch 211, Loss: 0.6809340715408325\n",
      "Epoch 11, Batch 212, Loss: 0.533379852771759\n",
      "Epoch 11, Batch 213, Loss: 0.5169790387153625\n",
      "Epoch 11, Batch 214, Loss: 0.3976549804210663\n",
      "Epoch 11, Batch 215, Loss: 0.35861241817474365\n",
      "Epoch 11, Batch 216, Loss: 0.5119217038154602\n",
      "Epoch 11, Batch 217, Loss: 0.6866699457168579\n",
      "Epoch 11, Batch 218, Loss: 0.5882965326309204\n",
      "Epoch 11, Batch 219, Loss: 0.29672667384147644\n",
      "Epoch 11, Batch 220, Loss: 0.5431170463562012\n",
      "Epoch 11, Batch 221, Loss: 0.5191060304641724\n",
      "Epoch 11, Batch 222, Loss: 0.5063071250915527\n",
      "Epoch 11, Batch 223, Loss: 0.5457190871238708\n",
      "Epoch 11, Batch 224, Loss: 0.4974054992198944\n",
      "Epoch 11, Batch 225, Loss: 0.427590012550354\n",
      "Epoch 11, Batch 226, Loss: 0.5340020060539246\n",
      "Epoch 11, Batch 227, Loss: 0.5552465319633484\n",
      "Epoch 11, Batch 228, Loss: 0.6432303190231323\n",
      "Epoch 11, Batch 229, Loss: 0.43940991163253784\n",
      "Epoch 11, Batch 230, Loss: 0.6858652830123901\n",
      "Epoch 11, Batch 231, Loss: 0.2862052321434021\n",
      "Epoch 11, Batch 232, Loss: 0.6036292910575867\n",
      "Epoch 11, Batch 233, Loss: 0.40741342306137085\n",
      "Epoch 11, Batch 234, Loss: 0.6376850008964539\n",
      "Epoch 11, Batch 235, Loss: 0.706325113773346\n",
      "Epoch 11, Batch 236, Loss: 0.27954909205436707\n",
      "Epoch 11, Batch 237, Loss: 0.40006065368652344\n",
      "Epoch 11, Batch 238, Loss: 0.4137248992919922\n",
      "Epoch 11, Batch 239, Loss: 0.5545129776000977\n",
      "Epoch 11, Batch 240, Loss: 0.49715185165405273\n",
      "Epoch 11, Batch 241, Loss: 0.4803895354270935\n",
      "Epoch 11, Batch 242, Loss: 0.500119149684906\n",
      "Epoch 11, Batch 243, Loss: 0.49105003476142883\n",
      "Epoch 11, Batch 244, Loss: 0.41844260692596436\n",
      "Epoch 11, Batch 245, Loss: 0.2951996922492981\n",
      "Epoch 11, Batch 246, Loss: 0.40767598152160645\n",
      "Epoch 11, Batch 247, Loss: 0.5362550020217896\n",
      "Epoch 11, Batch 248, Loss: 0.41808995604515076\n",
      "Epoch 11, Batch 249, Loss: 0.49833279848098755\n",
      "Epoch 11, Batch 250, Loss: 0.4002755582332611\n",
      "Epoch 11, Batch 251, Loss: 0.4310015141963959\n",
      "Epoch 11, Batch 252, Loss: 0.3485817015171051\n",
      "Epoch 11, Batch 253, Loss: 0.39042502641677856\n",
      "Epoch 11, Batch 254, Loss: 0.3798253536224365\n",
      "Epoch 11, Batch 255, Loss: 0.3922012150287628\n",
      "Epoch 11, Batch 256, Loss: 0.5837692022323608\n",
      "Epoch 11, Batch 257, Loss: 0.3949912190437317\n",
      "Epoch 11, Batch 258, Loss: 0.42829033732414246\n",
      "Epoch 11, Batch 259, Loss: 0.5827307105064392\n",
      "Epoch 11, Batch 260, Loss: 0.28269144892692566\n",
      "Epoch 11, Batch 261, Loss: 0.49299439787864685\n",
      "Epoch 11, Batch 262, Loss: 0.555339515209198\n",
      "Epoch 11, Batch 263, Loss: 0.6757635474205017\n",
      "Epoch 11, Batch 264, Loss: 0.3415504992008209\n",
      "Epoch 11, Batch 265, Loss: 0.624077320098877\n",
      "Epoch 11, Batch 266, Loss: 0.4001871645450592\n",
      "Epoch 11, Batch 267, Loss: 0.5841434001922607\n",
      "Epoch 11, Batch 268, Loss: 0.4520256519317627\n",
      "Epoch 11, Batch 269, Loss: 0.45711320638656616\n",
      "Epoch 11, Batch 270, Loss: 0.4783068597316742\n",
      "Epoch 11, Batch 271, Loss: 0.44632863998413086\n",
      "Epoch 11, Batch 272, Loss: 0.48427364230155945\n",
      "Epoch 11, Batch 273, Loss: 0.4579615294933319\n",
      "Epoch 11, Batch 274, Loss: 0.4617229700088501\n",
      "Epoch 11, Batch 275, Loss: 0.5036093592643738\n",
      "Epoch 11, Batch 276, Loss: 0.3704456388950348\n",
      "Epoch 11, Batch 277, Loss: 0.4587814211845398\n",
      "Epoch 11, Batch 278, Loss: 0.6717767715454102\n",
      "Epoch 11, Batch 279, Loss: 0.4613136053085327\n",
      "Epoch 11, Batch 280, Loss: 0.688845157623291\n",
      "Epoch 11, Batch 281, Loss: 0.4681316912174225\n",
      "Epoch 11, Batch 282, Loss: 0.49742448329925537\n",
      "Epoch 11, Batch 283, Loss: 0.6170755624771118\n",
      "Epoch 11, Batch 284, Loss: 0.5303806662559509\n",
      "Epoch 11, Batch 285, Loss: 0.5006924867630005\n",
      "Epoch 11, Batch 286, Loss: 0.2853110432624817\n",
      "Epoch 11, Batch 287, Loss: 0.5499998927116394\n",
      "Epoch 11, Batch 288, Loss: 0.5002618432044983\n",
      "Epoch 11, Batch 289, Loss: 0.5607057213783264\n",
      "Epoch 11, Batch 290, Loss: 0.42934900522232056\n",
      "Epoch 11, Batch 291, Loss: 0.521026074886322\n",
      "Epoch 11, Batch 292, Loss: 0.45955559611320496\n",
      "Epoch 11, Batch 293, Loss: 0.5201300382614136\n",
      "Epoch 11, Batch 294, Loss: 0.3787062466144562\n",
      "Epoch 11, Batch 295, Loss: 0.5392733812332153\n",
      "Epoch 11, Batch 296, Loss: 0.38664764165878296\n",
      "Epoch 11, Batch 297, Loss: 0.7118328809738159\n",
      "Epoch 11, Batch 298, Loss: 0.5687480568885803\n",
      "Epoch 11, Batch 299, Loss: 0.4777795970439911\n",
      "Epoch 11, Batch 300, Loss: 0.530819296836853\n",
      "Epoch 11, Batch 301, Loss: 0.38738176226615906\n",
      "Epoch 11, Batch 302, Loss: 0.4929770827293396\n",
      "Epoch 11, Batch 303, Loss: 0.4907829165458679\n",
      "Epoch 11, Batch 304, Loss: 0.4587036073207855\n",
      "Epoch 11, Batch 305, Loss: 0.4411737024784088\n",
      "Epoch 11, Batch 306, Loss: 0.6735038757324219\n",
      "Epoch 11, Batch 307, Loss: 0.7711228132247925\n",
      "Epoch 11, Batch 308, Loss: 0.5969334840774536\n",
      "Epoch 11, Batch 309, Loss: 0.335172176361084\n",
      "Epoch 11, Batch 310, Loss: 0.5619595050811768\n",
      "Epoch 11, Batch 311, Loss: 0.7083445191383362\n",
      "Epoch 11, Batch 312, Loss: 0.41751304268836975\n",
      "Epoch 11, Batch 313, Loss: 0.6486355662345886\n",
      "Epoch 11, Batch 314, Loss: 0.6448668241500854\n",
      "Epoch 11, Batch 315, Loss: 0.5212329626083374\n",
      "Epoch 11, Batch 316, Loss: 0.4669954478740692\n",
      "Epoch 11, Batch 317, Loss: 0.683293342590332\n",
      "Epoch 11, Batch 318, Loss: 0.5508358478546143\n",
      "Epoch 11, Batch 319, Loss: 0.4005888104438782\n",
      "Epoch 11, Batch 320, Loss: 0.5915889143943787\n",
      "Epoch 11, Batch 321, Loss: 0.5348231196403503\n",
      "Epoch 11, Batch 322, Loss: 0.6027462482452393\n",
      "Epoch 11, Batch 323, Loss: 0.6093260049819946\n",
      "Epoch 11, Batch 324, Loss: 0.6109060645103455\n",
      "Epoch 11, Batch 325, Loss: 0.5389010906219482\n",
      "Epoch 11, Batch 326, Loss: 0.4603218734264374\n",
      "Epoch 11, Batch 327, Loss: 0.6035569310188293\n",
      "Epoch 11, Batch 328, Loss: 0.5705996155738831\n",
      "Epoch 11, Batch 329, Loss: 0.3247239887714386\n",
      "Epoch 11, Batch 330, Loss: 0.49573540687561035\n",
      "Epoch 11, Batch 331, Loss: 0.5374534130096436\n",
      "Epoch 11, Batch 332, Loss: 0.4964173138141632\n",
      "Epoch 11, Batch 333, Loss: 0.5154182314872742\n",
      "Epoch 11, Batch 334, Loss: 0.44159382581710815\n",
      "Epoch 11, Batch 335, Loss: 0.5331363677978516\n",
      "Epoch 11, Batch 336, Loss: 0.5525698065757751\n",
      "Epoch 11, Batch 337, Loss: 0.4492551386356354\n",
      "Epoch 11, Batch 338, Loss: 0.5763514041900635\n",
      "Epoch 11, Batch 339, Loss: 0.5738818049430847\n",
      "Epoch 11, Batch 340, Loss: 0.5875885486602783\n",
      "Epoch 11, Batch 341, Loss: 0.49615928530693054\n",
      "Epoch 11, Batch 342, Loss: 0.7067794799804688\n",
      "Epoch 11, Batch 343, Loss: 0.40203022956848145\n",
      "Epoch 11, Batch 344, Loss: 0.5676445364952087\n",
      "Epoch 11, Batch 345, Loss: 0.32793280482292175\n",
      "Epoch 11, Batch 346, Loss: 0.3796979784965515\n",
      "Epoch 11, Batch 347, Loss: 0.38529911637306213\n",
      "Epoch 11, Batch 348, Loss: 0.41004490852355957\n",
      "Epoch 11, Batch 349, Loss: 0.46654555201530457\n",
      "Epoch 11, Batch 350, Loss: 0.4694792926311493\n",
      "Epoch 11, Batch 351, Loss: 0.45171332359313965\n",
      "Epoch 11, Batch 352, Loss: 0.48192092776298523\n",
      "Epoch 11, Batch 353, Loss: 0.4091724455356598\n",
      "Epoch 11, Batch 354, Loss: 0.6156246662139893\n",
      "Epoch 11, Batch 355, Loss: 0.6531050205230713\n",
      "Epoch 11, Batch 356, Loss: 0.47804051637649536\n",
      "Epoch 11, Batch 357, Loss: 0.5408793687820435\n",
      "Epoch 11, Batch 358, Loss: 0.5525495409965515\n",
      "Epoch 11, Batch 359, Loss: 0.5612685084342957\n",
      "Epoch 11, Batch 360, Loss: 0.44161584973335266\n",
      "Epoch 11, Batch 361, Loss: 0.4922701120376587\n",
      "Epoch 11, Batch 362, Loss: 0.4498101770877838\n",
      "Epoch 11, Batch 363, Loss: 0.6139898300170898\n",
      "Epoch 11, Batch 364, Loss: 0.807050347328186\n",
      "Epoch 11, Batch 365, Loss: 0.5782336592674255\n",
      "Epoch 11, Batch 366, Loss: 0.6039677858352661\n",
      "Epoch 11, Batch 367, Loss: 0.45924699306488037\n",
      "Epoch 11, Batch 368, Loss: 0.6129514575004578\n",
      "Epoch 11, Batch 369, Loss: 0.4134899377822876\n",
      "Epoch 11, Batch 370, Loss: 0.6525631546974182\n",
      "Epoch 11, Batch 371, Loss: 0.5908909440040588\n",
      "Epoch 11, Batch 372, Loss: 0.753649890422821\n",
      "Epoch 11, Batch 373, Loss: 0.6293411254882812\n",
      "Epoch 11, Batch 374, Loss: 0.7958047389984131\n",
      "Epoch 11, Batch 375, Loss: 0.4446794390678406\n",
      "Epoch 11, Batch 376, Loss: 0.6289655566215515\n",
      "Epoch 11, Batch 377, Loss: 0.4927384853363037\n",
      "Epoch 11, Batch 378, Loss: 0.4318159520626068\n",
      "Epoch 11, Batch 379, Loss: 0.45340609550476074\n",
      "Epoch 11, Batch 380, Loss: 0.4465198218822479\n",
      "Epoch 11, Batch 381, Loss: 0.4175272583961487\n",
      "Epoch 11, Batch 382, Loss: 0.3192199468612671\n",
      "Epoch 11, Batch 383, Loss: 0.7611979842185974\n",
      "Epoch 11, Batch 384, Loss: 0.5098795890808105\n",
      "Epoch 11, Batch 385, Loss: 0.7447698712348938\n",
      "Epoch 11, Batch 386, Loss: 0.4539923667907715\n",
      "Epoch 11, Batch 387, Loss: 0.4376666247844696\n",
      "Epoch 11, Batch 388, Loss: 0.534422755241394\n",
      "Epoch 11, Batch 389, Loss: 0.5945258140563965\n",
      "Epoch 11, Batch 390, Loss: 0.5591748952865601\n",
      "Epoch 11, Batch 391, Loss: 0.49834340810775757\n",
      "Epoch 11, Batch 392, Loss: 0.46421024203300476\n",
      "Epoch 11, Batch 393, Loss: 0.4670145809650421\n",
      "Epoch 11, Batch 394, Loss: 0.4376298189163208\n",
      "Epoch 11, Batch 395, Loss: 0.49714693427085876\n",
      "Epoch 11, Batch 396, Loss: 0.5154281854629517\n",
      "Epoch 11, Batch 397, Loss: 0.6927447319030762\n",
      "Epoch 11, Batch 398, Loss: 0.5789372324943542\n",
      "Epoch 11, Batch 399, Loss: 0.4423556923866272\n",
      "Epoch 11, Batch 400, Loss: 0.4637967646121979\n",
      "Epoch 11, Batch 401, Loss: 0.4619026184082031\n",
      "Epoch 11, Batch 402, Loss: 0.5509969592094421\n",
      "Epoch 11, Batch 403, Loss: 0.36198461055755615\n",
      "Epoch 11, Batch 404, Loss: 0.5226765275001526\n",
      "Epoch 11, Batch 405, Loss: 0.412008672952652\n",
      "Epoch 11, Batch 406, Loss: 0.8481395244598389\n",
      "Epoch 11, Batch 407, Loss: 0.5114492774009705\n",
      "Epoch 11, Batch 408, Loss: 0.7631171345710754\n",
      "Epoch 11, Batch 409, Loss: 0.49111342430114746\n",
      "Epoch 11, Batch 410, Loss: 0.5625303387641907\n",
      "Epoch 11, Batch 411, Loss: 0.3700527846813202\n",
      "Epoch 11, Batch 412, Loss: 0.45753639936447144\n",
      "Epoch 11, Batch 413, Loss: 0.469146728515625\n",
      "Epoch 11, Batch 414, Loss: 0.5230691432952881\n",
      "Epoch 11, Batch 415, Loss: 0.5465144515037537\n",
      "Epoch 11, Batch 416, Loss: 0.8950493335723877\n",
      "Epoch 11, Batch 417, Loss: 0.40034765005111694\n",
      "Epoch 11, Batch 418, Loss: 0.5300219058990479\n",
      "Epoch 11, Batch 419, Loss: 0.49414539337158203\n",
      "Epoch 11, Batch 420, Loss: 0.6743598580360413\n",
      "Epoch 11, Batch 421, Loss: 0.37720760703086853\n",
      "Epoch 11, Batch 422, Loss: 0.3701290190219879\n",
      "Epoch 11, Batch 423, Loss: 0.4838303327560425\n",
      "Epoch 11, Batch 424, Loss: 0.5454867482185364\n",
      "Epoch 11, Batch 425, Loss: 0.5864822864532471\n",
      "Epoch 11, Batch 426, Loss: 0.40371042490005493\n",
      "Epoch 11, Batch 427, Loss: 0.4862994849681854\n",
      "Epoch 11, Batch 428, Loss: 0.4120606780052185\n",
      "Epoch 11, Batch 429, Loss: 0.5237018465995789\n",
      "Epoch 11, Batch 430, Loss: 0.35495805740356445\n",
      "Epoch 11, Batch 431, Loss: 0.4738677144050598\n",
      "Epoch 11, Batch 432, Loss: 0.4329470992088318\n",
      "Epoch 11, Batch 433, Loss: 0.618992805480957\n",
      "Epoch 11, Batch 434, Loss: 0.5679846405982971\n",
      "Epoch 11, Batch 435, Loss: 0.6296683549880981\n",
      "Epoch 11, Batch 436, Loss: 0.3239118456840515\n",
      "Epoch 11, Batch 437, Loss: 0.3359419107437134\n",
      "Epoch 11, Batch 438, Loss: 0.534109354019165\n",
      "Epoch 11, Batch 439, Loss: 0.42913007736206055\n",
      "Epoch 11, Batch 440, Loss: 0.4275030493736267\n",
      "Epoch 11, Batch 441, Loss: 0.48360949754714966\n",
      "Epoch 11, Batch 442, Loss: 0.5314861536026001\n",
      "Epoch 11, Batch 443, Loss: 0.3697032034397125\n",
      "Epoch 11, Batch 444, Loss: 0.4486317038536072\n",
      "Epoch 11, Batch 445, Loss: 0.40650132298469543\n",
      "Epoch 11, Batch 446, Loss: 0.385563462972641\n",
      "Epoch 11, Batch 447, Loss: 0.6338648200035095\n",
      "Epoch 11, Batch 448, Loss: 0.42148512601852417\n",
      "Epoch 11, Batch 449, Loss: 0.5946975946426392\n",
      "Epoch 11, Batch 450, Loss: 0.5443127751350403\n",
      "Epoch 11, Batch 451, Loss: 0.49060070514678955\n",
      "Epoch 11, Batch 452, Loss: 0.6398099660873413\n",
      "Epoch 11, Batch 453, Loss: 0.5068157315254211\n",
      "Epoch 11, Batch 454, Loss: 0.7232192754745483\n",
      "Epoch 11, Batch 455, Loss: 0.3111138343811035\n",
      "Epoch 11, Batch 456, Loss: 0.7086477875709534\n",
      "Epoch 11, Batch 457, Loss: 0.39346814155578613\n",
      "Epoch 11, Batch 458, Loss: 0.5540034174919128\n",
      "Epoch 11, Batch 459, Loss: 0.4855225086212158\n",
      "Epoch 11, Batch 460, Loss: 0.36403483152389526\n",
      "Epoch 11, Batch 461, Loss: 0.48169344663619995\n",
      "Epoch 11, Batch 462, Loss: 0.3961219787597656\n",
      "Epoch 11, Batch 463, Loss: 0.5581685900688171\n",
      "Epoch 11, Batch 464, Loss: 0.5257402062416077\n",
      "Epoch 11, Batch 465, Loss: 0.31831276416778564\n",
      "Epoch 11, Batch 466, Loss: 0.5664384365081787\n",
      "Epoch 11, Batch 467, Loss: 0.4946932792663574\n",
      "Epoch 11, Batch 468, Loss: 0.6042798161506653\n",
      "Epoch 11, Batch 469, Loss: 0.35334479808807373\n",
      "Epoch 11, Batch 470, Loss: 0.3548404276371002\n",
      "Epoch 11, Batch 471, Loss: 0.3522484600543976\n",
      "Epoch 11, Batch 472, Loss: 0.5176304578781128\n",
      "Epoch 11, Batch 473, Loss: 0.5891116857528687\n",
      "Epoch 11, Batch 474, Loss: 0.48708558082580566\n",
      "Epoch 11, Batch 475, Loss: 0.5067678093910217\n",
      "Epoch 11, Batch 476, Loss: 0.3918795883655548\n",
      "Epoch 11, Batch 477, Loss: 0.5856013298034668\n",
      "Epoch 11, Batch 478, Loss: 0.42745649814605713\n",
      "Epoch 11, Batch 479, Loss: 0.6189820170402527\n",
      "Epoch 11, Batch 480, Loss: 0.6459977626800537\n",
      "Epoch 11, Batch 481, Loss: 0.3187904357910156\n",
      "Epoch 11, Batch 482, Loss: 0.6850267648696899\n",
      "Epoch 11, Batch 483, Loss: 0.4289763271808624\n",
      "Epoch 11, Batch 484, Loss: 0.4009198248386383\n",
      "Epoch 11, Batch 485, Loss: 0.4757852852344513\n",
      "Epoch 11, Batch 486, Loss: 0.47323477268218994\n",
      "Epoch 11, Batch 487, Loss: 0.4484735429286957\n",
      "Epoch 11, Batch 488, Loss: 0.42355024814605713\n",
      "Epoch 11, Batch 489, Loss: 0.3651725649833679\n",
      "Epoch 11, Batch 490, Loss: 0.5883607864379883\n",
      "Epoch 11, Batch 491, Loss: 0.5904222130775452\n",
      "Epoch 11, Batch 492, Loss: 0.4791138768196106\n",
      "Epoch 11, Batch 493, Loss: 0.49631986021995544\n",
      "Epoch 11, Batch 494, Loss: 0.725936770439148\n",
      "Epoch 11, Batch 495, Loss: 0.888959527015686\n",
      "Epoch 11, Batch 496, Loss: 0.37402471899986267\n",
      "Epoch 11, Batch 497, Loss: 0.6437503099441528\n",
      "Epoch 11, Batch 498, Loss: 0.5322667956352234\n",
      "Epoch 11, Batch 499, Loss: 0.552341103553772\n",
      "Epoch 11, Batch 500, Loss: 0.6743001341819763\n",
      "Epoch 11, Batch 501, Loss: 0.6674100756645203\n",
      "Epoch 11, Batch 502, Loss: 0.42351967096328735\n",
      "Epoch 11, Batch 503, Loss: 0.4610331356525421\n",
      "Epoch 11, Batch 504, Loss: 0.4356359839439392\n",
      "Epoch 11, Batch 505, Loss: 0.45174217224121094\n",
      "Epoch 11, Batch 506, Loss: 0.6380512714385986\n",
      "Epoch 11, Batch 507, Loss: 0.3953460454940796\n",
      "Epoch 11, Batch 508, Loss: 0.3969927132129669\n",
      "Epoch 11, Batch 509, Loss: 0.45892056822776794\n",
      "Epoch 11, Batch 510, Loss: 0.5831297636032104\n",
      "Epoch 11, Batch 511, Loss: 0.4985218644142151\n",
      "Epoch 11, Batch 512, Loss: 0.5915287733078003\n",
      "Epoch 11, Batch 513, Loss: 0.535308837890625\n",
      "Epoch 11, Batch 514, Loss: 0.4247526228427887\n",
      "Epoch 11, Batch 515, Loss: 0.4762333035469055\n",
      "Epoch 11, Batch 516, Loss: 0.5414726734161377\n",
      "Epoch 11, Batch 517, Loss: 0.5642922520637512\n",
      "Epoch 11, Batch 518, Loss: 0.4524846374988556\n",
      "Epoch 11, Batch 519, Loss: 0.4462622404098511\n",
      "Epoch 11, Batch 520, Loss: 0.5096713900566101\n",
      "Epoch 11, Batch 521, Loss: 0.5705916285514832\n",
      "Epoch 11, Batch 522, Loss: 0.38188624382019043\n",
      "Epoch 11, Batch 523, Loss: 0.5054445862770081\n",
      "Epoch 11, Batch 524, Loss: 0.5794254541397095\n",
      "Epoch 11, Batch 525, Loss: 0.5487437844276428\n",
      "Epoch 11, Batch 526, Loss: 0.4724036753177643\n",
      "Epoch 11, Batch 527, Loss: 0.4852444529533386\n",
      "Epoch 11, Batch 528, Loss: 0.49234285950660706\n",
      "Epoch 11, Batch 529, Loss: 0.36362549662590027\n",
      "Epoch 11, Batch 530, Loss: 0.5246101021766663\n",
      "Epoch 11, Batch 531, Loss: 0.5618875622749329\n",
      "Epoch 11, Batch 532, Loss: 0.5124475359916687\n",
      "Epoch 11, Batch 533, Loss: 0.5319601893424988\n",
      "Epoch 11, Batch 534, Loss: 0.5428054332733154\n",
      "Epoch 11, Batch 535, Loss: 0.5080000758171082\n",
      "Epoch 11, Batch 536, Loss: 0.40885525941848755\n",
      "Epoch 11, Batch 537, Loss: 0.5328595638275146\n",
      "Epoch 11, Batch 538, Loss: 0.4094773530960083\n",
      "Epoch 11, Batch 539, Loss: 0.3102104067802429\n",
      "Epoch 11, Batch 540, Loss: 0.3419061005115509\n",
      "Epoch 11, Batch 541, Loss: 0.5168910026550293\n",
      "Epoch 11, Batch 542, Loss: 0.7296485900878906\n",
      "Epoch 11, Batch 543, Loss: 0.49110937118530273\n",
      "Epoch 11, Batch 544, Loss: 0.5200117230415344\n",
      "Epoch 11, Batch 545, Loss: 0.5314716100692749\n",
      "Epoch 11, Batch 546, Loss: 0.4070931673049927\n",
      "Epoch 11, Batch 547, Loss: 0.6209902763366699\n",
      "Epoch 11, Batch 548, Loss: 0.4417634606361389\n",
      "Epoch 11, Batch 549, Loss: 0.35749128460884094\n",
      "Epoch 11, Batch 550, Loss: 0.4690425992012024\n",
      "Epoch 11, Batch 551, Loss: 0.601231038570404\n",
      "Epoch 11, Batch 552, Loss: 0.8124275207519531\n",
      "Epoch 11, Batch 553, Loss: 0.6734125018119812\n",
      "Epoch 11, Batch 554, Loss: 0.4339452087879181\n",
      "Epoch 11, Batch 555, Loss: 0.4409845471382141\n",
      "Epoch 11, Batch 556, Loss: 0.5203248858451843\n",
      "Epoch 11, Batch 557, Loss: 0.531154215335846\n",
      "Epoch 11, Batch 558, Loss: 0.5161101222038269\n",
      "Epoch 11, Batch 559, Loss: 0.6024981737136841\n",
      "Epoch 11, Batch 560, Loss: 0.5692415237426758\n",
      "Epoch 11, Batch 561, Loss: 0.5879692435264587\n",
      "Epoch 11, Batch 562, Loss: 0.5110334157943726\n",
      "Epoch 11, Batch 563, Loss: 0.4499739408493042\n",
      "Epoch 11, Batch 564, Loss: 0.48540759086608887\n",
      "Epoch 11, Batch 565, Loss: 0.5703370571136475\n",
      "Epoch 11, Batch 566, Loss: 0.49091941118240356\n",
      "Epoch 11, Batch 567, Loss: 0.5684142708778381\n",
      "Epoch 11, Batch 568, Loss: 0.521843433380127\n",
      "Epoch 11, Batch 569, Loss: 0.3079853951931\n",
      "Epoch 11, Batch 570, Loss: 0.5503351092338562\n",
      "Epoch 11, Batch 571, Loss: 0.46117961406707764\n",
      "Epoch 11, Batch 572, Loss: 0.4900401830673218\n",
      "Epoch 11, Batch 573, Loss: 0.3531292676925659\n",
      "Epoch 11, Batch 574, Loss: 0.4039662182331085\n",
      "Epoch 11, Batch 575, Loss: 0.49771106243133545\n",
      "Epoch 11, Batch 576, Loss: 0.3843691647052765\n",
      "Epoch 11, Batch 577, Loss: 0.49631184339523315\n",
      "Epoch 11, Batch 578, Loss: 0.6305685043334961\n",
      "Epoch 11, Batch 579, Loss: 0.5675950050354004\n",
      "Epoch 11, Batch 580, Loss: 0.4735201299190521\n",
      "Epoch 11, Batch 581, Loss: 0.557878851890564\n",
      "Epoch 11, Batch 582, Loss: 0.7304880023002625\n",
      "Epoch 11, Batch 583, Loss: 0.48506370186805725\n",
      "Epoch 11, Batch 584, Loss: 0.592607855796814\n",
      "Epoch 11, Batch 585, Loss: 0.5632333755493164\n",
      "Epoch 11, Batch 586, Loss: 0.5460768938064575\n",
      "Epoch 11, Batch 587, Loss: 0.5367379188537598\n",
      "Epoch 11, Batch 588, Loss: 0.49300292134284973\n",
      "Epoch 11, Batch 589, Loss: 0.37004703283309937\n",
      "Epoch 11, Batch 590, Loss: 0.4570779800415039\n",
      "Epoch 11, Batch 591, Loss: 0.5787884593009949\n",
      "Epoch 11, Batch 592, Loss: 0.5726702809333801\n",
      "Epoch 11, Batch 593, Loss: 0.3518920838832855\n",
      "Epoch 11, Batch 594, Loss: 0.5945039987564087\n",
      "Epoch 11, Batch 595, Loss: 0.5496581792831421\n",
      "Epoch 11, Batch 596, Loss: 0.41835474967956543\n",
      "Epoch 11, Batch 597, Loss: 0.48141321539878845\n",
      "Epoch 11, Batch 598, Loss: 0.5295928716659546\n",
      "Epoch 11, Batch 599, Loss: 0.5782647728919983\n",
      "Epoch 11, Batch 600, Loss: 0.6267602443695068\n",
      "Epoch 11, Batch 601, Loss: 0.31322768330574036\n",
      "Epoch 11, Batch 602, Loss: 0.43990349769592285\n",
      "Epoch 11, Batch 603, Loss: 0.3929857015609741\n",
      "Epoch 11, Batch 604, Loss: 0.4578295946121216\n",
      "Epoch 11, Batch 605, Loss: 0.5143820643424988\n",
      "Epoch 11, Batch 606, Loss: 0.626312792301178\n",
      "Epoch 11, Batch 607, Loss: 0.4493860602378845\n",
      "Epoch 11, Batch 608, Loss: 0.6148536801338196\n",
      "Epoch 11, Batch 609, Loss: 0.6911193132400513\n",
      "Epoch 11, Batch 610, Loss: 0.34683525562286377\n",
      "Epoch 11, Batch 611, Loss: 0.4994432330131531\n",
      "Epoch 11, Batch 612, Loss: 0.37707430124282837\n",
      "Epoch 11, Batch 613, Loss: 0.4363800585269928\n",
      "Epoch 11, Batch 614, Loss: 0.6176135540008545\n",
      "Epoch 11, Batch 615, Loss: 0.46017372608184814\n",
      "Epoch 11, Batch 616, Loss: 0.41373902559280396\n",
      "Epoch 11, Batch 617, Loss: 0.3160208761692047\n",
      "Epoch 11, Batch 618, Loss: 0.5443190336227417\n",
      "Epoch 11, Batch 619, Loss: 0.5602860450744629\n",
      "Epoch 11, Batch 620, Loss: 0.6768203973770142\n",
      "Epoch 11, Batch 621, Loss: 0.44076570868492126\n",
      "Epoch 11, Batch 622, Loss: 0.4841564893722534\n",
      "Epoch 11, Batch 623, Loss: 0.5295355319976807\n",
      "Epoch 11, Batch 624, Loss: 0.5250058174133301\n",
      "Epoch 11, Batch 625, Loss: 0.5541242957115173\n",
      "Epoch 11, Batch 626, Loss: 0.5821356773376465\n",
      "Epoch 11, Batch 627, Loss: 0.38787612318992615\n",
      "Epoch 11, Batch 628, Loss: 0.35749825835227966\n",
      "Epoch 11, Batch 629, Loss: 0.5472962856292725\n",
      "Epoch 11, Batch 630, Loss: 0.45549276471138\n",
      "Epoch 11, Batch 631, Loss: 0.4053887128829956\n",
      "Epoch 11, Batch 632, Loss: 0.7226232290267944\n",
      "Epoch 11, Batch 633, Loss: 0.41351318359375\n",
      "Epoch 11, Batch 634, Loss: 0.4573636054992676\n",
      "Epoch 11, Batch 635, Loss: 0.5081781148910522\n",
      "Epoch 11, Batch 636, Loss: 0.3621029853820801\n",
      "Epoch 11, Batch 637, Loss: 0.3819102942943573\n",
      "Epoch 11, Batch 638, Loss: 0.439322829246521\n",
      "Epoch 11, Batch 639, Loss: 0.4524398148059845\n",
      "Epoch 11, Batch 640, Loss: 0.5254084467887878\n",
      "Epoch 11, Batch 641, Loss: 0.8043184876441956\n",
      "Epoch 11, Batch 642, Loss: 0.6903302669525146\n",
      "Epoch 11, Batch 643, Loss: 0.48822617530822754\n",
      "Epoch 11, Batch 644, Loss: 0.4855725169181824\n",
      "Epoch 11, Batch 645, Loss: 0.42666947841644287\n",
      "Epoch 11, Batch 646, Loss: 0.48023006319999695\n",
      "Epoch 11, Batch 647, Loss: 0.7329120635986328\n",
      "Epoch 11, Batch 648, Loss: 0.5478440523147583\n",
      "Epoch 11, Batch 649, Loss: 0.5309559106826782\n",
      "Epoch 11, Batch 650, Loss: 0.49064192175865173\n",
      "Epoch 11, Batch 651, Loss: 0.5141578912734985\n",
      "Epoch 11, Batch 652, Loss: 0.5911097526550293\n",
      "Epoch 11, Batch 653, Loss: 0.5849075317382812\n",
      "Epoch 11, Batch 654, Loss: 0.5631242394447327\n",
      "Epoch 11, Batch 655, Loss: 0.6428499221801758\n",
      "Epoch 11, Batch 656, Loss: 0.6035619378089905\n",
      "Epoch 11, Batch 657, Loss: 0.4758515954017639\n",
      "Epoch 11, Batch 658, Loss: 0.5501420497894287\n",
      "Epoch 11, Batch 659, Loss: 0.4725867211818695\n",
      "Epoch 11, Batch 660, Loss: 0.5336681604385376\n",
      "Epoch 11, Batch 661, Loss: 0.6073793768882751\n",
      "Epoch 11, Batch 662, Loss: 0.4207373261451721\n",
      "Epoch 11, Batch 663, Loss: 0.5562050342559814\n",
      "Epoch 11, Batch 664, Loss: 0.5001329183578491\n",
      "Epoch 11, Batch 665, Loss: 0.6036998629570007\n",
      "Epoch 11, Batch 666, Loss: 0.4222000241279602\n",
      "Epoch 11, Batch 667, Loss: 0.5159787535667419\n",
      "Epoch 11, Batch 668, Loss: 0.43596693873405457\n",
      "Epoch 11, Batch 669, Loss: 0.7285546660423279\n",
      "Epoch 11, Batch 670, Loss: 0.5392423868179321\n",
      "Epoch 11, Batch 671, Loss: 0.5254899859428406\n",
      "Epoch 11, Batch 672, Loss: 0.39241844415664673\n",
      "Epoch 11, Batch 673, Loss: 0.3830311894416809\n",
      "Epoch 11, Batch 674, Loss: 0.5983855128288269\n",
      "Epoch 11, Batch 675, Loss: 0.5043181777000427\n",
      "Epoch 11, Batch 676, Loss: 0.3967600464820862\n",
      "Epoch 11, Batch 677, Loss: 0.6604579091072083\n",
      "Epoch 11, Batch 678, Loss: 0.3702096939086914\n",
      "Epoch 11, Batch 679, Loss: 0.4108501374721527\n",
      "Epoch 11, Batch 680, Loss: 0.4172498285770416\n",
      "Epoch 11, Batch 681, Loss: 0.4399477541446686\n",
      "Epoch 11, Batch 682, Loss: 0.40839287638664246\n",
      "Epoch 11, Batch 683, Loss: 0.487762451171875\n",
      "Epoch 11, Batch 684, Loss: 0.504501223564148\n",
      "Epoch 11, Batch 685, Loss: 0.44730114936828613\n",
      "Epoch 11, Batch 686, Loss: 0.5588568449020386\n",
      "Epoch 11, Batch 687, Loss: 0.30261358618736267\n",
      "Epoch 11, Batch 688, Loss: 0.40020716190338135\n",
      "Epoch 11, Batch 689, Loss: 0.41252872347831726\n",
      "Epoch 11, Batch 690, Loss: 0.35563188791275024\n",
      "Epoch 11, Batch 691, Loss: 0.4476257562637329\n",
      "Epoch 11, Batch 692, Loss: 0.6226305961608887\n",
      "Epoch 11, Batch 693, Loss: 0.5502451658248901\n",
      "Epoch 11, Batch 694, Loss: 0.619624137878418\n",
      "Epoch 11, Batch 695, Loss: 0.6183813810348511\n",
      "Epoch 11, Batch 696, Loss: 0.4306401014328003\n",
      "Epoch 11, Batch 697, Loss: 0.5324074029922485\n",
      "Epoch 11, Batch 698, Loss: 0.5187781453132629\n",
      "Epoch 11, Batch 699, Loss: 0.3630889654159546\n",
      "Epoch 11, Batch 700, Loss: 0.572084367275238\n",
      "Epoch 11, Batch 701, Loss: 0.6032049655914307\n",
      "Epoch 11, Batch 702, Loss: 0.49252772331237793\n",
      "Epoch 11, Batch 703, Loss: 0.4954524338245392\n",
      "Epoch 11, Batch 704, Loss: 0.5231772661209106\n",
      "Epoch 11, Batch 705, Loss: 0.788905143737793\n",
      "Epoch 11, Batch 706, Loss: 0.5770673155784607\n",
      "Epoch 11, Batch 707, Loss: 0.34212660789489746\n",
      "Epoch 11, Batch 708, Loss: 0.36631232500076294\n",
      "Epoch 11, Batch 709, Loss: 0.41337496042251587\n",
      "Epoch 11, Batch 710, Loss: 0.4527897238731384\n",
      "Epoch 11, Batch 711, Loss: 0.39772552251815796\n",
      "Epoch 11, Batch 712, Loss: 0.35245391726493835\n",
      "Epoch 11, Batch 713, Loss: 0.5509138703346252\n",
      "Epoch 11, Batch 714, Loss: 0.47137317061424255\n",
      "Epoch 11, Batch 715, Loss: 0.5503914952278137\n",
      "Epoch 11, Batch 716, Loss: 0.6854402422904968\n",
      "Epoch 11, Batch 717, Loss: 0.4421520531177521\n",
      "Epoch 11, Batch 718, Loss: 0.39445605874061584\n",
      "Epoch 11, Batch 719, Loss: 0.5479620695114136\n",
      "Epoch 11, Batch 720, Loss: 0.4590591788291931\n",
      "Epoch 11, Batch 721, Loss: 0.6772475838661194\n",
      "Epoch 11, Batch 722, Loss: 0.41629013419151306\n",
      "Epoch 11, Batch 723, Loss: 0.7014139890670776\n",
      "Epoch 11, Batch 724, Loss: 0.3626185953617096\n",
      "Epoch 11, Batch 725, Loss: 0.6941198706626892\n",
      "Epoch 11, Batch 726, Loss: 0.41515064239501953\n",
      "Epoch 11, Batch 727, Loss: 0.6085290312767029\n",
      "Epoch 11, Batch 728, Loss: 0.4591449499130249\n",
      "Epoch 11, Batch 729, Loss: 0.43889543414115906\n",
      "Epoch 11, Batch 730, Loss: 0.4705057144165039\n",
      "Epoch 11, Batch 731, Loss: 0.40928876399993896\n",
      "Epoch 11, Batch 732, Loss: 0.402111679315567\n",
      "Epoch 11, Batch 733, Loss: 0.5750535130500793\n",
      "Epoch 11, Batch 734, Loss: 0.529076337814331\n",
      "Epoch 11, Batch 735, Loss: 0.5301390886306763\n",
      "Epoch 11, Batch 736, Loss: 0.51859050989151\n",
      "Epoch 11, Batch 737, Loss: 0.49515625834465027\n",
      "Epoch 11, Batch 738, Loss: 0.4632585048675537\n",
      "Epoch 11, Batch 739, Loss: 0.6117939949035645\n",
      "Epoch 11, Batch 740, Loss: 0.5171660780906677\n",
      "Epoch 11, Batch 741, Loss: 0.5546522736549377\n",
      "Epoch 11, Batch 742, Loss: 0.47445136308670044\n",
      "Epoch 11, Batch 743, Loss: 0.4213474988937378\n",
      "Epoch 11, Batch 744, Loss: 0.40152832865715027\n",
      "Epoch 11, Batch 745, Loss: 0.257765531539917\n",
      "Epoch 11, Batch 746, Loss: 0.6601073145866394\n",
      "Epoch 11, Batch 747, Loss: 0.5527759194374084\n",
      "Epoch 11, Batch 748, Loss: 0.38071227073669434\n",
      "Epoch 11, Batch 749, Loss: 0.4404613673686981\n",
      "Epoch 11, Batch 750, Loss: 0.6915611028671265\n",
      "Epoch 11, Batch 751, Loss: 0.5686240196228027\n",
      "Epoch 11, Batch 752, Loss: 0.6627068519592285\n",
      "Epoch 11, Batch 753, Loss: 0.533513605594635\n",
      "Epoch 11, Batch 754, Loss: 0.646422266960144\n",
      "Epoch 11, Batch 755, Loss: 0.5871450304985046\n",
      "Epoch 11, Batch 756, Loss: 0.531502366065979\n",
      "Epoch 11, Batch 757, Loss: 0.3319660425186157\n",
      "Epoch 11, Batch 758, Loss: 0.5084733963012695\n",
      "Epoch 11, Batch 759, Loss: 0.4989878535270691\n",
      "Epoch 11, Batch 760, Loss: 0.6272280216217041\n",
      "Epoch 11, Batch 761, Loss: 0.30133944749832153\n",
      "Epoch 11, Batch 762, Loss: 0.56577467918396\n",
      "Epoch 11, Batch 763, Loss: 0.3760545253753662\n",
      "Epoch 11, Batch 764, Loss: 0.4942951202392578\n",
      "Epoch 11, Batch 765, Loss: 0.39705708622932434\n",
      "Epoch 11, Batch 766, Loss: 0.4800993800163269\n",
      "Epoch 11, Batch 767, Loss: 0.5284407734870911\n",
      "Epoch 11, Batch 768, Loss: 0.5118289589881897\n",
      "Epoch 11, Batch 769, Loss: 0.6271438002586365\n",
      "Epoch 11, Batch 770, Loss: 0.4543866217136383\n",
      "Epoch 11, Batch 771, Loss: 0.4612967371940613\n",
      "Epoch 11, Batch 772, Loss: 0.5222207307815552\n",
      "Epoch 11, Batch 773, Loss: 0.549278736114502\n",
      "Epoch 11, Batch 774, Loss: 0.7553464770317078\n",
      "Epoch 11, Batch 775, Loss: 0.597813606262207\n",
      "Epoch 11, Batch 776, Loss: 0.5862265229225159\n",
      "Epoch 11, Batch 777, Loss: 0.5985062718391418\n",
      "Epoch 11, Batch 778, Loss: 0.5265573859214783\n",
      "Epoch 11, Batch 779, Loss: 0.5392875075340271\n",
      "Epoch 11, Batch 780, Loss: 0.47060245275497437\n",
      "Epoch 11, Batch 781, Loss: 0.553409993648529\n",
      "Epoch 11, Batch 782, Loss: 0.410745769739151\n",
      "Epoch 11, Batch 783, Loss: 0.5989484190940857\n",
      "Epoch 11, Batch 784, Loss: 0.4161100685596466\n",
      "Epoch 11, Batch 785, Loss: 0.4958155155181885\n",
      "Epoch 11, Batch 786, Loss: 0.5389217138290405\n",
      "Epoch 11, Batch 787, Loss: 0.438726007938385\n",
      "Epoch 11, Batch 788, Loss: 0.5286591053009033\n",
      "Epoch 11, Batch 789, Loss: 0.44430622458457947\n",
      "Epoch 11, Batch 790, Loss: 0.7427288293838501\n",
      "Epoch 11, Batch 791, Loss: 0.46587005257606506\n",
      "Epoch 11, Batch 792, Loss: 0.5366220474243164\n",
      "Epoch 11, Batch 793, Loss: 0.3885795474052429\n",
      "Epoch 11, Batch 794, Loss: 0.49249377846717834\n",
      "Epoch 11, Batch 795, Loss: 0.5278239250183105\n",
      "Epoch 11, Batch 796, Loss: 0.5635966658592224\n",
      "Epoch 11, Batch 797, Loss: 0.35286059975624084\n",
      "Epoch 11, Batch 798, Loss: 0.7219790816307068\n",
      "Epoch 11, Batch 799, Loss: 0.4666503965854645\n",
      "Epoch 11, Batch 800, Loss: 0.5714860558509827\n",
      "Epoch 11, Batch 801, Loss: 0.7409219741821289\n",
      "Epoch 11, Batch 802, Loss: 0.6401180624961853\n",
      "Epoch 11, Batch 803, Loss: 0.5431782007217407\n",
      "Epoch 11, Batch 804, Loss: 0.4898392856121063\n",
      "Epoch 11, Batch 805, Loss: 0.5385190844535828\n",
      "Epoch 11, Batch 806, Loss: 0.35152187943458557\n",
      "Epoch 11, Batch 807, Loss: 0.47739312052726746\n",
      "Epoch 11, Batch 808, Loss: 0.38613107800483704\n",
      "Epoch 11, Batch 809, Loss: 0.5822629928588867\n",
      "Epoch 11, Batch 810, Loss: 0.4114648699760437\n",
      "Epoch 11, Batch 811, Loss: 0.5221823453903198\n",
      "Epoch 11, Batch 812, Loss: 0.876075804233551\n",
      "Epoch 11, Batch 813, Loss: 0.43225204944610596\n",
      "Epoch 11, Batch 814, Loss: 0.43875357508659363\n",
      "Epoch 11, Batch 815, Loss: 0.5949890613555908\n",
      "Epoch 11, Batch 816, Loss: 0.4683702886104584\n",
      "Epoch 11, Batch 817, Loss: 0.42345738410949707\n",
      "Epoch 11, Batch 818, Loss: 0.7027389407157898\n",
      "Epoch 11, Batch 819, Loss: 0.712002694606781\n",
      "Epoch 11, Batch 820, Loss: 0.64754718542099\n",
      "Epoch 11, Batch 821, Loss: 0.5441501140594482\n",
      "Epoch 11, Batch 822, Loss: 0.43248578906059265\n",
      "Epoch 11, Batch 823, Loss: 0.6795323491096497\n",
      "Epoch 11, Batch 824, Loss: 0.48734933137893677\n",
      "Epoch 11, Batch 825, Loss: 0.33237022161483765\n",
      "Epoch 11, Batch 826, Loss: 0.43993139266967773\n",
      "Epoch 11, Batch 827, Loss: 0.6504582762718201\n",
      "Epoch 11, Batch 828, Loss: 0.5360881090164185\n",
      "Epoch 11, Batch 829, Loss: 0.40789783000946045\n",
      "Epoch 11, Batch 830, Loss: 0.37000542879104614\n",
      "Epoch 11, Batch 831, Loss: 0.5162132978439331\n",
      "Epoch 11, Batch 832, Loss: 0.42127251625061035\n",
      "Epoch 11, Batch 833, Loss: 0.5156437158584595\n",
      "Epoch 11, Batch 834, Loss: 0.515889048576355\n",
      "Epoch 11, Batch 835, Loss: 0.5217921137809753\n",
      "Epoch 11, Batch 836, Loss: 0.5008129477500916\n",
      "Epoch 11, Batch 837, Loss: 0.37703460454940796\n",
      "Epoch 11, Batch 838, Loss: 0.618676483631134\n",
      "Epoch 11, Batch 839, Loss: 0.4286429286003113\n",
      "Epoch 11, Batch 840, Loss: 0.5992006659507751\n",
      "Epoch 11, Batch 841, Loss: 0.4629112482070923\n",
      "Epoch 11, Batch 842, Loss: 0.5181361436843872\n",
      "Epoch 11, Batch 843, Loss: 0.4030344784259796\n",
      "Epoch 11, Batch 844, Loss: 0.417674720287323\n",
      "Epoch 11, Batch 845, Loss: 0.524642825126648\n",
      "Epoch 11, Batch 846, Loss: 0.6785770654678345\n",
      "Epoch 11, Batch 847, Loss: 0.44398027658462524\n",
      "Epoch 11, Batch 848, Loss: 0.5003283619880676\n",
      "Epoch 11, Batch 849, Loss: 0.5054489374160767\n",
      "Epoch 11, Batch 850, Loss: 0.6700721383094788\n",
      "Epoch 11, Batch 851, Loss: 0.5955326557159424\n",
      "Epoch 11, Batch 852, Loss: 0.42670494318008423\n",
      "Epoch 11, Batch 853, Loss: 0.3797794580459595\n",
      "Epoch 11, Batch 854, Loss: 0.36696121096611023\n",
      "Epoch 11, Batch 855, Loss: 0.6350496411323547\n",
      "Epoch 11, Batch 856, Loss: 0.34953004121780396\n",
      "Epoch 11, Batch 857, Loss: 0.33447393774986267\n",
      "Epoch 11, Batch 858, Loss: 0.48094701766967773\n",
      "Epoch 11, Batch 859, Loss: 0.3564921021461487\n",
      "Epoch 11, Batch 860, Loss: 0.5540810823440552\n",
      "Epoch 11, Batch 861, Loss: 0.5182920098304749\n",
      "Epoch 11, Batch 862, Loss: 0.3545108437538147\n",
      "Epoch 11, Batch 863, Loss: 0.4611414074897766\n",
      "Epoch 11, Batch 864, Loss: 0.4654580354690552\n",
      "Epoch 11, Batch 865, Loss: 0.434690922498703\n",
      "Epoch 11, Batch 866, Loss: 0.6001332402229309\n",
      "Epoch 11, Batch 867, Loss: 0.4571307599544525\n",
      "Epoch 11, Batch 868, Loss: 0.4605233073234558\n",
      "Epoch 11, Batch 869, Loss: 0.6187313199043274\n",
      "Epoch 11, Batch 870, Loss: 0.37593403458595276\n",
      "Epoch 11, Batch 871, Loss: 0.537156343460083\n",
      "Epoch 11, Batch 872, Loss: 0.5110689997673035\n",
      "Epoch 11, Batch 873, Loss: 0.633346676826477\n",
      "Epoch 11, Batch 874, Loss: 0.658210039138794\n",
      "Epoch 11, Batch 875, Loss: 0.6732921004295349\n",
      "Epoch 11, Batch 876, Loss: 0.5286687016487122\n",
      "Epoch 11, Batch 877, Loss: 0.4530983865261078\n",
      "Epoch 11, Batch 878, Loss: 0.43753886222839355\n",
      "Epoch 11, Batch 879, Loss: 0.5294408202171326\n",
      "Epoch 11, Batch 880, Loss: 0.5546081066131592\n",
      "Epoch 11, Batch 881, Loss: 0.5661832690238953\n",
      "Epoch 11, Batch 882, Loss: 0.5229901075363159\n",
      "Epoch 11, Batch 883, Loss: 0.44591569900512695\n",
      "Epoch 11, Batch 884, Loss: 0.47078144550323486\n",
      "Epoch 11, Batch 885, Loss: 0.4475357234477997\n",
      "Epoch 11, Batch 886, Loss: 0.426323264837265\n",
      "Epoch 11, Batch 887, Loss: 0.5783272981643677\n",
      "Epoch 11, Batch 888, Loss: 0.47872787714004517\n",
      "Epoch 11, Batch 889, Loss: 0.4611780643463135\n",
      "Epoch 11, Batch 890, Loss: 0.4308139681816101\n",
      "Epoch 11, Batch 891, Loss: 0.39620500802993774\n",
      "Epoch 11, Batch 892, Loss: 0.5696793794631958\n",
      "Epoch 11, Batch 893, Loss: 0.437666118144989\n",
      "Epoch 11, Batch 894, Loss: 0.4305404722690582\n",
      "Epoch 11, Batch 895, Loss: 0.44508469104766846\n",
      "Epoch 11, Batch 896, Loss: 0.41960155963897705\n",
      "Epoch 11, Batch 897, Loss: 1.042859673500061\n",
      "Epoch 11, Batch 898, Loss: 0.4910438358783722\n",
      "Epoch 11, Batch 899, Loss: 0.5167471766471863\n",
      "Epoch 11, Batch 900, Loss: 0.489059716463089\n",
      "Epoch 11, Batch 901, Loss: 0.4626201093196869\n",
      "Epoch 11, Batch 902, Loss: 0.3373265266418457\n",
      "Epoch 11, Batch 903, Loss: 0.721926212310791\n",
      "Epoch 11, Batch 904, Loss: 0.65894615650177\n",
      "Epoch 11, Batch 905, Loss: 0.5677862167358398\n",
      "Epoch 11, Batch 906, Loss: 0.5562217235565186\n",
      "Epoch 11, Batch 907, Loss: 0.35264813899993896\n",
      "Epoch 11, Batch 908, Loss: 0.4341306686401367\n",
      "Epoch 11, Batch 909, Loss: 0.4680315852165222\n",
      "Epoch 11, Batch 910, Loss: 0.4257097840309143\n",
      "Epoch 11, Batch 911, Loss: 0.6197150945663452\n",
      "Epoch 11, Batch 912, Loss: 0.6628031134605408\n",
      "Epoch 11, Batch 913, Loss: 0.5154317617416382\n",
      "Epoch 11, Batch 914, Loss: 0.4752441942691803\n",
      "Epoch 11, Batch 915, Loss: 0.6621599197387695\n",
      "Epoch 11, Batch 916, Loss: 0.4164322018623352\n",
      "Epoch 11, Batch 917, Loss: 0.41447532176971436\n",
      "Epoch 11, Batch 918, Loss: 0.6450738310813904\n",
      "Epoch 11, Batch 919, Loss: 0.4812190532684326\n",
      "Epoch 11, Batch 920, Loss: 0.31182631850242615\n",
      "Epoch 11, Batch 921, Loss: 0.38069942593574524\n",
      "Epoch 11, Batch 922, Loss: 0.48028403520584106\n",
      "Epoch 11, Batch 923, Loss: 0.586505651473999\n",
      "Epoch 11, Batch 924, Loss: 0.5216000080108643\n",
      "Epoch 11, Batch 925, Loss: 0.727340579032898\n",
      "Epoch 11, Batch 926, Loss: 0.4350128769874573\n",
      "Epoch 11, Batch 927, Loss: 0.3477481007575989\n",
      "Epoch 11, Batch 928, Loss: 0.30433112382888794\n",
      "Epoch 11, Batch 929, Loss: 0.4954310357570648\n",
      "Epoch 11, Batch 930, Loss: 0.5426174998283386\n",
      "Epoch 11, Batch 931, Loss: 0.4027291536331177\n",
      "Epoch 11, Batch 932, Loss: 0.3354377746582031\n",
      "Epoch 11, Batch 933, Loss: 0.3817225694656372\n",
      "Epoch 11, Batch 934, Loss: 0.5578671097755432\n",
      "Epoch 11, Batch 935, Loss: 0.5702451467514038\n",
      "Epoch 11, Batch 936, Loss: 0.3833466172218323\n",
      "Epoch 11, Batch 937, Loss: 0.3945944607257843\n",
      "Epoch 11, Batch 938, Loss: 0.8254954814910889\n",
      "Accuracy of train set: 0.823\n",
      "Epoch 11, Batch 1, Test Loss: 0.5097894668579102\n",
      "Epoch 11, Batch 2, Test Loss: 0.5423225164413452\n",
      "Epoch 11, Batch 3, Test Loss: 0.5040964484214783\n",
      "Epoch 11, Batch 4, Test Loss: 0.5354055762290955\n",
      "Epoch 11, Batch 5, Test Loss: 0.5152307152748108\n",
      "Epoch 11, Batch 6, Test Loss: 0.5161267518997192\n",
      "Epoch 11, Batch 7, Test Loss: 0.49424251914024353\n",
      "Epoch 11, Batch 8, Test Loss: 0.3541181981563568\n",
      "Epoch 11, Batch 9, Test Loss: 0.5004334449768066\n",
      "Epoch 11, Batch 10, Test Loss: 0.49519145488739014\n",
      "Epoch 11, Batch 11, Test Loss: 0.5207749009132385\n",
      "Epoch 11, Batch 12, Test Loss: 0.4783242642879486\n",
      "Epoch 11, Batch 13, Test Loss: 0.5388550162315369\n",
      "Epoch 11, Batch 14, Test Loss: 0.35263311862945557\n",
      "Epoch 11, Batch 15, Test Loss: 0.5676381587982178\n",
      "Epoch 11, Batch 16, Test Loss: 0.499739408493042\n",
      "Epoch 11, Batch 17, Test Loss: 0.46685829758644104\n",
      "Epoch 11, Batch 18, Test Loss: 0.5368363261222839\n",
      "Epoch 11, Batch 19, Test Loss: 0.36263731122016907\n",
      "Epoch 11, Batch 20, Test Loss: 0.492513507604599\n",
      "Epoch 11, Batch 21, Test Loss: 0.5639429092407227\n",
      "Epoch 11, Batch 22, Test Loss: 0.5649657249450684\n",
      "Epoch 11, Batch 23, Test Loss: 0.4833454489707947\n",
      "Epoch 11, Batch 24, Test Loss: 0.379142701625824\n",
      "Epoch 11, Batch 25, Test Loss: 0.6914047002792358\n",
      "Epoch 11, Batch 26, Test Loss: 0.7125468254089355\n",
      "Epoch 11, Batch 27, Test Loss: 0.5497074127197266\n",
      "Epoch 11, Batch 28, Test Loss: 0.610789954662323\n",
      "Epoch 11, Batch 29, Test Loss: 0.4279094636440277\n",
      "Epoch 11, Batch 30, Test Loss: 0.4677548408508301\n",
      "Epoch 11, Batch 31, Test Loss: 0.36142414808273315\n",
      "Epoch 11, Batch 32, Test Loss: 0.45145201683044434\n",
      "Epoch 11, Batch 33, Test Loss: 0.3564663827419281\n",
      "Epoch 11, Batch 34, Test Loss: 0.48526352643966675\n",
      "Epoch 11, Batch 35, Test Loss: 0.4345046281814575\n",
      "Epoch 11, Batch 36, Test Loss: 0.3517623245716095\n",
      "Epoch 11, Batch 37, Test Loss: 0.40695613622665405\n",
      "Epoch 11, Batch 38, Test Loss: 0.6113490462303162\n",
      "Epoch 11, Batch 39, Test Loss: 0.3493161201477051\n",
      "Epoch 11, Batch 40, Test Loss: 0.821521520614624\n",
      "Epoch 11, Batch 41, Test Loss: 0.7721452713012695\n",
      "Epoch 11, Batch 42, Test Loss: 0.5077372789382935\n",
      "Epoch 11, Batch 43, Test Loss: 0.539037823677063\n",
      "Epoch 11, Batch 44, Test Loss: 0.513441801071167\n",
      "Epoch 11, Batch 45, Test Loss: 0.5233906507492065\n",
      "Epoch 11, Batch 46, Test Loss: 0.6107674837112427\n",
      "Epoch 11, Batch 47, Test Loss: 0.464897096157074\n",
      "Epoch 11, Batch 48, Test Loss: 0.5826668739318848\n",
      "Epoch 11, Batch 49, Test Loss: 0.47169923782348633\n",
      "Epoch 11, Batch 50, Test Loss: 0.5358309745788574\n",
      "Epoch 11, Batch 51, Test Loss: 0.5558272004127502\n",
      "Epoch 11, Batch 52, Test Loss: 0.5153216123580933\n",
      "Epoch 11, Batch 53, Test Loss: 0.6465165019035339\n",
      "Epoch 11, Batch 54, Test Loss: 0.46375733613967896\n",
      "Epoch 11, Batch 55, Test Loss: 0.41640594601631165\n",
      "Epoch 11, Batch 56, Test Loss: 0.41113758087158203\n",
      "Epoch 11, Batch 57, Test Loss: 0.3611379563808441\n",
      "Epoch 11, Batch 58, Test Loss: 0.5058577060699463\n",
      "Epoch 11, Batch 59, Test Loss: 0.355471670627594\n",
      "Epoch 11, Batch 60, Test Loss: 0.5119311809539795\n",
      "Epoch 11, Batch 61, Test Loss: 0.6885682940483093\n",
      "Epoch 11, Batch 62, Test Loss: 0.6354004144668579\n",
      "Epoch 11, Batch 63, Test Loss: 0.6353145837783813\n",
      "Epoch 11, Batch 64, Test Loss: 0.6900928616523743\n",
      "Epoch 11, Batch 65, Test Loss: 0.46980881690979004\n",
      "Epoch 11, Batch 66, Test Loss: 0.5919859409332275\n",
      "Epoch 11, Batch 67, Test Loss: 0.4494152069091797\n",
      "Epoch 11, Batch 68, Test Loss: 0.5976990461349487\n",
      "Epoch 11, Batch 69, Test Loss: 0.5586129426956177\n",
      "Epoch 11, Batch 70, Test Loss: 0.546445906162262\n",
      "Epoch 11, Batch 71, Test Loss: 0.6901111006736755\n",
      "Epoch 11, Batch 72, Test Loss: 0.6194769144058228\n",
      "Epoch 11, Batch 73, Test Loss: 0.4542781114578247\n",
      "Epoch 11, Batch 74, Test Loss: 0.5370457172393799\n",
      "Epoch 11, Batch 75, Test Loss: 0.723380446434021\n",
      "Epoch 11, Batch 76, Test Loss: 0.6716676354408264\n",
      "Epoch 11, Batch 77, Test Loss: 0.57981276512146\n",
      "Epoch 11, Batch 78, Test Loss: 0.5389249324798584\n",
      "Epoch 11, Batch 79, Test Loss: 0.41699379682540894\n",
      "Epoch 11, Batch 80, Test Loss: 0.6877188682556152\n",
      "Epoch 11, Batch 81, Test Loss: 0.5250283479690552\n",
      "Epoch 11, Batch 82, Test Loss: 0.4714231491088867\n",
      "Epoch 11, Batch 83, Test Loss: 0.4462398588657379\n",
      "Epoch 11, Batch 84, Test Loss: 0.4551464915275574\n",
      "Epoch 11, Batch 85, Test Loss: 0.46266576647758484\n",
      "Epoch 11, Batch 86, Test Loss: 0.6192668080329895\n",
      "Epoch 11, Batch 87, Test Loss: 0.6584147810935974\n",
      "Epoch 11, Batch 88, Test Loss: 0.37917473912239075\n",
      "Epoch 11, Batch 89, Test Loss: 0.5649573802947998\n",
      "Epoch 11, Batch 90, Test Loss: 0.42385876178741455\n",
      "Epoch 11, Batch 91, Test Loss: 0.4681802988052368\n",
      "Epoch 11, Batch 92, Test Loss: 0.3847792446613312\n",
      "Epoch 11, Batch 93, Test Loss: 0.7292479872703552\n",
      "Epoch 11, Batch 94, Test Loss: 0.6352107524871826\n",
      "Epoch 11, Batch 95, Test Loss: 0.5485132932662964\n",
      "Epoch 11, Batch 96, Test Loss: 0.46680334210395813\n",
      "Epoch 11, Batch 97, Test Loss: 0.4489472210407257\n",
      "Epoch 11, Batch 98, Test Loss: 0.3802032172679901\n",
      "Epoch 11, Batch 99, Test Loss: 0.539523720741272\n",
      "Epoch 11, Batch 100, Test Loss: 0.34584516286849976\n",
      "Epoch 11, Batch 101, Test Loss: 0.5787956118583679\n",
      "Epoch 11, Batch 102, Test Loss: 0.2803570628166199\n",
      "Epoch 11, Batch 103, Test Loss: 0.6088786125183105\n",
      "Epoch 11, Batch 104, Test Loss: 0.5442551970481873\n",
      "Epoch 11, Batch 105, Test Loss: 0.32993873953819275\n",
      "Epoch 11, Batch 106, Test Loss: 0.422595739364624\n",
      "Epoch 11, Batch 107, Test Loss: 0.4034276008605957\n",
      "Epoch 11, Batch 108, Test Loss: 0.42948925495147705\n",
      "Epoch 11, Batch 109, Test Loss: 0.4876917898654938\n",
      "Epoch 11, Batch 110, Test Loss: 0.6406952142715454\n",
      "Epoch 11, Batch 111, Test Loss: 0.4730750620365143\n",
      "Epoch 11, Batch 112, Test Loss: 0.37007761001586914\n",
      "Epoch 11, Batch 113, Test Loss: 0.6553362607955933\n",
      "Epoch 11, Batch 114, Test Loss: 0.7350909113883972\n",
      "Epoch 11, Batch 115, Test Loss: 0.6537603139877319\n",
      "Epoch 11, Batch 116, Test Loss: 0.4750770330429077\n",
      "Epoch 11, Batch 117, Test Loss: 0.4694933593273163\n",
      "Epoch 11, Batch 118, Test Loss: 0.6828223466873169\n",
      "Epoch 11, Batch 119, Test Loss: 0.4767891466617584\n",
      "Epoch 11, Batch 120, Test Loss: 0.4984087347984314\n",
      "Epoch 11, Batch 121, Test Loss: 0.3420303463935852\n",
      "Epoch 11, Batch 122, Test Loss: 0.4013696610927582\n",
      "Epoch 11, Batch 123, Test Loss: 0.5265437364578247\n",
      "Epoch 11, Batch 124, Test Loss: 0.5245263576507568\n",
      "Epoch 11, Batch 125, Test Loss: 0.5698847770690918\n",
      "Epoch 11, Batch 126, Test Loss: 0.5897027850151062\n",
      "Epoch 11, Batch 127, Test Loss: 0.5499074459075928\n",
      "Epoch 11, Batch 128, Test Loss: 0.35999083518981934\n",
      "Epoch 11, Batch 129, Test Loss: 0.5681033134460449\n",
      "Epoch 11, Batch 130, Test Loss: 0.40322333574295044\n",
      "Epoch 11, Batch 131, Test Loss: 0.5255017280578613\n",
      "Epoch 11, Batch 132, Test Loss: 0.5416531562805176\n",
      "Epoch 11, Batch 133, Test Loss: 0.5218526721000671\n",
      "Epoch 11, Batch 134, Test Loss: 0.44337084889411926\n",
      "Epoch 11, Batch 135, Test Loss: 0.4218703806400299\n",
      "Epoch 11, Batch 136, Test Loss: 0.4662705063819885\n",
      "Epoch 11, Batch 137, Test Loss: 0.6248053908348083\n",
      "Epoch 11, Batch 138, Test Loss: 0.5122746229171753\n",
      "Epoch 11, Batch 139, Test Loss: 0.6031669974327087\n",
      "Epoch 11, Batch 140, Test Loss: 0.5587088465690613\n",
      "Epoch 11, Batch 141, Test Loss: 0.4869157075881958\n",
      "Epoch 11, Batch 142, Test Loss: 0.6951526999473572\n",
      "Epoch 11, Batch 143, Test Loss: 0.47370195388793945\n",
      "Epoch 11, Batch 144, Test Loss: 0.5429335236549377\n",
      "Epoch 11, Batch 145, Test Loss: 0.4450744390487671\n",
      "Epoch 11, Batch 146, Test Loss: 0.43153148889541626\n",
      "Epoch 11, Batch 147, Test Loss: 0.5904720425605774\n",
      "Epoch 11, Batch 148, Test Loss: 0.6218743920326233\n",
      "Epoch 11, Batch 149, Test Loss: 0.5036339163780212\n",
      "Epoch 11, Batch 150, Test Loss: 0.49909448623657227\n",
      "Epoch 11, Batch 151, Test Loss: 0.628957986831665\n",
      "Epoch 11, Batch 152, Test Loss: 0.483054518699646\n",
      "Epoch 11, Batch 153, Test Loss: 0.6124328970909119\n",
      "Epoch 11, Batch 154, Test Loss: 0.45932650566101074\n",
      "Epoch 11, Batch 155, Test Loss: 0.5117197632789612\n",
      "Epoch 11, Batch 156, Test Loss: 0.32377803325653076\n",
      "Epoch 11, Batch 157, Test Loss: 0.3877407908439636\n",
      "Epoch 11, Batch 158, Test Loss: 0.456775039434433\n",
      "Epoch 11, Batch 159, Test Loss: 0.5950735211372375\n",
      "Epoch 11, Batch 160, Test Loss: 0.5423193573951721\n",
      "Epoch 11, Batch 161, Test Loss: 0.6145086288452148\n",
      "Epoch 11, Batch 162, Test Loss: 0.3878842890262604\n",
      "Epoch 11, Batch 163, Test Loss: 0.43757206201553345\n",
      "Epoch 11, Batch 164, Test Loss: 0.5431728363037109\n",
      "Epoch 11, Batch 165, Test Loss: 0.4064229428768158\n",
      "Epoch 11, Batch 166, Test Loss: 0.24053409695625305\n",
      "Epoch 11, Batch 167, Test Loss: 0.3436605930328369\n",
      "Epoch 11, Batch 168, Test Loss: 0.5062218308448792\n",
      "Epoch 11, Batch 169, Test Loss: 0.7847462296485901\n",
      "Epoch 11, Batch 170, Test Loss: 0.4805634617805481\n",
      "Epoch 11, Batch 171, Test Loss: 0.6258408427238464\n",
      "Epoch 11, Batch 172, Test Loss: 0.32365721464157104\n",
      "Epoch 11, Batch 173, Test Loss: 0.6278650164604187\n",
      "Epoch 11, Batch 174, Test Loss: 0.4983863830566406\n",
      "Epoch 11, Batch 175, Test Loss: 0.3940320611000061\n",
      "Epoch 11, Batch 176, Test Loss: 0.4798935055732727\n",
      "Epoch 11, Batch 177, Test Loss: 0.5081225037574768\n",
      "Epoch 11, Batch 178, Test Loss: 0.3118688762187958\n",
      "Epoch 11, Batch 179, Test Loss: 0.3609778881072998\n",
      "Epoch 11, Batch 180, Test Loss: 0.5734182596206665\n",
      "Epoch 11, Batch 181, Test Loss: 0.5286779403686523\n",
      "Epoch 11, Batch 182, Test Loss: 0.6464828252792358\n",
      "Epoch 11, Batch 183, Test Loss: 0.5532733798027039\n",
      "Epoch 11, Batch 184, Test Loss: 0.5733832120895386\n",
      "Epoch 11, Batch 185, Test Loss: 0.4895634055137634\n",
      "Epoch 11, Batch 186, Test Loss: 0.4613121747970581\n",
      "Epoch 11, Batch 187, Test Loss: 0.6296334862709045\n",
      "Epoch 11, Batch 188, Test Loss: 0.6615985631942749\n",
      "Epoch 11, Batch 189, Test Loss: 0.5134737491607666\n",
      "Epoch 11, Batch 190, Test Loss: 0.47840985655784607\n",
      "Epoch 11, Batch 191, Test Loss: 0.5627716779708862\n",
      "Epoch 11, Batch 192, Test Loss: 0.53980553150177\n",
      "Epoch 11, Batch 193, Test Loss: 0.5505331754684448\n",
      "Epoch 11, Batch 194, Test Loss: 0.430855393409729\n",
      "Epoch 11, Batch 195, Test Loss: 0.7579442858695984\n",
      "Epoch 11, Batch 196, Test Loss: 0.4286959767341614\n",
      "Epoch 11, Batch 197, Test Loss: 0.4193449914455414\n",
      "Epoch 11, Batch 198, Test Loss: 0.6546319723129272\n",
      "Epoch 11, Batch 199, Test Loss: 0.4678072929382324\n",
      "Epoch 11, Batch 200, Test Loss: 0.35467031598091125\n",
      "Epoch 11, Batch 201, Test Loss: 0.3865433633327484\n",
      "Epoch 11, Batch 202, Test Loss: 0.5715272426605225\n",
      "Epoch 11, Batch 203, Test Loss: 0.5067607760429382\n",
      "Epoch 11, Batch 204, Test Loss: 0.35710182785987854\n",
      "Epoch 11, Batch 205, Test Loss: 0.5835590362548828\n",
      "Epoch 11, Batch 206, Test Loss: 0.44879382848739624\n",
      "Epoch 11, Batch 207, Test Loss: 0.5403538942337036\n",
      "Epoch 11, Batch 208, Test Loss: 0.39244046807289124\n",
      "Epoch 11, Batch 209, Test Loss: 0.5518802404403687\n",
      "Epoch 11, Batch 210, Test Loss: 0.3638085126876831\n",
      "Epoch 11, Batch 211, Test Loss: 0.5935989022254944\n",
      "Epoch 11, Batch 212, Test Loss: 0.5282000303268433\n",
      "Epoch 11, Batch 213, Test Loss: 0.5341470837593079\n",
      "Epoch 11, Batch 214, Test Loss: 0.6084945797920227\n",
      "Epoch 11, Batch 215, Test Loss: 0.63607257604599\n",
      "Epoch 11, Batch 216, Test Loss: 0.41117048263549805\n",
      "Epoch 11, Batch 217, Test Loss: 0.3478702902793884\n",
      "Epoch 11, Batch 218, Test Loss: 0.7395013570785522\n",
      "Epoch 11, Batch 219, Test Loss: 0.6591343879699707\n",
      "Epoch 11, Batch 220, Test Loss: 0.5473737716674805\n",
      "Epoch 11, Batch 221, Test Loss: 0.47808587551116943\n",
      "Epoch 11, Batch 222, Test Loss: 0.560791015625\n",
      "Epoch 11, Batch 223, Test Loss: 0.4641527235507965\n",
      "Epoch 11, Batch 224, Test Loss: 0.4047580659389496\n",
      "Epoch 11, Batch 225, Test Loss: 0.867821991443634\n",
      "Epoch 11, Batch 226, Test Loss: 0.38553479313850403\n",
      "Epoch 11, Batch 227, Test Loss: 0.5342147350311279\n",
      "Epoch 11, Batch 228, Test Loss: 0.4840790927410126\n",
      "Epoch 11, Batch 229, Test Loss: 0.7317857146263123\n",
      "Epoch 11, Batch 230, Test Loss: 0.34514120221138\n",
      "Epoch 11, Batch 231, Test Loss: 0.8515012860298157\n",
      "Epoch 11, Batch 232, Test Loss: 0.5745528340339661\n",
      "Epoch 11, Batch 233, Test Loss: 0.4582047164440155\n",
      "Epoch 11, Batch 234, Test Loss: 0.5229832530021667\n",
      "Epoch 11, Batch 235, Test Loss: 0.7491858005523682\n",
      "Epoch 11, Batch 236, Test Loss: 0.6480991244316101\n",
      "Epoch 11, Batch 237, Test Loss: 0.4807148575782776\n",
      "Epoch 11, Batch 238, Test Loss: 0.4943698048591614\n",
      "Epoch 11, Batch 239, Test Loss: 0.4875527024269104\n",
      "Epoch 11, Batch 240, Test Loss: 0.5802304744720459\n",
      "Epoch 11, Batch 241, Test Loss: 0.554999828338623\n",
      "Epoch 11, Batch 242, Test Loss: 0.7188612222671509\n",
      "Epoch 11, Batch 243, Test Loss: 0.5697176456451416\n",
      "Epoch 11, Batch 244, Test Loss: 0.4831850230693817\n",
      "Epoch 11, Batch 245, Test Loss: 0.416248083114624\n",
      "Epoch 11, Batch 246, Test Loss: 0.40893054008483887\n",
      "Epoch 11, Batch 247, Test Loss: 0.3883763551712036\n",
      "Epoch 11, Batch 248, Test Loss: 0.4300119876861572\n",
      "Epoch 11, Batch 249, Test Loss: 0.39051130414009094\n",
      "Epoch 11, Batch 250, Test Loss: 0.490639865398407\n",
      "Epoch 11, Batch 251, Test Loss: 0.4704905152320862\n",
      "Epoch 11, Batch 252, Test Loss: 0.5426322817802429\n",
      "Epoch 11, Batch 253, Test Loss: 0.36224108934402466\n",
      "Epoch 11, Batch 254, Test Loss: 0.41661009192466736\n",
      "Epoch 11, Batch 255, Test Loss: 0.47569072246551514\n",
      "Epoch 11, Batch 256, Test Loss: 0.4838752746582031\n",
      "Epoch 11, Batch 257, Test Loss: 0.5603717565536499\n",
      "Epoch 11, Batch 258, Test Loss: 0.549182653427124\n",
      "Epoch 11, Batch 259, Test Loss: 0.6244012117385864\n",
      "Epoch 11, Batch 260, Test Loss: 0.412117063999176\n",
      "Epoch 11, Batch 261, Test Loss: 0.5656846761703491\n",
      "Epoch 11, Batch 262, Test Loss: 0.2891361117362976\n",
      "Epoch 11, Batch 263, Test Loss: 0.3945235311985016\n",
      "Epoch 11, Batch 264, Test Loss: 0.4210832715034485\n",
      "Epoch 11, Batch 265, Test Loss: 0.4064631164073944\n",
      "Epoch 11, Batch 266, Test Loss: 0.4441482722759247\n",
      "Epoch 11, Batch 267, Test Loss: 0.4672914743423462\n",
      "Epoch 11, Batch 268, Test Loss: 0.42982637882232666\n",
      "Epoch 11, Batch 269, Test Loss: 0.39665859937667847\n",
      "Epoch 11, Batch 270, Test Loss: 0.3773294985294342\n",
      "Epoch 11, Batch 271, Test Loss: 0.46434757113456726\n",
      "Epoch 11, Batch 272, Test Loss: 0.5599612593650818\n",
      "Epoch 11, Batch 273, Test Loss: 0.40981772541999817\n",
      "Epoch 11, Batch 274, Test Loss: 0.5689092874526978\n",
      "Epoch 11, Batch 275, Test Loss: 0.4411843419075012\n",
      "Epoch 11, Batch 276, Test Loss: 0.48785027861595154\n",
      "Epoch 11, Batch 277, Test Loss: 0.46188321709632874\n",
      "Epoch 11, Batch 278, Test Loss: 0.3739829659461975\n",
      "Epoch 11, Batch 279, Test Loss: 0.4556950628757477\n",
      "Epoch 11, Batch 280, Test Loss: 0.3442201018333435\n",
      "Epoch 11, Batch 281, Test Loss: 0.37082332372665405\n",
      "Epoch 11, Batch 282, Test Loss: 0.4221564829349518\n",
      "Epoch 11, Batch 283, Test Loss: 0.6251997947692871\n",
      "Epoch 11, Batch 284, Test Loss: 0.6320551037788391\n",
      "Epoch 11, Batch 285, Test Loss: 0.47838523983955383\n",
      "Epoch 11, Batch 286, Test Loss: 0.4282929599285126\n",
      "Epoch 11, Batch 287, Test Loss: 0.5034844279289246\n",
      "Epoch 11, Batch 288, Test Loss: 0.44711631536483765\n",
      "Epoch 11, Batch 289, Test Loss: 0.428067684173584\n",
      "Epoch 11, Batch 290, Test Loss: 0.6555226445198059\n",
      "Epoch 11, Batch 291, Test Loss: 0.45080938935279846\n",
      "Epoch 11, Batch 292, Test Loss: 0.42529594898223877\n",
      "Epoch 11, Batch 293, Test Loss: 0.5162138938903809\n",
      "Epoch 11, Batch 294, Test Loss: 0.47103309631347656\n",
      "Epoch 11, Batch 295, Test Loss: 0.4851396083831787\n",
      "Epoch 11, Batch 296, Test Loss: 0.47846269607543945\n",
      "Epoch 11, Batch 297, Test Loss: 0.49979960918426514\n",
      "Epoch 11, Batch 298, Test Loss: 0.35416945815086365\n",
      "Epoch 11, Batch 299, Test Loss: 0.405428946018219\n",
      "Epoch 11, Batch 300, Test Loss: 0.4398605525493622\n",
      "Epoch 11, Batch 301, Test Loss: 0.4551886320114136\n",
      "Epoch 11, Batch 302, Test Loss: 0.420074462890625\n",
      "Epoch 11, Batch 303, Test Loss: 0.5081713199615479\n",
      "Epoch 11, Batch 304, Test Loss: 0.5837533473968506\n",
      "Epoch 11, Batch 305, Test Loss: 0.46220821142196655\n",
      "Epoch 11, Batch 306, Test Loss: 0.49712157249450684\n",
      "Epoch 11, Batch 307, Test Loss: 0.47437790036201477\n",
      "Epoch 11, Batch 308, Test Loss: 0.6106814742088318\n",
      "Epoch 11, Batch 309, Test Loss: 0.40936729311943054\n",
      "Epoch 11, Batch 310, Test Loss: 0.4324336647987366\n",
      "Epoch 11, Batch 311, Test Loss: 0.42051684856414795\n",
      "Epoch 11, Batch 312, Test Loss: 0.7366478443145752\n",
      "Epoch 11, Batch 313, Test Loss: 0.7392147183418274\n",
      "Epoch 11, Batch 314, Test Loss: 0.4339864253997803\n",
      "Epoch 11, Batch 315, Test Loss: 0.5253280401229858\n",
      "Epoch 11, Batch 316, Test Loss: 0.42871806025505066\n",
      "Epoch 11, Batch 317, Test Loss: 0.6284695863723755\n",
      "Epoch 11, Batch 318, Test Loss: 0.47583016753196716\n",
      "Epoch 11, Batch 319, Test Loss: 0.4963623881340027\n",
      "Epoch 11, Batch 320, Test Loss: 0.42106783390045166\n",
      "Epoch 11, Batch 321, Test Loss: 0.5430260896682739\n",
      "Epoch 11, Batch 322, Test Loss: 0.5161336064338684\n",
      "Epoch 11, Batch 323, Test Loss: 0.5594725012779236\n",
      "Epoch 11, Batch 324, Test Loss: 0.5251753330230713\n",
      "Epoch 11, Batch 325, Test Loss: 0.4460145831108093\n",
      "Epoch 11, Batch 326, Test Loss: 0.37638601660728455\n",
      "Epoch 11, Batch 327, Test Loss: 0.7088531851768494\n",
      "Epoch 11, Batch 328, Test Loss: 0.498434841632843\n",
      "Epoch 11, Batch 329, Test Loss: 0.6024836897850037\n",
      "Epoch 11, Batch 330, Test Loss: 0.638626217842102\n",
      "Epoch 11, Batch 331, Test Loss: 0.4203771948814392\n",
      "Epoch 11, Batch 332, Test Loss: 0.4339664876461029\n",
      "Epoch 11, Batch 333, Test Loss: 0.39914652705192566\n",
      "Epoch 11, Batch 334, Test Loss: 0.33482611179351807\n",
      "Epoch 11, Batch 335, Test Loss: 0.2676069736480713\n",
      "Epoch 11, Batch 336, Test Loss: 0.5595294237136841\n",
      "Epoch 11, Batch 337, Test Loss: 0.665318489074707\n",
      "Epoch 11, Batch 338, Test Loss: 0.4430197775363922\n",
      "Epoch 11, Batch 339, Test Loss: 0.35860899090766907\n",
      "Epoch 11, Batch 340, Test Loss: 0.4898037612438202\n",
      "Epoch 11, Batch 341, Test Loss: 0.47230446338653564\n",
      "Epoch 11, Batch 342, Test Loss: 0.31151461601257324\n",
      "Epoch 11, Batch 343, Test Loss: 0.4709256887435913\n",
      "Epoch 11, Batch 344, Test Loss: 0.5301234126091003\n",
      "Epoch 11, Batch 345, Test Loss: 0.368175745010376\n",
      "Epoch 11, Batch 346, Test Loss: 0.42881855368614197\n",
      "Epoch 11, Batch 347, Test Loss: 0.5080732107162476\n",
      "Epoch 11, Batch 348, Test Loss: 0.3124346137046814\n",
      "Epoch 11, Batch 349, Test Loss: 0.43355807662010193\n",
      "Epoch 11, Batch 350, Test Loss: 0.7930183410644531\n",
      "Epoch 11, Batch 351, Test Loss: 0.48481446504592896\n",
      "Epoch 11, Batch 352, Test Loss: 0.4625880718231201\n",
      "Epoch 11, Batch 353, Test Loss: 0.4813690185546875\n",
      "Epoch 11, Batch 354, Test Loss: 0.4455185532569885\n",
      "Epoch 11, Batch 355, Test Loss: 0.3863699436187744\n",
      "Epoch 11, Batch 356, Test Loss: 0.5547335743904114\n",
      "Epoch 11, Batch 357, Test Loss: 0.7971153259277344\n",
      "Epoch 11, Batch 358, Test Loss: 0.44388872385025024\n",
      "Epoch 11, Batch 359, Test Loss: 0.520427405834198\n",
      "Epoch 11, Batch 360, Test Loss: 0.544359028339386\n",
      "Epoch 11, Batch 361, Test Loss: 0.513770580291748\n",
      "Epoch 11, Batch 362, Test Loss: 0.6127676963806152\n",
      "Epoch 11, Batch 363, Test Loss: 0.5362751483917236\n",
      "Epoch 11, Batch 364, Test Loss: 0.48885172605514526\n",
      "Epoch 11, Batch 365, Test Loss: 0.4844092130661011\n",
      "Epoch 11, Batch 366, Test Loss: 0.7698082327842712\n",
      "Epoch 11, Batch 367, Test Loss: 0.3657454252243042\n",
      "Epoch 11, Batch 368, Test Loss: 0.4678027033805847\n",
      "Epoch 11, Batch 369, Test Loss: 0.5121659636497498\n",
      "Epoch 11, Batch 370, Test Loss: 0.4744603633880615\n",
      "Epoch 11, Batch 371, Test Loss: 0.5722856521606445\n",
      "Epoch 11, Batch 372, Test Loss: 0.5238679647445679\n",
      "Epoch 11, Batch 373, Test Loss: 0.4118722081184387\n",
      "Epoch 11, Batch 374, Test Loss: 0.6078789234161377\n",
      "Epoch 11, Batch 375, Test Loss: 0.49869105219841003\n",
      "Epoch 11, Batch 376, Test Loss: 0.4081605076789856\n",
      "Epoch 11, Batch 377, Test Loss: 0.5411282181739807\n",
      "Epoch 11, Batch 378, Test Loss: 0.4769471287727356\n",
      "Epoch 11, Batch 379, Test Loss: 0.4833624064922333\n",
      "Epoch 11, Batch 380, Test Loss: 0.4369269013404846\n",
      "Epoch 11, Batch 381, Test Loss: 0.4082339406013489\n",
      "Epoch 11, Batch 382, Test Loss: 0.501372218132019\n",
      "Epoch 11, Batch 383, Test Loss: 0.6505839824676514\n",
      "Epoch 11, Batch 384, Test Loss: 0.4783702492713928\n",
      "Epoch 11, Batch 385, Test Loss: 0.3178229331970215\n",
      "Epoch 11, Batch 386, Test Loss: 0.6275518536567688\n",
      "Epoch 11, Batch 387, Test Loss: 0.5952571630477905\n",
      "Epoch 11, Batch 388, Test Loss: 0.719951868057251\n",
      "Epoch 11, Batch 389, Test Loss: 0.679896891117096\n",
      "Epoch 11, Batch 390, Test Loss: 0.4677371382713318\n",
      "Epoch 11, Batch 391, Test Loss: 0.4271882176399231\n",
      "Epoch 11, Batch 392, Test Loss: 0.5234959721565247\n",
      "Epoch 11, Batch 393, Test Loss: 0.46154022216796875\n",
      "Epoch 11, Batch 394, Test Loss: 0.4531845152378082\n",
      "Epoch 11, Batch 395, Test Loss: 0.5616551637649536\n",
      "Epoch 11, Batch 396, Test Loss: 0.44970011711120605\n",
      "Epoch 11, Batch 397, Test Loss: 0.5355937480926514\n",
      "Epoch 11, Batch 398, Test Loss: 0.6695619821548462\n",
      "Epoch 11, Batch 399, Test Loss: 0.40824443101882935\n",
      "Epoch 11, Batch 400, Test Loss: 0.47856950759887695\n",
      "Epoch 11, Batch 401, Test Loss: 0.454806923866272\n",
      "Epoch 11, Batch 402, Test Loss: 0.5466452836990356\n",
      "Epoch 11, Batch 403, Test Loss: 0.37839287519454956\n",
      "Epoch 11, Batch 404, Test Loss: 0.5368953943252563\n",
      "Epoch 11, Batch 405, Test Loss: 0.557580828666687\n",
      "Epoch 11, Batch 406, Test Loss: 0.7544466853141785\n",
      "Epoch 11, Batch 407, Test Loss: 0.30260169506073\n",
      "Epoch 11, Batch 408, Test Loss: 0.525371789932251\n",
      "Epoch 11, Batch 409, Test Loss: 0.7146376371383667\n",
      "Epoch 11, Batch 410, Test Loss: 0.3621065318584442\n",
      "Epoch 11, Batch 411, Test Loss: 0.4166528284549713\n",
      "Epoch 11, Batch 412, Test Loss: 0.49089136719703674\n",
      "Epoch 11, Batch 413, Test Loss: 0.5187002420425415\n",
      "Epoch 11, Batch 414, Test Loss: 0.3713190257549286\n",
      "Epoch 11, Batch 415, Test Loss: 0.5236439108848572\n",
      "Epoch 11, Batch 416, Test Loss: 0.619258463382721\n",
      "Epoch 11, Batch 417, Test Loss: 0.38562434911727905\n",
      "Epoch 11, Batch 418, Test Loss: 0.31678998470306396\n",
      "Epoch 11, Batch 419, Test Loss: 0.625182032585144\n",
      "Epoch 11, Batch 420, Test Loss: 0.5757628083229065\n",
      "Epoch 11, Batch 421, Test Loss: 0.5294396877288818\n",
      "Epoch 11, Batch 422, Test Loss: 0.47899460792541504\n",
      "Epoch 11, Batch 423, Test Loss: 0.7009554505348206\n",
      "Epoch 11, Batch 424, Test Loss: 0.46786296367645264\n",
      "Epoch 11, Batch 425, Test Loss: 0.394878625869751\n",
      "Epoch 11, Batch 426, Test Loss: 0.6295608878135681\n",
      "Epoch 11, Batch 427, Test Loss: 0.6082863211631775\n",
      "Epoch 11, Batch 428, Test Loss: 0.561215341091156\n",
      "Epoch 11, Batch 429, Test Loss: 0.4237349033355713\n",
      "Epoch 11, Batch 430, Test Loss: 0.485215961933136\n",
      "Epoch 11, Batch 431, Test Loss: 0.8163151144981384\n",
      "Epoch 11, Batch 432, Test Loss: 0.6184713244438171\n",
      "Epoch 11, Batch 433, Test Loss: 0.4947051703929901\n",
      "Epoch 11, Batch 434, Test Loss: 0.3188449740409851\n",
      "Epoch 11, Batch 435, Test Loss: 0.5769401788711548\n",
      "Epoch 11, Batch 436, Test Loss: 0.5241344571113586\n",
      "Epoch 11, Batch 437, Test Loss: 0.5135687589645386\n",
      "Epoch 11, Batch 438, Test Loss: 0.4472079873085022\n",
      "Epoch 11, Batch 439, Test Loss: 0.40547412633895874\n",
      "Epoch 11, Batch 440, Test Loss: 0.4922664165496826\n",
      "Epoch 11, Batch 441, Test Loss: 0.5496325492858887\n",
      "Epoch 11, Batch 442, Test Loss: 0.44488176703453064\n",
      "Epoch 11, Batch 443, Test Loss: 0.3948439955711365\n",
      "Epoch 11, Batch 444, Test Loss: 0.46968376636505127\n",
      "Epoch 11, Batch 445, Test Loss: 0.42034924030303955\n",
      "Epoch 11, Batch 446, Test Loss: 0.5041934847831726\n",
      "Epoch 11, Batch 447, Test Loss: 0.5131633281707764\n",
      "Epoch 11, Batch 448, Test Loss: 0.4693225622177124\n",
      "Epoch 11, Batch 449, Test Loss: 0.5542503595352173\n",
      "Epoch 11, Batch 450, Test Loss: 0.3459227979183197\n",
      "Epoch 11, Batch 451, Test Loss: 0.5273678302764893\n",
      "Epoch 11, Batch 452, Test Loss: 0.36426588892936707\n",
      "Epoch 11, Batch 453, Test Loss: 0.41048040986061096\n",
      "Epoch 11, Batch 454, Test Loss: 0.6164643168449402\n",
      "Epoch 11, Batch 455, Test Loss: 0.4416908919811249\n",
      "Epoch 11, Batch 456, Test Loss: 0.4987163245677948\n",
      "Epoch 11, Batch 457, Test Loss: 0.386140376329422\n",
      "Epoch 11, Batch 458, Test Loss: 0.4857133626937866\n",
      "Epoch 11, Batch 459, Test Loss: 0.6331742405891418\n",
      "Epoch 11, Batch 460, Test Loss: 0.6714169979095459\n",
      "Epoch 11, Batch 461, Test Loss: 0.7161251902580261\n",
      "Epoch 11, Batch 462, Test Loss: 0.4043654501438141\n",
      "Epoch 11, Batch 463, Test Loss: 0.5208749175071716\n",
      "Epoch 11, Batch 464, Test Loss: 0.41152265667915344\n",
      "Epoch 11, Batch 465, Test Loss: 0.4624214172363281\n",
      "Epoch 11, Batch 466, Test Loss: 0.39183953404426575\n",
      "Epoch 11, Batch 467, Test Loss: 0.4705696105957031\n",
      "Epoch 11, Batch 468, Test Loss: 0.5145658254623413\n",
      "Epoch 11, Batch 469, Test Loss: 0.40842586755752563\n",
      "Epoch 11, Batch 470, Test Loss: 0.36004844307899475\n",
      "Epoch 11, Batch 471, Test Loss: 0.39346617460250854\n",
      "Epoch 11, Batch 472, Test Loss: 0.5291380286216736\n",
      "Epoch 11, Batch 473, Test Loss: 0.6861924529075623\n",
      "Epoch 11, Batch 474, Test Loss: 0.5323123931884766\n",
      "Epoch 11, Batch 475, Test Loss: 0.7338364720344543\n",
      "Epoch 11, Batch 476, Test Loss: 0.5983814597129822\n",
      "Epoch 11, Batch 477, Test Loss: 0.5231012105941772\n",
      "Epoch 11, Batch 478, Test Loss: 0.526788055896759\n",
      "Epoch 11, Batch 479, Test Loss: 0.3146078288555145\n",
      "Epoch 11, Batch 480, Test Loss: 0.45355668663978577\n",
      "Epoch 11, Batch 481, Test Loss: 0.5806061625480652\n",
      "Epoch 11, Batch 482, Test Loss: 0.36182889342308044\n",
      "Epoch 11, Batch 483, Test Loss: 0.5143075585365295\n",
      "Epoch 11, Batch 484, Test Loss: 0.48408669233322144\n",
      "Epoch 11, Batch 485, Test Loss: 0.5502505898475647\n",
      "Epoch 11, Batch 486, Test Loss: 0.4206942319869995\n",
      "Epoch 11, Batch 487, Test Loss: 0.43523430824279785\n",
      "Epoch 11, Batch 488, Test Loss: 0.5020669102668762\n",
      "Epoch 11, Batch 489, Test Loss: 0.39216354489326477\n",
      "Epoch 11, Batch 490, Test Loss: 0.5092803835868835\n",
      "Epoch 11, Batch 491, Test Loss: 0.5156781673431396\n",
      "Epoch 11, Batch 492, Test Loss: 0.4156053066253662\n",
      "Epoch 11, Batch 493, Test Loss: 0.42089056968688965\n",
      "Epoch 11, Batch 494, Test Loss: 0.7129818201065063\n",
      "Epoch 11, Batch 495, Test Loss: 0.5718443393707275\n",
      "Epoch 11, Batch 496, Test Loss: 0.5270951986312866\n",
      "Epoch 11, Batch 497, Test Loss: 0.3683074712753296\n",
      "Epoch 11, Batch 498, Test Loss: 0.4798591434955597\n",
      "Epoch 11, Batch 499, Test Loss: 0.4950244426727295\n",
      "Epoch 11, Batch 500, Test Loss: 0.5765334367752075\n",
      "Epoch 11, Batch 501, Test Loss: 0.5122981071472168\n",
      "Epoch 11, Batch 502, Test Loss: 0.5824933052062988\n",
      "Epoch 11, Batch 503, Test Loss: 0.6696733236312866\n",
      "Epoch 11, Batch 504, Test Loss: 0.4467884302139282\n",
      "Epoch 11, Batch 505, Test Loss: 0.4599703848361969\n",
      "Epoch 11, Batch 506, Test Loss: 0.6091090440750122\n",
      "Epoch 11, Batch 507, Test Loss: 0.5880496501922607\n",
      "Epoch 11, Batch 508, Test Loss: 0.4116190969944\n",
      "Epoch 11, Batch 509, Test Loss: 0.49383029341697693\n",
      "Epoch 11, Batch 510, Test Loss: 0.6622129678726196\n",
      "Epoch 11, Batch 511, Test Loss: 0.4792563319206238\n",
      "Epoch 11, Batch 512, Test Loss: 0.4661618173122406\n",
      "Epoch 11, Batch 513, Test Loss: 0.4201701581478119\n",
      "Epoch 11, Batch 514, Test Loss: 0.7703331708908081\n",
      "Epoch 11, Batch 515, Test Loss: 0.7319122552871704\n",
      "Epoch 11, Batch 516, Test Loss: 0.42475375533103943\n",
      "Epoch 11, Batch 517, Test Loss: 0.5623077154159546\n",
      "Epoch 11, Batch 518, Test Loss: 0.42270928621292114\n",
      "Epoch 11, Batch 519, Test Loss: 0.3486991226673126\n",
      "Epoch 11, Batch 520, Test Loss: 0.6646237373352051\n",
      "Epoch 11, Batch 521, Test Loss: 0.3222407102584839\n",
      "Epoch 11, Batch 522, Test Loss: 0.5992290377616882\n",
      "Epoch 11, Batch 523, Test Loss: 0.5712536573410034\n",
      "Epoch 11, Batch 524, Test Loss: 0.4064408242702484\n",
      "Epoch 11, Batch 525, Test Loss: 0.6571746468544006\n",
      "Epoch 11, Batch 526, Test Loss: 0.5375404357910156\n",
      "Epoch 11, Batch 527, Test Loss: 0.6217963695526123\n",
      "Epoch 11, Batch 528, Test Loss: 0.4933604300022125\n",
      "Epoch 11, Batch 529, Test Loss: 0.4494004249572754\n",
      "Epoch 11, Batch 530, Test Loss: 0.416273295879364\n",
      "Epoch 11, Batch 531, Test Loss: 0.3301168978214264\n",
      "Epoch 11, Batch 532, Test Loss: 0.28428977727890015\n",
      "Epoch 11, Batch 533, Test Loss: 0.4258027672767639\n",
      "Epoch 11, Batch 534, Test Loss: 0.5055105090141296\n",
      "Epoch 11, Batch 535, Test Loss: 0.403506338596344\n",
      "Epoch 11, Batch 536, Test Loss: 0.4360463619232178\n",
      "Epoch 11, Batch 537, Test Loss: 0.532178521156311\n",
      "Epoch 11, Batch 538, Test Loss: 0.4357304573059082\n",
      "Epoch 11, Batch 539, Test Loss: 0.4969860315322876\n",
      "Epoch 11, Batch 540, Test Loss: 0.3125486373901367\n",
      "Epoch 11, Batch 541, Test Loss: 0.39019104838371277\n",
      "Epoch 11, Batch 542, Test Loss: 0.3555166721343994\n",
      "Epoch 11, Batch 543, Test Loss: 0.4273872375488281\n",
      "Epoch 11, Batch 544, Test Loss: 0.5264623165130615\n",
      "Epoch 11, Batch 545, Test Loss: 0.672005295753479\n",
      "Epoch 11, Batch 546, Test Loss: 0.37633177638053894\n",
      "Epoch 11, Batch 547, Test Loss: 0.691476583480835\n",
      "Epoch 11, Batch 548, Test Loss: 0.5833233594894409\n",
      "Epoch 11, Batch 549, Test Loss: 0.6573531031608582\n",
      "Epoch 11, Batch 550, Test Loss: 0.6760125756263733\n",
      "Epoch 11, Batch 551, Test Loss: 0.44021812081336975\n",
      "Epoch 11, Batch 552, Test Loss: 0.5557560920715332\n",
      "Epoch 11, Batch 553, Test Loss: 0.3369468152523041\n",
      "Epoch 11, Batch 554, Test Loss: 0.6036679744720459\n",
      "Epoch 11, Batch 555, Test Loss: 0.5617402791976929\n",
      "Epoch 11, Batch 556, Test Loss: 0.6394251585006714\n",
      "Epoch 11, Batch 557, Test Loss: 0.40069136023521423\n",
      "Epoch 11, Batch 558, Test Loss: 0.4512552320957184\n",
      "Epoch 11, Batch 559, Test Loss: 0.42459189891815186\n",
      "Epoch 11, Batch 560, Test Loss: 0.3439948558807373\n",
      "Epoch 11, Batch 561, Test Loss: 0.41585034132003784\n",
      "Epoch 11, Batch 562, Test Loss: 0.37227460741996765\n",
      "Epoch 11, Batch 563, Test Loss: 0.46996620297431946\n",
      "Epoch 11, Batch 564, Test Loss: 0.4584904611110687\n",
      "Epoch 11, Batch 565, Test Loss: 0.3608197569847107\n",
      "Epoch 11, Batch 566, Test Loss: 0.5294588804244995\n",
      "Epoch 11, Batch 567, Test Loss: 0.658322274684906\n",
      "Epoch 11, Batch 568, Test Loss: 0.5602646470069885\n",
      "Epoch 11, Batch 569, Test Loss: 0.36807090044021606\n",
      "Epoch 11, Batch 570, Test Loss: 0.47374939918518066\n",
      "Epoch 11, Batch 571, Test Loss: 0.458384245634079\n",
      "Epoch 11, Batch 572, Test Loss: 0.44475799798965454\n",
      "Epoch 11, Batch 573, Test Loss: 0.3822363615036011\n",
      "Epoch 11, Batch 574, Test Loss: 0.4659375548362732\n",
      "Epoch 11, Batch 575, Test Loss: 0.4574381411075592\n",
      "Epoch 11, Batch 576, Test Loss: 0.572812557220459\n",
      "Epoch 11, Batch 577, Test Loss: 0.6167452931404114\n",
      "Epoch 11, Batch 578, Test Loss: 0.5046427845954895\n",
      "Epoch 11, Batch 579, Test Loss: 0.5704213976860046\n",
      "Epoch 11, Batch 580, Test Loss: 0.6421509385108948\n",
      "Epoch 11, Batch 581, Test Loss: 0.5321739315986633\n",
      "Epoch 11, Batch 582, Test Loss: 0.4455642104148865\n",
      "Epoch 11, Batch 583, Test Loss: 0.6030846834182739\n",
      "Epoch 11, Batch 584, Test Loss: 0.5674626231193542\n",
      "Epoch 11, Batch 585, Test Loss: 0.5418953895568848\n",
      "Epoch 11, Batch 586, Test Loss: 0.4969508647918701\n",
      "Epoch 11, Batch 587, Test Loss: 0.440549373626709\n",
      "Epoch 11, Batch 588, Test Loss: 0.6087448596954346\n",
      "Epoch 11, Batch 589, Test Loss: 0.5506493449211121\n",
      "Epoch 11, Batch 590, Test Loss: 0.42350468039512634\n",
      "Epoch 11, Batch 591, Test Loss: 0.5584038496017456\n",
      "Epoch 11, Batch 592, Test Loss: 0.46677660942077637\n",
      "Epoch 11, Batch 593, Test Loss: 0.3867530822753906\n",
      "Epoch 11, Batch 594, Test Loss: 0.4987766146659851\n",
      "Epoch 11, Batch 595, Test Loss: 0.7058290243148804\n",
      "Epoch 11, Batch 596, Test Loss: 0.35195785760879517\n",
      "Epoch 11, Batch 597, Test Loss: 0.46937575936317444\n",
      "Epoch 11, Batch 598, Test Loss: 0.5299302935600281\n",
      "Epoch 11, Batch 599, Test Loss: 0.40325409173965454\n",
      "Epoch 11, Batch 600, Test Loss: 0.47347235679626465\n",
      "Epoch 11, Batch 601, Test Loss: 0.34962865710258484\n",
      "Epoch 11, Batch 602, Test Loss: 0.598103404045105\n",
      "Epoch 11, Batch 603, Test Loss: 0.4135599136352539\n",
      "Epoch 11, Batch 604, Test Loss: 0.47067534923553467\n",
      "Epoch 11, Batch 605, Test Loss: 0.5213118195533752\n",
      "Epoch 11, Batch 606, Test Loss: 0.6830313801765442\n",
      "Epoch 11, Batch 607, Test Loss: 0.4075140357017517\n",
      "Epoch 11, Batch 608, Test Loss: 0.6060271263122559\n",
      "Epoch 11, Batch 609, Test Loss: 0.3554414212703705\n",
      "Epoch 11, Batch 610, Test Loss: 0.5467363595962524\n",
      "Epoch 11, Batch 611, Test Loss: 0.6216812133789062\n",
      "Epoch 11, Batch 612, Test Loss: 0.3730927109718323\n",
      "Epoch 11, Batch 613, Test Loss: 0.4551003575325012\n",
      "Epoch 11, Batch 614, Test Loss: 0.3289693593978882\n",
      "Epoch 11, Batch 615, Test Loss: 0.4669768810272217\n",
      "Epoch 11, Batch 616, Test Loss: 0.4690718352794647\n",
      "Epoch 11, Batch 617, Test Loss: 0.44671422243118286\n",
      "Epoch 11, Batch 618, Test Loss: 0.4383009970188141\n",
      "Epoch 11, Batch 619, Test Loss: 0.5019195675849915\n",
      "Epoch 11, Batch 620, Test Loss: 0.5988701581954956\n",
      "Epoch 11, Batch 621, Test Loss: 0.4478040337562561\n",
      "Epoch 11, Batch 622, Test Loss: 0.46166154742240906\n",
      "Epoch 11, Batch 623, Test Loss: 0.5287644267082214\n",
      "Epoch 11, Batch 624, Test Loss: 0.5650382041931152\n",
      "Epoch 11, Batch 625, Test Loss: 0.5040448307991028\n",
      "Epoch 11, Batch 626, Test Loss: 0.4150199890136719\n",
      "Epoch 11, Batch 627, Test Loss: 0.4562041759490967\n",
      "Epoch 11, Batch 628, Test Loss: 0.6394584774971008\n",
      "Epoch 11, Batch 629, Test Loss: 0.4841800332069397\n",
      "Epoch 11, Batch 630, Test Loss: 0.6053184866905212\n",
      "Epoch 11, Batch 631, Test Loss: 0.4321277141571045\n",
      "Epoch 11, Batch 632, Test Loss: 0.5316095352172852\n",
      "Epoch 11, Batch 633, Test Loss: 0.5752096176147461\n",
      "Epoch 11, Batch 634, Test Loss: 0.47444188594818115\n",
      "Epoch 11, Batch 635, Test Loss: 0.4367348551750183\n",
      "Epoch 11, Batch 636, Test Loss: 0.5705516934394836\n",
      "Epoch 11, Batch 637, Test Loss: 0.35637110471725464\n",
      "Epoch 11, Batch 638, Test Loss: 0.47860875725746155\n",
      "Epoch 11, Batch 639, Test Loss: 0.5226554274559021\n",
      "Epoch 11, Batch 640, Test Loss: 0.5072325468063354\n",
      "Epoch 11, Batch 641, Test Loss: 0.5773892402648926\n",
      "Epoch 11, Batch 642, Test Loss: 0.48019909858703613\n",
      "Epoch 11, Batch 643, Test Loss: 0.39037564396858215\n",
      "Epoch 11, Batch 644, Test Loss: 0.5850523710250854\n",
      "Epoch 11, Batch 645, Test Loss: 0.40411534905433655\n",
      "Epoch 11, Batch 646, Test Loss: 0.4147965908050537\n",
      "Epoch 11, Batch 647, Test Loss: 0.43710678815841675\n",
      "Epoch 11, Batch 648, Test Loss: 0.606614351272583\n",
      "Epoch 11, Batch 649, Test Loss: 0.5914974212646484\n",
      "Epoch 11, Batch 650, Test Loss: 0.45002666115760803\n",
      "Epoch 11, Batch 651, Test Loss: 0.625645101070404\n",
      "Epoch 11, Batch 652, Test Loss: 0.45871925354003906\n",
      "Epoch 11, Batch 653, Test Loss: 0.41635972261428833\n",
      "Epoch 11, Batch 654, Test Loss: 0.46956899762153625\n",
      "Epoch 11, Batch 655, Test Loss: 0.47376763820648193\n",
      "Epoch 11, Batch 656, Test Loss: 0.48802512884140015\n",
      "Epoch 11, Batch 657, Test Loss: 0.48775017261505127\n",
      "Epoch 11, Batch 658, Test Loss: 0.7316410541534424\n",
      "Epoch 11, Batch 659, Test Loss: 0.45044055581092834\n",
      "Epoch 11, Batch 660, Test Loss: 0.4708276689052582\n",
      "Epoch 11, Batch 661, Test Loss: 0.40410834550857544\n",
      "Epoch 11, Batch 662, Test Loss: 0.5702676177024841\n",
      "Epoch 11, Batch 663, Test Loss: 0.5813149213790894\n",
      "Epoch 11, Batch 664, Test Loss: 0.3281692862510681\n",
      "Epoch 11, Batch 665, Test Loss: 0.39368486404418945\n",
      "Epoch 11, Batch 666, Test Loss: 0.47633883357048035\n",
      "Epoch 11, Batch 667, Test Loss: 0.38291698694229126\n",
      "Epoch 11, Batch 668, Test Loss: 0.36310702562332153\n",
      "Epoch 11, Batch 669, Test Loss: 0.6346373558044434\n",
      "Epoch 11, Batch 670, Test Loss: 0.37595126032829285\n",
      "Epoch 11, Batch 671, Test Loss: 0.5247370004653931\n",
      "Epoch 11, Batch 672, Test Loss: 0.42944851517677307\n",
      "Epoch 11, Batch 673, Test Loss: 0.5539300441741943\n",
      "Epoch 11, Batch 674, Test Loss: 0.5208749771118164\n",
      "Epoch 11, Batch 675, Test Loss: 0.35628554224967957\n",
      "Epoch 11, Batch 676, Test Loss: 0.665066123008728\n",
      "Epoch 11, Batch 677, Test Loss: 0.731091320514679\n",
      "Epoch 11, Batch 678, Test Loss: 0.5883522033691406\n",
      "Epoch 11, Batch 679, Test Loss: 0.6212317943572998\n",
      "Epoch 11, Batch 680, Test Loss: 0.5712717175483704\n",
      "Epoch 11, Batch 681, Test Loss: 0.6478363275527954\n",
      "Epoch 11, Batch 682, Test Loss: 0.4123415946960449\n",
      "Epoch 11, Batch 683, Test Loss: 0.49815917015075684\n",
      "Epoch 11, Batch 684, Test Loss: 0.3162945806980133\n",
      "Epoch 11, Batch 685, Test Loss: 0.5659385323524475\n",
      "Epoch 11, Batch 686, Test Loss: 0.4848470091819763\n",
      "Epoch 11, Batch 687, Test Loss: 0.3793242275714874\n",
      "Epoch 11, Batch 688, Test Loss: 0.3597511649131775\n",
      "Epoch 11, Batch 689, Test Loss: 0.3784548342227936\n",
      "Epoch 11, Batch 690, Test Loss: 0.4977762699127197\n",
      "Epoch 11, Batch 691, Test Loss: 0.7958902716636658\n",
      "Epoch 11, Batch 692, Test Loss: 0.5350065231323242\n",
      "Epoch 11, Batch 693, Test Loss: 0.6333286166191101\n",
      "Epoch 11, Batch 694, Test Loss: 0.4305783808231354\n",
      "Epoch 11, Batch 695, Test Loss: 0.37125182151794434\n",
      "Epoch 11, Batch 696, Test Loss: 0.48279955983161926\n",
      "Epoch 11, Batch 697, Test Loss: 0.5693557858467102\n",
      "Epoch 11, Batch 698, Test Loss: 0.44402146339416504\n",
      "Epoch 11, Batch 699, Test Loss: 0.4227331876754761\n",
      "Epoch 11, Batch 700, Test Loss: 0.3812720775604248\n",
      "Epoch 11, Batch 701, Test Loss: 0.21422633528709412\n",
      "Epoch 11, Batch 702, Test Loss: 0.3269954323768616\n",
      "Epoch 11, Batch 703, Test Loss: 0.4910825490951538\n",
      "Epoch 11, Batch 704, Test Loss: 0.5078015923500061\n",
      "Epoch 11, Batch 705, Test Loss: 0.45836353302001953\n",
      "Epoch 11, Batch 706, Test Loss: 0.5465293526649475\n",
      "Epoch 11, Batch 707, Test Loss: 0.7612281441688538\n",
      "Epoch 11, Batch 708, Test Loss: 0.4474013149738312\n",
      "Epoch 11, Batch 709, Test Loss: 0.40194544196128845\n",
      "Epoch 11, Batch 710, Test Loss: 0.4649654030799866\n",
      "Epoch 11, Batch 711, Test Loss: 0.4850088655948639\n",
      "Epoch 11, Batch 712, Test Loss: 0.3562258780002594\n",
      "Epoch 11, Batch 713, Test Loss: 0.4318840801715851\n",
      "Epoch 11, Batch 714, Test Loss: 0.5506125688552856\n",
      "Epoch 11, Batch 715, Test Loss: 0.6489552855491638\n",
      "Epoch 11, Batch 716, Test Loss: 0.5333972573280334\n",
      "Epoch 11, Batch 717, Test Loss: 0.5458129048347473\n",
      "Epoch 11, Batch 718, Test Loss: 0.5160866379737854\n",
      "Epoch 11, Batch 719, Test Loss: 0.4242391586303711\n",
      "Epoch 11, Batch 720, Test Loss: 0.5450855493545532\n",
      "Epoch 11, Batch 721, Test Loss: 0.3888176679611206\n",
      "Epoch 11, Batch 722, Test Loss: 0.4437798261642456\n",
      "Epoch 11, Batch 723, Test Loss: 0.5047746896743774\n",
      "Epoch 11, Batch 724, Test Loss: 0.5115520358085632\n",
      "Epoch 11, Batch 725, Test Loss: 0.42746931314468384\n",
      "Epoch 11, Batch 726, Test Loss: 0.48448213934898376\n",
      "Epoch 11, Batch 727, Test Loss: 0.5777604579925537\n",
      "Epoch 11, Batch 728, Test Loss: 0.5384283065795898\n",
      "Epoch 11, Batch 729, Test Loss: 0.4145049750804901\n",
      "Epoch 11, Batch 730, Test Loss: 0.3804435431957245\n",
      "Epoch 11, Batch 731, Test Loss: 0.38767001032829285\n",
      "Epoch 11, Batch 732, Test Loss: 0.6078031063079834\n",
      "Epoch 11, Batch 733, Test Loss: 0.319299578666687\n",
      "Epoch 11, Batch 734, Test Loss: 0.3659718632698059\n",
      "Epoch 11, Batch 735, Test Loss: 0.3387983441352844\n",
      "Epoch 11, Batch 736, Test Loss: 0.5503937005996704\n",
      "Epoch 11, Batch 737, Test Loss: 0.5298744440078735\n",
      "Epoch 11, Batch 738, Test Loss: 0.5225232243537903\n",
      "Epoch 11, Batch 739, Test Loss: 0.4940827786922455\n",
      "Epoch 11, Batch 740, Test Loss: 0.5513028502464294\n",
      "Epoch 11, Batch 741, Test Loss: 0.6400379538536072\n",
      "Epoch 11, Batch 742, Test Loss: 0.5133610963821411\n",
      "Epoch 11, Batch 743, Test Loss: 0.4435432553291321\n",
      "Epoch 11, Batch 744, Test Loss: 0.5900636911392212\n",
      "Epoch 11, Batch 745, Test Loss: 0.5148894786834717\n",
      "Epoch 11, Batch 746, Test Loss: 0.4462531507015228\n",
      "Epoch 11, Batch 747, Test Loss: 0.3537106215953827\n",
      "Epoch 11, Batch 748, Test Loss: 0.47551441192626953\n",
      "Epoch 11, Batch 749, Test Loss: 0.5451899766921997\n",
      "Epoch 11, Batch 750, Test Loss: 0.42473840713500977\n",
      "Epoch 11, Batch 751, Test Loss: 0.4213736951351166\n",
      "Epoch 11, Batch 752, Test Loss: 0.5342252850532532\n",
      "Epoch 11, Batch 753, Test Loss: 0.5481578707695007\n",
      "Epoch 11, Batch 754, Test Loss: 0.7319331169128418\n",
      "Epoch 11, Batch 755, Test Loss: 0.5164301991462708\n",
      "Epoch 11, Batch 756, Test Loss: 0.7405708432197571\n",
      "Epoch 11, Batch 757, Test Loss: 0.20796087384223938\n",
      "Epoch 11, Batch 758, Test Loss: 0.4848100543022156\n",
      "Epoch 11, Batch 759, Test Loss: 0.35160940885543823\n",
      "Epoch 11, Batch 760, Test Loss: 0.5823681354522705\n",
      "Epoch 11, Batch 761, Test Loss: 0.5943924188613892\n",
      "Epoch 11, Batch 762, Test Loss: 0.43717947602272034\n",
      "Epoch 11, Batch 763, Test Loss: 0.721750020980835\n",
      "Epoch 11, Batch 764, Test Loss: 0.2917374074459076\n",
      "Epoch 11, Batch 765, Test Loss: 0.3113110065460205\n",
      "Epoch 11, Batch 766, Test Loss: 0.38425111770629883\n",
      "Epoch 11, Batch 767, Test Loss: 0.481560617685318\n",
      "Epoch 11, Batch 768, Test Loss: 0.611350953578949\n",
      "Epoch 11, Batch 769, Test Loss: 0.45231208205223083\n",
      "Epoch 11, Batch 770, Test Loss: 0.4211828410625458\n",
      "Epoch 11, Batch 771, Test Loss: 0.5399519205093384\n",
      "Epoch 11, Batch 772, Test Loss: 0.41508159041404724\n",
      "Epoch 11, Batch 773, Test Loss: 0.5648518204689026\n",
      "Epoch 11, Batch 774, Test Loss: 0.7818441390991211\n",
      "Epoch 11, Batch 775, Test Loss: 0.40580782294273376\n",
      "Epoch 11, Batch 776, Test Loss: 0.5854187607765198\n",
      "Epoch 11, Batch 777, Test Loss: 0.5207663178443909\n",
      "Epoch 11, Batch 778, Test Loss: 0.455113023519516\n",
      "Epoch 11, Batch 779, Test Loss: 0.44016212224960327\n",
      "Epoch 11, Batch 780, Test Loss: 0.4726009964942932\n",
      "Epoch 11, Batch 781, Test Loss: 0.5547709465026855\n",
      "Epoch 11, Batch 782, Test Loss: 0.48044663667678833\n",
      "Epoch 11, Batch 783, Test Loss: 0.3871949017047882\n",
      "Epoch 11, Batch 784, Test Loss: 0.35954174399375916\n",
      "Epoch 11, Batch 785, Test Loss: 0.5642465949058533\n",
      "Epoch 11, Batch 786, Test Loss: 0.3833223879337311\n",
      "Epoch 11, Batch 787, Test Loss: 0.5816628336906433\n",
      "Epoch 11, Batch 788, Test Loss: 0.4682886004447937\n",
      "Epoch 11, Batch 789, Test Loss: 0.44570812582969666\n",
      "Epoch 11, Batch 790, Test Loss: 0.598193347454071\n",
      "Epoch 11, Batch 791, Test Loss: 0.2583776116371155\n",
      "Epoch 11, Batch 792, Test Loss: 0.40921780467033386\n",
      "Epoch 11, Batch 793, Test Loss: 0.3331824839115143\n",
      "Epoch 11, Batch 794, Test Loss: 0.37811949849128723\n",
      "Epoch 11, Batch 795, Test Loss: 0.7226053476333618\n",
      "Epoch 11, Batch 796, Test Loss: 0.5093511939048767\n",
      "Epoch 11, Batch 797, Test Loss: 0.33795976638793945\n",
      "Epoch 11, Batch 798, Test Loss: 0.4140170216560364\n",
      "Epoch 11, Batch 799, Test Loss: 0.5165258049964905\n",
      "Epoch 11, Batch 800, Test Loss: 0.5552427768707275\n",
      "Epoch 11, Batch 801, Test Loss: 0.3769181966781616\n",
      "Epoch 11, Batch 802, Test Loss: 0.6805275082588196\n",
      "Epoch 11, Batch 803, Test Loss: 0.48477986454963684\n",
      "Epoch 11, Batch 804, Test Loss: 0.7089078426361084\n",
      "Epoch 11, Batch 805, Test Loss: 0.4939488470554352\n",
      "Epoch 11, Batch 806, Test Loss: 0.5537927150726318\n",
      "Epoch 11, Batch 807, Test Loss: 0.42189154028892517\n",
      "Epoch 11, Batch 808, Test Loss: 0.3614899814128876\n",
      "Epoch 11, Batch 809, Test Loss: 0.5052067637443542\n",
      "Epoch 11, Batch 810, Test Loss: 0.39295533299446106\n",
      "Epoch 11, Batch 811, Test Loss: 0.36684858798980713\n",
      "Epoch 11, Batch 812, Test Loss: 0.3109520971775055\n",
      "Epoch 11, Batch 813, Test Loss: 0.49674639105796814\n",
      "Epoch 11, Batch 814, Test Loss: 0.5576440095901489\n",
      "Epoch 11, Batch 815, Test Loss: 0.4803438186645508\n",
      "Epoch 11, Batch 816, Test Loss: 0.5425771474838257\n",
      "Epoch 11, Batch 817, Test Loss: 0.6304557919502258\n",
      "Epoch 11, Batch 818, Test Loss: 0.33884739875793457\n",
      "Epoch 11, Batch 819, Test Loss: 0.5211113691329956\n",
      "Epoch 11, Batch 820, Test Loss: 0.30773237347602844\n",
      "Epoch 11, Batch 821, Test Loss: 0.4261241555213928\n",
      "Epoch 11, Batch 822, Test Loss: 0.6421632766723633\n",
      "Epoch 11, Batch 823, Test Loss: 0.37925541400909424\n",
      "Epoch 11, Batch 824, Test Loss: 0.46384134888648987\n",
      "Epoch 11, Batch 825, Test Loss: 0.4834716320037842\n",
      "Epoch 11, Batch 826, Test Loss: 0.4462307393550873\n",
      "Epoch 11, Batch 827, Test Loss: 0.2892475724220276\n",
      "Epoch 11, Batch 828, Test Loss: 0.36799752712249756\n",
      "Epoch 11, Batch 829, Test Loss: 0.37717705965042114\n",
      "Epoch 11, Batch 830, Test Loss: 0.5234626531600952\n",
      "Epoch 11, Batch 831, Test Loss: 0.5231045484542847\n",
      "Epoch 11, Batch 832, Test Loss: 0.5145967602729797\n",
      "Epoch 11, Batch 833, Test Loss: 0.3648885488510132\n",
      "Epoch 11, Batch 834, Test Loss: 0.6470472812652588\n",
      "Epoch 11, Batch 835, Test Loss: 0.5797474980354309\n",
      "Epoch 11, Batch 836, Test Loss: 0.3601301908493042\n",
      "Epoch 11, Batch 837, Test Loss: 0.793850302696228\n",
      "Epoch 11, Batch 838, Test Loss: 0.7305360436439514\n",
      "Epoch 11, Batch 839, Test Loss: 0.41380178928375244\n",
      "Epoch 11, Batch 840, Test Loss: 0.40239495038986206\n",
      "Epoch 11, Batch 841, Test Loss: 0.5023717880249023\n",
      "Epoch 11, Batch 842, Test Loss: 0.4711395502090454\n",
      "Epoch 11, Batch 843, Test Loss: 0.44351983070373535\n",
      "Epoch 11, Batch 844, Test Loss: 0.4842955470085144\n",
      "Epoch 11, Batch 845, Test Loss: 0.39860111474990845\n",
      "Epoch 11, Batch 846, Test Loss: 0.4031490981578827\n",
      "Epoch 11, Batch 847, Test Loss: 0.4869232475757599\n",
      "Epoch 11, Batch 848, Test Loss: 0.5733813047409058\n",
      "Epoch 11, Batch 849, Test Loss: 0.4781281054019928\n",
      "Epoch 11, Batch 850, Test Loss: 0.5674278140068054\n",
      "Epoch 11, Batch 851, Test Loss: 0.4954076409339905\n",
      "Epoch 11, Batch 852, Test Loss: 0.5805855393409729\n",
      "Epoch 11, Batch 853, Test Loss: 0.48980391025543213\n",
      "Epoch 11, Batch 854, Test Loss: 0.6630915403366089\n",
      "Epoch 11, Batch 855, Test Loss: 0.4990711808204651\n",
      "Epoch 11, Batch 856, Test Loss: 0.846931517124176\n",
      "Epoch 11, Batch 857, Test Loss: 0.5701009035110474\n",
      "Epoch 11, Batch 858, Test Loss: 0.5028662085533142\n",
      "Epoch 11, Batch 859, Test Loss: 0.6811689138412476\n",
      "Epoch 11, Batch 860, Test Loss: 0.5944569706916809\n",
      "Epoch 11, Batch 861, Test Loss: 0.3629835844039917\n",
      "Epoch 11, Batch 862, Test Loss: 0.4750185012817383\n",
      "Epoch 11, Batch 863, Test Loss: 0.4749605059623718\n",
      "Epoch 11, Batch 864, Test Loss: 0.6042253375053406\n",
      "Epoch 11, Batch 865, Test Loss: 0.3222702145576477\n",
      "Epoch 11, Batch 866, Test Loss: 0.637993574142456\n",
      "Epoch 11, Batch 867, Test Loss: 0.5319346785545349\n",
      "Epoch 11, Batch 868, Test Loss: 0.514033317565918\n",
      "Epoch 11, Batch 869, Test Loss: 0.4875040650367737\n",
      "Epoch 11, Batch 870, Test Loss: 0.6426264643669128\n",
      "Epoch 11, Batch 871, Test Loss: 0.5797092914581299\n",
      "Epoch 11, Batch 872, Test Loss: 0.389669269323349\n",
      "Epoch 11, Batch 873, Test Loss: 0.4728773832321167\n",
      "Epoch 11, Batch 874, Test Loss: 0.33689433336257935\n",
      "Epoch 11, Batch 875, Test Loss: 0.4563634991645813\n",
      "Epoch 11, Batch 876, Test Loss: 0.4336615800857544\n",
      "Epoch 11, Batch 877, Test Loss: 0.44723448157310486\n",
      "Epoch 11, Batch 878, Test Loss: 0.37449854612350464\n",
      "Epoch 11, Batch 879, Test Loss: 0.46560564637184143\n",
      "Epoch 11, Batch 880, Test Loss: 0.4880305826663971\n",
      "Epoch 11, Batch 881, Test Loss: 0.3877696990966797\n",
      "Epoch 11, Batch 882, Test Loss: 0.43403735756874084\n",
      "Epoch 11, Batch 883, Test Loss: 0.971532940864563\n",
      "Epoch 11, Batch 884, Test Loss: 0.5313655138015747\n",
      "Epoch 11, Batch 885, Test Loss: 0.541165292263031\n",
      "Epoch 11, Batch 886, Test Loss: 0.37510889768600464\n",
      "Epoch 11, Batch 887, Test Loss: 0.38789719343185425\n",
      "Epoch 11, Batch 888, Test Loss: 0.39847812056541443\n",
      "Epoch 11, Batch 889, Test Loss: 0.5424526333808899\n",
      "Epoch 11, Batch 890, Test Loss: 0.45200133323669434\n",
      "Epoch 11, Batch 891, Test Loss: 0.3960580825805664\n",
      "Epoch 11, Batch 892, Test Loss: 0.48652926087379456\n",
      "Epoch 11, Batch 893, Test Loss: 0.6020433902740479\n",
      "Epoch 11, Batch 894, Test Loss: 0.43515533208847046\n",
      "Epoch 11, Batch 895, Test Loss: 0.49686548113822937\n",
      "Epoch 11, Batch 896, Test Loss: 0.3446347713470459\n",
      "Epoch 11, Batch 897, Test Loss: 0.42199277877807617\n",
      "Epoch 11, Batch 898, Test Loss: 0.4299989342689514\n",
      "Epoch 11, Batch 899, Test Loss: 0.47795286774635315\n",
      "Epoch 11, Batch 900, Test Loss: 0.48290395736694336\n",
      "Epoch 11, Batch 901, Test Loss: 0.6899741291999817\n",
      "Epoch 11, Batch 902, Test Loss: 0.4653550088405609\n",
      "Epoch 11, Batch 903, Test Loss: 0.4279821813106537\n",
      "Epoch 11, Batch 904, Test Loss: 0.4428479075431824\n",
      "Epoch 11, Batch 905, Test Loss: 0.5207304358482361\n",
      "Epoch 11, Batch 906, Test Loss: 0.5599371194839478\n",
      "Epoch 11, Batch 907, Test Loss: 0.6881148219108582\n",
      "Epoch 11, Batch 908, Test Loss: 0.45505455136299133\n",
      "Epoch 11, Batch 909, Test Loss: 0.3823166489601135\n",
      "Epoch 11, Batch 910, Test Loss: 0.4609837532043457\n",
      "Epoch 11, Batch 911, Test Loss: 0.45553886890411377\n",
      "Epoch 11, Batch 912, Test Loss: 0.45162829756736755\n",
      "Epoch 11, Batch 913, Test Loss: 0.2736164629459381\n",
      "Epoch 11, Batch 914, Test Loss: 0.4910493791103363\n",
      "Epoch 11, Batch 915, Test Loss: 0.4955093562602997\n",
      "Epoch 11, Batch 916, Test Loss: 0.49271106719970703\n",
      "Epoch 11, Batch 917, Test Loss: 0.5844453573226929\n",
      "Epoch 11, Batch 918, Test Loss: 0.5714897513389587\n",
      "Epoch 11, Batch 919, Test Loss: 0.40635618567466736\n",
      "Epoch 11, Batch 920, Test Loss: 0.4261586666107178\n",
      "Epoch 11, Batch 921, Test Loss: 0.4721153676509857\n",
      "Epoch 11, Batch 922, Test Loss: 0.48502564430236816\n",
      "Epoch 11, Batch 923, Test Loss: 0.6162708401679993\n",
      "Epoch 11, Batch 924, Test Loss: 0.36907079815864563\n",
      "Epoch 11, Batch 925, Test Loss: 0.47748827934265137\n",
      "Epoch 11, Batch 926, Test Loss: 0.39073899388313293\n",
      "Epoch 11, Batch 927, Test Loss: 0.49300962686538696\n",
      "Epoch 11, Batch 928, Test Loss: 0.5652633905410767\n",
      "Epoch 11, Batch 929, Test Loss: 0.5859739780426025\n",
      "Epoch 11, Batch 930, Test Loss: 0.2919446527957916\n",
      "Epoch 11, Batch 931, Test Loss: 0.46127554774284363\n",
      "Epoch 11, Batch 932, Test Loss: 0.47827184200286865\n",
      "Epoch 11, Batch 933, Test Loss: 0.632135272026062\n",
      "Epoch 11, Batch 934, Test Loss: 0.34526219964027405\n",
      "Epoch 11, Batch 935, Test Loss: 0.33285051584243774\n",
      "Epoch 11, Batch 936, Test Loss: 0.4464374780654907\n",
      "Epoch 11, Batch 937, Test Loss: 0.6687129139900208\n",
      "Epoch 11, Batch 938, Test Loss: 0.22467216849327087\n",
      "Accuracy of Test set: 0.8270333333333333\n",
      "Epoch 12, Batch 1, Loss: 0.4966694712638855\n",
      "Epoch 12, Batch 2, Loss: 0.6078701615333557\n",
      "Epoch 12, Batch 3, Loss: 0.5057380795478821\n",
      "Epoch 12, Batch 4, Loss: 0.515166163444519\n",
      "Epoch 12, Batch 5, Loss: 0.429511696100235\n",
      "Epoch 12, Batch 6, Loss: 0.675821840763092\n",
      "Epoch 12, Batch 7, Loss: 0.44671857357025146\n",
      "Epoch 12, Batch 8, Loss: 0.3242645263671875\n",
      "Epoch 12, Batch 9, Loss: 0.47634029388427734\n",
      "Epoch 12, Batch 10, Loss: 0.5751438140869141\n",
      "Epoch 12, Batch 11, Loss: 0.489073783159256\n",
      "Epoch 12, Batch 12, Loss: 0.623023509979248\n",
      "Epoch 12, Batch 13, Loss: 0.4011545479297638\n",
      "Epoch 12, Batch 14, Loss: 0.7002379298210144\n",
      "Epoch 12, Batch 15, Loss: 0.3437480330467224\n",
      "Epoch 12, Batch 16, Loss: 0.49606993794441223\n",
      "Epoch 12, Batch 17, Loss: 0.6240770220756531\n",
      "Epoch 12, Batch 18, Loss: 0.6150052547454834\n",
      "Epoch 12, Batch 19, Loss: 0.5483803749084473\n",
      "Epoch 12, Batch 20, Loss: 0.5072194933891296\n",
      "Epoch 12, Batch 21, Loss: 0.5167021751403809\n",
      "Epoch 12, Batch 22, Loss: 0.5506110191345215\n",
      "Epoch 12, Batch 23, Loss: 0.4983624815940857\n",
      "Epoch 12, Batch 24, Loss: 0.3868238031864166\n",
      "Epoch 12, Batch 25, Loss: 0.7628650665283203\n",
      "Epoch 12, Batch 26, Loss: 0.5305643677711487\n",
      "Epoch 12, Batch 27, Loss: 0.37103044986724854\n",
      "Epoch 12, Batch 28, Loss: 0.5972492694854736\n",
      "Epoch 12, Batch 29, Loss: 0.3518894612789154\n",
      "Epoch 12, Batch 30, Loss: 0.5537799596786499\n",
      "Epoch 12, Batch 31, Loss: 0.6165248155593872\n",
      "Epoch 12, Batch 32, Loss: 0.46202823519706726\n",
      "Epoch 12, Batch 33, Loss: 0.47651082277297974\n",
      "Epoch 12, Batch 34, Loss: 0.5126502513885498\n",
      "Epoch 12, Batch 35, Loss: 0.4530941843986511\n",
      "Epoch 12, Batch 36, Loss: 0.42597877979278564\n",
      "Epoch 12, Batch 37, Loss: 0.49524372816085815\n",
      "Epoch 12, Batch 38, Loss: 0.46303513646125793\n",
      "Epoch 12, Batch 39, Loss: 0.5791915059089661\n",
      "Epoch 12, Batch 40, Loss: 0.4435916543006897\n",
      "Epoch 12, Batch 41, Loss: 0.4963301420211792\n",
      "Epoch 12, Batch 42, Loss: 0.4215167164802551\n",
      "Epoch 12, Batch 43, Loss: 0.7008099555969238\n",
      "Epoch 12, Batch 44, Loss: 0.5754678845405579\n",
      "Epoch 12, Batch 45, Loss: 0.4853130280971527\n",
      "Epoch 12, Batch 46, Loss: 0.4800419509410858\n",
      "Epoch 12, Batch 47, Loss: 0.5709108114242554\n",
      "Epoch 12, Batch 48, Loss: 0.7431592345237732\n",
      "Epoch 12, Batch 49, Loss: 0.36970868706703186\n",
      "Epoch 12, Batch 50, Loss: 0.6142092943191528\n",
      "Epoch 12, Batch 51, Loss: 0.45959198474884033\n",
      "Epoch 12, Batch 52, Loss: 0.529922366142273\n",
      "Epoch 12, Batch 53, Loss: 0.7571731209754944\n",
      "Epoch 12, Batch 54, Loss: 0.5069733262062073\n",
      "Epoch 12, Batch 55, Loss: 0.62008136510849\n",
      "Epoch 12, Batch 56, Loss: 0.4820079207420349\n",
      "Epoch 12, Batch 57, Loss: 0.3911605477333069\n",
      "Epoch 12, Batch 58, Loss: 0.47662055492401123\n",
      "Epoch 12, Batch 59, Loss: 0.574529767036438\n",
      "Epoch 12, Batch 60, Loss: 0.48033344745635986\n",
      "Epoch 12, Batch 61, Loss: 0.4523049592971802\n",
      "Epoch 12, Batch 62, Loss: 0.38471370935440063\n",
      "Epoch 12, Batch 63, Loss: 0.5480238199234009\n",
      "Epoch 12, Batch 64, Loss: 0.42608413100242615\n",
      "Epoch 12, Batch 65, Loss: 0.5401569604873657\n",
      "Epoch 12, Batch 66, Loss: 0.43892428278923035\n",
      "Epoch 12, Batch 67, Loss: 0.5552717447280884\n",
      "Epoch 12, Batch 68, Loss: 0.3934531807899475\n",
      "Epoch 12, Batch 69, Loss: 0.5337132811546326\n",
      "Epoch 12, Batch 70, Loss: 0.49235981702804565\n",
      "Epoch 12, Batch 71, Loss: 0.47117823362350464\n",
      "Epoch 12, Batch 72, Loss: 0.6282824873924255\n",
      "Epoch 12, Batch 73, Loss: 0.5394653081893921\n",
      "Epoch 12, Batch 74, Loss: 0.4963025152683258\n",
      "Epoch 12, Batch 75, Loss: 0.3155845105648041\n",
      "Epoch 12, Batch 76, Loss: 0.528343677520752\n",
      "Epoch 12, Batch 77, Loss: 0.534692108631134\n",
      "Epoch 12, Batch 78, Loss: 0.4793091118335724\n",
      "Epoch 12, Batch 79, Loss: 0.5287790298461914\n",
      "Epoch 12, Batch 80, Loss: 0.556119441986084\n",
      "Epoch 12, Batch 81, Loss: 0.38539889454841614\n",
      "Epoch 12, Batch 82, Loss: 0.6965660452842712\n",
      "Epoch 12, Batch 83, Loss: 0.3379032611846924\n",
      "Epoch 12, Batch 84, Loss: 0.5319629907608032\n",
      "Epoch 12, Batch 85, Loss: 0.5990636348724365\n",
      "Epoch 12, Batch 86, Loss: 0.6184796690940857\n",
      "Epoch 12, Batch 87, Loss: 0.4763563275337219\n",
      "Epoch 12, Batch 88, Loss: 0.5989735722541809\n",
      "Epoch 12, Batch 89, Loss: 0.6571107506752014\n",
      "Epoch 12, Batch 90, Loss: 0.538942813873291\n",
      "Epoch 12, Batch 91, Loss: 0.5586867332458496\n",
      "Epoch 12, Batch 92, Loss: 0.7010642290115356\n",
      "Epoch 12, Batch 93, Loss: 0.30497676134109497\n",
      "Epoch 12, Batch 94, Loss: 0.5780552625656128\n",
      "Epoch 12, Batch 95, Loss: 0.5731521248817444\n",
      "Epoch 12, Batch 96, Loss: 0.42296314239501953\n",
      "Epoch 12, Batch 97, Loss: 0.46686750650405884\n",
      "Epoch 12, Batch 98, Loss: 0.4616326689720154\n",
      "Epoch 12, Batch 99, Loss: 0.4517136514186859\n",
      "Epoch 12, Batch 100, Loss: 0.2934093475341797\n",
      "Epoch 12, Batch 101, Loss: 0.4987698197364807\n",
      "Epoch 12, Batch 102, Loss: 0.47048676013946533\n",
      "Epoch 12, Batch 103, Loss: 0.5119250416755676\n",
      "Epoch 12, Batch 104, Loss: 0.35461002588272095\n",
      "Epoch 12, Batch 105, Loss: 0.42057448625564575\n",
      "Epoch 12, Batch 106, Loss: 0.38334548473358154\n",
      "Epoch 12, Batch 107, Loss: 0.44196322560310364\n",
      "Epoch 12, Batch 108, Loss: 0.5910655856132507\n",
      "Epoch 12, Batch 109, Loss: 0.5342645049095154\n",
      "Epoch 12, Batch 110, Loss: 0.6841480731964111\n",
      "Epoch 12, Batch 111, Loss: 0.5431433916091919\n",
      "Epoch 12, Batch 112, Loss: 0.7044888734817505\n",
      "Epoch 12, Batch 113, Loss: 0.44972386956214905\n",
      "Epoch 12, Batch 114, Loss: 0.5880070924758911\n",
      "Epoch 12, Batch 115, Loss: 0.458117812871933\n",
      "Epoch 12, Batch 116, Loss: 0.6355292797088623\n",
      "Epoch 12, Batch 117, Loss: 0.646048903465271\n",
      "Epoch 12, Batch 118, Loss: 0.616352915763855\n",
      "Epoch 12, Batch 119, Loss: 0.5919795036315918\n",
      "Epoch 12, Batch 120, Loss: 0.4808250665664673\n",
      "Epoch 12, Batch 121, Loss: 0.5903607606887817\n",
      "Epoch 12, Batch 122, Loss: 0.4074402451515198\n",
      "Epoch 12, Batch 123, Loss: 0.4368920624256134\n",
      "Epoch 12, Batch 124, Loss: 0.44007644057273865\n",
      "Epoch 12, Batch 125, Loss: 0.3871344327926636\n",
      "Epoch 12, Batch 126, Loss: 0.38877928256988525\n",
      "Epoch 12, Batch 127, Loss: 0.4449214041233063\n",
      "Epoch 12, Batch 128, Loss: 0.5317005515098572\n",
      "Epoch 12, Batch 129, Loss: 0.4083840548992157\n",
      "Epoch 12, Batch 130, Loss: 0.7991970777511597\n",
      "Epoch 12, Batch 131, Loss: 0.5168307423591614\n",
      "Epoch 12, Batch 132, Loss: 0.4807910919189453\n",
      "Epoch 12, Batch 133, Loss: 0.39075082540512085\n",
      "Epoch 12, Batch 134, Loss: 0.585900068283081\n",
      "Epoch 12, Batch 135, Loss: 0.43514376878738403\n",
      "Epoch 12, Batch 136, Loss: 0.375397264957428\n",
      "Epoch 12, Batch 137, Loss: 0.4108271300792694\n",
      "Epoch 12, Batch 138, Loss: 0.3721321225166321\n",
      "Epoch 12, Batch 139, Loss: 0.5609347224235535\n",
      "Epoch 12, Batch 140, Loss: 0.6128400564193726\n",
      "Epoch 12, Batch 141, Loss: 0.39749905467033386\n",
      "Epoch 12, Batch 142, Loss: 0.37369561195373535\n",
      "Epoch 12, Batch 143, Loss: 0.5197016596794128\n",
      "Epoch 12, Batch 144, Loss: 0.39172813296318054\n",
      "Epoch 12, Batch 145, Loss: 0.46764689683914185\n",
      "Epoch 12, Batch 146, Loss: 0.502609133720398\n",
      "Epoch 12, Batch 147, Loss: 0.5415194630622864\n",
      "Epoch 12, Batch 148, Loss: 0.4516097903251648\n",
      "Epoch 12, Batch 149, Loss: 0.4243716597557068\n",
      "Epoch 12, Batch 150, Loss: 0.541363537311554\n",
      "Epoch 12, Batch 151, Loss: 0.6402227878570557\n",
      "Epoch 12, Batch 152, Loss: 0.40870359539985657\n",
      "Epoch 12, Batch 153, Loss: 0.5080749988555908\n",
      "Epoch 12, Batch 154, Loss: 0.521439790725708\n",
      "Epoch 12, Batch 155, Loss: 0.40160518884658813\n",
      "Epoch 12, Batch 156, Loss: 0.5737882852554321\n",
      "Epoch 12, Batch 157, Loss: 0.4208530783653259\n",
      "Epoch 12, Batch 158, Loss: 0.6892910003662109\n",
      "Epoch 12, Batch 159, Loss: 0.732248842716217\n",
      "Epoch 12, Batch 160, Loss: 0.3745317757129669\n",
      "Epoch 12, Batch 161, Loss: 0.4733457565307617\n",
      "Epoch 12, Batch 162, Loss: 0.5765634179115295\n",
      "Epoch 12, Batch 163, Loss: 0.5370700359344482\n",
      "Epoch 12, Batch 164, Loss: 0.34663787484169006\n",
      "Epoch 12, Batch 165, Loss: 0.37391528487205505\n",
      "Epoch 12, Batch 166, Loss: 0.41418617963790894\n",
      "Epoch 12, Batch 167, Loss: 0.44308382272720337\n",
      "Epoch 12, Batch 168, Loss: 0.5655901432037354\n",
      "Epoch 12, Batch 169, Loss: 0.39730092883110046\n",
      "Epoch 12, Batch 170, Loss: 0.4820924699306488\n",
      "Epoch 12, Batch 171, Loss: 0.8306001424789429\n",
      "Epoch 12, Batch 172, Loss: 0.6075027585029602\n",
      "Epoch 12, Batch 173, Loss: 0.48767104744911194\n",
      "Epoch 12, Batch 174, Loss: 0.5522747039794922\n",
      "Epoch 12, Batch 175, Loss: 0.29267334938049316\n",
      "Epoch 12, Batch 176, Loss: 0.3513948321342468\n",
      "Epoch 12, Batch 177, Loss: 0.569816529750824\n",
      "Epoch 12, Batch 178, Loss: 0.4715906083583832\n",
      "Epoch 12, Batch 179, Loss: 0.5223508477210999\n",
      "Epoch 12, Batch 180, Loss: 0.46044763922691345\n",
      "Epoch 12, Batch 181, Loss: 0.32918843626976013\n",
      "Epoch 12, Batch 182, Loss: 0.5349949598312378\n",
      "Epoch 12, Batch 183, Loss: 0.47329479455947876\n",
      "Epoch 12, Batch 184, Loss: 0.4055560529232025\n",
      "Epoch 12, Batch 185, Loss: 0.4176093339920044\n",
      "Epoch 12, Batch 186, Loss: 0.3477420508861542\n",
      "Epoch 12, Batch 187, Loss: 0.6559063196182251\n",
      "Epoch 12, Batch 188, Loss: 0.40269094705581665\n",
      "Epoch 12, Batch 189, Loss: 0.5605814456939697\n",
      "Epoch 12, Batch 190, Loss: 0.40585118532180786\n",
      "Epoch 12, Batch 191, Loss: 0.4138965308666229\n",
      "Epoch 12, Batch 192, Loss: 0.33896759152412415\n",
      "Epoch 12, Batch 193, Loss: 0.43206915259361267\n",
      "Epoch 12, Batch 194, Loss: 0.5320348739624023\n",
      "Epoch 12, Batch 195, Loss: 0.5610201358795166\n",
      "Epoch 12, Batch 196, Loss: 0.6283212304115295\n",
      "Epoch 12, Batch 197, Loss: 0.4157301187515259\n",
      "Epoch 12, Batch 198, Loss: 0.6451603174209595\n",
      "Epoch 12, Batch 199, Loss: 0.6451900005340576\n",
      "Epoch 12, Batch 200, Loss: 0.31415945291519165\n",
      "Epoch 12, Batch 201, Loss: 0.39938342571258545\n",
      "Epoch 12, Batch 202, Loss: 0.6158777475357056\n",
      "Epoch 12, Batch 203, Loss: 0.7922883033752441\n",
      "Epoch 12, Batch 204, Loss: 0.5360298156738281\n",
      "Epoch 12, Batch 205, Loss: 0.503410816192627\n",
      "Epoch 12, Batch 206, Loss: 0.47871243953704834\n",
      "Epoch 12, Batch 207, Loss: 0.5189800262451172\n",
      "Epoch 12, Batch 208, Loss: 0.5825731158256531\n",
      "Epoch 12, Batch 209, Loss: 0.4699736535549164\n",
      "Epoch 12, Batch 210, Loss: 0.4675741493701935\n",
      "Epoch 12, Batch 211, Loss: 0.6390977501869202\n",
      "Epoch 12, Batch 212, Loss: 0.46329039335250854\n",
      "Epoch 12, Batch 213, Loss: 0.4407537579536438\n",
      "Epoch 12, Batch 214, Loss: 0.7581332921981812\n",
      "Epoch 12, Batch 215, Loss: 0.5670793056488037\n",
      "Epoch 12, Batch 216, Loss: 0.4809018075466156\n",
      "Epoch 12, Batch 217, Loss: 0.5571470260620117\n",
      "Epoch 12, Batch 218, Loss: 0.454958438873291\n",
      "Epoch 12, Batch 219, Loss: 0.5882276892662048\n",
      "Epoch 12, Batch 220, Loss: 0.5153964757919312\n",
      "Epoch 12, Batch 221, Loss: 0.5197094082832336\n",
      "Epoch 12, Batch 222, Loss: 0.6018291711807251\n",
      "Epoch 12, Batch 223, Loss: 0.47539782524108887\n",
      "Epoch 12, Batch 224, Loss: 0.5314366221427917\n",
      "Epoch 12, Batch 225, Loss: 0.3146888017654419\n",
      "Epoch 12, Batch 226, Loss: 0.460041344165802\n",
      "Epoch 12, Batch 227, Loss: 0.3808712959289551\n",
      "Epoch 12, Batch 228, Loss: 0.5217109322547913\n",
      "Epoch 12, Batch 229, Loss: 0.5593154430389404\n",
      "Epoch 12, Batch 230, Loss: 0.49421846866607666\n",
      "Epoch 12, Batch 231, Loss: 0.4644972085952759\n",
      "Epoch 12, Batch 232, Loss: 0.6529338955879211\n",
      "Epoch 12, Batch 233, Loss: 0.5114181041717529\n",
      "Epoch 12, Batch 234, Loss: 0.4889204502105713\n",
      "Epoch 12, Batch 235, Loss: 0.6318875551223755\n",
      "Epoch 12, Batch 236, Loss: 0.34239447116851807\n",
      "Epoch 12, Batch 237, Loss: 0.6458879709243774\n",
      "Epoch 12, Batch 238, Loss: 0.592269241809845\n",
      "Epoch 12, Batch 239, Loss: 0.45923471450805664\n",
      "Epoch 12, Batch 240, Loss: 0.4901556074619293\n",
      "Epoch 12, Batch 241, Loss: 0.45970505475997925\n",
      "Epoch 12, Batch 242, Loss: 0.6295113563537598\n",
      "Epoch 12, Batch 243, Loss: 0.39841920137405396\n",
      "Epoch 12, Batch 244, Loss: 0.5524160265922546\n",
      "Epoch 12, Batch 245, Loss: 0.3759738802909851\n",
      "Epoch 12, Batch 246, Loss: 0.5185468196868896\n",
      "Epoch 12, Batch 247, Loss: 0.5342985987663269\n",
      "Epoch 12, Batch 248, Loss: 0.6172983646392822\n",
      "Epoch 12, Batch 249, Loss: 0.5314168930053711\n",
      "Epoch 12, Batch 250, Loss: 0.6905122995376587\n",
      "Epoch 12, Batch 251, Loss: 0.3656306564807892\n",
      "Epoch 12, Batch 252, Loss: 0.5495946407318115\n",
      "Epoch 12, Batch 253, Loss: 0.4996228814125061\n",
      "Epoch 12, Batch 254, Loss: 0.4785386323928833\n",
      "Epoch 12, Batch 255, Loss: 0.5614604949951172\n",
      "Epoch 12, Batch 256, Loss: 0.5328840613365173\n",
      "Epoch 12, Batch 257, Loss: 0.6165226101875305\n",
      "Epoch 12, Batch 258, Loss: 0.5125652551651001\n",
      "Epoch 12, Batch 259, Loss: 0.43897104263305664\n",
      "Epoch 12, Batch 260, Loss: 0.5537111163139343\n",
      "Epoch 12, Batch 261, Loss: 0.43481647968292236\n",
      "Epoch 12, Batch 262, Loss: 0.3738223612308502\n",
      "Epoch 12, Batch 263, Loss: 0.6225993633270264\n",
      "Epoch 12, Batch 264, Loss: 0.4567188620567322\n",
      "Epoch 12, Batch 265, Loss: 0.5736704468727112\n",
      "Epoch 12, Batch 266, Loss: 0.5883191823959351\n",
      "Epoch 12, Batch 267, Loss: 0.3596099615097046\n",
      "Epoch 12, Batch 268, Loss: 0.5284088253974915\n",
      "Epoch 12, Batch 269, Loss: 0.5379577875137329\n",
      "Epoch 12, Batch 270, Loss: 0.39247989654541016\n",
      "Epoch 12, Batch 271, Loss: 0.4049731492996216\n",
      "Epoch 12, Batch 272, Loss: 0.5293314456939697\n",
      "Epoch 12, Batch 273, Loss: 0.4569333493709564\n",
      "Epoch 12, Batch 274, Loss: 0.5158945918083191\n",
      "Epoch 12, Batch 275, Loss: 0.6141506433486938\n",
      "Epoch 12, Batch 276, Loss: 0.604008138179779\n",
      "Epoch 12, Batch 277, Loss: 0.31431981921195984\n",
      "Epoch 12, Batch 278, Loss: 0.5140513181686401\n",
      "Epoch 12, Batch 279, Loss: 0.28748810291290283\n",
      "Epoch 12, Batch 280, Loss: 0.5748233795166016\n",
      "Epoch 12, Batch 281, Loss: 0.4134944677352905\n",
      "Epoch 12, Batch 282, Loss: 0.45148858428001404\n",
      "Epoch 12, Batch 283, Loss: 0.4551553726196289\n",
      "Epoch 12, Batch 284, Loss: 0.7506694793701172\n",
      "Epoch 12, Batch 285, Loss: 0.5564107298851013\n",
      "Epoch 12, Batch 286, Loss: 0.4626758396625519\n",
      "Epoch 12, Batch 287, Loss: 0.6234127283096313\n",
      "Epoch 12, Batch 288, Loss: 0.507192850112915\n",
      "Epoch 12, Batch 289, Loss: 0.3046652674674988\n",
      "Epoch 12, Batch 290, Loss: 0.6908489465713501\n",
      "Epoch 12, Batch 291, Loss: 0.4815938174724579\n",
      "Epoch 12, Batch 292, Loss: 0.49226757884025574\n",
      "Epoch 12, Batch 293, Loss: 0.42268049716949463\n",
      "Epoch 12, Batch 294, Loss: 0.5463504195213318\n",
      "Epoch 12, Batch 295, Loss: 0.38202598690986633\n",
      "Epoch 12, Batch 296, Loss: 0.4545823633670807\n",
      "Epoch 12, Batch 297, Loss: 0.4351749122142792\n",
      "Epoch 12, Batch 298, Loss: 0.3997153043746948\n",
      "Epoch 12, Batch 299, Loss: 0.4053734838962555\n",
      "Epoch 12, Batch 300, Loss: 0.5612336993217468\n",
      "Epoch 12, Batch 301, Loss: 0.32052910327911377\n",
      "Epoch 12, Batch 302, Loss: 0.6106007695198059\n",
      "Epoch 12, Batch 303, Loss: 0.40117529034614563\n",
      "Epoch 12, Batch 304, Loss: 0.30559617280960083\n",
      "Epoch 12, Batch 305, Loss: 0.49278056621551514\n",
      "Epoch 12, Batch 306, Loss: 0.3928048312664032\n",
      "Epoch 12, Batch 307, Loss: 0.5283553004264832\n",
      "Epoch 12, Batch 308, Loss: 0.48580873012542725\n",
      "Epoch 12, Batch 309, Loss: 0.5075948238372803\n",
      "Epoch 12, Batch 310, Loss: 0.6430412530899048\n",
      "Epoch 12, Batch 311, Loss: 0.3374313712120056\n",
      "Epoch 12, Batch 312, Loss: 0.7120847702026367\n",
      "Epoch 12, Batch 313, Loss: 0.5257718563079834\n",
      "Epoch 12, Batch 314, Loss: 0.5045740604400635\n",
      "Epoch 12, Batch 315, Loss: 0.49419835209846497\n",
      "Epoch 12, Batch 316, Loss: 0.5698340535163879\n",
      "Epoch 12, Batch 317, Loss: 0.47174590826034546\n",
      "Epoch 12, Batch 318, Loss: 0.6459794044494629\n",
      "Epoch 12, Batch 319, Loss: 0.7120778560638428\n",
      "Epoch 12, Batch 320, Loss: 0.396872341632843\n",
      "Epoch 12, Batch 321, Loss: 0.46153852343559265\n",
      "Epoch 12, Batch 322, Loss: 0.5564076900482178\n",
      "Epoch 12, Batch 323, Loss: 0.4594004154205322\n",
      "Epoch 12, Batch 324, Loss: 0.5927901268005371\n",
      "Epoch 12, Batch 325, Loss: 0.4326423406600952\n",
      "Epoch 12, Batch 326, Loss: 0.6217641830444336\n",
      "Epoch 12, Batch 327, Loss: 0.5369657278060913\n",
      "Epoch 12, Batch 328, Loss: 0.5200027227401733\n",
      "Epoch 12, Batch 329, Loss: 0.6550328731536865\n",
      "Epoch 12, Batch 330, Loss: 0.3945302963256836\n",
      "Epoch 12, Batch 331, Loss: 0.42337411642074585\n",
      "Epoch 12, Batch 332, Loss: 0.5480526685714722\n",
      "Epoch 12, Batch 333, Loss: 0.7173267006874084\n",
      "Epoch 12, Batch 334, Loss: 0.3956948518753052\n",
      "Epoch 12, Batch 335, Loss: 0.32070472836494446\n",
      "Epoch 12, Batch 336, Loss: 0.6794001460075378\n",
      "Epoch 12, Batch 337, Loss: 0.4522169232368469\n",
      "Epoch 12, Batch 338, Loss: 0.556085467338562\n",
      "Epoch 12, Batch 339, Loss: 0.591672420501709\n",
      "Epoch 12, Batch 340, Loss: 0.5691503286361694\n",
      "Epoch 12, Batch 341, Loss: 0.8424991965293884\n",
      "Epoch 12, Batch 342, Loss: 0.4210968315601349\n",
      "Epoch 12, Batch 343, Loss: 0.5683853030204773\n",
      "Epoch 12, Batch 344, Loss: 0.3680431842803955\n",
      "Epoch 12, Batch 345, Loss: 0.3353583812713623\n",
      "Epoch 12, Batch 346, Loss: 0.7100945711135864\n",
      "Epoch 12, Batch 347, Loss: 0.4005768299102783\n",
      "Epoch 12, Batch 348, Loss: 0.46797648072242737\n",
      "Epoch 12, Batch 349, Loss: 0.43661975860595703\n",
      "Epoch 12, Batch 350, Loss: 0.5478283166885376\n",
      "Epoch 12, Batch 351, Loss: 0.4897465109825134\n",
      "Epoch 12, Batch 352, Loss: 0.5070536732673645\n",
      "Epoch 12, Batch 353, Loss: 0.434787780046463\n",
      "Epoch 12, Batch 354, Loss: 0.42524364590644836\n",
      "Epoch 12, Batch 355, Loss: 0.46020254492759705\n",
      "Epoch 12, Batch 356, Loss: 0.6442127227783203\n",
      "Epoch 12, Batch 357, Loss: 0.5414065718650818\n",
      "Epoch 12, Batch 358, Loss: 0.5705793499946594\n",
      "Epoch 12, Batch 359, Loss: 0.7574514150619507\n",
      "Epoch 12, Batch 360, Loss: 0.4588760733604431\n",
      "Epoch 12, Batch 361, Loss: 0.47362419962882996\n",
      "Epoch 12, Batch 362, Loss: 0.626456081867218\n",
      "Epoch 12, Batch 363, Loss: 0.6109234094619751\n",
      "Epoch 12, Batch 364, Loss: 0.3953726291656494\n",
      "Epoch 12, Batch 365, Loss: 0.42628851532936096\n",
      "Epoch 12, Batch 366, Loss: 0.5175133347511292\n",
      "Epoch 12, Batch 367, Loss: 0.4430846869945526\n",
      "Epoch 12, Batch 368, Loss: 0.5233204960823059\n",
      "Epoch 12, Batch 369, Loss: 0.3160163164138794\n",
      "Epoch 12, Batch 370, Loss: 0.47330984473228455\n",
      "Epoch 12, Batch 371, Loss: 0.4397946894168854\n",
      "Epoch 12, Batch 372, Loss: 0.4359742999076843\n",
      "Epoch 12, Batch 373, Loss: 0.4813649654388428\n",
      "Epoch 12, Batch 374, Loss: 0.4269685447216034\n",
      "Epoch 12, Batch 375, Loss: 0.39894601702690125\n",
      "Epoch 12, Batch 376, Loss: 0.555220365524292\n",
      "Epoch 12, Batch 377, Loss: 0.48848602175712585\n",
      "Epoch 12, Batch 378, Loss: 0.38313597440719604\n",
      "Epoch 12, Batch 379, Loss: 0.5770836472511292\n",
      "Epoch 12, Batch 380, Loss: 0.7408777475357056\n",
      "Epoch 12, Batch 381, Loss: 0.598831295967102\n",
      "Epoch 12, Batch 382, Loss: 0.3793281316757202\n",
      "Epoch 12, Batch 383, Loss: 0.5069935321807861\n",
      "Epoch 12, Batch 384, Loss: 0.5664241313934326\n",
      "Epoch 12, Batch 385, Loss: 0.36832430958747864\n",
      "Epoch 12, Batch 386, Loss: 0.5951575040817261\n",
      "Epoch 12, Batch 387, Loss: 0.4363638162612915\n",
      "Epoch 12, Batch 388, Loss: 0.38156187534332275\n",
      "Epoch 12, Batch 389, Loss: 0.48392483592033386\n",
      "Epoch 12, Batch 390, Loss: 0.42875415086746216\n",
      "Epoch 12, Batch 391, Loss: 0.6387251615524292\n",
      "Epoch 12, Batch 392, Loss: 0.6172116994857788\n",
      "Epoch 12, Batch 393, Loss: 0.6487661004066467\n",
      "Epoch 12, Batch 394, Loss: 0.5111043453216553\n",
      "Epoch 12, Batch 395, Loss: 0.49607789516448975\n",
      "Epoch 12, Batch 396, Loss: 0.28466320037841797\n",
      "Epoch 12, Batch 397, Loss: 0.3703874349594116\n",
      "Epoch 12, Batch 398, Loss: 0.5671582221984863\n",
      "Epoch 12, Batch 399, Loss: 0.5636054277420044\n",
      "Epoch 12, Batch 400, Loss: 0.37730881571769714\n",
      "Epoch 12, Batch 401, Loss: 0.6636268496513367\n",
      "Epoch 12, Batch 402, Loss: 0.3970974087715149\n",
      "Epoch 12, Batch 403, Loss: 0.42197492718696594\n",
      "Epoch 12, Batch 404, Loss: 0.5684467554092407\n",
      "Epoch 12, Batch 405, Loss: 0.4839450716972351\n",
      "Epoch 12, Batch 406, Loss: 0.6252573728561401\n",
      "Epoch 12, Batch 407, Loss: 0.6390925049781799\n",
      "Epoch 12, Batch 408, Loss: 0.44860753417015076\n",
      "Epoch 12, Batch 409, Loss: 0.35921087861061096\n",
      "Epoch 12, Batch 410, Loss: 0.6996167302131653\n",
      "Epoch 12, Batch 411, Loss: 0.4118398427963257\n",
      "Epoch 12, Batch 412, Loss: 0.3786797821521759\n",
      "Epoch 12, Batch 413, Loss: 0.3439723253250122\n",
      "Epoch 12, Batch 414, Loss: 0.3360596001148224\n",
      "Epoch 12, Batch 415, Loss: 0.40092167258262634\n",
      "Epoch 12, Batch 416, Loss: 0.5739538669586182\n",
      "Epoch 12, Batch 417, Loss: 0.5087957978248596\n",
      "Epoch 12, Batch 418, Loss: 0.4887579083442688\n",
      "Epoch 12, Batch 419, Loss: 0.35266169905662537\n",
      "Epoch 12, Batch 420, Loss: 0.5244626998901367\n",
      "Epoch 12, Batch 421, Loss: 0.26995009183883667\n",
      "Epoch 12, Batch 422, Loss: 0.5437443852424622\n",
      "Epoch 12, Batch 423, Loss: 0.6233610510826111\n",
      "Epoch 12, Batch 424, Loss: 0.5370834469795227\n",
      "Epoch 12, Batch 425, Loss: 0.6349277496337891\n",
      "Epoch 12, Batch 426, Loss: 0.5570151209831238\n",
      "Epoch 12, Batch 427, Loss: 0.4376445412635803\n",
      "Epoch 12, Batch 428, Loss: 0.45633551478385925\n",
      "Epoch 12, Batch 429, Loss: 0.45310553908348083\n",
      "Epoch 12, Batch 430, Loss: 0.27783024311065674\n",
      "Epoch 12, Batch 431, Loss: 0.5653907656669617\n",
      "Epoch 12, Batch 432, Loss: 0.5249324440956116\n",
      "Epoch 12, Batch 433, Loss: 0.529204249382019\n",
      "Epoch 12, Batch 434, Loss: 0.41720402240753174\n",
      "Epoch 12, Batch 435, Loss: 0.7484863996505737\n",
      "Epoch 12, Batch 436, Loss: 0.5905904173851013\n",
      "Epoch 12, Batch 437, Loss: 0.6201395988464355\n",
      "Epoch 12, Batch 438, Loss: 0.5869249105453491\n",
      "Epoch 12, Batch 439, Loss: 0.5239537358283997\n",
      "Epoch 12, Batch 440, Loss: 0.5877866744995117\n",
      "Epoch 12, Batch 441, Loss: 0.42266136407852173\n",
      "Epoch 12, Batch 442, Loss: 0.39657819271087646\n",
      "Epoch 12, Batch 443, Loss: 0.4004821181297302\n",
      "Epoch 12, Batch 444, Loss: 0.5121464133262634\n",
      "Epoch 12, Batch 445, Loss: 0.37068796157836914\n",
      "Epoch 12, Batch 446, Loss: 0.42616158723831177\n",
      "Epoch 12, Batch 447, Loss: 0.6418702602386475\n",
      "Epoch 12, Batch 448, Loss: 0.5538662672042847\n",
      "Epoch 12, Batch 449, Loss: 0.5797867774963379\n",
      "Epoch 12, Batch 450, Loss: 0.3928671181201935\n",
      "Epoch 12, Batch 451, Loss: 0.628120481967926\n",
      "Epoch 12, Batch 452, Loss: 0.5074745416641235\n",
      "Epoch 12, Batch 453, Loss: 0.46301883459091187\n",
      "Epoch 12, Batch 454, Loss: 0.5839881896972656\n",
      "Epoch 12, Batch 455, Loss: 0.5693684816360474\n",
      "Epoch 12, Batch 456, Loss: 0.42750388383865356\n",
      "Epoch 12, Batch 457, Loss: 0.422624409198761\n",
      "Epoch 12, Batch 458, Loss: 0.6215378642082214\n",
      "Epoch 12, Batch 459, Loss: 0.4804782271385193\n",
      "Epoch 12, Batch 460, Loss: 0.44431331753730774\n",
      "Epoch 12, Batch 461, Loss: 0.7215684056282043\n",
      "Epoch 12, Batch 462, Loss: 0.5610801577568054\n",
      "Epoch 12, Batch 463, Loss: 0.4346518814563751\n",
      "Epoch 12, Batch 464, Loss: 0.47694844007492065\n",
      "Epoch 12, Batch 465, Loss: 0.6045250296592712\n",
      "Epoch 12, Batch 466, Loss: 0.3952629864215851\n",
      "Epoch 12, Batch 467, Loss: 0.47183990478515625\n",
      "Epoch 12, Batch 468, Loss: 0.4359399080276489\n",
      "Epoch 12, Batch 469, Loss: 0.4534362554550171\n",
      "Epoch 12, Batch 470, Loss: 0.3706381320953369\n",
      "Epoch 12, Batch 471, Loss: 0.44991257786750793\n",
      "Epoch 12, Batch 472, Loss: 0.4839589595794678\n",
      "Epoch 12, Batch 473, Loss: 0.4818255603313446\n",
      "Epoch 12, Batch 474, Loss: 0.5325058102607727\n",
      "Epoch 12, Batch 475, Loss: 0.38737383484840393\n",
      "Epoch 12, Batch 476, Loss: 0.4234597086906433\n",
      "Epoch 12, Batch 477, Loss: 0.5836707353591919\n",
      "Epoch 12, Batch 478, Loss: 0.6089801788330078\n",
      "Epoch 12, Batch 479, Loss: 0.5029038190841675\n",
      "Epoch 12, Batch 480, Loss: 0.5236764550209045\n",
      "Epoch 12, Batch 481, Loss: 0.32636013627052307\n",
      "Epoch 12, Batch 482, Loss: 0.9213374853134155\n",
      "Epoch 12, Batch 483, Loss: 0.5400789976119995\n",
      "Epoch 12, Batch 484, Loss: 0.4072863459587097\n",
      "Epoch 12, Batch 485, Loss: 0.5693871974945068\n",
      "Epoch 12, Batch 486, Loss: 0.4439072608947754\n",
      "Epoch 12, Batch 487, Loss: 0.49163785576820374\n",
      "Epoch 12, Batch 488, Loss: 0.5093348622322083\n",
      "Epoch 12, Batch 489, Loss: 0.5106486082077026\n",
      "Epoch 12, Batch 490, Loss: 0.5169828534126282\n",
      "Epoch 12, Batch 491, Loss: 0.4229426681995392\n",
      "Epoch 12, Batch 492, Loss: 0.47494226694107056\n",
      "Epoch 12, Batch 493, Loss: 0.5636307001113892\n",
      "Epoch 12, Batch 494, Loss: 0.36156782507896423\n",
      "Epoch 12, Batch 495, Loss: 0.5421755909919739\n",
      "Epoch 12, Batch 496, Loss: 0.542884111404419\n",
      "Epoch 12, Batch 497, Loss: 0.49785324931144714\n",
      "Epoch 12, Batch 498, Loss: 0.46045058965682983\n",
      "Epoch 12, Batch 499, Loss: 0.3945773243904114\n",
      "Epoch 12, Batch 500, Loss: 0.7284365892410278\n",
      "Epoch 12, Batch 501, Loss: 0.4623381197452545\n",
      "Epoch 12, Batch 502, Loss: 0.48223549127578735\n",
      "Epoch 12, Batch 503, Loss: 0.4392620325088501\n",
      "Epoch 12, Batch 504, Loss: 0.35886505246162415\n",
      "Epoch 12, Batch 505, Loss: 0.43105143308639526\n",
      "Epoch 12, Batch 506, Loss: 0.5992698669433594\n",
      "Epoch 12, Batch 507, Loss: 0.39471131563186646\n",
      "Epoch 12, Batch 508, Loss: 0.4571322500705719\n",
      "Epoch 12, Batch 509, Loss: 0.3402242660522461\n",
      "Epoch 12, Batch 510, Loss: 0.4368011951446533\n",
      "Epoch 12, Batch 511, Loss: 0.32184523344039917\n",
      "Epoch 12, Batch 512, Loss: 0.3368830680847168\n",
      "Epoch 12, Batch 513, Loss: 0.4895569086074829\n",
      "Epoch 12, Batch 514, Loss: 0.7445377707481384\n",
      "Epoch 12, Batch 515, Loss: 0.3775239884853363\n",
      "Epoch 12, Batch 516, Loss: 0.46366745233535767\n",
      "Epoch 12, Batch 517, Loss: 0.3241598904132843\n",
      "Epoch 12, Batch 518, Loss: 0.4603213667869568\n",
      "Epoch 12, Batch 519, Loss: 0.3926932215690613\n",
      "Epoch 12, Batch 520, Loss: 0.5771204233169556\n",
      "Epoch 12, Batch 521, Loss: 0.4247831702232361\n",
      "Epoch 12, Batch 522, Loss: 0.7019113302230835\n",
      "Epoch 12, Batch 523, Loss: 0.37281107902526855\n",
      "Epoch 12, Batch 524, Loss: 0.3842755854129791\n",
      "Epoch 12, Batch 525, Loss: 0.35613083839416504\n",
      "Epoch 12, Batch 526, Loss: 0.4576626420021057\n",
      "Epoch 12, Batch 527, Loss: 0.3564348518848419\n",
      "Epoch 12, Batch 528, Loss: 0.5680820941925049\n",
      "Epoch 12, Batch 529, Loss: 0.623097836971283\n",
      "Epoch 12, Batch 530, Loss: 0.44346144795417786\n",
      "Epoch 12, Batch 531, Loss: 0.630459725856781\n",
      "Epoch 12, Batch 532, Loss: 0.4558309316635132\n",
      "Epoch 12, Batch 533, Loss: 0.5218607783317566\n",
      "Epoch 12, Batch 534, Loss: 0.42213499546051025\n",
      "Epoch 12, Batch 535, Loss: 0.46709445118904114\n",
      "Epoch 12, Batch 536, Loss: 0.5223734378814697\n",
      "Epoch 12, Batch 537, Loss: 0.5206501483917236\n",
      "Epoch 12, Batch 538, Loss: 0.4250844120979309\n",
      "Epoch 12, Batch 539, Loss: 0.4934712052345276\n",
      "Epoch 12, Batch 540, Loss: 0.6100285649299622\n",
      "Epoch 12, Batch 541, Loss: 0.4034929871559143\n",
      "Epoch 12, Batch 542, Loss: 0.49563828110694885\n",
      "Epoch 12, Batch 543, Loss: 0.46671637892723083\n",
      "Epoch 12, Batch 544, Loss: 0.5623425841331482\n",
      "Epoch 12, Batch 545, Loss: 0.4847428798675537\n",
      "Epoch 12, Batch 546, Loss: 0.26078060269355774\n",
      "Epoch 12, Batch 547, Loss: 0.48814448714256287\n",
      "Epoch 12, Batch 548, Loss: 0.5191405415534973\n",
      "Epoch 12, Batch 549, Loss: 0.456033319234848\n",
      "Epoch 12, Batch 550, Loss: 0.38906824588775635\n",
      "Epoch 12, Batch 551, Loss: 0.5104143023490906\n",
      "Epoch 12, Batch 552, Loss: 0.5086692571640015\n",
      "Epoch 12, Batch 553, Loss: 0.4866398572921753\n",
      "Epoch 12, Batch 554, Loss: 0.4051368236541748\n",
      "Epoch 12, Batch 555, Loss: 0.651688814163208\n",
      "Epoch 12, Batch 556, Loss: 0.5228589773178101\n",
      "Epoch 12, Batch 557, Loss: 0.3664252758026123\n",
      "Epoch 12, Batch 558, Loss: 0.4868212044239044\n",
      "Epoch 12, Batch 559, Loss: 0.48791903257369995\n",
      "Epoch 12, Batch 560, Loss: 0.5114251375198364\n",
      "Epoch 12, Batch 561, Loss: 0.5514534115791321\n",
      "Epoch 12, Batch 562, Loss: 0.4503120183944702\n",
      "Epoch 12, Batch 563, Loss: 0.44033315777778625\n",
      "Epoch 12, Batch 564, Loss: 0.5547362565994263\n",
      "Epoch 12, Batch 565, Loss: 0.5148468613624573\n",
      "Epoch 12, Batch 566, Loss: 0.632403552532196\n",
      "Epoch 12, Batch 567, Loss: 0.5561910271644592\n",
      "Epoch 12, Batch 568, Loss: 0.6204731464385986\n",
      "Epoch 12, Batch 569, Loss: 0.4868847131729126\n",
      "Epoch 12, Batch 570, Loss: 0.3976645767688751\n",
      "Epoch 12, Batch 571, Loss: 0.6815142631530762\n",
      "Epoch 12, Batch 572, Loss: 0.5684797167778015\n",
      "Epoch 12, Batch 573, Loss: 0.48927995562553406\n",
      "Epoch 12, Batch 574, Loss: 0.5189671516418457\n",
      "Epoch 12, Batch 575, Loss: 0.4275907278060913\n",
      "Epoch 12, Batch 576, Loss: 0.5013636350631714\n",
      "Epoch 12, Batch 577, Loss: 0.5452255010604858\n",
      "Epoch 12, Batch 578, Loss: 0.4832632839679718\n",
      "Epoch 12, Batch 579, Loss: 0.6703351736068726\n",
      "Epoch 12, Batch 580, Loss: 0.4624654948711395\n",
      "Epoch 12, Batch 581, Loss: 0.33854198455810547\n",
      "Epoch 12, Batch 582, Loss: 0.4273512065410614\n",
      "Epoch 12, Batch 583, Loss: 0.6815539002418518\n",
      "Epoch 12, Batch 584, Loss: 0.6545264720916748\n",
      "Epoch 12, Batch 585, Loss: 0.48034238815307617\n",
      "Epoch 12, Batch 586, Loss: 0.5608120560646057\n",
      "Epoch 12, Batch 587, Loss: 0.5470060110092163\n",
      "Epoch 12, Batch 588, Loss: 0.39811909198760986\n",
      "Epoch 12, Batch 589, Loss: 0.5897035598754883\n",
      "Epoch 12, Batch 590, Loss: 0.46466946601867676\n",
      "Epoch 12, Batch 591, Loss: 0.3894214630126953\n",
      "Epoch 12, Batch 592, Loss: 0.5133230090141296\n",
      "Epoch 12, Batch 593, Loss: 0.4296523332595825\n",
      "Epoch 12, Batch 594, Loss: 0.44264182448387146\n",
      "Epoch 12, Batch 595, Loss: 0.5895524621009827\n",
      "Epoch 12, Batch 596, Loss: 0.4597533941268921\n",
      "Epoch 12, Batch 597, Loss: 0.4386255741119385\n",
      "Epoch 12, Batch 598, Loss: 0.4264717102050781\n",
      "Epoch 12, Batch 599, Loss: 0.4433157742023468\n",
      "Epoch 12, Batch 600, Loss: 0.42695480585098267\n",
      "Epoch 12, Batch 601, Loss: 0.5357741713523865\n",
      "Epoch 12, Batch 602, Loss: 0.477830171585083\n",
      "Epoch 12, Batch 603, Loss: 0.40397822856903076\n",
      "Epoch 12, Batch 604, Loss: 0.354516863822937\n",
      "Epoch 12, Batch 605, Loss: 0.5295408368110657\n",
      "Epoch 12, Batch 606, Loss: 0.7280877828598022\n",
      "Epoch 12, Batch 607, Loss: 0.45481616258621216\n",
      "Epoch 12, Batch 608, Loss: 0.5730882287025452\n",
      "Epoch 12, Batch 609, Loss: 0.5987857580184937\n",
      "Epoch 12, Batch 610, Loss: 0.5246187448501587\n",
      "Epoch 12, Batch 611, Loss: 0.5428736805915833\n",
      "Epoch 12, Batch 612, Loss: 0.47792649269104004\n",
      "Epoch 12, Batch 613, Loss: 0.4488695561885834\n",
      "Epoch 12, Batch 614, Loss: 0.47538045048713684\n",
      "Epoch 12, Batch 615, Loss: 0.4374765157699585\n",
      "Epoch 12, Batch 616, Loss: 0.5362319946289062\n",
      "Epoch 12, Batch 617, Loss: 0.41700661182403564\n",
      "Epoch 12, Batch 618, Loss: 0.5025753974914551\n",
      "Epoch 12, Batch 619, Loss: 0.2948801517486572\n",
      "Epoch 12, Batch 620, Loss: 0.5457530617713928\n",
      "Epoch 12, Batch 621, Loss: 0.4584290683269501\n",
      "Epoch 12, Batch 622, Loss: 0.33533385396003723\n",
      "Epoch 12, Batch 623, Loss: 0.39702868461608887\n",
      "Epoch 12, Batch 624, Loss: 0.5928530097007751\n",
      "Epoch 12, Batch 625, Loss: 0.7869667410850525\n",
      "Epoch 12, Batch 626, Loss: 0.36766257882118225\n",
      "Epoch 12, Batch 627, Loss: 0.2676685154438019\n",
      "Epoch 12, Batch 628, Loss: 0.3489917814731598\n",
      "Epoch 12, Batch 629, Loss: 0.4176385700702667\n",
      "Epoch 12, Batch 630, Loss: 0.38579925894737244\n",
      "Epoch 12, Batch 631, Loss: 0.48323166370391846\n",
      "Epoch 12, Batch 632, Loss: 0.4426145851612091\n",
      "Epoch 12, Batch 633, Loss: 0.8467042446136475\n",
      "Epoch 12, Batch 634, Loss: 0.39659881591796875\n",
      "Epoch 12, Batch 635, Loss: 0.46428948640823364\n",
      "Epoch 12, Batch 636, Loss: 0.5508309006690979\n",
      "Epoch 12, Batch 637, Loss: 0.5425160527229309\n",
      "Epoch 12, Batch 638, Loss: 0.6079633831977844\n",
      "Epoch 12, Batch 639, Loss: 0.397769957780838\n",
      "Epoch 12, Batch 640, Loss: 0.5636628270149231\n",
      "Epoch 12, Batch 641, Loss: 0.5010046362876892\n",
      "Epoch 12, Batch 642, Loss: 0.4362906217575073\n",
      "Epoch 12, Batch 643, Loss: 0.41217517852783203\n",
      "Epoch 12, Batch 644, Loss: 0.6301408410072327\n",
      "Epoch 12, Batch 645, Loss: 0.47654712200164795\n",
      "Epoch 12, Batch 646, Loss: 0.6152779459953308\n",
      "Epoch 12, Batch 647, Loss: 0.45295125246047974\n",
      "Epoch 12, Batch 648, Loss: 0.42031580209732056\n",
      "Epoch 12, Batch 649, Loss: 0.4965059459209442\n",
      "Epoch 12, Batch 650, Loss: 0.5186947584152222\n",
      "Epoch 12, Batch 651, Loss: 0.6119495630264282\n",
      "Epoch 12, Batch 652, Loss: 0.44333702325820923\n",
      "Epoch 12, Batch 653, Loss: 0.4343743324279785\n",
      "Epoch 12, Batch 654, Loss: 0.3853466510772705\n",
      "Epoch 12, Batch 655, Loss: 0.5457807183265686\n",
      "Epoch 12, Batch 656, Loss: 0.4862477481365204\n",
      "Epoch 12, Batch 657, Loss: 0.38122713565826416\n",
      "Epoch 12, Batch 658, Loss: 0.5120506882667542\n",
      "Epoch 12, Batch 659, Loss: 0.4693544805049896\n",
      "Epoch 12, Batch 660, Loss: 0.34282249212265015\n",
      "Epoch 12, Batch 661, Loss: 0.49015018343925476\n",
      "Epoch 12, Batch 662, Loss: 0.3870750069618225\n",
      "Epoch 12, Batch 663, Loss: 0.4293608069419861\n",
      "Epoch 12, Batch 664, Loss: 0.5888525247573853\n",
      "Epoch 12, Batch 665, Loss: 0.5258322358131409\n",
      "Epoch 12, Batch 666, Loss: 0.5137928128242493\n",
      "Epoch 12, Batch 667, Loss: 0.708729088306427\n",
      "Epoch 12, Batch 668, Loss: 0.5220327973365784\n",
      "Epoch 12, Batch 669, Loss: 0.3698163628578186\n",
      "Epoch 12, Batch 670, Loss: 0.4739479124546051\n",
      "Epoch 12, Batch 671, Loss: 0.5447444915771484\n",
      "Epoch 12, Batch 672, Loss: 0.40476399660110474\n",
      "Epoch 12, Batch 673, Loss: 0.398438960313797\n",
      "Epoch 12, Batch 674, Loss: 0.5022506713867188\n",
      "Epoch 12, Batch 675, Loss: 0.4854245185852051\n",
      "Epoch 12, Batch 676, Loss: 0.369048535823822\n",
      "Epoch 12, Batch 677, Loss: 0.4254666864871979\n",
      "Epoch 12, Batch 678, Loss: 0.42808836698532104\n",
      "Epoch 12, Batch 679, Loss: 0.595215380191803\n",
      "Epoch 12, Batch 680, Loss: 0.6129829287528992\n",
      "Epoch 12, Batch 681, Loss: 0.44548141956329346\n",
      "Epoch 12, Batch 682, Loss: 0.6254459619522095\n",
      "Epoch 12, Batch 683, Loss: 0.46325814723968506\n",
      "Epoch 12, Batch 684, Loss: 0.5724477171897888\n",
      "Epoch 12, Batch 685, Loss: 0.497745156288147\n",
      "Epoch 12, Batch 686, Loss: 0.3543607294559479\n",
      "Epoch 12, Batch 687, Loss: 0.36378219723701477\n",
      "Epoch 12, Batch 688, Loss: 0.26711681485176086\n",
      "Epoch 12, Batch 689, Loss: 0.5054410099983215\n",
      "Epoch 12, Batch 690, Loss: 0.5051832795143127\n",
      "Epoch 12, Batch 691, Loss: 0.5861457586288452\n",
      "Epoch 12, Batch 692, Loss: 0.706121563911438\n",
      "Epoch 12, Batch 693, Loss: 0.5575757026672363\n",
      "Epoch 12, Batch 694, Loss: 0.543121874332428\n",
      "Epoch 12, Batch 695, Loss: 0.43803608417510986\n",
      "Epoch 12, Batch 696, Loss: 0.47674185037612915\n",
      "Epoch 12, Batch 697, Loss: 0.5664430260658264\n",
      "Epoch 12, Batch 698, Loss: 0.6881976127624512\n",
      "Epoch 12, Batch 699, Loss: 0.7973049879074097\n",
      "Epoch 12, Batch 700, Loss: 0.48627111315727234\n",
      "Epoch 12, Batch 701, Loss: 0.45743390917778015\n",
      "Epoch 12, Batch 702, Loss: 0.36700141429901123\n",
      "Epoch 12, Batch 703, Loss: 0.5689804553985596\n",
      "Epoch 12, Batch 704, Loss: 0.3730517327785492\n",
      "Epoch 12, Batch 705, Loss: 0.39563873410224915\n",
      "Epoch 12, Batch 706, Loss: 0.5598584413528442\n",
      "Epoch 12, Batch 707, Loss: 0.5290323495864868\n",
      "Epoch 12, Batch 708, Loss: 0.7268725037574768\n",
      "Epoch 12, Batch 709, Loss: 0.4629839360713959\n",
      "Epoch 12, Batch 710, Loss: 0.3810406029224396\n",
      "Epoch 12, Batch 711, Loss: 0.391876220703125\n",
      "Epoch 12, Batch 712, Loss: 0.47808682918548584\n",
      "Epoch 12, Batch 713, Loss: 0.5000722408294678\n",
      "Epoch 12, Batch 714, Loss: 0.5884820818901062\n",
      "Epoch 12, Batch 715, Loss: 0.49046191573143005\n",
      "Epoch 12, Batch 716, Loss: 0.41498690843582153\n",
      "Epoch 12, Batch 717, Loss: 0.4907815456390381\n",
      "Epoch 12, Batch 718, Loss: 0.5545362830162048\n",
      "Epoch 12, Batch 719, Loss: 0.3372804522514343\n",
      "Epoch 12, Batch 720, Loss: 0.5776353478431702\n",
      "Epoch 12, Batch 721, Loss: 0.4726826548576355\n",
      "Epoch 12, Batch 722, Loss: 0.7315429449081421\n",
      "Epoch 12, Batch 723, Loss: 0.4701780676841736\n",
      "Epoch 12, Batch 724, Loss: 0.39296412467956543\n",
      "Epoch 12, Batch 725, Loss: 0.3946838974952698\n",
      "Epoch 12, Batch 726, Loss: 0.44429638981819153\n",
      "Epoch 12, Batch 727, Loss: 0.6980774402618408\n",
      "Epoch 12, Batch 728, Loss: 0.49004220962524414\n",
      "Epoch 12, Batch 729, Loss: 0.4393576681613922\n",
      "Epoch 12, Batch 730, Loss: 0.4139261543750763\n",
      "Epoch 12, Batch 731, Loss: 0.3718344569206238\n",
      "Epoch 12, Batch 732, Loss: 0.8072553277015686\n",
      "Epoch 12, Batch 733, Loss: 0.683445394039154\n",
      "Epoch 12, Batch 734, Loss: 0.5550854802131653\n",
      "Epoch 12, Batch 735, Loss: 0.7547546029090881\n",
      "Epoch 12, Batch 736, Loss: 0.5623601078987122\n",
      "Epoch 12, Batch 737, Loss: 0.5474331974983215\n",
      "Epoch 12, Batch 738, Loss: 0.5175377130508423\n",
      "Epoch 12, Batch 739, Loss: 0.3049792945384979\n",
      "Epoch 12, Batch 740, Loss: 0.6480987668037415\n",
      "Epoch 12, Batch 741, Loss: 0.24136769771575928\n",
      "Epoch 12, Batch 742, Loss: 0.37341880798339844\n",
      "Epoch 12, Batch 743, Loss: 0.4587303400039673\n",
      "Epoch 12, Batch 744, Loss: 0.6156361699104309\n",
      "Epoch 12, Batch 745, Loss: 0.5966837406158447\n",
      "Epoch 12, Batch 746, Loss: 0.3837089240550995\n",
      "Epoch 12, Batch 747, Loss: 0.44619667530059814\n",
      "Epoch 12, Batch 748, Loss: 0.643666684627533\n",
      "Epoch 12, Batch 749, Loss: 0.49830007553100586\n",
      "Epoch 12, Batch 750, Loss: 0.5414345264434814\n",
      "Epoch 12, Batch 751, Loss: 0.5227872729301453\n",
      "Epoch 12, Batch 752, Loss: 0.48832663893699646\n",
      "Epoch 12, Batch 753, Loss: 0.4222685396671295\n",
      "Epoch 12, Batch 754, Loss: 0.5128845572471619\n",
      "Epoch 12, Batch 755, Loss: 0.4800044894218445\n",
      "Epoch 12, Batch 756, Loss: 0.527353823184967\n",
      "Epoch 12, Batch 757, Loss: 0.40352383255958557\n",
      "Epoch 12, Batch 758, Loss: 0.3705189824104309\n",
      "Epoch 12, Batch 759, Loss: 0.5100985765457153\n",
      "Epoch 12, Batch 760, Loss: 0.5040477514266968\n",
      "Epoch 12, Batch 761, Loss: 0.45483696460723877\n",
      "Epoch 12, Batch 762, Loss: 0.5483725070953369\n",
      "Epoch 12, Batch 763, Loss: 0.5001441836357117\n",
      "Epoch 12, Batch 764, Loss: 0.4946667551994324\n",
      "Epoch 12, Batch 765, Loss: 0.5369313359260559\n",
      "Epoch 12, Batch 766, Loss: 0.3816194534301758\n",
      "Epoch 12, Batch 767, Loss: 0.45966461300849915\n",
      "Epoch 12, Batch 768, Loss: 0.42670148611068726\n",
      "Epoch 12, Batch 769, Loss: 0.3629320561885834\n",
      "Epoch 12, Batch 770, Loss: 0.43232280015945435\n",
      "Epoch 12, Batch 771, Loss: 0.5506249666213989\n",
      "Epoch 12, Batch 772, Loss: 0.5575302839279175\n",
      "Epoch 12, Batch 773, Loss: 0.333976686000824\n",
      "Epoch 12, Batch 774, Loss: 0.4664086401462555\n",
      "Epoch 12, Batch 775, Loss: 0.5845236778259277\n",
      "Epoch 12, Batch 776, Loss: 0.6405524015426636\n",
      "Epoch 12, Batch 777, Loss: 0.2533954679965973\n",
      "Epoch 12, Batch 778, Loss: 0.4093009829521179\n",
      "Epoch 12, Batch 779, Loss: 0.45029884576797485\n",
      "Epoch 12, Batch 780, Loss: 0.6714053750038147\n",
      "Epoch 12, Batch 781, Loss: 0.6751090884208679\n",
      "Epoch 12, Batch 782, Loss: 0.5217062830924988\n",
      "Epoch 12, Batch 783, Loss: 0.4307185411453247\n",
      "Epoch 12, Batch 784, Loss: 0.6475489139556885\n",
      "Epoch 12, Batch 785, Loss: 0.5194841027259827\n",
      "Epoch 12, Batch 786, Loss: 0.5569323897361755\n",
      "Epoch 12, Batch 787, Loss: 0.47578302025794983\n",
      "Epoch 12, Batch 788, Loss: 0.5639805197715759\n",
      "Epoch 12, Batch 789, Loss: 0.501960813999176\n",
      "Epoch 12, Batch 790, Loss: 0.5160882472991943\n",
      "Epoch 12, Batch 791, Loss: 0.6104735136032104\n",
      "Epoch 12, Batch 792, Loss: 0.47985807061195374\n",
      "Epoch 12, Batch 793, Loss: 0.3720434904098511\n",
      "Epoch 12, Batch 794, Loss: 0.34293079376220703\n",
      "Epoch 12, Batch 795, Loss: 0.40945979952812195\n",
      "Epoch 12, Batch 796, Loss: 0.5439152717590332\n",
      "Epoch 12, Batch 797, Loss: 0.5025113224983215\n",
      "Epoch 12, Batch 798, Loss: 0.5158268809318542\n",
      "Epoch 12, Batch 799, Loss: 0.4140034317970276\n",
      "Epoch 12, Batch 800, Loss: 0.2580379545688629\n",
      "Epoch 12, Batch 801, Loss: 0.40043920278549194\n",
      "Epoch 12, Batch 802, Loss: 0.6830925941467285\n",
      "Epoch 12, Batch 803, Loss: 0.7492356896400452\n",
      "Epoch 12, Batch 804, Loss: 0.5432671308517456\n",
      "Epoch 12, Batch 805, Loss: 0.5304722785949707\n",
      "Epoch 12, Batch 806, Loss: 0.49012482166290283\n",
      "Epoch 12, Batch 807, Loss: 0.4403199255466461\n",
      "Epoch 12, Batch 808, Loss: 0.3008551597595215\n",
      "Epoch 12, Batch 809, Loss: 0.45267748832702637\n",
      "Epoch 12, Batch 810, Loss: 0.41793736815452576\n",
      "Epoch 12, Batch 811, Loss: 0.2930242121219635\n",
      "Epoch 12, Batch 812, Loss: 0.45561683177948\n",
      "Epoch 12, Batch 813, Loss: 0.5087326169013977\n",
      "Epoch 12, Batch 814, Loss: 0.5914740562438965\n",
      "Epoch 12, Batch 815, Loss: 0.45645058155059814\n",
      "Epoch 12, Batch 816, Loss: 0.6566355228424072\n",
      "Epoch 12, Batch 817, Loss: 0.5453442335128784\n",
      "Epoch 12, Batch 818, Loss: 0.2935197353363037\n",
      "Epoch 12, Batch 819, Loss: 0.5052462816238403\n",
      "Epoch 12, Batch 820, Loss: 0.38883456587791443\n",
      "Epoch 12, Batch 821, Loss: 0.49451205134391785\n",
      "Epoch 12, Batch 822, Loss: 0.3832685947418213\n",
      "Epoch 12, Batch 823, Loss: 0.4619138240814209\n",
      "Epoch 12, Batch 824, Loss: 0.586451530456543\n",
      "Epoch 12, Batch 825, Loss: 0.5736996531486511\n",
      "Epoch 12, Batch 826, Loss: 0.5026763677597046\n",
      "Epoch 12, Batch 827, Loss: 0.4460669159889221\n",
      "Epoch 12, Batch 828, Loss: 0.4857444167137146\n",
      "Epoch 12, Batch 829, Loss: 0.5310630202293396\n",
      "Epoch 12, Batch 830, Loss: 0.715789258480072\n",
      "Epoch 12, Batch 831, Loss: 0.4533287286758423\n",
      "Epoch 12, Batch 832, Loss: 0.4145267605781555\n",
      "Epoch 12, Batch 833, Loss: 0.6191753149032593\n",
      "Epoch 12, Batch 834, Loss: 0.7521085739135742\n",
      "Epoch 12, Batch 835, Loss: 0.32193776965141296\n",
      "Epoch 12, Batch 836, Loss: 0.5262389779090881\n",
      "Epoch 12, Batch 837, Loss: 0.4652332067489624\n",
      "Epoch 12, Batch 838, Loss: 0.4971308708190918\n",
      "Epoch 12, Batch 839, Loss: 0.5055747032165527\n",
      "Epoch 12, Batch 840, Loss: 0.6276257038116455\n",
      "Epoch 12, Batch 841, Loss: 0.5012617111206055\n",
      "Epoch 12, Batch 842, Loss: 0.47178590297698975\n",
      "Epoch 12, Batch 843, Loss: 0.5105569362640381\n",
      "Epoch 12, Batch 844, Loss: 0.5305101871490479\n",
      "Epoch 12, Batch 845, Loss: 0.5341300964355469\n",
      "Epoch 12, Batch 846, Loss: 0.5218325257301331\n",
      "Epoch 12, Batch 847, Loss: 0.22351494431495667\n",
      "Epoch 12, Batch 848, Loss: 0.4604008197784424\n",
      "Epoch 12, Batch 849, Loss: 0.4288555383682251\n",
      "Epoch 12, Batch 850, Loss: 0.706860363483429\n",
      "Epoch 12, Batch 851, Loss: 0.37915492057800293\n",
      "Epoch 12, Batch 852, Loss: 0.525425374507904\n",
      "Epoch 12, Batch 853, Loss: 0.456034779548645\n",
      "Epoch 12, Batch 854, Loss: 0.5767505764961243\n",
      "Epoch 12, Batch 855, Loss: 0.47944337129592896\n",
      "Epoch 12, Batch 856, Loss: 0.35521677136421204\n",
      "Epoch 12, Batch 857, Loss: 0.4668324589729309\n",
      "Epoch 12, Batch 858, Loss: 0.40265700221061707\n",
      "Epoch 12, Batch 859, Loss: 0.5071364045143127\n",
      "Epoch 12, Batch 860, Loss: 0.3951141834259033\n",
      "Epoch 12, Batch 861, Loss: 0.7702656984329224\n",
      "Epoch 12, Batch 862, Loss: 0.6602867841720581\n",
      "Epoch 12, Batch 863, Loss: 0.5627062916755676\n",
      "Epoch 12, Batch 864, Loss: 0.5527525544166565\n",
      "Epoch 12, Batch 865, Loss: 0.6419448852539062\n",
      "Epoch 12, Batch 866, Loss: 0.5050017833709717\n",
      "Epoch 12, Batch 867, Loss: 0.5364328026771545\n",
      "Epoch 12, Batch 868, Loss: 0.4038071632385254\n",
      "Epoch 12, Batch 869, Loss: 0.6234939694404602\n",
      "Epoch 12, Batch 870, Loss: 0.6132196187973022\n",
      "Epoch 12, Batch 871, Loss: 0.4028119742870331\n",
      "Epoch 12, Batch 872, Loss: 0.654593825340271\n",
      "Epoch 12, Batch 873, Loss: 0.5219869017601013\n",
      "Epoch 12, Batch 874, Loss: 0.4002370834350586\n",
      "Epoch 12, Batch 875, Loss: 0.5702744126319885\n",
      "Epoch 12, Batch 876, Loss: 0.4424892067909241\n",
      "Epoch 12, Batch 877, Loss: 0.38254857063293457\n",
      "Epoch 12, Batch 878, Loss: 0.4086645841598511\n",
      "Epoch 12, Batch 879, Loss: 0.4234123229980469\n",
      "Epoch 12, Batch 880, Loss: 0.5903957486152649\n",
      "Epoch 12, Batch 881, Loss: 0.5029612183570862\n",
      "Epoch 12, Batch 882, Loss: 0.5077641606330872\n",
      "Epoch 12, Batch 883, Loss: 0.4124007821083069\n",
      "Epoch 12, Batch 884, Loss: 0.3967243731021881\n",
      "Epoch 12, Batch 885, Loss: 0.3438494801521301\n",
      "Epoch 12, Batch 886, Loss: 0.35168910026550293\n",
      "Epoch 12, Batch 887, Loss: 0.47476404905319214\n",
      "Epoch 12, Batch 888, Loss: 0.419742614030838\n",
      "Epoch 12, Batch 889, Loss: 0.3577250838279724\n",
      "Epoch 12, Batch 890, Loss: 0.46157771348953247\n",
      "Epoch 12, Batch 891, Loss: 0.5028218030929565\n",
      "Epoch 12, Batch 892, Loss: 0.678947925567627\n",
      "Epoch 12, Batch 893, Loss: 0.4736579358577728\n",
      "Epoch 12, Batch 894, Loss: 0.5619661211967468\n",
      "Epoch 12, Batch 895, Loss: 0.4382952153682709\n",
      "Epoch 12, Batch 896, Loss: 0.5059692859649658\n",
      "Epoch 12, Batch 897, Loss: 0.3834967613220215\n",
      "Epoch 12, Batch 898, Loss: 0.5678239464759827\n",
      "Epoch 12, Batch 899, Loss: 0.48360520601272583\n",
      "Epoch 12, Batch 900, Loss: 0.4799681305885315\n",
      "Epoch 12, Batch 901, Loss: 0.3815339207649231\n",
      "Epoch 12, Batch 902, Loss: 0.7006152868270874\n",
      "Epoch 12, Batch 903, Loss: 0.5890922546386719\n",
      "Epoch 12, Batch 904, Loss: 0.4891465902328491\n",
      "Epoch 12, Batch 905, Loss: 0.4824688136577606\n",
      "Epoch 12, Batch 906, Loss: 0.6047273278236389\n",
      "Epoch 12, Batch 907, Loss: 0.5399847626686096\n",
      "Epoch 12, Batch 908, Loss: 0.3847835958003998\n",
      "Epoch 12, Batch 909, Loss: 0.37318897247314453\n",
      "Epoch 12, Batch 910, Loss: 0.43736571073532104\n",
      "Epoch 12, Batch 911, Loss: 0.5153023600578308\n",
      "Epoch 12, Batch 912, Loss: 0.34535688161849976\n",
      "Epoch 12, Batch 913, Loss: 0.5061020851135254\n",
      "Epoch 12, Batch 914, Loss: 0.436170756816864\n",
      "Epoch 12, Batch 915, Loss: 0.47511982917785645\n",
      "Epoch 12, Batch 916, Loss: 0.5549627542495728\n",
      "Epoch 12, Batch 917, Loss: 0.36573541164398193\n",
      "Epoch 12, Batch 918, Loss: 0.548673152923584\n",
      "Epoch 12, Batch 919, Loss: 0.40149691700935364\n",
      "Epoch 12, Batch 920, Loss: 0.46464547514915466\n",
      "Epoch 12, Batch 921, Loss: 0.5490780472755432\n",
      "Epoch 12, Batch 922, Loss: 0.47097665071487427\n",
      "Epoch 12, Batch 923, Loss: 0.4027763605117798\n",
      "Epoch 12, Batch 924, Loss: 0.4516931176185608\n",
      "Epoch 12, Batch 925, Loss: 0.5529348254203796\n",
      "Epoch 12, Batch 926, Loss: 0.4137950539588928\n",
      "Epoch 12, Batch 927, Loss: 0.3189573884010315\n",
      "Epoch 12, Batch 928, Loss: 0.6943861246109009\n",
      "Epoch 12, Batch 929, Loss: 0.607582688331604\n",
      "Epoch 12, Batch 930, Loss: 0.4060039520263672\n",
      "Epoch 12, Batch 931, Loss: 0.5114514231681824\n",
      "Epoch 12, Batch 932, Loss: 0.3004620671272278\n",
      "Epoch 12, Batch 933, Loss: 0.4977636933326721\n",
      "Epoch 12, Batch 934, Loss: 0.3359249234199524\n",
      "Epoch 12, Batch 935, Loss: 0.4063076972961426\n",
      "Epoch 12, Batch 936, Loss: 0.4793560206890106\n",
      "Epoch 12, Batch 937, Loss: 0.5342152714729309\n",
      "Epoch 12, Batch 938, Loss: 0.36472663283348083\n",
      "Accuracy of train set: 0.8269333333333333\n",
      "Epoch 12, Batch 1, Test Loss: 0.435479998588562\n",
      "Epoch 12, Batch 2, Test Loss: 0.4819810688495636\n",
      "Epoch 12, Batch 3, Test Loss: 0.5650016069412231\n",
      "Epoch 12, Batch 4, Test Loss: 0.3597683012485504\n",
      "Epoch 12, Batch 5, Test Loss: 0.5081359148025513\n",
      "Epoch 12, Batch 6, Test Loss: 0.5553489327430725\n",
      "Epoch 12, Batch 7, Test Loss: 0.3828698992729187\n",
      "Epoch 12, Batch 8, Test Loss: 0.39829179644584656\n",
      "Epoch 12, Batch 9, Test Loss: 0.5449478030204773\n",
      "Epoch 12, Batch 10, Test Loss: 0.47389161586761475\n",
      "Epoch 12, Batch 11, Test Loss: 0.5589601397514343\n",
      "Epoch 12, Batch 12, Test Loss: 0.4973290264606476\n",
      "Epoch 12, Batch 13, Test Loss: 0.5086872577667236\n",
      "Epoch 12, Batch 14, Test Loss: 0.48959362506866455\n",
      "Epoch 12, Batch 15, Test Loss: 0.46666261553764343\n",
      "Epoch 12, Batch 16, Test Loss: 0.5629541277885437\n",
      "Epoch 12, Batch 17, Test Loss: 0.3961140215396881\n",
      "Epoch 12, Batch 18, Test Loss: 0.7821961641311646\n",
      "Epoch 12, Batch 19, Test Loss: 0.5027120113372803\n",
      "Epoch 12, Batch 20, Test Loss: 0.4309643507003784\n",
      "Epoch 12, Batch 21, Test Loss: 0.4543668031692505\n",
      "Epoch 12, Batch 22, Test Loss: 0.3127348721027374\n",
      "Epoch 12, Batch 23, Test Loss: 0.3361271023750305\n",
      "Epoch 12, Batch 24, Test Loss: 0.5032476782798767\n",
      "Epoch 12, Batch 25, Test Loss: 0.5141223669052124\n",
      "Epoch 12, Batch 26, Test Loss: 0.47320622205734253\n",
      "Epoch 12, Batch 27, Test Loss: 0.35266628861427307\n",
      "Epoch 12, Batch 28, Test Loss: 0.7645502090454102\n",
      "Epoch 12, Batch 29, Test Loss: 0.5360386967658997\n",
      "Epoch 12, Batch 30, Test Loss: 0.551884651184082\n",
      "Epoch 12, Batch 31, Test Loss: 0.5890788435935974\n",
      "Epoch 12, Batch 32, Test Loss: 0.493776798248291\n",
      "Epoch 12, Batch 33, Test Loss: 0.6267423629760742\n",
      "Epoch 12, Batch 34, Test Loss: 0.28679797053337097\n",
      "Epoch 12, Batch 35, Test Loss: 0.45073235034942627\n",
      "Epoch 12, Batch 36, Test Loss: 0.3623250722885132\n",
      "Epoch 12, Batch 37, Test Loss: 0.4683898091316223\n",
      "Epoch 12, Batch 38, Test Loss: 0.42776328325271606\n",
      "Epoch 12, Batch 39, Test Loss: 0.5848912596702576\n",
      "Epoch 12, Batch 40, Test Loss: 0.42512238025665283\n",
      "Epoch 12, Batch 41, Test Loss: 0.5747074484825134\n",
      "Epoch 12, Batch 42, Test Loss: 0.429282009601593\n",
      "Epoch 12, Batch 43, Test Loss: 0.271368145942688\n",
      "Epoch 12, Batch 44, Test Loss: 0.4712832570075989\n",
      "Epoch 12, Batch 45, Test Loss: 0.6513407826423645\n",
      "Epoch 12, Batch 46, Test Loss: 0.45917174220085144\n",
      "Epoch 12, Batch 47, Test Loss: 0.6337482333183289\n",
      "Epoch 12, Batch 48, Test Loss: 0.45172837376594543\n",
      "Epoch 12, Batch 49, Test Loss: 0.5169011950492859\n",
      "Epoch 12, Batch 50, Test Loss: 0.4180300235748291\n",
      "Epoch 12, Batch 51, Test Loss: 0.5505293607711792\n",
      "Epoch 12, Batch 52, Test Loss: 0.4247668385505676\n",
      "Epoch 12, Batch 53, Test Loss: 0.37985846400260925\n",
      "Epoch 12, Batch 54, Test Loss: 0.49255192279815674\n",
      "Epoch 12, Batch 55, Test Loss: 0.6113566756248474\n",
      "Epoch 12, Batch 56, Test Loss: 0.4505925476551056\n",
      "Epoch 12, Batch 57, Test Loss: 0.43157270550727844\n",
      "Epoch 12, Batch 58, Test Loss: 0.47868403792381287\n",
      "Epoch 12, Batch 59, Test Loss: 0.47741231322288513\n",
      "Epoch 12, Batch 60, Test Loss: 0.4284931421279907\n",
      "Epoch 12, Batch 61, Test Loss: 0.5443810820579529\n",
      "Epoch 12, Batch 62, Test Loss: 0.40330541133880615\n",
      "Epoch 12, Batch 63, Test Loss: 0.5820844769477844\n",
      "Epoch 12, Batch 64, Test Loss: 0.5569654107093811\n",
      "Epoch 12, Batch 65, Test Loss: 0.4510071873664856\n",
      "Epoch 12, Batch 66, Test Loss: 0.46086496114730835\n",
      "Epoch 12, Batch 67, Test Loss: 0.473136842250824\n",
      "Epoch 12, Batch 68, Test Loss: 0.43305695056915283\n",
      "Epoch 12, Batch 69, Test Loss: 0.2757326066493988\n",
      "Epoch 12, Batch 70, Test Loss: 0.5512403845787048\n",
      "Epoch 12, Batch 71, Test Loss: 0.4580959677696228\n",
      "Epoch 12, Batch 72, Test Loss: 0.45534273982048035\n",
      "Epoch 12, Batch 73, Test Loss: 0.6001729965209961\n",
      "Epoch 12, Batch 74, Test Loss: 0.5374546051025391\n",
      "Epoch 12, Batch 75, Test Loss: 0.2872753143310547\n",
      "Epoch 12, Batch 76, Test Loss: 0.5417174696922302\n",
      "Epoch 12, Batch 77, Test Loss: 0.5102525949478149\n",
      "Epoch 12, Batch 78, Test Loss: 0.5061469078063965\n",
      "Epoch 12, Batch 79, Test Loss: 0.5058699250221252\n",
      "Epoch 12, Batch 80, Test Loss: 0.41655051708221436\n",
      "Epoch 12, Batch 81, Test Loss: 0.34948983788490295\n",
      "Epoch 12, Batch 82, Test Loss: 0.5340486764907837\n",
      "Epoch 12, Batch 83, Test Loss: 0.5992873907089233\n",
      "Epoch 12, Batch 84, Test Loss: 0.5058220028877258\n",
      "Epoch 12, Batch 85, Test Loss: 0.5366926193237305\n",
      "Epoch 12, Batch 86, Test Loss: 0.49805378913879395\n",
      "Epoch 12, Batch 87, Test Loss: 0.2898235321044922\n",
      "Epoch 12, Batch 88, Test Loss: 0.43248194456100464\n",
      "Epoch 12, Batch 89, Test Loss: 0.6731300354003906\n",
      "Epoch 12, Batch 90, Test Loss: 0.45976436138153076\n",
      "Epoch 12, Batch 91, Test Loss: 0.5251342058181763\n",
      "Epoch 12, Batch 92, Test Loss: 0.44563567638397217\n",
      "Epoch 12, Batch 93, Test Loss: 0.29071682691574097\n",
      "Epoch 12, Batch 94, Test Loss: 0.39738261699676514\n",
      "Epoch 12, Batch 95, Test Loss: 0.46120256185531616\n",
      "Epoch 12, Batch 96, Test Loss: 0.4766274690628052\n",
      "Epoch 12, Batch 97, Test Loss: 0.653107762336731\n",
      "Epoch 12, Batch 98, Test Loss: 0.45635393261909485\n",
      "Epoch 12, Batch 99, Test Loss: 0.46681898832321167\n",
      "Epoch 12, Batch 100, Test Loss: 0.4237954616546631\n",
      "Epoch 12, Batch 101, Test Loss: 0.24213753640651703\n",
      "Epoch 12, Batch 102, Test Loss: 0.4571078419685364\n",
      "Epoch 12, Batch 103, Test Loss: 0.4067000448703766\n",
      "Epoch 12, Batch 104, Test Loss: 0.3488597869873047\n",
      "Epoch 12, Batch 105, Test Loss: 0.5896273255348206\n",
      "Epoch 12, Batch 106, Test Loss: 0.469777375459671\n",
      "Epoch 12, Batch 107, Test Loss: 0.44941025972366333\n",
      "Epoch 12, Batch 108, Test Loss: 0.3747262954711914\n",
      "Epoch 12, Batch 109, Test Loss: 0.3497263193130493\n",
      "Epoch 12, Batch 110, Test Loss: 0.4745492935180664\n",
      "Epoch 12, Batch 111, Test Loss: 0.5345894694328308\n",
      "Epoch 12, Batch 112, Test Loss: 0.37473779916763306\n",
      "Epoch 12, Batch 113, Test Loss: 0.41668522357940674\n",
      "Epoch 12, Batch 114, Test Loss: 0.5290619134902954\n",
      "Epoch 12, Batch 115, Test Loss: 0.44590383768081665\n",
      "Epoch 12, Batch 116, Test Loss: 0.3768344521522522\n",
      "Epoch 12, Batch 117, Test Loss: 0.41616374254226685\n",
      "Epoch 12, Batch 118, Test Loss: 0.5704806447029114\n",
      "Epoch 12, Batch 119, Test Loss: 0.4563279449939728\n",
      "Epoch 12, Batch 120, Test Loss: 0.49135246872901917\n",
      "Epoch 12, Batch 121, Test Loss: 0.3500789701938629\n",
      "Epoch 12, Batch 122, Test Loss: 0.6607900261878967\n",
      "Epoch 12, Batch 123, Test Loss: 0.3885059356689453\n",
      "Epoch 12, Batch 124, Test Loss: 0.5040112137794495\n",
      "Epoch 12, Batch 125, Test Loss: 0.2451779544353485\n",
      "Epoch 12, Batch 126, Test Loss: 0.7573692798614502\n",
      "Epoch 12, Batch 127, Test Loss: 0.5258787870407104\n",
      "Epoch 12, Batch 128, Test Loss: 0.6854767203330994\n",
      "Epoch 12, Batch 129, Test Loss: 0.5609815120697021\n",
      "Epoch 12, Batch 130, Test Loss: 0.7290104627609253\n",
      "Epoch 12, Batch 131, Test Loss: 0.2959786653518677\n",
      "Epoch 12, Batch 132, Test Loss: 0.4543009400367737\n",
      "Epoch 12, Batch 133, Test Loss: 0.6124172806739807\n",
      "Epoch 12, Batch 134, Test Loss: 0.5257236361503601\n",
      "Epoch 12, Batch 135, Test Loss: 0.4824768900871277\n",
      "Epoch 12, Batch 136, Test Loss: 0.45959779620170593\n",
      "Epoch 12, Batch 137, Test Loss: 0.424349308013916\n",
      "Epoch 12, Batch 138, Test Loss: 0.5745906233787537\n",
      "Epoch 12, Batch 139, Test Loss: 0.6832142472267151\n",
      "Epoch 12, Batch 140, Test Loss: 0.5189839601516724\n",
      "Epoch 12, Batch 141, Test Loss: 0.525962233543396\n",
      "Epoch 12, Batch 142, Test Loss: 0.46424275636672974\n",
      "Epoch 12, Batch 143, Test Loss: 0.5075920224189758\n",
      "Epoch 12, Batch 144, Test Loss: 0.3521535396575928\n",
      "Epoch 12, Batch 145, Test Loss: 0.44284263253211975\n",
      "Epoch 12, Batch 146, Test Loss: 0.4458402395248413\n",
      "Epoch 12, Batch 147, Test Loss: 0.6483659148216248\n",
      "Epoch 12, Batch 148, Test Loss: 0.48817604780197144\n",
      "Epoch 12, Batch 149, Test Loss: 0.39477694034576416\n",
      "Epoch 12, Batch 150, Test Loss: 0.3034123182296753\n",
      "Epoch 12, Batch 151, Test Loss: 0.48776423931121826\n",
      "Epoch 12, Batch 152, Test Loss: 0.5460849404335022\n",
      "Epoch 12, Batch 153, Test Loss: 0.42402881383895874\n",
      "Epoch 12, Batch 154, Test Loss: 0.3367336392402649\n",
      "Epoch 12, Batch 155, Test Loss: 0.6530288457870483\n",
      "Epoch 12, Batch 156, Test Loss: 0.43201255798339844\n",
      "Epoch 12, Batch 157, Test Loss: 0.48588111996650696\n",
      "Epoch 12, Batch 158, Test Loss: 0.3609081506729126\n",
      "Epoch 12, Batch 159, Test Loss: 0.7445700764656067\n",
      "Epoch 12, Batch 160, Test Loss: 0.6284921169281006\n",
      "Epoch 12, Batch 161, Test Loss: 0.34156498312950134\n",
      "Epoch 12, Batch 162, Test Loss: 0.4808266758918762\n",
      "Epoch 12, Batch 163, Test Loss: 0.4828746020793915\n",
      "Epoch 12, Batch 164, Test Loss: 0.5080214738845825\n",
      "Epoch 12, Batch 165, Test Loss: 0.31577131152153015\n",
      "Epoch 12, Batch 166, Test Loss: 0.4385661482810974\n",
      "Epoch 12, Batch 167, Test Loss: 0.4991823136806488\n",
      "Epoch 12, Batch 168, Test Loss: 0.49592310190200806\n",
      "Epoch 12, Batch 169, Test Loss: 0.39909642934799194\n",
      "Epoch 12, Batch 170, Test Loss: 0.5008304715156555\n",
      "Epoch 12, Batch 171, Test Loss: 0.5571877956390381\n",
      "Epoch 12, Batch 172, Test Loss: 0.42463043332099915\n",
      "Epoch 12, Batch 173, Test Loss: 0.5472304821014404\n",
      "Epoch 12, Batch 174, Test Loss: 0.3037351667881012\n",
      "Epoch 12, Batch 175, Test Loss: 0.5084661841392517\n",
      "Epoch 12, Batch 176, Test Loss: 0.5042428374290466\n",
      "Epoch 12, Batch 177, Test Loss: 0.33078163862228394\n",
      "Epoch 12, Batch 178, Test Loss: 0.36312785744667053\n",
      "Epoch 12, Batch 179, Test Loss: 0.4384995400905609\n",
      "Epoch 12, Batch 180, Test Loss: 0.4712134599685669\n",
      "Epoch 12, Batch 181, Test Loss: 0.48738542199134827\n",
      "Epoch 12, Batch 182, Test Loss: 0.7777931094169617\n",
      "Epoch 12, Batch 183, Test Loss: 0.6111760139465332\n",
      "Epoch 12, Batch 184, Test Loss: 0.336638867855072\n",
      "Epoch 12, Batch 185, Test Loss: 0.46051672101020813\n",
      "Epoch 12, Batch 186, Test Loss: 0.4525054097175598\n",
      "Epoch 12, Batch 187, Test Loss: 0.5199368000030518\n",
      "Epoch 12, Batch 188, Test Loss: 0.6108075380325317\n",
      "Epoch 12, Batch 189, Test Loss: 0.3766596019268036\n",
      "Epoch 12, Batch 190, Test Loss: 0.46159422397613525\n",
      "Epoch 12, Batch 191, Test Loss: 0.35741308331489563\n",
      "Epoch 12, Batch 192, Test Loss: 0.5352074503898621\n",
      "Epoch 12, Batch 193, Test Loss: 0.36797526478767395\n",
      "Epoch 12, Batch 194, Test Loss: 0.4101540148258209\n",
      "Epoch 12, Batch 195, Test Loss: 0.3627525866031647\n",
      "Epoch 12, Batch 196, Test Loss: 0.552140474319458\n",
      "Epoch 12, Batch 197, Test Loss: 0.3783116042613983\n",
      "Epoch 12, Batch 198, Test Loss: 0.5138217210769653\n",
      "Epoch 12, Batch 199, Test Loss: 0.46583929657936096\n",
      "Epoch 12, Batch 200, Test Loss: 0.34402987360954285\n",
      "Epoch 12, Batch 201, Test Loss: 0.3205036520957947\n",
      "Epoch 12, Batch 202, Test Loss: 0.4348663091659546\n",
      "Epoch 12, Batch 203, Test Loss: 0.4332094192504883\n",
      "Epoch 12, Batch 204, Test Loss: 0.5059818029403687\n",
      "Epoch 12, Batch 205, Test Loss: 0.39248785376548767\n",
      "Epoch 12, Batch 206, Test Loss: 0.4538704454898834\n",
      "Epoch 12, Batch 207, Test Loss: 0.5457606315612793\n",
      "Epoch 12, Batch 208, Test Loss: 0.4164591431617737\n",
      "Epoch 12, Batch 209, Test Loss: 0.3798518478870392\n",
      "Epoch 12, Batch 210, Test Loss: 0.5587109923362732\n",
      "Epoch 12, Batch 211, Test Loss: 0.3735300898551941\n",
      "Epoch 12, Batch 212, Test Loss: 0.5044568181037903\n",
      "Epoch 12, Batch 213, Test Loss: 0.3754655122756958\n",
      "Epoch 12, Batch 214, Test Loss: 0.48887374997138977\n",
      "Epoch 12, Batch 215, Test Loss: 0.5784960389137268\n",
      "Epoch 12, Batch 216, Test Loss: 0.43643510341644287\n",
      "Epoch 12, Batch 217, Test Loss: 0.4186348617076874\n",
      "Epoch 12, Batch 218, Test Loss: 0.49281203746795654\n",
      "Epoch 12, Batch 219, Test Loss: 0.28713950514793396\n",
      "Epoch 12, Batch 220, Test Loss: 0.3270820379257202\n",
      "Epoch 12, Batch 221, Test Loss: 0.490064799785614\n",
      "Epoch 12, Batch 222, Test Loss: 0.4730897545814514\n",
      "Epoch 12, Batch 223, Test Loss: 0.4653080701828003\n",
      "Epoch 12, Batch 224, Test Loss: 0.4960702657699585\n",
      "Epoch 12, Batch 225, Test Loss: 0.5536217093467712\n",
      "Epoch 12, Batch 226, Test Loss: 0.7145540118217468\n",
      "Epoch 12, Batch 227, Test Loss: 0.3214799165725708\n",
      "Epoch 12, Batch 228, Test Loss: 0.4360974133014679\n",
      "Epoch 12, Batch 229, Test Loss: 0.43213263154029846\n",
      "Epoch 12, Batch 230, Test Loss: 0.3683360815048218\n",
      "Epoch 12, Batch 231, Test Loss: 0.5496254563331604\n",
      "Epoch 12, Batch 232, Test Loss: 0.5002233386039734\n",
      "Epoch 12, Batch 233, Test Loss: 0.49397262930870056\n",
      "Epoch 12, Batch 234, Test Loss: 0.5385168790817261\n",
      "Epoch 12, Batch 235, Test Loss: 0.5138623714447021\n",
      "Epoch 12, Batch 236, Test Loss: 0.6989798545837402\n",
      "Epoch 12, Batch 237, Test Loss: 0.5760024189949036\n",
      "Epoch 12, Batch 238, Test Loss: 0.5818164944648743\n",
      "Epoch 12, Batch 239, Test Loss: 0.42981094121932983\n",
      "Epoch 12, Batch 240, Test Loss: 0.3450815677642822\n",
      "Epoch 12, Batch 241, Test Loss: 0.3740498721599579\n",
      "Epoch 12, Batch 242, Test Loss: 0.6052296161651611\n",
      "Epoch 12, Batch 243, Test Loss: 0.5279617309570312\n",
      "Epoch 12, Batch 244, Test Loss: 0.4096836447715759\n",
      "Epoch 12, Batch 245, Test Loss: 0.5784879326820374\n",
      "Epoch 12, Batch 246, Test Loss: 0.5196250677108765\n",
      "Epoch 12, Batch 247, Test Loss: 0.492987722158432\n",
      "Epoch 12, Batch 248, Test Loss: 0.3737559914588928\n",
      "Epoch 12, Batch 249, Test Loss: 0.39515572786331177\n",
      "Epoch 12, Batch 250, Test Loss: 0.5355979204177856\n",
      "Epoch 12, Batch 251, Test Loss: 0.3775251507759094\n",
      "Epoch 12, Batch 252, Test Loss: 0.41868624091148376\n",
      "Epoch 12, Batch 253, Test Loss: 0.5722623467445374\n",
      "Epoch 12, Batch 254, Test Loss: 0.5295917987823486\n",
      "Epoch 12, Batch 255, Test Loss: 0.4127074182033539\n",
      "Epoch 12, Batch 256, Test Loss: 0.5259933471679688\n",
      "Epoch 12, Batch 257, Test Loss: 0.5305206775665283\n",
      "Epoch 12, Batch 258, Test Loss: 0.4670497477054596\n",
      "Epoch 12, Batch 259, Test Loss: 0.5235641598701477\n",
      "Epoch 12, Batch 260, Test Loss: 0.31205081939697266\n",
      "Epoch 12, Batch 261, Test Loss: 0.5290008783340454\n",
      "Epoch 12, Batch 262, Test Loss: 0.4927143454551697\n",
      "Epoch 12, Batch 263, Test Loss: 0.39709943532943726\n",
      "Epoch 12, Batch 264, Test Loss: 0.5186794400215149\n",
      "Epoch 12, Batch 265, Test Loss: 0.6376168727874756\n",
      "Epoch 12, Batch 266, Test Loss: 0.3832380771636963\n",
      "Epoch 12, Batch 267, Test Loss: 0.5432707667350769\n",
      "Epoch 12, Batch 268, Test Loss: 0.79721599817276\n",
      "Epoch 12, Batch 269, Test Loss: 0.4226481020450592\n",
      "Epoch 12, Batch 270, Test Loss: 0.515853762626648\n",
      "Epoch 12, Batch 271, Test Loss: 0.40946558117866516\n",
      "Epoch 12, Batch 272, Test Loss: 0.4640042185783386\n",
      "Epoch 12, Batch 273, Test Loss: 0.2880812883377075\n",
      "Epoch 12, Batch 274, Test Loss: 0.49809712171554565\n",
      "Epoch 12, Batch 275, Test Loss: 0.5465718507766724\n",
      "Epoch 12, Batch 276, Test Loss: 0.5477362275123596\n",
      "Epoch 12, Batch 277, Test Loss: 0.7958614826202393\n",
      "Epoch 12, Batch 278, Test Loss: 0.5256937742233276\n",
      "Epoch 12, Batch 279, Test Loss: 0.5498133301734924\n",
      "Epoch 12, Batch 280, Test Loss: 0.5330087542533875\n",
      "Epoch 12, Batch 281, Test Loss: 0.4648275375366211\n",
      "Epoch 12, Batch 282, Test Loss: 0.5105491876602173\n",
      "Epoch 12, Batch 283, Test Loss: 0.5217639207839966\n",
      "Epoch 12, Batch 284, Test Loss: 0.5541847944259644\n",
      "Epoch 12, Batch 285, Test Loss: 0.5756248831748962\n",
      "Epoch 12, Batch 286, Test Loss: 0.41554245352745056\n",
      "Epoch 12, Batch 287, Test Loss: 0.4561036229133606\n",
      "Epoch 12, Batch 288, Test Loss: 0.5160664319992065\n",
      "Epoch 12, Batch 289, Test Loss: 0.5872321128845215\n",
      "Epoch 12, Batch 290, Test Loss: 0.3575108051300049\n",
      "Epoch 12, Batch 291, Test Loss: 0.3802512586116791\n",
      "Epoch 12, Batch 292, Test Loss: 0.5174658298492432\n",
      "Epoch 12, Batch 293, Test Loss: 0.6315330266952515\n",
      "Epoch 12, Batch 294, Test Loss: 0.47513890266418457\n",
      "Epoch 12, Batch 295, Test Loss: 0.3991182744503021\n",
      "Epoch 12, Batch 296, Test Loss: 0.33609136939048767\n",
      "Epoch 12, Batch 297, Test Loss: 0.6276341676712036\n",
      "Epoch 12, Batch 298, Test Loss: 0.39748504757881165\n",
      "Epoch 12, Batch 299, Test Loss: 0.3717474341392517\n",
      "Epoch 12, Batch 300, Test Loss: 0.3883644640445709\n",
      "Epoch 12, Batch 301, Test Loss: 0.6050688028335571\n",
      "Epoch 12, Batch 302, Test Loss: 0.4372382164001465\n",
      "Epoch 12, Batch 303, Test Loss: 0.43950241804122925\n",
      "Epoch 12, Batch 304, Test Loss: 0.44344305992126465\n",
      "Epoch 12, Batch 305, Test Loss: 0.3365773856639862\n",
      "Epoch 12, Batch 306, Test Loss: 0.5473319292068481\n",
      "Epoch 12, Batch 307, Test Loss: 0.3958072364330292\n",
      "Epoch 12, Batch 308, Test Loss: 0.332830011844635\n",
      "Epoch 12, Batch 309, Test Loss: 0.519014298915863\n",
      "Epoch 12, Batch 310, Test Loss: 0.5243160724639893\n",
      "Epoch 12, Batch 311, Test Loss: 0.3876684904098511\n",
      "Epoch 12, Batch 312, Test Loss: 0.6159247159957886\n",
      "Epoch 12, Batch 313, Test Loss: 0.34459078311920166\n",
      "Epoch 12, Batch 314, Test Loss: 0.46548956632614136\n",
      "Epoch 12, Batch 315, Test Loss: 0.3406684100627899\n",
      "Epoch 12, Batch 316, Test Loss: 0.46568310260772705\n",
      "Epoch 12, Batch 317, Test Loss: 0.4802262783050537\n",
      "Epoch 12, Batch 318, Test Loss: 0.47304800152778625\n",
      "Epoch 12, Batch 319, Test Loss: 0.45126646757125854\n",
      "Epoch 12, Batch 320, Test Loss: 0.5081771612167358\n",
      "Epoch 12, Batch 321, Test Loss: 0.5165750980377197\n",
      "Epoch 12, Batch 322, Test Loss: 0.45591726899147034\n",
      "Epoch 12, Batch 323, Test Loss: 0.5041916966438293\n",
      "Epoch 12, Batch 324, Test Loss: 0.39805254340171814\n",
      "Epoch 12, Batch 325, Test Loss: 0.5342915654182434\n",
      "Epoch 12, Batch 326, Test Loss: 0.5375142693519592\n",
      "Epoch 12, Batch 327, Test Loss: 0.5756909847259521\n",
      "Epoch 12, Batch 328, Test Loss: 0.5385849475860596\n",
      "Epoch 12, Batch 329, Test Loss: 0.4551320970058441\n",
      "Epoch 12, Batch 330, Test Loss: 0.5160751938819885\n",
      "Epoch 12, Batch 331, Test Loss: 0.5987930297851562\n",
      "Epoch 12, Batch 332, Test Loss: 0.4522743821144104\n",
      "Epoch 12, Batch 333, Test Loss: 0.7265217304229736\n",
      "Epoch 12, Batch 334, Test Loss: 0.4143170714378357\n",
      "Epoch 12, Batch 335, Test Loss: 0.47941017150878906\n",
      "Epoch 12, Batch 336, Test Loss: 0.48880577087402344\n",
      "Epoch 12, Batch 337, Test Loss: 0.43895551562309265\n",
      "Epoch 12, Batch 338, Test Loss: 0.6806216239929199\n",
      "Epoch 12, Batch 339, Test Loss: 0.3950108587741852\n",
      "Epoch 12, Batch 340, Test Loss: 0.5327203869819641\n",
      "Epoch 12, Batch 341, Test Loss: 0.32417476177215576\n",
      "Epoch 12, Batch 342, Test Loss: 0.37334975600242615\n",
      "Epoch 12, Batch 343, Test Loss: 0.49766767024993896\n",
      "Epoch 12, Batch 344, Test Loss: 0.622427225112915\n",
      "Epoch 12, Batch 345, Test Loss: 0.4012553095817566\n",
      "Epoch 12, Batch 346, Test Loss: 0.4690000116825104\n",
      "Epoch 12, Batch 347, Test Loss: 0.5558297634124756\n",
      "Epoch 12, Batch 348, Test Loss: 0.703523576259613\n",
      "Epoch 12, Batch 349, Test Loss: 0.6741728782653809\n",
      "Epoch 12, Batch 350, Test Loss: 0.35862457752227783\n",
      "Epoch 12, Batch 351, Test Loss: 0.34944918751716614\n",
      "Epoch 12, Batch 352, Test Loss: 0.31009891629219055\n",
      "Epoch 12, Batch 353, Test Loss: 0.4410446882247925\n",
      "Epoch 12, Batch 354, Test Loss: 0.584043025970459\n",
      "Epoch 12, Batch 355, Test Loss: 0.39214658737182617\n",
      "Epoch 12, Batch 356, Test Loss: 0.48487770557403564\n",
      "Epoch 12, Batch 357, Test Loss: 0.45059481263160706\n",
      "Epoch 12, Batch 358, Test Loss: 0.2879871129989624\n",
      "Epoch 12, Batch 359, Test Loss: 0.5177586674690247\n",
      "Epoch 12, Batch 360, Test Loss: 0.33303648233413696\n",
      "Epoch 12, Batch 361, Test Loss: 0.4626443386077881\n",
      "Epoch 12, Batch 362, Test Loss: 0.44586965441703796\n",
      "Epoch 12, Batch 363, Test Loss: 0.5472972989082336\n",
      "Epoch 12, Batch 364, Test Loss: 0.697641134262085\n",
      "Epoch 12, Batch 365, Test Loss: 0.4231942296028137\n",
      "Epoch 12, Batch 366, Test Loss: 0.5211818218231201\n",
      "Epoch 12, Batch 367, Test Loss: 0.3975100517272949\n",
      "Epoch 12, Batch 368, Test Loss: 0.5397639274597168\n",
      "Epoch 12, Batch 369, Test Loss: 0.486065536737442\n",
      "Epoch 12, Batch 370, Test Loss: 0.4910740554332733\n",
      "Epoch 12, Batch 371, Test Loss: 0.40173450112342834\n",
      "Epoch 12, Batch 372, Test Loss: 0.524347722530365\n",
      "Epoch 12, Batch 373, Test Loss: 0.45243698358535767\n",
      "Epoch 12, Batch 374, Test Loss: 0.40597304701805115\n",
      "Epoch 12, Batch 375, Test Loss: 0.4245884120464325\n",
      "Epoch 12, Batch 376, Test Loss: 0.39122748374938965\n",
      "Epoch 12, Batch 377, Test Loss: 0.40933090448379517\n",
      "Epoch 12, Batch 378, Test Loss: 0.4165728688240051\n",
      "Epoch 12, Batch 379, Test Loss: 0.5794127583503723\n",
      "Epoch 12, Batch 380, Test Loss: 0.4422827661037445\n",
      "Epoch 12, Batch 381, Test Loss: 0.37021109461784363\n",
      "Epoch 12, Batch 382, Test Loss: 0.26905274391174316\n",
      "Epoch 12, Batch 383, Test Loss: 0.4454977512359619\n",
      "Epoch 12, Batch 384, Test Loss: 0.5801690816879272\n",
      "Epoch 12, Batch 385, Test Loss: 0.3673038184642792\n",
      "Epoch 12, Batch 386, Test Loss: 0.5528969168663025\n",
      "Epoch 12, Batch 387, Test Loss: 0.5987585783004761\n",
      "Epoch 12, Batch 388, Test Loss: 0.532773494720459\n",
      "Epoch 12, Batch 389, Test Loss: 0.40371468663215637\n",
      "Epoch 12, Batch 390, Test Loss: 0.5794314742088318\n",
      "Epoch 12, Batch 391, Test Loss: 0.6583569049835205\n",
      "Epoch 12, Batch 392, Test Loss: 0.2787661552429199\n",
      "Epoch 12, Batch 393, Test Loss: 0.39200901985168457\n",
      "Epoch 12, Batch 394, Test Loss: 0.2950575053691864\n",
      "Epoch 12, Batch 395, Test Loss: 0.48094066977500916\n",
      "Epoch 12, Batch 396, Test Loss: 0.6730736494064331\n",
      "Epoch 12, Batch 397, Test Loss: 0.46812164783477783\n",
      "Epoch 12, Batch 398, Test Loss: 0.45494315028190613\n",
      "Epoch 12, Batch 399, Test Loss: 0.47570186853408813\n",
      "Epoch 12, Batch 400, Test Loss: 0.31644055247306824\n",
      "Epoch 12, Batch 401, Test Loss: 0.47419843077659607\n",
      "Epoch 12, Batch 402, Test Loss: 0.4787996709346771\n",
      "Epoch 12, Batch 403, Test Loss: 0.7022600769996643\n",
      "Epoch 12, Batch 404, Test Loss: 0.465396523475647\n",
      "Epoch 12, Batch 405, Test Loss: 0.5195670127868652\n",
      "Epoch 12, Batch 406, Test Loss: 0.6088089346885681\n",
      "Epoch 12, Batch 407, Test Loss: 0.5340408682823181\n",
      "Epoch 12, Batch 408, Test Loss: 0.4217948615550995\n",
      "Epoch 12, Batch 409, Test Loss: 0.678849995136261\n",
      "Epoch 12, Batch 410, Test Loss: 0.6153760552406311\n",
      "Epoch 12, Batch 411, Test Loss: 0.4940602481365204\n",
      "Epoch 12, Batch 412, Test Loss: 0.6406758427619934\n",
      "Epoch 12, Batch 413, Test Loss: 0.36392953991889954\n",
      "Epoch 12, Batch 414, Test Loss: 0.5869395136833191\n",
      "Epoch 12, Batch 415, Test Loss: 0.39417505264282227\n",
      "Epoch 12, Batch 416, Test Loss: 0.4004482626914978\n",
      "Epoch 12, Batch 417, Test Loss: 0.3846489489078522\n",
      "Epoch 12, Batch 418, Test Loss: 0.4483276903629303\n",
      "Epoch 12, Batch 419, Test Loss: 0.42508843541145325\n",
      "Epoch 12, Batch 420, Test Loss: 0.3463820517063141\n",
      "Epoch 12, Batch 421, Test Loss: 0.38770681619644165\n",
      "Epoch 12, Batch 422, Test Loss: 0.5369834303855896\n",
      "Epoch 12, Batch 423, Test Loss: 0.6626531481742859\n",
      "Epoch 12, Batch 424, Test Loss: 0.3360784649848938\n",
      "Epoch 12, Batch 425, Test Loss: 0.5004535913467407\n",
      "Epoch 12, Batch 426, Test Loss: 0.7972561717033386\n",
      "Epoch 12, Batch 427, Test Loss: 0.47593486309051514\n",
      "Epoch 12, Batch 428, Test Loss: 0.4815709590911865\n",
      "Epoch 12, Batch 429, Test Loss: 0.31951552629470825\n",
      "Epoch 12, Batch 430, Test Loss: 0.4669717252254486\n",
      "Epoch 12, Batch 431, Test Loss: 0.4629344642162323\n",
      "Epoch 12, Batch 432, Test Loss: 0.44973400235176086\n",
      "Epoch 12, Batch 433, Test Loss: 0.5375312566757202\n",
      "Epoch 12, Batch 434, Test Loss: 0.3395639657974243\n",
      "Epoch 12, Batch 435, Test Loss: 0.720748782157898\n",
      "Epoch 12, Batch 436, Test Loss: 0.5480050444602966\n",
      "Epoch 12, Batch 437, Test Loss: 0.5651842355728149\n",
      "Epoch 12, Batch 438, Test Loss: 0.596784234046936\n",
      "Epoch 12, Batch 439, Test Loss: 0.5388674139976501\n",
      "Epoch 12, Batch 440, Test Loss: 0.47091981768608093\n",
      "Epoch 12, Batch 441, Test Loss: 0.3953968584537506\n",
      "Epoch 12, Batch 442, Test Loss: 0.6671187877655029\n",
      "Epoch 12, Batch 443, Test Loss: 0.5541190505027771\n",
      "Epoch 12, Batch 444, Test Loss: 0.50455242395401\n",
      "Epoch 12, Batch 445, Test Loss: 0.744313657283783\n",
      "Epoch 12, Batch 446, Test Loss: 0.4063078463077545\n",
      "Epoch 12, Batch 447, Test Loss: 0.3425367474555969\n",
      "Epoch 12, Batch 448, Test Loss: 0.3837905526161194\n",
      "Epoch 12, Batch 449, Test Loss: 0.7163952589035034\n",
      "Epoch 12, Batch 450, Test Loss: 0.5224117040634155\n",
      "Epoch 12, Batch 451, Test Loss: 0.33025920391082764\n",
      "Epoch 12, Batch 452, Test Loss: 0.4020661413669586\n",
      "Epoch 12, Batch 453, Test Loss: 0.6630336046218872\n",
      "Epoch 12, Batch 454, Test Loss: 0.5726404786109924\n",
      "Epoch 12, Batch 455, Test Loss: 0.2687101662158966\n",
      "Epoch 12, Batch 456, Test Loss: 0.4186723828315735\n",
      "Epoch 12, Batch 457, Test Loss: 0.5157999992370605\n",
      "Epoch 12, Batch 458, Test Loss: 0.5813093781471252\n",
      "Epoch 12, Batch 459, Test Loss: 0.3482329547405243\n",
      "Epoch 12, Batch 460, Test Loss: 0.45655709505081177\n",
      "Epoch 12, Batch 461, Test Loss: 0.4707818925380707\n",
      "Epoch 12, Batch 462, Test Loss: 0.5544800162315369\n",
      "Epoch 12, Batch 463, Test Loss: 0.5219707489013672\n",
      "Epoch 12, Batch 464, Test Loss: 0.37898990511894226\n",
      "Epoch 12, Batch 465, Test Loss: 0.5544487833976746\n",
      "Epoch 12, Batch 466, Test Loss: 0.435409814119339\n",
      "Epoch 12, Batch 467, Test Loss: 0.6293398141860962\n",
      "Epoch 12, Batch 468, Test Loss: 0.6104612946510315\n",
      "Epoch 12, Batch 469, Test Loss: 0.5273292660713196\n",
      "Epoch 12, Batch 470, Test Loss: 0.42952463030815125\n",
      "Epoch 12, Batch 471, Test Loss: 0.5210328102111816\n",
      "Epoch 12, Batch 472, Test Loss: 0.3952163755893707\n",
      "Epoch 12, Batch 473, Test Loss: 0.38699454069137573\n",
      "Epoch 12, Batch 474, Test Loss: 0.4724031686782837\n",
      "Epoch 12, Batch 475, Test Loss: 0.4985056519508362\n",
      "Epoch 12, Batch 476, Test Loss: 0.5392638444900513\n",
      "Epoch 12, Batch 477, Test Loss: 0.6958914995193481\n",
      "Epoch 12, Batch 478, Test Loss: 0.36256489157676697\n",
      "Epoch 12, Batch 479, Test Loss: 0.4086013734340668\n",
      "Epoch 12, Batch 480, Test Loss: 0.48539629578590393\n",
      "Epoch 12, Batch 481, Test Loss: 0.41791296005249023\n",
      "Epoch 12, Batch 482, Test Loss: 0.3947748839855194\n",
      "Epoch 12, Batch 483, Test Loss: 0.38855040073394775\n",
      "Epoch 12, Batch 484, Test Loss: 0.478261798620224\n",
      "Epoch 12, Batch 485, Test Loss: 0.3994135856628418\n",
      "Epoch 12, Batch 486, Test Loss: 0.34814101457595825\n",
      "Epoch 12, Batch 487, Test Loss: 0.5914801955223083\n",
      "Epoch 12, Batch 488, Test Loss: 0.40385010838508606\n",
      "Epoch 12, Batch 489, Test Loss: 0.46973684430122375\n",
      "Epoch 12, Batch 490, Test Loss: 0.5663245916366577\n",
      "Epoch 12, Batch 491, Test Loss: 0.38278740644454956\n",
      "Epoch 12, Batch 492, Test Loss: 0.34314027428627014\n",
      "Epoch 12, Batch 493, Test Loss: 0.8078524470329285\n",
      "Epoch 12, Batch 494, Test Loss: 0.6620590686798096\n",
      "Epoch 12, Batch 495, Test Loss: 0.41730695962905884\n",
      "Epoch 12, Batch 496, Test Loss: 0.3547249734401703\n",
      "Epoch 12, Batch 497, Test Loss: 0.4214012324810028\n",
      "Epoch 12, Batch 498, Test Loss: 0.5690867900848389\n",
      "Epoch 12, Batch 499, Test Loss: 0.49413275718688965\n",
      "Epoch 12, Batch 500, Test Loss: 0.4517787992954254\n",
      "Epoch 12, Batch 501, Test Loss: 0.4880022406578064\n",
      "Epoch 12, Batch 502, Test Loss: 0.3757840394973755\n",
      "Epoch 12, Batch 503, Test Loss: 0.6296341419219971\n",
      "Epoch 12, Batch 504, Test Loss: 0.5228990316390991\n",
      "Epoch 12, Batch 505, Test Loss: 0.4369233250617981\n",
      "Epoch 12, Batch 506, Test Loss: 0.42118608951568604\n",
      "Epoch 12, Batch 507, Test Loss: 0.41175326704978943\n",
      "Epoch 12, Batch 508, Test Loss: 0.40248772501945496\n",
      "Epoch 12, Batch 509, Test Loss: 0.40737614035606384\n",
      "Epoch 12, Batch 510, Test Loss: 0.49917078018188477\n",
      "Epoch 12, Batch 511, Test Loss: 0.3954657316207886\n",
      "Epoch 12, Batch 512, Test Loss: 0.4214746356010437\n",
      "Epoch 12, Batch 513, Test Loss: 0.496940016746521\n",
      "Epoch 12, Batch 514, Test Loss: 0.659871518611908\n",
      "Epoch 12, Batch 515, Test Loss: 0.39516574144363403\n",
      "Epoch 12, Batch 516, Test Loss: 0.5785543918609619\n",
      "Epoch 12, Batch 517, Test Loss: 0.5018457174301147\n",
      "Epoch 12, Batch 518, Test Loss: 0.49746817350387573\n",
      "Epoch 12, Batch 519, Test Loss: 0.511719822883606\n",
      "Epoch 12, Batch 520, Test Loss: 0.5280141234397888\n",
      "Epoch 12, Batch 521, Test Loss: 0.5064224004745483\n",
      "Epoch 12, Batch 522, Test Loss: 0.44501087069511414\n",
      "Epoch 12, Batch 523, Test Loss: 0.4490289092063904\n",
      "Epoch 12, Batch 524, Test Loss: 0.4819941520690918\n",
      "Epoch 12, Batch 525, Test Loss: 0.3525462746620178\n",
      "Epoch 12, Batch 526, Test Loss: 0.5489736795425415\n",
      "Epoch 12, Batch 527, Test Loss: 0.5615625977516174\n",
      "Epoch 12, Batch 528, Test Loss: 0.47714895009994507\n",
      "Epoch 12, Batch 529, Test Loss: 0.4945530593395233\n",
      "Epoch 12, Batch 530, Test Loss: 0.36020007729530334\n",
      "Epoch 12, Batch 531, Test Loss: 0.3041897416114807\n",
      "Epoch 12, Batch 532, Test Loss: 0.46714797616004944\n",
      "Epoch 12, Batch 533, Test Loss: 0.6329659223556519\n",
      "Epoch 12, Batch 534, Test Loss: 0.3534240424633026\n",
      "Epoch 12, Batch 535, Test Loss: 0.7273702025413513\n",
      "Epoch 12, Batch 536, Test Loss: 0.6286574602127075\n",
      "Epoch 12, Batch 537, Test Loss: 0.5617880821228027\n",
      "Epoch 12, Batch 538, Test Loss: 0.5055730938911438\n",
      "Epoch 12, Batch 539, Test Loss: 0.709159255027771\n",
      "Epoch 12, Batch 540, Test Loss: 0.44757044315338135\n",
      "Epoch 12, Batch 541, Test Loss: 0.4733824133872986\n",
      "Epoch 12, Batch 542, Test Loss: 0.7406702637672424\n",
      "Epoch 12, Batch 543, Test Loss: 0.46221670508384705\n",
      "Epoch 12, Batch 544, Test Loss: 0.4055711627006531\n",
      "Epoch 12, Batch 545, Test Loss: 0.437870591878891\n",
      "Epoch 12, Batch 546, Test Loss: 0.41745615005493164\n",
      "Epoch 12, Batch 547, Test Loss: 0.37302714586257935\n",
      "Epoch 12, Batch 548, Test Loss: 0.3999975919723511\n",
      "Epoch 12, Batch 549, Test Loss: 0.6655288934707642\n",
      "Epoch 12, Batch 550, Test Loss: 0.44786393642425537\n",
      "Epoch 12, Batch 551, Test Loss: 0.48877477645874023\n",
      "Epoch 12, Batch 552, Test Loss: 0.5456041693687439\n",
      "Epoch 12, Batch 553, Test Loss: 0.3869384825229645\n",
      "Epoch 12, Batch 554, Test Loss: 0.48037734627723694\n",
      "Epoch 12, Batch 555, Test Loss: 0.4268447756767273\n",
      "Epoch 12, Batch 556, Test Loss: 0.6016603112220764\n",
      "Epoch 12, Batch 557, Test Loss: 0.5424278378486633\n",
      "Epoch 12, Batch 558, Test Loss: 0.5246245861053467\n",
      "Epoch 12, Batch 559, Test Loss: 0.5451959371566772\n",
      "Epoch 12, Batch 560, Test Loss: 0.4625704884529114\n",
      "Epoch 12, Batch 561, Test Loss: 0.5028377771377563\n",
      "Epoch 12, Batch 562, Test Loss: 0.5596306324005127\n",
      "Epoch 12, Batch 563, Test Loss: 0.39424851536750793\n",
      "Epoch 12, Batch 564, Test Loss: 0.49800586700439453\n",
      "Epoch 12, Batch 565, Test Loss: 0.5651418566703796\n",
      "Epoch 12, Batch 566, Test Loss: 0.5300915241241455\n",
      "Epoch 12, Batch 567, Test Loss: 0.49673494696617126\n",
      "Epoch 12, Batch 568, Test Loss: 0.508604884147644\n",
      "Epoch 12, Batch 569, Test Loss: 0.43025606870651245\n",
      "Epoch 12, Batch 570, Test Loss: 0.4433765113353729\n",
      "Epoch 12, Batch 571, Test Loss: 0.41754066944122314\n",
      "Epoch 12, Batch 572, Test Loss: 0.5270023941993713\n",
      "Epoch 12, Batch 573, Test Loss: 0.368565171957016\n",
      "Epoch 12, Batch 574, Test Loss: 0.3527224659919739\n",
      "Epoch 12, Batch 575, Test Loss: 0.4753400385379791\n",
      "Epoch 12, Batch 576, Test Loss: 0.41524744033813477\n",
      "Epoch 12, Batch 577, Test Loss: 0.47081252932548523\n",
      "Epoch 12, Batch 578, Test Loss: 0.589025616645813\n",
      "Epoch 12, Batch 579, Test Loss: 0.5183732509613037\n",
      "Epoch 12, Batch 580, Test Loss: 0.6565980315208435\n",
      "Epoch 12, Batch 581, Test Loss: 0.44819176197052\n",
      "Epoch 12, Batch 582, Test Loss: 0.4166390895843506\n",
      "Epoch 12, Batch 583, Test Loss: 0.5391998887062073\n",
      "Epoch 12, Batch 584, Test Loss: 0.5335206389427185\n",
      "Epoch 12, Batch 585, Test Loss: 0.4199960231781006\n",
      "Epoch 12, Batch 586, Test Loss: 0.44462838768959045\n",
      "Epoch 12, Batch 587, Test Loss: 0.4428291916847229\n",
      "Epoch 12, Batch 588, Test Loss: 0.5569737553596497\n",
      "Epoch 12, Batch 589, Test Loss: 0.43319424986839294\n",
      "Epoch 12, Batch 590, Test Loss: 0.40647608041763306\n",
      "Epoch 12, Batch 591, Test Loss: 0.5822548270225525\n",
      "Epoch 12, Batch 592, Test Loss: 0.38295701146125793\n",
      "Epoch 12, Batch 593, Test Loss: 0.4787777066230774\n",
      "Epoch 12, Batch 594, Test Loss: 0.6859058737754822\n",
      "Epoch 12, Batch 595, Test Loss: 0.612599790096283\n",
      "Epoch 12, Batch 596, Test Loss: 0.36616402864456177\n",
      "Epoch 12, Batch 597, Test Loss: 0.37668612599372864\n",
      "Epoch 12, Batch 598, Test Loss: 0.5747514367103577\n",
      "Epoch 12, Batch 599, Test Loss: 0.4421594440937042\n",
      "Epoch 12, Batch 600, Test Loss: 0.5320322513580322\n",
      "Epoch 12, Batch 601, Test Loss: 0.5022637844085693\n",
      "Epoch 12, Batch 602, Test Loss: 0.7082856893539429\n",
      "Epoch 12, Batch 603, Test Loss: 0.6738179922103882\n",
      "Epoch 12, Batch 604, Test Loss: 0.5179002285003662\n",
      "Epoch 12, Batch 605, Test Loss: 0.41872069239616394\n",
      "Epoch 12, Batch 606, Test Loss: 0.3554111421108246\n",
      "Epoch 12, Batch 607, Test Loss: 0.3899889588356018\n",
      "Epoch 12, Batch 608, Test Loss: 0.4759802222251892\n",
      "Epoch 12, Batch 609, Test Loss: 0.5353471636772156\n",
      "Epoch 12, Batch 610, Test Loss: 0.5095487236976624\n",
      "Epoch 12, Batch 611, Test Loss: 0.5447242259979248\n",
      "Epoch 12, Batch 612, Test Loss: 0.3973609209060669\n",
      "Epoch 12, Batch 613, Test Loss: 0.4243391156196594\n",
      "Epoch 12, Batch 614, Test Loss: 0.45704853534698486\n",
      "Epoch 12, Batch 615, Test Loss: 0.3899425268173218\n",
      "Epoch 12, Batch 616, Test Loss: 0.37481972575187683\n",
      "Epoch 12, Batch 617, Test Loss: 0.5114303827285767\n",
      "Epoch 12, Batch 618, Test Loss: 0.31861525774002075\n",
      "Epoch 12, Batch 619, Test Loss: 0.4916914999485016\n",
      "Epoch 12, Batch 620, Test Loss: 0.5574581623077393\n",
      "Epoch 12, Batch 621, Test Loss: 0.3589314818382263\n",
      "Epoch 12, Batch 622, Test Loss: 0.45748698711395264\n",
      "Epoch 12, Batch 623, Test Loss: 0.4750182032585144\n",
      "Epoch 12, Batch 624, Test Loss: 0.4857354760169983\n",
      "Epoch 12, Batch 625, Test Loss: 0.6011866331100464\n",
      "Epoch 12, Batch 626, Test Loss: 0.30662575364112854\n",
      "Epoch 12, Batch 627, Test Loss: 0.4238314926624298\n",
      "Epoch 12, Batch 628, Test Loss: 0.49506717920303345\n",
      "Epoch 12, Batch 629, Test Loss: 0.43593382835388184\n",
      "Epoch 12, Batch 630, Test Loss: 0.5660161972045898\n",
      "Epoch 12, Batch 631, Test Loss: 0.4144126772880554\n",
      "Epoch 12, Batch 632, Test Loss: 0.3724551200866699\n",
      "Epoch 12, Batch 633, Test Loss: 0.614943265914917\n",
      "Epoch 12, Batch 634, Test Loss: 0.5297264456748962\n",
      "Epoch 12, Batch 635, Test Loss: 0.26535114645957947\n",
      "Epoch 12, Batch 636, Test Loss: 0.4062694311141968\n",
      "Epoch 12, Batch 637, Test Loss: 0.49200597405433655\n",
      "Epoch 12, Batch 638, Test Loss: 0.44227319955825806\n",
      "Epoch 12, Batch 639, Test Loss: 0.5774251818656921\n",
      "Epoch 12, Batch 640, Test Loss: 0.4864848554134369\n",
      "Epoch 12, Batch 641, Test Loss: 0.6005172729492188\n",
      "Epoch 12, Batch 642, Test Loss: 0.6612256169319153\n",
      "Epoch 12, Batch 643, Test Loss: 0.7071623206138611\n",
      "Epoch 12, Batch 644, Test Loss: 0.5403702855110168\n",
      "Epoch 12, Batch 645, Test Loss: 0.30404892563819885\n",
      "Epoch 12, Batch 646, Test Loss: 0.4422380328178406\n",
      "Epoch 12, Batch 647, Test Loss: 0.5301243662834167\n",
      "Epoch 12, Batch 648, Test Loss: 0.5047484040260315\n",
      "Epoch 12, Batch 649, Test Loss: 0.43142327666282654\n",
      "Epoch 12, Batch 650, Test Loss: 0.2798791229724884\n",
      "Epoch 12, Batch 651, Test Loss: 0.4448601007461548\n",
      "Epoch 12, Batch 652, Test Loss: 0.3901413679122925\n",
      "Epoch 12, Batch 653, Test Loss: 0.538561999797821\n",
      "Epoch 12, Batch 654, Test Loss: 0.4432888925075531\n",
      "Epoch 12, Batch 655, Test Loss: 0.6398849487304688\n",
      "Epoch 12, Batch 656, Test Loss: 0.7894496917724609\n",
      "Epoch 12, Batch 657, Test Loss: 0.4661449193954468\n",
      "Epoch 12, Batch 658, Test Loss: 0.528752863407135\n",
      "Epoch 12, Batch 659, Test Loss: 0.5356314778327942\n",
      "Epoch 12, Batch 660, Test Loss: 0.6205910444259644\n",
      "Epoch 12, Batch 661, Test Loss: 0.4107057452201843\n",
      "Epoch 12, Batch 662, Test Loss: 0.4351893365383148\n",
      "Epoch 12, Batch 663, Test Loss: 0.5515609979629517\n",
      "Epoch 12, Batch 664, Test Loss: 0.4318714141845703\n",
      "Epoch 12, Batch 665, Test Loss: 0.39732053875923157\n",
      "Epoch 12, Batch 666, Test Loss: 0.44587236642837524\n",
      "Epoch 12, Batch 667, Test Loss: 0.33725520968437195\n",
      "Epoch 12, Batch 668, Test Loss: 0.49466800689697266\n",
      "Epoch 12, Batch 669, Test Loss: 0.4620884954929352\n",
      "Epoch 12, Batch 670, Test Loss: 0.5319774746894836\n",
      "Epoch 12, Batch 671, Test Loss: 0.5927438139915466\n",
      "Epoch 12, Batch 672, Test Loss: 0.346905916929245\n",
      "Epoch 12, Batch 673, Test Loss: 0.5997675061225891\n",
      "Epoch 12, Batch 674, Test Loss: 0.38406896591186523\n",
      "Epoch 12, Batch 675, Test Loss: 0.6018006205558777\n",
      "Epoch 12, Batch 676, Test Loss: 0.3257060647010803\n",
      "Epoch 12, Batch 677, Test Loss: 0.3884213864803314\n",
      "Epoch 12, Batch 678, Test Loss: 0.5985300540924072\n",
      "Epoch 12, Batch 679, Test Loss: 0.5037271976470947\n",
      "Epoch 12, Batch 680, Test Loss: 0.8748641014099121\n",
      "Epoch 12, Batch 681, Test Loss: 0.3966854512691498\n",
      "Epoch 12, Batch 682, Test Loss: 0.6672047972679138\n",
      "Epoch 12, Batch 683, Test Loss: 0.6649999022483826\n",
      "Epoch 12, Batch 684, Test Loss: 0.36959317326545715\n",
      "Epoch 12, Batch 685, Test Loss: 0.38779184222221375\n",
      "Epoch 12, Batch 686, Test Loss: 0.510411262512207\n",
      "Epoch 12, Batch 687, Test Loss: 0.4838407337665558\n",
      "Epoch 12, Batch 688, Test Loss: 0.4994773268699646\n",
      "Epoch 12, Batch 689, Test Loss: 0.559838056564331\n",
      "Epoch 12, Batch 690, Test Loss: 0.5918737649917603\n",
      "Epoch 12, Batch 691, Test Loss: 0.5416051149368286\n",
      "Epoch 12, Batch 692, Test Loss: 0.5131314396858215\n",
      "Epoch 12, Batch 693, Test Loss: 0.32855331897735596\n",
      "Epoch 12, Batch 694, Test Loss: 0.4340595006942749\n",
      "Epoch 12, Batch 695, Test Loss: 0.3785691261291504\n",
      "Epoch 12, Batch 696, Test Loss: 0.5201506018638611\n",
      "Epoch 12, Batch 697, Test Loss: 0.5233100056648254\n",
      "Epoch 12, Batch 698, Test Loss: 0.6093700528144836\n",
      "Epoch 12, Batch 699, Test Loss: 0.4592321515083313\n",
      "Epoch 12, Batch 700, Test Loss: 0.47301802039146423\n",
      "Epoch 12, Batch 701, Test Loss: 0.4083032011985779\n",
      "Epoch 12, Batch 702, Test Loss: 0.3785057067871094\n",
      "Epoch 12, Batch 703, Test Loss: 0.4378145933151245\n",
      "Epoch 12, Batch 704, Test Loss: 0.5364027619361877\n",
      "Epoch 12, Batch 705, Test Loss: 0.648940920829773\n",
      "Epoch 12, Batch 706, Test Loss: 0.5708184838294983\n",
      "Epoch 12, Batch 707, Test Loss: 0.5176939964294434\n",
      "Epoch 12, Batch 708, Test Loss: 0.4113044738769531\n",
      "Epoch 12, Batch 709, Test Loss: 0.7443875670433044\n",
      "Epoch 12, Batch 710, Test Loss: 0.3826003670692444\n",
      "Epoch 12, Batch 711, Test Loss: 0.4453964829444885\n",
      "Epoch 12, Batch 712, Test Loss: 0.6263445019721985\n",
      "Epoch 12, Batch 713, Test Loss: 0.5464357137680054\n",
      "Epoch 12, Batch 714, Test Loss: 0.42056548595428467\n",
      "Epoch 12, Batch 715, Test Loss: 0.5904712080955505\n",
      "Epoch 12, Batch 716, Test Loss: 0.3973402678966522\n",
      "Epoch 12, Batch 717, Test Loss: 0.5294775366783142\n",
      "Epoch 12, Batch 718, Test Loss: 0.5083034634590149\n",
      "Epoch 12, Batch 719, Test Loss: 0.550502359867096\n",
      "Epoch 12, Batch 720, Test Loss: 0.3591788411140442\n",
      "Epoch 12, Batch 721, Test Loss: 0.5972892045974731\n",
      "Epoch 12, Batch 722, Test Loss: 0.3149058222770691\n",
      "Epoch 12, Batch 723, Test Loss: 0.40244898200035095\n",
      "Epoch 12, Batch 724, Test Loss: 0.5979447960853577\n",
      "Epoch 12, Batch 725, Test Loss: 0.4275505244731903\n",
      "Epoch 12, Batch 726, Test Loss: 0.5818383097648621\n",
      "Epoch 12, Batch 727, Test Loss: 0.40163058042526245\n",
      "Epoch 12, Batch 728, Test Loss: 0.4715765714645386\n",
      "Epoch 12, Batch 729, Test Loss: 0.7683554291725159\n",
      "Epoch 12, Batch 730, Test Loss: 0.44780588150024414\n",
      "Epoch 12, Batch 731, Test Loss: 0.44596436619758606\n",
      "Epoch 12, Batch 732, Test Loss: 0.4291457533836365\n",
      "Epoch 12, Batch 733, Test Loss: 0.3707656264305115\n",
      "Epoch 12, Batch 734, Test Loss: 0.39670318365097046\n",
      "Epoch 12, Batch 735, Test Loss: 0.44132325053215027\n",
      "Epoch 12, Batch 736, Test Loss: 0.4561600089073181\n",
      "Epoch 12, Batch 737, Test Loss: 0.46678900718688965\n",
      "Epoch 12, Batch 738, Test Loss: 0.6883924007415771\n",
      "Epoch 12, Batch 739, Test Loss: 0.3815784454345703\n",
      "Epoch 12, Batch 740, Test Loss: 0.43719515204429626\n",
      "Epoch 12, Batch 741, Test Loss: 0.4006427526473999\n",
      "Epoch 12, Batch 742, Test Loss: 0.3944479823112488\n",
      "Epoch 12, Batch 743, Test Loss: 0.4941082298755646\n",
      "Epoch 12, Batch 744, Test Loss: 0.2810252010822296\n",
      "Epoch 12, Batch 745, Test Loss: 0.6406112909317017\n",
      "Epoch 12, Batch 746, Test Loss: 0.8358232975006104\n",
      "Epoch 12, Batch 747, Test Loss: 0.3710346221923828\n",
      "Epoch 12, Batch 748, Test Loss: 0.6963080167770386\n",
      "Epoch 12, Batch 749, Test Loss: 0.6185147166252136\n",
      "Epoch 12, Batch 750, Test Loss: 0.5532804727554321\n",
      "Epoch 12, Batch 751, Test Loss: 0.4366666376590729\n",
      "Epoch 12, Batch 752, Test Loss: 0.3454572558403015\n",
      "Epoch 12, Batch 753, Test Loss: 0.7971276044845581\n",
      "Epoch 12, Batch 754, Test Loss: 0.4327171742916107\n",
      "Epoch 12, Batch 755, Test Loss: 0.4502440392971039\n",
      "Epoch 12, Batch 756, Test Loss: 0.504166305065155\n",
      "Epoch 12, Batch 757, Test Loss: 0.34650570154190063\n",
      "Epoch 12, Batch 758, Test Loss: 0.5201647281646729\n",
      "Epoch 12, Batch 759, Test Loss: 0.764378547668457\n",
      "Epoch 12, Batch 760, Test Loss: 0.5078901648521423\n",
      "Epoch 12, Batch 761, Test Loss: 0.33397141098976135\n",
      "Epoch 12, Batch 762, Test Loss: 0.469407320022583\n",
      "Epoch 12, Batch 763, Test Loss: 0.490315318107605\n",
      "Epoch 12, Batch 764, Test Loss: 0.4333657920360565\n",
      "Epoch 12, Batch 765, Test Loss: 0.38283246755599976\n",
      "Epoch 12, Batch 766, Test Loss: 0.2996605932712555\n",
      "Epoch 12, Batch 767, Test Loss: 0.5538223385810852\n",
      "Epoch 12, Batch 768, Test Loss: 0.8340650796890259\n",
      "Epoch 12, Batch 769, Test Loss: 0.5815171003341675\n",
      "Epoch 12, Batch 770, Test Loss: 0.4198015034198761\n",
      "Epoch 12, Batch 771, Test Loss: 0.39450603723526\n",
      "Epoch 12, Batch 772, Test Loss: 0.28933286666870117\n",
      "Epoch 12, Batch 773, Test Loss: 0.40658894181251526\n",
      "Epoch 12, Batch 774, Test Loss: 0.41639891266822815\n",
      "Epoch 12, Batch 775, Test Loss: 0.5790591239929199\n",
      "Epoch 12, Batch 776, Test Loss: 0.35473403334617615\n",
      "Epoch 12, Batch 777, Test Loss: 0.3858354389667511\n",
      "Epoch 12, Batch 778, Test Loss: 0.3419135510921478\n",
      "Epoch 12, Batch 779, Test Loss: 0.3585071861743927\n",
      "Epoch 12, Batch 780, Test Loss: 0.4383261203765869\n",
      "Epoch 12, Batch 781, Test Loss: 0.5772728323936462\n",
      "Epoch 12, Batch 782, Test Loss: 0.3867206573486328\n",
      "Epoch 12, Batch 783, Test Loss: 0.30226558446884155\n",
      "Epoch 12, Batch 784, Test Loss: 0.4804927408695221\n",
      "Epoch 12, Batch 785, Test Loss: 0.5342666506767273\n",
      "Epoch 12, Batch 786, Test Loss: 0.34155982732772827\n",
      "Epoch 12, Batch 787, Test Loss: 0.6102184057235718\n",
      "Epoch 12, Batch 788, Test Loss: 0.332302987575531\n",
      "Epoch 12, Batch 789, Test Loss: 0.5338726043701172\n",
      "Epoch 12, Batch 790, Test Loss: 0.5332328081130981\n",
      "Epoch 12, Batch 791, Test Loss: 0.45925554633140564\n",
      "Epoch 12, Batch 792, Test Loss: 0.457335889339447\n",
      "Epoch 12, Batch 793, Test Loss: 0.3985092341899872\n",
      "Epoch 12, Batch 794, Test Loss: 0.47133100032806396\n",
      "Epoch 12, Batch 795, Test Loss: 0.34661027789115906\n",
      "Epoch 12, Batch 796, Test Loss: 0.3713308274745941\n",
      "Epoch 12, Batch 797, Test Loss: 0.30491480231285095\n",
      "Epoch 12, Batch 798, Test Loss: 0.5528358221054077\n",
      "Epoch 12, Batch 799, Test Loss: 0.45518869161605835\n",
      "Epoch 12, Batch 800, Test Loss: 0.3869158625602722\n",
      "Epoch 12, Batch 801, Test Loss: 0.5095301866531372\n",
      "Epoch 12, Batch 802, Test Loss: 0.35268932580947876\n",
      "Epoch 12, Batch 803, Test Loss: 0.3648754954338074\n",
      "Epoch 12, Batch 804, Test Loss: 0.44350191950798035\n",
      "Epoch 12, Batch 805, Test Loss: 0.5042312741279602\n",
      "Epoch 12, Batch 806, Test Loss: 0.4217231273651123\n",
      "Epoch 12, Batch 807, Test Loss: 0.4780692458152771\n",
      "Epoch 12, Batch 808, Test Loss: 0.545190155506134\n",
      "Epoch 12, Batch 809, Test Loss: 0.49967578053474426\n",
      "Epoch 12, Batch 810, Test Loss: 0.6655648946762085\n",
      "Epoch 12, Batch 811, Test Loss: 0.5586084127426147\n",
      "Epoch 12, Batch 812, Test Loss: 0.38271844387054443\n",
      "Epoch 12, Batch 813, Test Loss: 0.2773648798465729\n",
      "Epoch 12, Batch 814, Test Loss: 0.4636448621749878\n",
      "Epoch 12, Batch 815, Test Loss: 0.47242802381515503\n",
      "Epoch 12, Batch 816, Test Loss: 0.4260545074939728\n",
      "Epoch 12, Batch 817, Test Loss: 0.29648131132125854\n",
      "Epoch 12, Batch 818, Test Loss: 0.6430197954177856\n",
      "Epoch 12, Batch 819, Test Loss: 0.6028623580932617\n",
      "Epoch 12, Batch 820, Test Loss: 0.5560909509658813\n",
      "Epoch 12, Batch 821, Test Loss: 0.3612567186355591\n",
      "Epoch 12, Batch 822, Test Loss: 0.4645334482192993\n",
      "Epoch 12, Batch 823, Test Loss: 0.5853747129440308\n",
      "Epoch 12, Batch 824, Test Loss: 0.48325875401496887\n",
      "Epoch 12, Batch 825, Test Loss: 0.4491513967514038\n",
      "Epoch 12, Batch 826, Test Loss: 0.4583403170108795\n",
      "Epoch 12, Batch 827, Test Loss: 0.49643298983573914\n",
      "Epoch 12, Batch 828, Test Loss: 0.5529192686080933\n",
      "Epoch 12, Batch 829, Test Loss: 0.6014237999916077\n",
      "Epoch 12, Batch 830, Test Loss: 0.4737377464771271\n",
      "Epoch 12, Batch 831, Test Loss: 0.5373347997665405\n",
      "Epoch 12, Batch 832, Test Loss: 0.46250274777412415\n",
      "Epoch 12, Batch 833, Test Loss: 0.42562955617904663\n",
      "Epoch 12, Batch 834, Test Loss: 0.4456195831298828\n",
      "Epoch 12, Batch 835, Test Loss: 0.38441264629364014\n",
      "Epoch 12, Batch 836, Test Loss: 0.5205788612365723\n",
      "Epoch 12, Batch 837, Test Loss: 0.6475807428359985\n",
      "Epoch 12, Batch 838, Test Loss: 0.49608349800109863\n",
      "Epoch 12, Batch 839, Test Loss: 0.6940451860427856\n",
      "Epoch 12, Batch 840, Test Loss: 0.4141426682472229\n",
      "Epoch 12, Batch 841, Test Loss: 0.6381101608276367\n",
      "Epoch 12, Batch 842, Test Loss: 0.46019747853279114\n",
      "Epoch 12, Batch 843, Test Loss: 0.4585005044937134\n",
      "Epoch 12, Batch 844, Test Loss: 0.3747219741344452\n",
      "Epoch 12, Batch 845, Test Loss: 0.41208362579345703\n",
      "Epoch 12, Batch 846, Test Loss: 0.47842979431152344\n",
      "Epoch 12, Batch 847, Test Loss: 0.34087902307510376\n",
      "Epoch 12, Batch 848, Test Loss: 0.5057264566421509\n",
      "Epoch 12, Batch 849, Test Loss: 0.34271812438964844\n",
      "Epoch 12, Batch 850, Test Loss: 0.5766127109527588\n",
      "Epoch 12, Batch 851, Test Loss: 0.35630279779434204\n",
      "Epoch 12, Batch 852, Test Loss: 0.41431593894958496\n",
      "Epoch 12, Batch 853, Test Loss: 0.34081798791885376\n",
      "Epoch 12, Batch 854, Test Loss: 0.48572200536727905\n",
      "Epoch 12, Batch 855, Test Loss: 0.4037289619445801\n",
      "Epoch 12, Batch 856, Test Loss: 0.34201133251190186\n",
      "Epoch 12, Batch 857, Test Loss: 0.40362969040870667\n",
      "Epoch 12, Batch 858, Test Loss: 0.3148837387561798\n",
      "Epoch 12, Batch 859, Test Loss: 0.43560612201690674\n",
      "Epoch 12, Batch 860, Test Loss: 0.573317289352417\n",
      "Epoch 12, Batch 861, Test Loss: 0.22674497961997986\n",
      "Epoch 12, Batch 862, Test Loss: 0.6075147390365601\n",
      "Epoch 12, Batch 863, Test Loss: 0.4417235553264618\n",
      "Epoch 12, Batch 864, Test Loss: 0.5334247350692749\n",
      "Epoch 12, Batch 865, Test Loss: 0.42980527877807617\n",
      "Epoch 12, Batch 866, Test Loss: 0.586756706237793\n",
      "Epoch 12, Batch 867, Test Loss: 0.5898374319076538\n",
      "Epoch 12, Batch 868, Test Loss: 0.31441766023635864\n",
      "Epoch 12, Batch 869, Test Loss: 0.7512404322624207\n",
      "Epoch 12, Batch 870, Test Loss: 0.5549803376197815\n",
      "Epoch 12, Batch 871, Test Loss: 0.31073081493377686\n",
      "Epoch 12, Batch 872, Test Loss: 0.44585028290748596\n",
      "Epoch 12, Batch 873, Test Loss: 0.3773181736469269\n",
      "Epoch 12, Batch 874, Test Loss: 0.45485854148864746\n",
      "Epoch 12, Batch 875, Test Loss: 0.3850592076778412\n",
      "Epoch 12, Batch 876, Test Loss: 0.5179094672203064\n",
      "Epoch 12, Batch 877, Test Loss: 0.49440324306488037\n",
      "Epoch 12, Batch 878, Test Loss: 0.358394593000412\n",
      "Epoch 12, Batch 879, Test Loss: 0.49692803621292114\n",
      "Epoch 12, Batch 880, Test Loss: 0.41120386123657227\n",
      "Epoch 12, Batch 881, Test Loss: 0.5797839760780334\n",
      "Epoch 12, Batch 882, Test Loss: 0.49523496627807617\n",
      "Epoch 12, Batch 883, Test Loss: 0.5717612504959106\n",
      "Epoch 12, Batch 884, Test Loss: 0.4383998215198517\n",
      "Epoch 12, Batch 885, Test Loss: 0.4427565932273865\n",
      "Epoch 12, Batch 886, Test Loss: 0.41049790382385254\n",
      "Epoch 12, Batch 887, Test Loss: 0.545183539390564\n",
      "Epoch 12, Batch 888, Test Loss: 0.4097808599472046\n",
      "Epoch 12, Batch 889, Test Loss: 0.9124860763549805\n",
      "Epoch 12, Batch 890, Test Loss: 0.5468648672103882\n",
      "Epoch 12, Batch 891, Test Loss: 0.46105101704597473\n",
      "Epoch 12, Batch 892, Test Loss: 0.6527132987976074\n",
      "Epoch 12, Batch 893, Test Loss: 0.544573962688446\n",
      "Epoch 12, Batch 894, Test Loss: 0.5699805021286011\n",
      "Epoch 12, Batch 895, Test Loss: 0.4826890528202057\n",
      "Epoch 12, Batch 896, Test Loss: 0.31884732842445374\n",
      "Epoch 12, Batch 897, Test Loss: 0.32238849997520447\n",
      "Epoch 12, Batch 898, Test Loss: 0.44005149602890015\n",
      "Epoch 12, Batch 899, Test Loss: 0.6906391382217407\n",
      "Epoch 12, Batch 900, Test Loss: 0.5068685412406921\n",
      "Epoch 12, Batch 901, Test Loss: 0.3884097635746002\n",
      "Epoch 12, Batch 902, Test Loss: 0.42839184403419495\n",
      "Epoch 12, Batch 903, Test Loss: 0.297211617231369\n",
      "Epoch 12, Batch 904, Test Loss: 0.489166259765625\n",
      "Epoch 12, Batch 905, Test Loss: 0.637799084186554\n",
      "Epoch 12, Batch 906, Test Loss: 0.6030154824256897\n",
      "Epoch 12, Batch 907, Test Loss: 0.5164168477058411\n",
      "Epoch 12, Batch 908, Test Loss: 0.3609899580478668\n",
      "Epoch 12, Batch 909, Test Loss: 0.47541746497154236\n",
      "Epoch 12, Batch 910, Test Loss: 0.4680944085121155\n",
      "Epoch 12, Batch 911, Test Loss: 0.5725325345993042\n",
      "Epoch 12, Batch 912, Test Loss: 0.45979177951812744\n",
      "Epoch 12, Batch 913, Test Loss: 0.4378756880760193\n",
      "Epoch 12, Batch 914, Test Loss: 0.39524605870246887\n",
      "Epoch 12, Batch 915, Test Loss: 0.5521613955497742\n",
      "Epoch 12, Batch 916, Test Loss: 0.3868279457092285\n",
      "Epoch 12, Batch 917, Test Loss: 0.3356421887874603\n",
      "Epoch 12, Batch 918, Test Loss: 0.38274168968200684\n",
      "Epoch 12, Batch 919, Test Loss: 0.5284826755523682\n",
      "Epoch 12, Batch 920, Test Loss: 0.3346899747848511\n",
      "Epoch 12, Batch 921, Test Loss: 0.36074018478393555\n",
      "Epoch 12, Batch 922, Test Loss: 0.4841894209384918\n",
      "Epoch 12, Batch 923, Test Loss: 0.6502280235290527\n",
      "Epoch 12, Batch 924, Test Loss: 0.5221207737922668\n",
      "Epoch 12, Batch 925, Test Loss: 0.5957942008972168\n",
      "Epoch 12, Batch 926, Test Loss: 0.504540205001831\n",
      "Epoch 12, Batch 927, Test Loss: 0.5413554906845093\n",
      "Epoch 12, Batch 928, Test Loss: 0.41796499490737915\n",
      "Epoch 12, Batch 929, Test Loss: 0.5040167570114136\n",
      "Epoch 12, Batch 930, Test Loss: 0.5972438454627991\n",
      "Epoch 12, Batch 931, Test Loss: 0.5191132426261902\n",
      "Epoch 12, Batch 932, Test Loss: 0.41908320784568787\n",
      "Epoch 12, Batch 933, Test Loss: 0.8476152420043945\n",
      "Epoch 12, Batch 934, Test Loss: 0.3468000292778015\n",
      "Epoch 12, Batch 935, Test Loss: 0.6118769645690918\n",
      "Epoch 12, Batch 936, Test Loss: 0.5835684537887573\n",
      "Epoch 12, Batch 937, Test Loss: 0.48457881808280945\n",
      "Epoch 12, Batch 938, Test Loss: 0.4489559531211853\n",
      "Accuracy of Test set: 0.8340166666666666\n",
      "Epoch 13, Batch 1, Loss: 0.4689556658267975\n",
      "Epoch 13, Batch 2, Loss: 0.5183177590370178\n",
      "Epoch 13, Batch 3, Loss: 0.74883633852005\n",
      "Epoch 13, Batch 4, Loss: 0.7146188616752625\n",
      "Epoch 13, Batch 5, Loss: 0.3836158812046051\n",
      "Epoch 13, Batch 6, Loss: 0.5592702031135559\n",
      "Epoch 13, Batch 7, Loss: 0.7917404770851135\n",
      "Epoch 13, Batch 8, Loss: 0.44825443625450134\n",
      "Epoch 13, Batch 9, Loss: 0.415215402841568\n",
      "Epoch 13, Batch 10, Loss: 0.4944624602794647\n",
      "Epoch 13, Batch 11, Loss: 0.4845316410064697\n",
      "Epoch 13, Batch 12, Loss: 0.5394065976142883\n",
      "Epoch 13, Batch 13, Loss: 0.6505568027496338\n",
      "Epoch 13, Batch 14, Loss: 0.47953668236732483\n",
      "Epoch 13, Batch 15, Loss: 0.6156591773033142\n",
      "Epoch 13, Batch 16, Loss: 0.4902336299419403\n",
      "Epoch 13, Batch 17, Loss: 0.574484646320343\n",
      "Epoch 13, Batch 18, Loss: 0.37850460410118103\n",
      "Epoch 13, Batch 19, Loss: 0.9059258103370667\n",
      "Epoch 13, Batch 20, Loss: 0.38525334000587463\n",
      "Epoch 13, Batch 21, Loss: 0.6241595149040222\n",
      "Epoch 13, Batch 22, Loss: 0.5394372940063477\n",
      "Epoch 13, Batch 23, Loss: 0.28443700075149536\n",
      "Epoch 13, Batch 24, Loss: 0.36149880290031433\n",
      "Epoch 13, Batch 25, Loss: 0.6671459078788757\n",
      "Epoch 13, Batch 26, Loss: 0.45554107427597046\n",
      "Epoch 13, Batch 27, Loss: 0.4955748915672302\n",
      "Epoch 13, Batch 28, Loss: 0.531522274017334\n",
      "Epoch 13, Batch 29, Loss: 0.42244255542755127\n",
      "Epoch 13, Batch 30, Loss: 0.7298625707626343\n",
      "Epoch 13, Batch 31, Loss: 0.6673042178153992\n",
      "Epoch 13, Batch 32, Loss: 0.4448971450328827\n",
      "Epoch 13, Batch 33, Loss: 0.5337140560150146\n",
      "Epoch 13, Batch 34, Loss: 0.5715847611427307\n",
      "Epoch 13, Batch 35, Loss: 0.4159550964832306\n",
      "Epoch 13, Batch 36, Loss: 0.2876318693161011\n",
      "Epoch 13, Batch 37, Loss: 0.5549547076225281\n",
      "Epoch 13, Batch 38, Loss: 0.45060214400291443\n",
      "Epoch 13, Batch 39, Loss: 0.3749813437461853\n",
      "Epoch 13, Batch 40, Loss: 0.4449930787086487\n",
      "Epoch 13, Batch 41, Loss: 0.38074010610580444\n",
      "Epoch 13, Batch 42, Loss: 0.5611023902893066\n",
      "Epoch 13, Batch 43, Loss: 0.5033126473426819\n",
      "Epoch 13, Batch 44, Loss: 0.37487056851387024\n",
      "Epoch 13, Batch 45, Loss: 0.41199734807014465\n",
      "Epoch 13, Batch 46, Loss: 0.60415118932724\n",
      "Epoch 13, Batch 47, Loss: 0.49765744805336\n",
      "Epoch 13, Batch 48, Loss: 0.3075498342514038\n",
      "Epoch 13, Batch 49, Loss: 0.5283065438270569\n",
      "Epoch 13, Batch 50, Loss: 0.5507346987724304\n",
      "Epoch 13, Batch 51, Loss: 0.6637964844703674\n",
      "Epoch 13, Batch 52, Loss: 0.6778613924980164\n",
      "Epoch 13, Batch 53, Loss: 0.4107174575328827\n",
      "Epoch 13, Batch 54, Loss: 0.5246976613998413\n",
      "Epoch 13, Batch 55, Loss: 0.6033231019973755\n",
      "Epoch 13, Batch 56, Loss: 0.5095503330230713\n",
      "Epoch 13, Batch 57, Loss: 0.5127765536308289\n",
      "Epoch 13, Batch 58, Loss: 0.5309726595878601\n",
      "Epoch 13, Batch 59, Loss: 0.47160428762435913\n",
      "Epoch 13, Batch 60, Loss: 0.3615725636482239\n",
      "Epoch 13, Batch 61, Loss: 0.41502779722213745\n",
      "Epoch 13, Batch 62, Loss: 0.4903501868247986\n",
      "Epoch 13, Batch 63, Loss: 0.5112228989601135\n",
      "Epoch 13, Batch 64, Loss: 0.43242669105529785\n",
      "Epoch 13, Batch 65, Loss: 0.5872572660446167\n",
      "Epoch 13, Batch 66, Loss: 0.3200417160987854\n",
      "Epoch 13, Batch 67, Loss: 0.5768566727638245\n",
      "Epoch 13, Batch 68, Loss: 0.6467413902282715\n",
      "Epoch 13, Batch 69, Loss: 0.39983004331588745\n",
      "Epoch 13, Batch 70, Loss: 0.5465114116668701\n",
      "Epoch 13, Batch 71, Loss: 0.34960806369781494\n",
      "Epoch 13, Batch 72, Loss: 0.24847517907619476\n",
      "Epoch 13, Batch 73, Loss: 0.5486841797828674\n",
      "Epoch 13, Batch 74, Loss: 0.39013442397117615\n",
      "Epoch 13, Batch 75, Loss: 0.534054160118103\n",
      "Epoch 13, Batch 76, Loss: 0.6092653870582581\n",
      "Epoch 13, Batch 77, Loss: 0.5015541911125183\n",
      "Epoch 13, Batch 78, Loss: 0.49112531542778015\n",
      "Epoch 13, Batch 79, Loss: 0.3873524069786072\n",
      "Epoch 13, Batch 80, Loss: 0.5205195546150208\n",
      "Epoch 13, Batch 81, Loss: 0.5639306306838989\n",
      "Epoch 13, Batch 82, Loss: 0.4212507903575897\n",
      "Epoch 13, Batch 83, Loss: 0.5930086970329285\n",
      "Epoch 13, Batch 84, Loss: 0.5494654178619385\n",
      "Epoch 13, Batch 85, Loss: 0.5742968916893005\n",
      "Epoch 13, Batch 86, Loss: 0.6189605593681335\n",
      "Epoch 13, Batch 87, Loss: 0.48755139112472534\n",
      "Epoch 13, Batch 88, Loss: 0.4655650854110718\n",
      "Epoch 13, Batch 89, Loss: 0.38167092204093933\n",
      "Epoch 13, Batch 90, Loss: 0.46388372778892517\n",
      "Epoch 13, Batch 91, Loss: 0.6238201856613159\n",
      "Epoch 13, Batch 92, Loss: 0.42593133449554443\n",
      "Epoch 13, Batch 93, Loss: 0.4944559931755066\n",
      "Epoch 13, Batch 94, Loss: 0.5572207570075989\n",
      "Epoch 13, Batch 95, Loss: 0.5361232757568359\n",
      "Epoch 13, Batch 96, Loss: 0.5194923281669617\n",
      "Epoch 13, Batch 97, Loss: 0.467155396938324\n",
      "Epoch 13, Batch 98, Loss: 0.35512441396713257\n",
      "Epoch 13, Batch 99, Loss: 0.5302118062973022\n",
      "Epoch 13, Batch 100, Loss: 0.5780030488967896\n",
      "Epoch 13, Batch 101, Loss: 0.47683224081993103\n",
      "Epoch 13, Batch 102, Loss: 0.4793888330459595\n",
      "Epoch 13, Batch 103, Loss: 0.5148081183433533\n",
      "Epoch 13, Batch 104, Loss: 0.4397538900375366\n",
      "Epoch 13, Batch 105, Loss: 0.2899922728538513\n",
      "Epoch 13, Batch 106, Loss: 0.444914847612381\n",
      "Epoch 13, Batch 107, Loss: 0.44436970353126526\n",
      "Epoch 13, Batch 108, Loss: 0.421083003282547\n",
      "Epoch 13, Batch 109, Loss: 0.7637897729873657\n",
      "Epoch 13, Batch 110, Loss: 0.41638535261154175\n",
      "Epoch 13, Batch 111, Loss: 0.4108385741710663\n",
      "Epoch 13, Batch 112, Loss: 0.4439229369163513\n",
      "Epoch 13, Batch 113, Loss: 0.44298750162124634\n",
      "Epoch 13, Batch 114, Loss: 0.37213584780693054\n",
      "Epoch 13, Batch 115, Loss: 0.42240607738494873\n",
      "Epoch 13, Batch 116, Loss: 0.24519731104373932\n",
      "Epoch 13, Batch 117, Loss: 0.4872482717037201\n",
      "Epoch 13, Batch 118, Loss: 0.48884353041648865\n",
      "Epoch 13, Batch 119, Loss: 0.4670475721359253\n",
      "Epoch 13, Batch 120, Loss: 0.4013649523258209\n",
      "Epoch 13, Batch 121, Loss: 0.33269432187080383\n",
      "Epoch 13, Batch 122, Loss: 0.49256253242492676\n",
      "Epoch 13, Batch 123, Loss: 0.4551185369491577\n",
      "Epoch 13, Batch 124, Loss: 0.4562457203865051\n",
      "Epoch 13, Batch 125, Loss: 0.4101892113685608\n",
      "Epoch 13, Batch 126, Loss: 0.5989999175071716\n",
      "Epoch 13, Batch 127, Loss: 0.5553833246231079\n",
      "Epoch 13, Batch 128, Loss: 0.33932316303253174\n",
      "Epoch 13, Batch 129, Loss: 0.571833074092865\n",
      "Epoch 13, Batch 130, Loss: 0.40891754627227783\n",
      "Epoch 13, Batch 131, Loss: 0.7127525210380554\n",
      "Epoch 13, Batch 132, Loss: 0.5114381313323975\n",
      "Epoch 13, Batch 133, Loss: 0.5977421998977661\n",
      "Epoch 13, Batch 134, Loss: 0.5423590540885925\n",
      "Epoch 13, Batch 135, Loss: 0.4412577152252197\n",
      "Epoch 13, Batch 136, Loss: 0.5744302868843079\n",
      "Epoch 13, Batch 137, Loss: 0.39915215969085693\n",
      "Epoch 13, Batch 138, Loss: 0.44132131338119507\n",
      "Epoch 13, Batch 139, Loss: 0.4639414846897125\n",
      "Epoch 13, Batch 140, Loss: 0.571780800819397\n",
      "Epoch 13, Batch 141, Loss: 0.5673030614852905\n",
      "Epoch 13, Batch 142, Loss: 0.5832056403160095\n",
      "Epoch 13, Batch 143, Loss: 0.4717414081096649\n",
      "Epoch 13, Batch 144, Loss: 0.4012735188007355\n",
      "Epoch 13, Batch 145, Loss: 0.4609822630882263\n",
      "Epoch 13, Batch 146, Loss: 0.442156583070755\n",
      "Epoch 13, Batch 147, Loss: 0.6498458385467529\n",
      "Epoch 13, Batch 148, Loss: 0.46257078647613525\n",
      "Epoch 13, Batch 149, Loss: 0.3700687289237976\n",
      "Epoch 13, Batch 150, Loss: 0.4883842170238495\n",
      "Epoch 13, Batch 151, Loss: 0.361566424369812\n",
      "Epoch 13, Batch 152, Loss: 0.6385500431060791\n",
      "Epoch 13, Batch 153, Loss: 0.44925373792648315\n",
      "Epoch 13, Batch 154, Loss: 0.49144458770751953\n",
      "Epoch 13, Batch 155, Loss: 0.49689871072769165\n",
      "Epoch 13, Batch 156, Loss: 0.580938994884491\n",
      "Epoch 13, Batch 157, Loss: 0.48494449257850647\n",
      "Epoch 13, Batch 158, Loss: 0.48335206508636475\n",
      "Epoch 13, Batch 159, Loss: 0.5069776773452759\n",
      "Epoch 13, Batch 160, Loss: 0.4854607582092285\n",
      "Epoch 13, Batch 161, Loss: 0.6906018257141113\n",
      "Epoch 13, Batch 162, Loss: 0.5095216035842896\n",
      "Epoch 13, Batch 163, Loss: 0.48652637004852295\n",
      "Epoch 13, Batch 164, Loss: 0.5508734583854675\n",
      "Epoch 13, Batch 165, Loss: 0.4994577467441559\n",
      "Epoch 13, Batch 166, Loss: 0.5498937964439392\n",
      "Epoch 13, Batch 167, Loss: 0.6391035318374634\n",
      "Epoch 13, Batch 168, Loss: 0.34790822863578796\n",
      "Epoch 13, Batch 169, Loss: 0.48696550726890564\n",
      "Epoch 13, Batch 170, Loss: 0.46825307607650757\n",
      "Epoch 13, Batch 171, Loss: 0.42275843024253845\n",
      "Epoch 13, Batch 172, Loss: 0.42593440413475037\n",
      "Epoch 13, Batch 173, Loss: 0.5674885511398315\n",
      "Epoch 13, Batch 174, Loss: 0.6854233741760254\n",
      "Epoch 13, Batch 175, Loss: 0.37235280871391296\n",
      "Epoch 13, Batch 176, Loss: 0.4115276336669922\n",
      "Epoch 13, Batch 177, Loss: 0.4762383997440338\n",
      "Epoch 13, Batch 178, Loss: 0.4146801233291626\n",
      "Epoch 13, Batch 179, Loss: 0.5365673303604126\n",
      "Epoch 13, Batch 180, Loss: 0.3382793962955475\n",
      "Epoch 13, Batch 181, Loss: 0.38708439469337463\n",
      "Epoch 13, Batch 182, Loss: 0.36674565076828003\n",
      "Epoch 13, Batch 183, Loss: 0.5815029740333557\n",
      "Epoch 13, Batch 184, Loss: 0.4814864695072174\n",
      "Epoch 13, Batch 185, Loss: 0.5076967477798462\n",
      "Epoch 13, Batch 186, Loss: 0.3334345519542694\n",
      "Epoch 13, Batch 187, Loss: 0.5530779957771301\n",
      "Epoch 13, Batch 188, Loss: 0.22490587830543518\n",
      "Epoch 13, Batch 189, Loss: 0.5775748491287231\n",
      "Epoch 13, Batch 190, Loss: 0.46794936060905457\n",
      "Epoch 13, Batch 191, Loss: 0.5724808573722839\n",
      "Epoch 13, Batch 192, Loss: 0.5280513167381287\n",
      "Epoch 13, Batch 193, Loss: 0.5081049203872681\n",
      "Epoch 13, Batch 194, Loss: 0.4862481653690338\n",
      "Epoch 13, Batch 195, Loss: 0.4945816397666931\n",
      "Epoch 13, Batch 196, Loss: 0.6480278968811035\n",
      "Epoch 13, Batch 197, Loss: 0.6549752354621887\n",
      "Epoch 13, Batch 198, Loss: 0.49791601300239563\n",
      "Epoch 13, Batch 199, Loss: 0.6106832027435303\n",
      "Epoch 13, Batch 200, Loss: 0.4837612807750702\n",
      "Epoch 13, Batch 201, Loss: 0.3622983992099762\n",
      "Epoch 13, Batch 202, Loss: 0.4695478081703186\n",
      "Epoch 13, Batch 203, Loss: 0.5973020792007446\n",
      "Epoch 13, Batch 204, Loss: 0.4407089948654175\n",
      "Epoch 13, Batch 205, Loss: 0.6048760414123535\n",
      "Epoch 13, Batch 206, Loss: 0.3099815845489502\n",
      "Epoch 13, Batch 207, Loss: 0.3865513205528259\n",
      "Epoch 13, Batch 208, Loss: 0.3510378301143646\n",
      "Epoch 13, Batch 209, Loss: 0.4182142913341522\n",
      "Epoch 13, Batch 210, Loss: 0.519637405872345\n",
      "Epoch 13, Batch 211, Loss: 0.3540496826171875\n",
      "Epoch 13, Batch 212, Loss: 0.4116795063018799\n",
      "Epoch 13, Batch 213, Loss: 0.41223710775375366\n",
      "Epoch 13, Batch 214, Loss: 0.5488993525505066\n",
      "Epoch 13, Batch 215, Loss: 0.5799781084060669\n",
      "Epoch 13, Batch 216, Loss: 0.6068093776702881\n",
      "Epoch 13, Batch 217, Loss: 0.425189733505249\n",
      "Epoch 13, Batch 218, Loss: 0.4215812087059021\n",
      "Epoch 13, Batch 219, Loss: 0.5184085965156555\n",
      "Epoch 13, Batch 220, Loss: 0.5756800770759583\n",
      "Epoch 13, Batch 221, Loss: 0.5735723376274109\n",
      "Epoch 13, Batch 222, Loss: 0.41926389932632446\n",
      "Epoch 13, Batch 223, Loss: 0.4996813237667084\n",
      "Epoch 13, Batch 224, Loss: 0.5941454768180847\n",
      "Epoch 13, Batch 225, Loss: 0.5762938261032104\n",
      "Epoch 13, Batch 226, Loss: 0.4338327944278717\n",
      "Epoch 13, Batch 227, Loss: 0.556013286113739\n",
      "Epoch 13, Batch 228, Loss: 0.5992064476013184\n",
      "Epoch 13, Batch 229, Loss: 0.4515828490257263\n",
      "Epoch 13, Batch 230, Loss: 0.5049938559532166\n",
      "Epoch 13, Batch 231, Loss: 0.6642212867736816\n",
      "Epoch 13, Batch 232, Loss: 0.5788375735282898\n",
      "Epoch 13, Batch 233, Loss: 0.5584666728973389\n",
      "Epoch 13, Batch 234, Loss: 0.3549262285232544\n",
      "Epoch 13, Batch 235, Loss: 0.4605437219142914\n",
      "Epoch 13, Batch 236, Loss: 0.5221929550170898\n",
      "Epoch 13, Batch 237, Loss: 0.7164784669876099\n",
      "Epoch 13, Batch 238, Loss: 0.6059870719909668\n",
      "Epoch 13, Batch 239, Loss: 0.5285804271697998\n",
      "Epoch 13, Batch 240, Loss: 0.2877308428287506\n",
      "Epoch 13, Batch 241, Loss: 0.41994357109069824\n",
      "Epoch 13, Batch 242, Loss: 0.6249191164970398\n",
      "Epoch 13, Batch 243, Loss: 0.42111271619796753\n",
      "Epoch 13, Batch 244, Loss: 0.5225476622581482\n",
      "Epoch 13, Batch 245, Loss: 0.5781131386756897\n",
      "Epoch 13, Batch 246, Loss: 0.3133945167064667\n",
      "Epoch 13, Batch 247, Loss: 0.4613099992275238\n",
      "Epoch 13, Batch 248, Loss: 0.40055280923843384\n",
      "Epoch 13, Batch 249, Loss: 0.6475054025650024\n",
      "Epoch 13, Batch 250, Loss: 0.4935528635978699\n",
      "Epoch 13, Batch 251, Loss: 0.5262662768363953\n",
      "Epoch 13, Batch 252, Loss: 0.6099042296409607\n",
      "Epoch 13, Batch 253, Loss: 0.47191962599754333\n",
      "Epoch 13, Batch 254, Loss: 0.6195144057273865\n",
      "Epoch 13, Batch 255, Loss: 0.6722812056541443\n",
      "Epoch 13, Batch 256, Loss: 0.5770078897476196\n",
      "Epoch 13, Batch 257, Loss: 0.5512023568153381\n",
      "Epoch 13, Batch 258, Loss: 0.5607249736785889\n",
      "Epoch 13, Batch 259, Loss: 0.6770174503326416\n",
      "Epoch 13, Batch 260, Loss: 0.5175737738609314\n",
      "Epoch 13, Batch 261, Loss: 0.4239882230758667\n",
      "Epoch 13, Batch 262, Loss: 0.3576321303844452\n",
      "Epoch 13, Batch 263, Loss: 0.388670414686203\n",
      "Epoch 13, Batch 264, Loss: 0.4021749794483185\n",
      "Epoch 13, Batch 265, Loss: 0.3432348072528839\n",
      "Epoch 13, Batch 266, Loss: 0.409621924161911\n",
      "Epoch 13, Batch 267, Loss: 0.5605674386024475\n",
      "Epoch 13, Batch 268, Loss: 0.49342072010040283\n",
      "Epoch 13, Batch 269, Loss: 0.6019772887229919\n",
      "Epoch 13, Batch 270, Loss: 0.32726621627807617\n",
      "Epoch 13, Batch 271, Loss: 0.39325758814811707\n",
      "Epoch 13, Batch 272, Loss: 0.27647748589515686\n",
      "Epoch 13, Batch 273, Loss: 0.387928307056427\n",
      "Epoch 13, Batch 274, Loss: 0.7782477140426636\n",
      "Epoch 13, Batch 275, Loss: 0.4916987419128418\n",
      "Epoch 13, Batch 276, Loss: 0.5375784039497375\n",
      "Epoch 13, Batch 277, Loss: 0.4237730801105499\n",
      "Epoch 13, Batch 278, Loss: 0.5000513195991516\n",
      "Epoch 13, Batch 279, Loss: 0.5389232635498047\n",
      "Epoch 13, Batch 280, Loss: 0.633231520652771\n",
      "Epoch 13, Batch 281, Loss: 0.3890923261642456\n",
      "Epoch 13, Batch 282, Loss: 0.4734303057193756\n",
      "Epoch 13, Batch 283, Loss: 0.41078412532806396\n",
      "Epoch 13, Batch 284, Loss: 0.5652392506599426\n",
      "Epoch 13, Batch 285, Loss: 0.4132331311702728\n",
      "Epoch 13, Batch 286, Loss: 0.42414361238479614\n",
      "Epoch 13, Batch 287, Loss: 0.4463120996952057\n",
      "Epoch 13, Batch 288, Loss: 0.4375210106372833\n",
      "Epoch 13, Batch 289, Loss: 0.46472787857055664\n",
      "Epoch 13, Batch 290, Loss: 0.4033477306365967\n",
      "Epoch 13, Batch 291, Loss: 0.6495748162269592\n",
      "Epoch 13, Batch 292, Loss: 0.3201403319835663\n",
      "Epoch 13, Batch 293, Loss: 0.2980369031429291\n",
      "Epoch 13, Batch 294, Loss: 0.4777451455593109\n",
      "Epoch 13, Batch 295, Loss: 0.6653996706008911\n",
      "Epoch 13, Batch 296, Loss: 0.6318444013595581\n",
      "Epoch 13, Batch 297, Loss: 0.31869369745254517\n",
      "Epoch 13, Batch 298, Loss: 0.36445945501327515\n",
      "Epoch 13, Batch 299, Loss: 0.4592713415622711\n",
      "Epoch 13, Batch 300, Loss: 0.5392146706581116\n",
      "Epoch 13, Batch 301, Loss: 0.4857143759727478\n",
      "Epoch 13, Batch 302, Loss: 0.5888370275497437\n",
      "Epoch 13, Batch 303, Loss: 0.380445659160614\n",
      "Epoch 13, Batch 304, Loss: 0.4727579355239868\n",
      "Epoch 13, Batch 305, Loss: 0.6166089773178101\n",
      "Epoch 13, Batch 306, Loss: 0.47125139832496643\n",
      "Epoch 13, Batch 307, Loss: 0.5383473634719849\n",
      "Epoch 13, Batch 308, Loss: 0.6554437279701233\n",
      "Epoch 13, Batch 309, Loss: 0.5631146430969238\n",
      "Epoch 13, Batch 310, Loss: 0.7242098450660706\n",
      "Epoch 13, Batch 311, Loss: 0.5122935771942139\n",
      "Epoch 13, Batch 312, Loss: 0.49901437759399414\n",
      "Epoch 13, Batch 313, Loss: 0.5122868418693542\n",
      "Epoch 13, Batch 314, Loss: 0.4997778832912445\n",
      "Epoch 13, Batch 315, Loss: 0.5663036108016968\n",
      "Epoch 13, Batch 316, Loss: 0.7837346196174622\n",
      "Epoch 13, Batch 317, Loss: 0.5419830083847046\n",
      "Epoch 13, Batch 318, Loss: 0.5382247567176819\n",
      "Epoch 13, Batch 319, Loss: 0.29252612590789795\n",
      "Epoch 13, Batch 320, Loss: 0.527405858039856\n",
      "Epoch 13, Batch 321, Loss: 0.37596121430397034\n",
      "Epoch 13, Batch 322, Loss: 0.43258431553840637\n",
      "Epoch 13, Batch 323, Loss: 0.4767933785915375\n",
      "Epoch 13, Batch 324, Loss: 0.7544419765472412\n",
      "Epoch 13, Batch 325, Loss: 0.5325732231140137\n",
      "Epoch 13, Batch 326, Loss: 0.47507157921791077\n",
      "Epoch 13, Batch 327, Loss: 0.5922067165374756\n",
      "Epoch 13, Batch 328, Loss: 0.4893205165863037\n",
      "Epoch 13, Batch 329, Loss: 0.4560896158218384\n",
      "Epoch 13, Batch 330, Loss: 0.39341437816619873\n",
      "Epoch 13, Batch 331, Loss: 0.5724612474441528\n",
      "Epoch 13, Batch 332, Loss: 0.6082905530929565\n",
      "Epoch 13, Batch 333, Loss: 0.3738349378108978\n",
      "Epoch 13, Batch 334, Loss: 0.42596670985221863\n",
      "Epoch 13, Batch 335, Loss: 0.44429686665534973\n",
      "Epoch 13, Batch 336, Loss: 0.38560453057289124\n",
      "Epoch 13, Batch 337, Loss: 0.5359189510345459\n",
      "Epoch 13, Batch 338, Loss: 0.37790143489837646\n",
      "Epoch 13, Batch 339, Loss: 0.7822287082672119\n",
      "Epoch 13, Batch 340, Loss: 0.4666443467140198\n",
      "Epoch 13, Batch 341, Loss: 0.3632301986217499\n",
      "Epoch 13, Batch 342, Loss: 0.3992600440979004\n",
      "Epoch 13, Batch 343, Loss: 0.5585130453109741\n",
      "Epoch 13, Batch 344, Loss: 0.295102596282959\n",
      "Epoch 13, Batch 345, Loss: 0.5324561595916748\n",
      "Epoch 13, Batch 346, Loss: 0.4798961281776428\n",
      "Epoch 13, Batch 347, Loss: 0.5262100696563721\n",
      "Epoch 13, Batch 348, Loss: 0.3762451708316803\n",
      "Epoch 13, Batch 349, Loss: 0.35954219102859497\n",
      "Epoch 13, Batch 350, Loss: 0.4563763737678528\n",
      "Epoch 13, Batch 351, Loss: 0.514391303062439\n",
      "Epoch 13, Batch 352, Loss: 0.5352926850318909\n",
      "Epoch 13, Batch 353, Loss: 0.45394712686538696\n",
      "Epoch 13, Batch 354, Loss: 0.32809939980506897\n",
      "Epoch 13, Batch 355, Loss: 0.48274877667427063\n",
      "Epoch 13, Batch 356, Loss: 0.5108426809310913\n",
      "Epoch 13, Batch 357, Loss: 0.5892226696014404\n",
      "Epoch 13, Batch 358, Loss: 0.42383286356925964\n",
      "Epoch 13, Batch 359, Loss: 0.5610710382461548\n",
      "Epoch 13, Batch 360, Loss: 0.3979625999927521\n",
      "Epoch 13, Batch 361, Loss: 0.3367084860801697\n",
      "Epoch 13, Batch 362, Loss: 0.5448657870292664\n",
      "Epoch 13, Batch 363, Loss: 0.372623085975647\n",
      "Epoch 13, Batch 364, Loss: 0.27589523792266846\n",
      "Epoch 13, Batch 365, Loss: 0.6318809390068054\n",
      "Epoch 13, Batch 366, Loss: 0.4625922441482544\n",
      "Epoch 13, Batch 367, Loss: 0.6329635381698608\n",
      "Epoch 13, Batch 368, Loss: 0.5170401930809021\n",
      "Epoch 13, Batch 369, Loss: 0.5370903015136719\n",
      "Epoch 13, Batch 370, Loss: 0.6664774417877197\n",
      "Epoch 13, Batch 371, Loss: 0.5066656470298767\n",
      "Epoch 13, Batch 372, Loss: 0.4608142375946045\n",
      "Epoch 13, Batch 373, Loss: 0.7248175740242004\n",
      "Epoch 13, Batch 374, Loss: 0.3813316822052002\n",
      "Epoch 13, Batch 375, Loss: 0.50897216796875\n",
      "Epoch 13, Batch 376, Loss: 0.33486801385879517\n",
      "Epoch 13, Batch 377, Loss: 0.4539949297904968\n",
      "Epoch 13, Batch 378, Loss: 0.4809552729129791\n",
      "Epoch 13, Batch 379, Loss: 0.7754660844802856\n",
      "Epoch 13, Batch 380, Loss: 0.6011170148849487\n",
      "Epoch 13, Batch 381, Loss: 0.5250858664512634\n",
      "Epoch 13, Batch 382, Loss: 0.4831088185310364\n",
      "Epoch 13, Batch 383, Loss: 0.383609414100647\n",
      "Epoch 13, Batch 384, Loss: 0.529821515083313\n",
      "Epoch 13, Batch 385, Loss: 0.39391687512397766\n",
      "Epoch 13, Batch 386, Loss: 0.5770920515060425\n",
      "Epoch 13, Batch 387, Loss: 0.6236558556556702\n",
      "Epoch 13, Batch 388, Loss: 0.5064424276351929\n",
      "Epoch 13, Batch 389, Loss: 0.48000285029411316\n",
      "Epoch 13, Batch 390, Loss: 0.425855815410614\n",
      "Epoch 13, Batch 391, Loss: 0.4936380088329315\n",
      "Epoch 13, Batch 392, Loss: 0.26768991351127625\n",
      "Epoch 13, Batch 393, Loss: 0.3015386462211609\n",
      "Epoch 13, Batch 394, Loss: 0.4194992184638977\n",
      "Epoch 13, Batch 395, Loss: 0.2830203175544739\n",
      "Epoch 13, Batch 396, Loss: 0.3372046947479248\n",
      "Epoch 13, Batch 397, Loss: 0.4367118179798126\n",
      "Epoch 13, Batch 398, Loss: 0.7865976691246033\n",
      "Epoch 13, Batch 399, Loss: 0.40051472187042236\n",
      "Epoch 13, Batch 400, Loss: 0.565619707107544\n",
      "Epoch 13, Batch 401, Loss: 0.43729785084724426\n",
      "Epoch 13, Batch 402, Loss: 0.5483109354972839\n",
      "Epoch 13, Batch 403, Loss: 0.487740159034729\n",
      "Epoch 13, Batch 404, Loss: 0.5242304801940918\n",
      "Epoch 13, Batch 405, Loss: 0.3850085437297821\n",
      "Epoch 13, Batch 406, Loss: 0.43475645780563354\n",
      "Epoch 13, Batch 407, Loss: 0.46435481309890747\n",
      "Epoch 13, Batch 408, Loss: 0.8011307716369629\n",
      "Epoch 13, Batch 409, Loss: 0.4849400222301483\n",
      "Epoch 13, Batch 410, Loss: 0.2814921736717224\n",
      "Epoch 13, Batch 411, Loss: 0.3082038164138794\n",
      "Epoch 13, Batch 412, Loss: 0.3971188962459564\n",
      "Epoch 13, Batch 413, Loss: 0.28300875425338745\n",
      "Epoch 13, Batch 414, Loss: 0.5688827633857727\n",
      "Epoch 13, Batch 415, Loss: 0.4645925760269165\n",
      "Epoch 13, Batch 416, Loss: 0.5854889750480652\n",
      "Epoch 13, Batch 417, Loss: 0.5041804313659668\n",
      "Epoch 13, Batch 418, Loss: 0.5216169357299805\n",
      "Epoch 13, Batch 419, Loss: 0.543092668056488\n",
      "Epoch 13, Batch 420, Loss: 0.7737647294998169\n",
      "Epoch 13, Batch 421, Loss: 0.50931316614151\n",
      "Epoch 13, Batch 422, Loss: 0.5022168755531311\n",
      "Epoch 13, Batch 423, Loss: 0.5523688793182373\n",
      "Epoch 13, Batch 424, Loss: 0.7451021671295166\n",
      "Epoch 13, Batch 425, Loss: 0.26929157972335815\n",
      "Epoch 13, Batch 426, Loss: 0.5974285006523132\n",
      "Epoch 13, Batch 427, Loss: 0.48197510838508606\n",
      "Epoch 13, Batch 428, Loss: 0.4633401036262512\n",
      "Epoch 13, Batch 429, Loss: 0.5654305815696716\n",
      "Epoch 13, Batch 430, Loss: 0.4207773208618164\n",
      "Epoch 13, Batch 431, Loss: 0.4177599251270294\n",
      "Epoch 13, Batch 432, Loss: 0.4705567955970764\n",
      "Epoch 13, Batch 433, Loss: 0.6267069578170776\n",
      "Epoch 13, Batch 434, Loss: 0.608703076839447\n",
      "Epoch 13, Batch 435, Loss: 0.42887574434280396\n",
      "Epoch 13, Batch 436, Loss: 0.6524400115013123\n",
      "Epoch 13, Batch 437, Loss: 0.4418896436691284\n",
      "Epoch 13, Batch 438, Loss: 0.4887956976890564\n",
      "Epoch 13, Batch 439, Loss: 0.6122581362724304\n",
      "Epoch 13, Batch 440, Loss: 0.39032429456710815\n",
      "Epoch 13, Batch 441, Loss: 0.4013861417770386\n",
      "Epoch 13, Batch 442, Loss: 0.2561715543270111\n",
      "Epoch 13, Batch 443, Loss: 0.5634819269180298\n",
      "Epoch 13, Batch 444, Loss: 0.616154134273529\n",
      "Epoch 13, Batch 445, Loss: 0.4306873083114624\n",
      "Epoch 13, Batch 446, Loss: 0.4307895600795746\n",
      "Epoch 13, Batch 447, Loss: 0.6342836022377014\n",
      "Epoch 13, Batch 448, Loss: 0.5206305384635925\n",
      "Epoch 13, Batch 449, Loss: 0.4525318145751953\n",
      "Epoch 13, Batch 450, Loss: 0.4163554906845093\n",
      "Epoch 13, Batch 451, Loss: 0.5338334441184998\n",
      "Epoch 13, Batch 452, Loss: 0.4073023498058319\n",
      "Epoch 13, Batch 453, Loss: 0.48203045129776\n",
      "Epoch 13, Batch 454, Loss: 0.538830041885376\n",
      "Epoch 13, Batch 455, Loss: 0.38644614815711975\n",
      "Epoch 13, Batch 456, Loss: 0.41847383975982666\n",
      "Epoch 13, Batch 457, Loss: 0.4225504994392395\n",
      "Epoch 13, Batch 458, Loss: 0.4900705814361572\n",
      "Epoch 13, Batch 459, Loss: 0.4542368948459625\n",
      "Epoch 13, Batch 460, Loss: 0.4112972617149353\n",
      "Epoch 13, Batch 461, Loss: 0.41177698969841003\n",
      "Epoch 13, Batch 462, Loss: 0.42290636897087097\n",
      "Epoch 13, Batch 463, Loss: 0.49354806542396545\n",
      "Epoch 13, Batch 464, Loss: 0.5187439918518066\n",
      "Epoch 13, Batch 465, Loss: 0.45422253012657166\n",
      "Epoch 13, Batch 466, Loss: 0.42705249786376953\n",
      "Epoch 13, Batch 467, Loss: 0.6458474397659302\n",
      "Epoch 13, Batch 468, Loss: 0.48707273602485657\n",
      "Epoch 13, Batch 469, Loss: 0.3739578127861023\n",
      "Epoch 13, Batch 470, Loss: 0.38262104988098145\n",
      "Epoch 13, Batch 471, Loss: 0.39806681871414185\n",
      "Epoch 13, Batch 472, Loss: 0.3517597019672394\n",
      "Epoch 13, Batch 473, Loss: 0.5965099334716797\n",
      "Epoch 13, Batch 474, Loss: 0.41299089789390564\n",
      "Epoch 13, Batch 475, Loss: 0.30463895201683044\n",
      "Epoch 13, Batch 476, Loss: 0.5083785057067871\n",
      "Epoch 13, Batch 477, Loss: 0.6466583013534546\n",
      "Epoch 13, Batch 478, Loss: 0.5434226989746094\n",
      "Epoch 13, Batch 479, Loss: 0.3844298720359802\n",
      "Epoch 13, Batch 480, Loss: 0.4153222441673279\n",
      "Epoch 13, Batch 481, Loss: 0.502548098564148\n",
      "Epoch 13, Batch 482, Loss: 0.7211869359016418\n",
      "Epoch 13, Batch 483, Loss: 0.5014353394508362\n",
      "Epoch 13, Batch 484, Loss: 0.38693732023239136\n",
      "Epoch 13, Batch 485, Loss: 0.42060479521751404\n",
      "Epoch 13, Batch 486, Loss: 0.39777880907058716\n",
      "Epoch 13, Batch 487, Loss: 0.41652557253837585\n",
      "Epoch 13, Batch 488, Loss: 0.38639193773269653\n",
      "Epoch 13, Batch 489, Loss: 0.3567067086696625\n",
      "Epoch 13, Batch 490, Loss: 0.460426926612854\n",
      "Epoch 13, Batch 491, Loss: 0.3948257565498352\n",
      "Epoch 13, Batch 492, Loss: 0.5192927122116089\n",
      "Epoch 13, Batch 493, Loss: 0.7679358720779419\n",
      "Epoch 13, Batch 494, Loss: 0.3814953565597534\n",
      "Epoch 13, Batch 495, Loss: 0.5910985469818115\n",
      "Epoch 13, Batch 496, Loss: 0.4496583938598633\n",
      "Epoch 13, Batch 497, Loss: 0.39474740624427795\n",
      "Epoch 13, Batch 498, Loss: 0.5207801461219788\n",
      "Epoch 13, Batch 499, Loss: 0.35702359676361084\n",
      "Epoch 13, Batch 500, Loss: 0.3755359351634979\n",
      "Epoch 13, Batch 501, Loss: 0.7040207386016846\n",
      "Epoch 13, Batch 502, Loss: 0.5565989017486572\n",
      "Epoch 13, Batch 503, Loss: 0.3128736615180969\n",
      "Epoch 13, Batch 504, Loss: 0.4848635792732239\n",
      "Epoch 13, Batch 505, Loss: 0.3814556300640106\n",
      "Epoch 13, Batch 506, Loss: 0.3920923173427582\n",
      "Epoch 13, Batch 507, Loss: 0.5246832370758057\n",
      "Epoch 13, Batch 508, Loss: 0.44602832198143005\n",
      "Epoch 13, Batch 509, Loss: 0.47788846492767334\n",
      "Epoch 13, Batch 510, Loss: 0.783226490020752\n",
      "Epoch 13, Batch 511, Loss: 0.42799073457717896\n",
      "Epoch 13, Batch 512, Loss: 0.49046096205711365\n",
      "Epoch 13, Batch 513, Loss: 0.7548196911811829\n",
      "Epoch 13, Batch 514, Loss: 0.5348117351531982\n",
      "Epoch 13, Batch 515, Loss: 0.4011857211589813\n",
      "Epoch 13, Batch 516, Loss: 0.33978891372680664\n",
      "Epoch 13, Batch 517, Loss: 0.4959857761859894\n",
      "Epoch 13, Batch 518, Loss: 0.4592325687408447\n",
      "Epoch 13, Batch 519, Loss: 0.46089115738868713\n",
      "Epoch 13, Batch 520, Loss: 0.32652854919433594\n",
      "Epoch 13, Batch 521, Loss: 0.5262747406959534\n",
      "Epoch 13, Batch 522, Loss: 0.536865770816803\n",
      "Epoch 13, Batch 523, Loss: 0.4329884946346283\n",
      "Epoch 13, Batch 524, Loss: 0.5302449464797974\n",
      "Epoch 13, Batch 525, Loss: 0.4440191388130188\n",
      "Epoch 13, Batch 526, Loss: 0.4023941159248352\n",
      "Epoch 13, Batch 527, Loss: 0.5305424928665161\n",
      "Epoch 13, Batch 528, Loss: 0.3950466215610504\n",
      "Epoch 13, Batch 529, Loss: 0.45619094371795654\n",
      "Epoch 13, Batch 530, Loss: 0.33463501930236816\n",
      "Epoch 13, Batch 531, Loss: 0.6626521348953247\n",
      "Epoch 13, Batch 532, Loss: 0.3630765378475189\n",
      "Epoch 13, Batch 533, Loss: 0.48146021366119385\n",
      "Epoch 13, Batch 534, Loss: 0.5192160606384277\n",
      "Epoch 13, Batch 535, Loss: 0.44048699736595154\n",
      "Epoch 13, Batch 536, Loss: 0.3797916769981384\n",
      "Epoch 13, Batch 537, Loss: 0.4021892547607422\n",
      "Epoch 13, Batch 538, Loss: 0.5780512690544128\n",
      "Epoch 13, Batch 539, Loss: 0.6016772985458374\n",
      "Epoch 13, Batch 540, Loss: 0.5885074138641357\n",
      "Epoch 13, Batch 541, Loss: 0.3219459056854248\n",
      "Epoch 13, Batch 542, Loss: 0.40035852789878845\n",
      "Epoch 13, Batch 543, Loss: 0.49414992332458496\n",
      "Epoch 13, Batch 544, Loss: 0.3496592044830322\n",
      "Epoch 13, Batch 545, Loss: 0.420985609292984\n",
      "Epoch 13, Batch 546, Loss: 0.39266377687454224\n",
      "Epoch 13, Batch 547, Loss: 0.45244643092155457\n",
      "Epoch 13, Batch 548, Loss: 0.7204651832580566\n",
      "Epoch 13, Batch 549, Loss: 0.6515911817550659\n",
      "Epoch 13, Batch 550, Loss: 0.3713643550872803\n",
      "Epoch 13, Batch 551, Loss: 0.32901713252067566\n",
      "Epoch 13, Batch 552, Loss: 0.5536621809005737\n",
      "Epoch 13, Batch 553, Loss: 0.5031515955924988\n",
      "Epoch 13, Batch 554, Loss: 0.39432504773139954\n",
      "Epoch 13, Batch 555, Loss: 0.6318413019180298\n",
      "Epoch 13, Batch 556, Loss: 0.24434912204742432\n",
      "Epoch 13, Batch 557, Loss: 0.6028712391853333\n",
      "Epoch 13, Batch 558, Loss: 0.428708553314209\n",
      "Epoch 13, Batch 559, Loss: 0.39240410923957825\n",
      "Epoch 13, Batch 560, Loss: 0.3712120056152344\n",
      "Epoch 13, Batch 561, Loss: 0.5889041423797607\n",
      "Epoch 13, Batch 562, Loss: 0.37729135155677795\n",
      "Epoch 13, Batch 563, Loss: 0.42485371232032776\n",
      "Epoch 13, Batch 564, Loss: 0.4603276252746582\n",
      "Epoch 13, Batch 565, Loss: 0.3208639919757843\n",
      "Epoch 13, Batch 566, Loss: 0.4997367262840271\n",
      "Epoch 13, Batch 567, Loss: 0.42605453729629517\n",
      "Epoch 13, Batch 568, Loss: 0.4705840051174164\n",
      "Epoch 13, Batch 569, Loss: 0.5816781520843506\n",
      "Epoch 13, Batch 570, Loss: 0.3513532876968384\n",
      "Epoch 13, Batch 571, Loss: 0.4659190773963928\n",
      "Epoch 13, Batch 572, Loss: 0.5408433079719543\n",
      "Epoch 13, Batch 573, Loss: 0.6042035222053528\n",
      "Epoch 13, Batch 574, Loss: 0.5401099324226379\n",
      "Epoch 13, Batch 575, Loss: 0.312183141708374\n",
      "Epoch 13, Batch 576, Loss: 0.48883360624313354\n",
      "Epoch 13, Batch 577, Loss: 0.4615182876586914\n",
      "Epoch 13, Batch 578, Loss: 0.5130404233932495\n",
      "Epoch 13, Batch 579, Loss: 0.5167545080184937\n",
      "Epoch 13, Batch 580, Loss: 0.4226628839969635\n",
      "Epoch 13, Batch 581, Loss: 0.5451540946960449\n",
      "Epoch 13, Batch 582, Loss: 0.5743337869644165\n",
      "Epoch 13, Batch 583, Loss: 0.4053979218006134\n",
      "Epoch 13, Batch 584, Loss: 0.5621585845947266\n",
      "Epoch 13, Batch 585, Loss: 0.5740755796432495\n",
      "Epoch 13, Batch 586, Loss: 0.473273903131485\n",
      "Epoch 13, Batch 587, Loss: 0.29387637972831726\n",
      "Epoch 13, Batch 588, Loss: 0.6661760807037354\n",
      "Epoch 13, Batch 589, Loss: 0.5134626030921936\n",
      "Epoch 13, Batch 590, Loss: 0.46892106533050537\n",
      "Epoch 13, Batch 591, Loss: 0.4453428387641907\n",
      "Epoch 13, Batch 592, Loss: 0.44188833236694336\n",
      "Epoch 13, Batch 593, Loss: 0.42405399680137634\n",
      "Epoch 13, Batch 594, Loss: 0.5858004093170166\n",
      "Epoch 13, Batch 595, Loss: 0.38758736848831177\n",
      "Epoch 13, Batch 596, Loss: 0.6518334150314331\n",
      "Epoch 13, Batch 597, Loss: 0.29548513889312744\n",
      "Epoch 13, Batch 598, Loss: 0.6513522267341614\n",
      "Epoch 13, Batch 599, Loss: 0.5555676817893982\n",
      "Epoch 13, Batch 600, Loss: 0.4925599694252014\n",
      "Epoch 13, Batch 601, Loss: 0.3795238435268402\n",
      "Epoch 13, Batch 602, Loss: 0.3509974181652069\n",
      "Epoch 13, Batch 603, Loss: 0.3667287230491638\n",
      "Epoch 13, Batch 604, Loss: 0.3616091012954712\n",
      "Epoch 13, Batch 605, Loss: 0.377359002828598\n",
      "Epoch 13, Batch 606, Loss: 0.4036555588245392\n",
      "Epoch 13, Batch 607, Loss: 0.48613694310188293\n",
      "Epoch 13, Batch 608, Loss: 0.41594478487968445\n",
      "Epoch 13, Batch 609, Loss: 0.2821853458881378\n",
      "Epoch 13, Batch 610, Loss: 0.37784337997436523\n",
      "Epoch 13, Batch 611, Loss: 0.6755154132843018\n",
      "Epoch 13, Batch 612, Loss: 0.42560485005378723\n",
      "Epoch 13, Batch 613, Loss: 0.4235036373138428\n",
      "Epoch 13, Batch 614, Loss: 0.37149778008461\n",
      "Epoch 13, Batch 615, Loss: 0.514295756816864\n",
      "Epoch 13, Batch 616, Loss: 0.5865920782089233\n",
      "Epoch 13, Batch 617, Loss: 0.33349376916885376\n",
      "Epoch 13, Batch 618, Loss: 0.46111175417900085\n",
      "Epoch 13, Batch 619, Loss: 0.5317508578300476\n",
      "Epoch 13, Batch 620, Loss: 0.49820762872695923\n",
      "Epoch 13, Batch 621, Loss: 0.6118537783622742\n",
      "Epoch 13, Batch 622, Loss: 0.3990032374858856\n",
      "Epoch 13, Batch 623, Loss: 0.36248958110809326\n",
      "Epoch 13, Batch 624, Loss: 0.3980710804462433\n",
      "Epoch 13, Batch 625, Loss: 0.35210785269737244\n",
      "Epoch 13, Batch 626, Loss: 0.4508042335510254\n",
      "Epoch 13, Batch 627, Loss: 0.7032521963119507\n",
      "Epoch 13, Batch 628, Loss: 0.5449773073196411\n",
      "Epoch 13, Batch 629, Loss: 0.41002416610717773\n",
      "Epoch 13, Batch 630, Loss: 0.5001175403594971\n",
      "Epoch 13, Batch 631, Loss: 0.4134241044521332\n",
      "Epoch 13, Batch 632, Loss: 0.45105573534965515\n",
      "Epoch 13, Batch 633, Loss: 0.4096454381942749\n",
      "Epoch 13, Batch 634, Loss: 0.4830194115638733\n",
      "Epoch 13, Batch 635, Loss: 0.5641571283340454\n",
      "Epoch 13, Batch 636, Loss: 0.44572916626930237\n",
      "Epoch 13, Batch 637, Loss: 0.5084744095802307\n",
      "Epoch 13, Batch 638, Loss: 0.3849334418773651\n",
      "Epoch 13, Batch 639, Loss: 0.7245126962661743\n",
      "Epoch 13, Batch 640, Loss: 0.3807634115219116\n",
      "Epoch 13, Batch 641, Loss: 0.3916512727737427\n",
      "Epoch 13, Batch 642, Loss: 0.6238815188407898\n",
      "Epoch 13, Batch 643, Loss: 0.41538774967193604\n",
      "Epoch 13, Batch 644, Loss: 0.7026222944259644\n",
      "Epoch 13, Batch 645, Loss: 0.38497239351272583\n",
      "Epoch 13, Batch 646, Loss: 0.44046375155448914\n",
      "Epoch 13, Batch 647, Loss: 0.49784016609191895\n",
      "Epoch 13, Batch 648, Loss: 0.48234474658966064\n",
      "Epoch 13, Batch 649, Loss: 0.3325120508670807\n",
      "Epoch 13, Batch 650, Loss: 0.15627877414226532\n",
      "Epoch 13, Batch 651, Loss: 0.5117441415786743\n",
      "Epoch 13, Batch 652, Loss: 0.4406317174434662\n",
      "Epoch 13, Batch 653, Loss: 0.4397867023944855\n",
      "Epoch 13, Batch 654, Loss: 0.5275225043296814\n",
      "Epoch 13, Batch 655, Loss: 0.5224508047103882\n",
      "Epoch 13, Batch 656, Loss: 0.4288244843482971\n",
      "Epoch 13, Batch 657, Loss: 0.5495532751083374\n",
      "Epoch 13, Batch 658, Loss: 0.4710281491279602\n",
      "Epoch 13, Batch 659, Loss: 0.5564484000205994\n",
      "Epoch 13, Batch 660, Loss: 0.39174968004226685\n",
      "Epoch 13, Batch 661, Loss: 0.437633752822876\n",
      "Epoch 13, Batch 662, Loss: 0.37647974491119385\n",
      "Epoch 13, Batch 663, Loss: 0.5227064490318298\n",
      "Epoch 13, Batch 664, Loss: 0.513582706451416\n",
      "Epoch 13, Batch 665, Loss: 0.5718039274215698\n",
      "Epoch 13, Batch 666, Loss: 0.5265985131263733\n",
      "Epoch 13, Batch 667, Loss: 0.4591218829154968\n",
      "Epoch 13, Batch 668, Loss: 0.40679293870925903\n",
      "Epoch 13, Batch 669, Loss: 0.5286262631416321\n",
      "Epoch 13, Batch 670, Loss: 0.5278605818748474\n",
      "Epoch 13, Batch 671, Loss: 0.594298243522644\n",
      "Epoch 13, Batch 672, Loss: 0.4490850269794464\n",
      "Epoch 13, Batch 673, Loss: 0.46430450677871704\n",
      "Epoch 13, Batch 674, Loss: 0.6240332126617432\n",
      "Epoch 13, Batch 675, Loss: 0.42269450426101685\n",
      "Epoch 13, Batch 676, Loss: 0.6076763868331909\n",
      "Epoch 13, Batch 677, Loss: 0.380260705947876\n",
      "Epoch 13, Batch 678, Loss: 0.3684452176094055\n",
      "Epoch 13, Batch 679, Loss: 0.6659040451049805\n",
      "Epoch 13, Batch 680, Loss: 0.35326525568962097\n",
      "Epoch 13, Batch 681, Loss: 0.41612353920936584\n",
      "Epoch 13, Batch 682, Loss: 0.3028603792190552\n",
      "Epoch 13, Batch 683, Loss: 0.30398592352867126\n",
      "Epoch 13, Batch 684, Loss: 0.5966158509254456\n",
      "Epoch 13, Batch 685, Loss: 0.4665391743183136\n",
      "Epoch 13, Batch 686, Loss: 0.5241737365722656\n",
      "Epoch 13, Batch 687, Loss: 0.6595444679260254\n",
      "Epoch 13, Batch 688, Loss: 0.6676291227340698\n",
      "Epoch 13, Batch 689, Loss: 0.3085162341594696\n",
      "Epoch 13, Batch 690, Loss: 0.4017624855041504\n",
      "Epoch 13, Batch 691, Loss: 0.39902442693710327\n",
      "Epoch 13, Batch 692, Loss: 0.7470000982284546\n",
      "Epoch 13, Batch 693, Loss: 0.4885944426059723\n",
      "Epoch 13, Batch 694, Loss: 0.5188218355178833\n",
      "Epoch 13, Batch 695, Loss: 0.4917166233062744\n",
      "Epoch 13, Batch 696, Loss: 0.45296141505241394\n",
      "Epoch 13, Batch 697, Loss: 0.39042624831199646\n",
      "Epoch 13, Batch 698, Loss: 0.32922908663749695\n",
      "Epoch 13, Batch 699, Loss: 0.35273122787475586\n",
      "Epoch 13, Batch 700, Loss: 0.4967101812362671\n",
      "Epoch 13, Batch 701, Loss: 0.42216068506240845\n",
      "Epoch 13, Batch 702, Loss: 0.4453401565551758\n",
      "Epoch 13, Batch 703, Loss: 0.4154435694217682\n",
      "Epoch 13, Batch 704, Loss: 0.4937005639076233\n",
      "Epoch 13, Batch 705, Loss: 0.3791438937187195\n",
      "Epoch 13, Batch 706, Loss: 0.42106273770332336\n",
      "Epoch 13, Batch 707, Loss: 0.5669276118278503\n",
      "Epoch 13, Batch 708, Loss: 0.43324872851371765\n",
      "Epoch 13, Batch 709, Loss: 0.3593222498893738\n",
      "Epoch 13, Batch 710, Loss: 0.37143629789352417\n",
      "Epoch 13, Batch 711, Loss: 0.5084580183029175\n",
      "Epoch 13, Batch 712, Loss: 0.667730450630188\n",
      "Epoch 13, Batch 713, Loss: 0.8089521527290344\n",
      "Epoch 13, Batch 714, Loss: 0.5602330565452576\n",
      "Epoch 13, Batch 715, Loss: 0.5890285968780518\n",
      "Epoch 13, Batch 716, Loss: 0.5932313799858093\n",
      "Epoch 13, Batch 717, Loss: 0.48401203751564026\n",
      "Epoch 13, Batch 718, Loss: 0.47065281867980957\n",
      "Epoch 13, Batch 719, Loss: 0.4335413873195648\n",
      "Epoch 13, Batch 720, Loss: 0.44946470856666565\n",
      "Epoch 13, Batch 721, Loss: 0.48446425795555115\n",
      "Epoch 13, Batch 722, Loss: 0.5196270942687988\n",
      "Epoch 13, Batch 723, Loss: 0.6425840258598328\n",
      "Epoch 13, Batch 724, Loss: 0.6174471378326416\n",
      "Epoch 13, Batch 725, Loss: 0.28713780641555786\n",
      "Epoch 13, Batch 726, Loss: 0.7353344559669495\n",
      "Epoch 13, Batch 727, Loss: 0.41609466075897217\n",
      "Epoch 13, Batch 728, Loss: 0.5009102821350098\n",
      "Epoch 13, Batch 729, Loss: 0.3910817801952362\n",
      "Epoch 13, Batch 730, Loss: 0.5202352404594421\n",
      "Epoch 13, Batch 731, Loss: 0.6141066551208496\n",
      "Epoch 13, Batch 732, Loss: 0.4946385622024536\n",
      "Epoch 13, Batch 733, Loss: 0.44795915484428406\n",
      "Epoch 13, Batch 734, Loss: 0.4182182550430298\n",
      "Epoch 13, Batch 735, Loss: 0.3980059325695038\n",
      "Epoch 13, Batch 736, Loss: 0.35604187846183777\n",
      "Epoch 13, Batch 737, Loss: 0.5061436295509338\n",
      "Epoch 13, Batch 738, Loss: 0.6263683438301086\n",
      "Epoch 13, Batch 739, Loss: 0.4143778085708618\n",
      "Epoch 13, Batch 740, Loss: 0.5733529329299927\n",
      "Epoch 13, Batch 741, Loss: 0.40200817584991455\n",
      "Epoch 13, Batch 742, Loss: 0.3114282488822937\n",
      "Epoch 13, Batch 743, Loss: 0.7125501036643982\n",
      "Epoch 13, Batch 744, Loss: 0.6147035956382751\n",
      "Epoch 13, Batch 745, Loss: 0.5392948389053345\n",
      "Epoch 13, Batch 746, Loss: 0.6264834403991699\n",
      "Epoch 13, Batch 747, Loss: 0.4758710563182831\n",
      "Epoch 13, Batch 748, Loss: 0.5573790073394775\n",
      "Epoch 13, Batch 749, Loss: 0.31696727871894836\n",
      "Epoch 13, Batch 750, Loss: 0.5653532147407532\n",
      "Epoch 13, Batch 751, Loss: 0.5145796537399292\n",
      "Epoch 13, Batch 752, Loss: 0.5540848970413208\n",
      "Epoch 13, Batch 753, Loss: 0.5919517278671265\n",
      "Epoch 13, Batch 754, Loss: 0.4403418004512787\n",
      "Epoch 13, Batch 755, Loss: 0.4187000095844269\n",
      "Epoch 13, Batch 756, Loss: 0.4788826107978821\n",
      "Epoch 13, Batch 757, Loss: 0.4535432755947113\n",
      "Epoch 13, Batch 758, Loss: 0.6172810792922974\n",
      "Epoch 13, Batch 759, Loss: 0.38375529646873474\n",
      "Epoch 13, Batch 760, Loss: 0.5088977813720703\n",
      "Epoch 13, Batch 761, Loss: 0.5113901495933533\n",
      "Epoch 13, Batch 762, Loss: 0.38328778743743896\n",
      "Epoch 13, Batch 763, Loss: 0.3972000181674957\n",
      "Epoch 13, Batch 764, Loss: 0.439552903175354\n",
      "Epoch 13, Batch 765, Loss: 0.37679290771484375\n",
      "Epoch 13, Batch 766, Loss: 0.736445426940918\n",
      "Epoch 13, Batch 767, Loss: 0.36448416113853455\n",
      "Epoch 13, Batch 768, Loss: 0.35587579011917114\n",
      "Epoch 13, Batch 769, Loss: 0.49685508012771606\n",
      "Epoch 13, Batch 770, Loss: 0.3903701603412628\n",
      "Epoch 13, Batch 771, Loss: 0.515103816986084\n",
      "Epoch 13, Batch 772, Loss: 0.5014935731887817\n",
      "Epoch 13, Batch 773, Loss: 0.3815995752811432\n",
      "Epoch 13, Batch 774, Loss: 0.8342461585998535\n",
      "Epoch 13, Batch 775, Loss: 0.4385957419872284\n",
      "Epoch 13, Batch 776, Loss: 0.46207574009895325\n",
      "Epoch 13, Batch 777, Loss: 0.5114311575889587\n",
      "Epoch 13, Batch 778, Loss: 0.48172980546951294\n",
      "Epoch 13, Batch 779, Loss: 0.42046934366226196\n",
      "Epoch 13, Batch 780, Loss: 0.49181562662124634\n",
      "Epoch 13, Batch 781, Loss: 0.37188905477523804\n",
      "Epoch 13, Batch 782, Loss: 0.49720925092697144\n",
      "Epoch 13, Batch 783, Loss: 0.45421239733695984\n",
      "Epoch 13, Batch 784, Loss: 0.6578990817070007\n",
      "Epoch 13, Batch 785, Loss: 0.4562126398086548\n",
      "Epoch 13, Batch 786, Loss: 0.3456321060657501\n",
      "Epoch 13, Batch 787, Loss: 0.6772284507751465\n",
      "Epoch 13, Batch 788, Loss: 0.3105490803718567\n",
      "Epoch 13, Batch 789, Loss: 0.4945632815361023\n",
      "Epoch 13, Batch 790, Loss: 0.5330303311347961\n",
      "Epoch 13, Batch 791, Loss: 0.49062928557395935\n",
      "Epoch 13, Batch 792, Loss: 0.4618889391422272\n",
      "Epoch 13, Batch 793, Loss: 0.37247732281684875\n",
      "Epoch 13, Batch 794, Loss: 0.5612794756889343\n",
      "Epoch 13, Batch 795, Loss: 0.6616346836090088\n",
      "Epoch 13, Batch 796, Loss: 0.5702825784683228\n",
      "Epoch 13, Batch 797, Loss: 0.5688459277153015\n",
      "Epoch 13, Batch 798, Loss: 0.4995787739753723\n",
      "Epoch 13, Batch 799, Loss: 0.5366058945655823\n",
      "Epoch 13, Batch 800, Loss: 0.4561105966567993\n",
      "Epoch 13, Batch 801, Loss: 0.34205782413482666\n",
      "Epoch 13, Batch 802, Loss: 0.3858461081981659\n",
      "Epoch 13, Batch 803, Loss: 0.2952593266963959\n",
      "Epoch 13, Batch 804, Loss: 0.4727414846420288\n",
      "Epoch 13, Batch 805, Loss: 0.36970871686935425\n",
      "Epoch 13, Batch 806, Loss: 0.5060312747955322\n",
      "Epoch 13, Batch 807, Loss: 0.3753095269203186\n",
      "Epoch 13, Batch 808, Loss: 0.4996340870857239\n",
      "Epoch 13, Batch 809, Loss: 0.5394828915596008\n",
      "Epoch 13, Batch 810, Loss: 0.5292676091194153\n",
      "Epoch 13, Batch 811, Loss: 0.5654525756835938\n",
      "Epoch 13, Batch 812, Loss: 0.36642175912857056\n",
      "Epoch 13, Batch 813, Loss: 0.4710329473018646\n",
      "Epoch 13, Batch 814, Loss: 0.5660196542739868\n",
      "Epoch 13, Batch 815, Loss: 0.7051630020141602\n",
      "Epoch 13, Batch 816, Loss: 0.3491525650024414\n",
      "Epoch 13, Batch 817, Loss: 0.2902698218822479\n",
      "Epoch 13, Batch 818, Loss: 0.42675918340682983\n",
      "Epoch 13, Batch 819, Loss: 0.6069698333740234\n",
      "Epoch 13, Batch 820, Loss: 0.42910024523735046\n",
      "Epoch 13, Batch 821, Loss: 0.2663479447364807\n",
      "Epoch 13, Batch 822, Loss: 0.612822413444519\n",
      "Epoch 13, Batch 823, Loss: 0.43440014123916626\n",
      "Epoch 13, Batch 824, Loss: 0.47715380787849426\n",
      "Epoch 13, Batch 825, Loss: 0.724616289138794\n",
      "Epoch 13, Batch 826, Loss: 0.4678930342197418\n",
      "Epoch 13, Batch 827, Loss: 0.4545212388038635\n",
      "Epoch 13, Batch 828, Loss: 0.4236754775047302\n",
      "Epoch 13, Batch 829, Loss: 0.4261777400970459\n",
      "Epoch 13, Batch 830, Loss: 0.3856799006462097\n",
      "Epoch 13, Batch 831, Loss: 0.4570842981338501\n",
      "Epoch 13, Batch 832, Loss: 0.6290595531463623\n",
      "Epoch 13, Batch 833, Loss: 0.37442144751548767\n",
      "Epoch 13, Batch 834, Loss: 0.35624125599861145\n",
      "Epoch 13, Batch 835, Loss: 0.627159833908081\n",
      "Epoch 13, Batch 836, Loss: 0.5859070420265198\n",
      "Epoch 13, Batch 837, Loss: 0.6334656476974487\n",
      "Epoch 13, Batch 838, Loss: 0.36893948912620544\n",
      "Epoch 13, Batch 839, Loss: 0.4568594992160797\n",
      "Epoch 13, Batch 840, Loss: 0.47830361127853394\n",
      "Epoch 13, Batch 841, Loss: 0.518291175365448\n",
      "Epoch 13, Batch 842, Loss: 0.3603355884552002\n",
      "Epoch 13, Batch 843, Loss: 0.44287699460983276\n",
      "Epoch 13, Batch 844, Loss: 0.6507339477539062\n",
      "Epoch 13, Batch 845, Loss: 0.5017613768577576\n",
      "Epoch 13, Batch 846, Loss: 0.4881956875324249\n",
      "Epoch 13, Batch 847, Loss: 0.4549901485443115\n",
      "Epoch 13, Batch 848, Loss: 0.3864910900592804\n",
      "Epoch 13, Batch 849, Loss: 0.4494515061378479\n",
      "Epoch 13, Batch 850, Loss: 0.18840515613555908\n",
      "Epoch 13, Batch 851, Loss: 0.5771795511245728\n",
      "Epoch 13, Batch 852, Loss: 0.5036038756370544\n",
      "Epoch 13, Batch 853, Loss: 0.587454617023468\n",
      "Epoch 13, Batch 854, Loss: 0.5143671035766602\n",
      "Epoch 13, Batch 855, Loss: 0.430044561624527\n",
      "Epoch 13, Batch 856, Loss: 0.6473574638366699\n",
      "Epoch 13, Batch 857, Loss: 0.3998125195503235\n",
      "Epoch 13, Batch 858, Loss: 0.5563707947731018\n",
      "Epoch 13, Batch 859, Loss: 0.5326036214828491\n",
      "Epoch 13, Batch 860, Loss: 0.5663349032402039\n",
      "Epoch 13, Batch 861, Loss: 0.3756466209888458\n",
      "Epoch 13, Batch 862, Loss: 0.43865907192230225\n",
      "Epoch 13, Batch 863, Loss: 0.3431199789047241\n",
      "Epoch 13, Batch 864, Loss: 0.5673924684524536\n",
      "Epoch 13, Batch 865, Loss: 0.45883405208587646\n",
      "Epoch 13, Batch 866, Loss: 0.41848817467689514\n",
      "Epoch 13, Batch 867, Loss: 0.3485184907913208\n",
      "Epoch 13, Batch 868, Loss: 0.36992815136909485\n",
      "Epoch 13, Batch 869, Loss: 0.44056376814842224\n",
      "Epoch 13, Batch 870, Loss: 0.5657246112823486\n",
      "Epoch 13, Batch 871, Loss: 0.491394579410553\n",
      "Epoch 13, Batch 872, Loss: 0.4579806327819824\n",
      "Epoch 13, Batch 873, Loss: 0.417707234621048\n",
      "Epoch 13, Batch 874, Loss: 0.41927576065063477\n",
      "Epoch 13, Batch 875, Loss: 0.5196614861488342\n",
      "Epoch 13, Batch 876, Loss: 0.4584888815879822\n",
      "Epoch 13, Batch 877, Loss: 0.501897394657135\n",
      "Epoch 13, Batch 878, Loss: 0.6388434171676636\n",
      "Epoch 13, Batch 879, Loss: 0.5263357162475586\n",
      "Epoch 13, Batch 880, Loss: 0.5815849900245667\n",
      "Epoch 13, Batch 881, Loss: 0.42490336298942566\n",
      "Epoch 13, Batch 882, Loss: 0.4272346496582031\n",
      "Epoch 13, Batch 883, Loss: 0.4276314675807953\n",
      "Epoch 13, Batch 884, Loss: 0.4972490072250366\n",
      "Epoch 13, Batch 885, Loss: 0.4146929681301117\n",
      "Epoch 13, Batch 886, Loss: 0.39926785230636597\n",
      "Epoch 13, Batch 887, Loss: 0.5791271924972534\n",
      "Epoch 13, Batch 888, Loss: 0.4280666708946228\n",
      "Epoch 13, Batch 889, Loss: 0.466701477766037\n",
      "Epoch 13, Batch 890, Loss: 0.4416951537132263\n",
      "Epoch 13, Batch 891, Loss: 0.3932711184024811\n",
      "Epoch 13, Batch 892, Loss: 0.4736918807029724\n",
      "Epoch 13, Batch 893, Loss: 0.6628133654594421\n",
      "Epoch 13, Batch 894, Loss: 0.39027947187423706\n",
      "Epoch 13, Batch 895, Loss: 0.38767704367637634\n",
      "Epoch 13, Batch 896, Loss: 0.4501439332962036\n",
      "Epoch 13, Batch 897, Loss: 0.5601934194564819\n",
      "Epoch 13, Batch 898, Loss: 0.5128617286682129\n",
      "Epoch 13, Batch 899, Loss: 0.33216506242752075\n",
      "Epoch 13, Batch 900, Loss: 0.46364328265190125\n",
      "Epoch 13, Batch 901, Loss: 0.7314240336418152\n",
      "Epoch 13, Batch 902, Loss: 0.6775363683700562\n",
      "Epoch 13, Batch 903, Loss: 0.44482138752937317\n",
      "Epoch 13, Batch 904, Loss: 0.5765631794929504\n",
      "Epoch 13, Batch 905, Loss: 0.5240179300308228\n",
      "Epoch 13, Batch 906, Loss: 0.5367396473884583\n",
      "Epoch 13, Batch 907, Loss: 0.5401087999343872\n",
      "Epoch 13, Batch 908, Loss: 0.43139517307281494\n",
      "Epoch 13, Batch 909, Loss: 0.5141785740852356\n",
      "Epoch 13, Batch 910, Loss: 0.6096678972244263\n",
      "Epoch 13, Batch 911, Loss: 0.48290735483169556\n",
      "Epoch 13, Batch 912, Loss: 0.4250827431678772\n",
      "Epoch 13, Batch 913, Loss: 0.4222332835197449\n",
      "Epoch 13, Batch 914, Loss: 0.43089863657951355\n",
      "Epoch 13, Batch 915, Loss: 0.4299417734146118\n",
      "Epoch 13, Batch 916, Loss: 0.47801661491394043\n",
      "Epoch 13, Batch 917, Loss: 0.407157838344574\n",
      "Epoch 13, Batch 918, Loss: 0.4875853657722473\n",
      "Epoch 13, Batch 919, Loss: 0.3607975244522095\n",
      "Epoch 13, Batch 920, Loss: 0.404419481754303\n",
      "Epoch 13, Batch 921, Loss: 0.4206141531467438\n",
      "Epoch 13, Batch 922, Loss: 0.5887832045555115\n",
      "Epoch 13, Batch 923, Loss: 0.4999941289424896\n",
      "Epoch 13, Batch 924, Loss: 0.45141083002090454\n",
      "Epoch 13, Batch 925, Loss: 0.4293938875198364\n",
      "Epoch 13, Batch 926, Loss: 0.32341933250427246\n",
      "Epoch 13, Batch 927, Loss: 0.587680995464325\n",
      "Epoch 13, Batch 928, Loss: 0.46144503355026245\n",
      "Epoch 13, Batch 929, Loss: 0.502648115158081\n",
      "Epoch 13, Batch 930, Loss: 0.6404738426208496\n",
      "Epoch 13, Batch 931, Loss: 0.5376507043838501\n",
      "Epoch 13, Batch 932, Loss: 0.48635074496269226\n",
      "Epoch 13, Batch 933, Loss: 0.3912389278411865\n",
      "Epoch 13, Batch 934, Loss: 0.529403030872345\n",
      "Epoch 13, Batch 935, Loss: 0.5360988974571228\n",
      "Epoch 13, Batch 936, Loss: 0.6317392587661743\n",
      "Epoch 13, Batch 937, Loss: 0.4280887544155121\n",
      "Epoch 13, Batch 938, Loss: 0.21302656829357147\n",
      "Accuracy of train set: 0.8320833333333333\n",
      "Epoch 13, Batch 1, Test Loss: 0.4239951968193054\n",
      "Epoch 13, Batch 2, Test Loss: 0.4981577396392822\n",
      "Epoch 13, Batch 3, Test Loss: 0.4145117998123169\n",
      "Epoch 13, Batch 4, Test Loss: 0.4412751793861389\n",
      "Epoch 13, Batch 5, Test Loss: 0.5141074657440186\n",
      "Epoch 13, Batch 6, Test Loss: 0.3873067796230316\n",
      "Epoch 13, Batch 7, Test Loss: 0.3633444607257843\n",
      "Epoch 13, Batch 8, Test Loss: 0.7908273935317993\n",
      "Epoch 13, Batch 9, Test Loss: 0.2734808623790741\n",
      "Epoch 13, Batch 10, Test Loss: 0.5877589583396912\n",
      "Epoch 13, Batch 11, Test Loss: 0.2711696922779083\n",
      "Epoch 13, Batch 12, Test Loss: 0.4553329348564148\n",
      "Epoch 13, Batch 13, Test Loss: 0.7633387446403503\n",
      "Epoch 13, Batch 14, Test Loss: 0.7148342132568359\n",
      "Epoch 13, Batch 15, Test Loss: 0.357921302318573\n",
      "Epoch 13, Batch 16, Test Loss: 0.3801915645599365\n",
      "Epoch 13, Batch 17, Test Loss: 0.4325438141822815\n",
      "Epoch 13, Batch 18, Test Loss: 0.4993595480918884\n",
      "Epoch 13, Batch 19, Test Loss: 0.38052210211753845\n",
      "Epoch 13, Batch 20, Test Loss: 0.4239368438720703\n",
      "Epoch 13, Batch 21, Test Loss: 0.4167625606060028\n",
      "Epoch 13, Batch 22, Test Loss: 0.362295925617218\n",
      "Epoch 13, Batch 23, Test Loss: 0.3277609646320343\n",
      "Epoch 13, Batch 24, Test Loss: 0.5479153990745544\n",
      "Epoch 13, Batch 25, Test Loss: 0.2893396019935608\n",
      "Epoch 13, Batch 26, Test Loss: 0.29795876145362854\n",
      "Epoch 13, Batch 27, Test Loss: 0.49726057052612305\n",
      "Epoch 13, Batch 28, Test Loss: 0.5741114020347595\n",
      "Epoch 13, Batch 29, Test Loss: 0.42667078971862793\n",
      "Epoch 13, Batch 30, Test Loss: 0.49848899245262146\n",
      "Epoch 13, Batch 31, Test Loss: 0.46174877882003784\n",
      "Epoch 13, Batch 32, Test Loss: 0.37404268980026245\n",
      "Epoch 13, Batch 33, Test Loss: 0.37421730160713196\n",
      "Epoch 13, Batch 34, Test Loss: 0.44882914423942566\n",
      "Epoch 13, Batch 35, Test Loss: 0.559513509273529\n",
      "Epoch 13, Batch 36, Test Loss: 0.40434467792510986\n",
      "Epoch 13, Batch 37, Test Loss: 0.44882845878601074\n",
      "Epoch 13, Batch 38, Test Loss: 0.40386494994163513\n",
      "Epoch 13, Batch 39, Test Loss: 0.5999537706375122\n",
      "Epoch 13, Batch 40, Test Loss: 0.5476078987121582\n",
      "Epoch 13, Batch 41, Test Loss: 0.41554534435272217\n",
      "Epoch 13, Batch 42, Test Loss: 0.5573576092720032\n",
      "Epoch 13, Batch 43, Test Loss: 0.4192964434623718\n",
      "Epoch 13, Batch 44, Test Loss: 0.4161747097969055\n",
      "Epoch 13, Batch 45, Test Loss: 0.4889849126338959\n",
      "Epoch 13, Batch 46, Test Loss: 0.5321095585823059\n",
      "Epoch 13, Batch 47, Test Loss: 0.33563029766082764\n",
      "Epoch 13, Batch 48, Test Loss: 0.6327879428863525\n",
      "Epoch 13, Batch 49, Test Loss: 0.3880329132080078\n",
      "Epoch 13, Batch 50, Test Loss: 0.4265487790107727\n",
      "Epoch 13, Batch 51, Test Loss: 0.38433656096458435\n",
      "Epoch 13, Batch 52, Test Loss: 0.4304748773574829\n",
      "Epoch 13, Batch 53, Test Loss: 0.2995069622993469\n",
      "Epoch 13, Batch 54, Test Loss: 0.527271032333374\n",
      "Epoch 13, Batch 55, Test Loss: 0.5775046348571777\n",
      "Epoch 13, Batch 56, Test Loss: 0.4086757004261017\n",
      "Epoch 13, Batch 57, Test Loss: 0.5178622603416443\n",
      "Epoch 13, Batch 58, Test Loss: 0.4513337016105652\n",
      "Epoch 13, Batch 59, Test Loss: 0.5312324166297913\n",
      "Epoch 13, Batch 60, Test Loss: 0.6845570206642151\n",
      "Epoch 13, Batch 61, Test Loss: 0.46309229731559753\n",
      "Epoch 13, Batch 62, Test Loss: 0.3711452782154083\n",
      "Epoch 13, Batch 63, Test Loss: 0.5620149374008179\n",
      "Epoch 13, Batch 64, Test Loss: 0.5158089399337769\n",
      "Epoch 13, Batch 65, Test Loss: 0.3879670798778534\n",
      "Epoch 13, Batch 66, Test Loss: 0.6789727210998535\n",
      "Epoch 13, Batch 67, Test Loss: 0.5040374994277954\n",
      "Epoch 13, Batch 68, Test Loss: 0.8004499077796936\n",
      "Epoch 13, Batch 69, Test Loss: 0.3482322096824646\n",
      "Epoch 13, Batch 70, Test Loss: 0.4642447829246521\n",
      "Epoch 13, Batch 71, Test Loss: 0.3975147008895874\n",
      "Epoch 13, Batch 72, Test Loss: 0.4851692020893097\n",
      "Epoch 13, Batch 73, Test Loss: 0.6986407041549683\n",
      "Epoch 13, Batch 74, Test Loss: 0.46840140223503113\n",
      "Epoch 13, Batch 75, Test Loss: 0.31060922145843506\n",
      "Epoch 13, Batch 76, Test Loss: 0.38518601655960083\n",
      "Epoch 13, Batch 77, Test Loss: 0.5381397604942322\n",
      "Epoch 13, Batch 78, Test Loss: 0.5069208145141602\n",
      "Epoch 13, Batch 79, Test Loss: 0.4905441105365753\n",
      "Epoch 13, Batch 80, Test Loss: 0.37328001856803894\n",
      "Epoch 13, Batch 81, Test Loss: 0.5319167375564575\n",
      "Epoch 13, Batch 82, Test Loss: 0.3896437883377075\n",
      "Epoch 13, Batch 83, Test Loss: 0.6207276582717896\n",
      "Epoch 13, Batch 84, Test Loss: 0.48827290534973145\n",
      "Epoch 13, Batch 85, Test Loss: 0.3762817084789276\n",
      "Epoch 13, Batch 86, Test Loss: 0.6148748993873596\n",
      "Epoch 13, Batch 87, Test Loss: 0.6116958260536194\n",
      "Epoch 13, Batch 88, Test Loss: 0.7486555576324463\n",
      "Epoch 13, Batch 89, Test Loss: 0.4509841203689575\n",
      "Epoch 13, Batch 90, Test Loss: 0.44533395767211914\n",
      "Epoch 13, Batch 91, Test Loss: 0.690819263458252\n",
      "Epoch 13, Batch 92, Test Loss: 0.40296244621276855\n",
      "Epoch 13, Batch 93, Test Loss: 0.5294961333274841\n",
      "Epoch 13, Batch 94, Test Loss: 0.5354302525520325\n",
      "Epoch 13, Batch 95, Test Loss: 0.5439341068267822\n",
      "Epoch 13, Batch 96, Test Loss: 0.34337639808654785\n",
      "Epoch 13, Batch 97, Test Loss: 0.5634821653366089\n",
      "Epoch 13, Batch 98, Test Loss: 0.32711800932884216\n",
      "Epoch 13, Batch 99, Test Loss: 0.572777509689331\n",
      "Epoch 13, Batch 100, Test Loss: 0.4934920072555542\n",
      "Epoch 13, Batch 101, Test Loss: 0.4725015163421631\n",
      "Epoch 13, Batch 102, Test Loss: 0.37010514736175537\n",
      "Epoch 13, Batch 103, Test Loss: 0.5292485952377319\n",
      "Epoch 13, Batch 104, Test Loss: 0.3370928168296814\n",
      "Epoch 13, Batch 105, Test Loss: 0.5322344899177551\n",
      "Epoch 13, Batch 106, Test Loss: 0.5010913610458374\n",
      "Epoch 13, Batch 107, Test Loss: 0.3296179473400116\n",
      "Epoch 13, Batch 108, Test Loss: 0.21957948803901672\n",
      "Epoch 13, Batch 109, Test Loss: 0.5281413793563843\n",
      "Epoch 13, Batch 110, Test Loss: 0.5546644926071167\n",
      "Epoch 13, Batch 111, Test Loss: 0.3586304783821106\n",
      "Epoch 13, Batch 112, Test Loss: 0.3524259924888611\n",
      "Epoch 13, Batch 113, Test Loss: 0.5306667685508728\n",
      "Epoch 13, Batch 114, Test Loss: 0.5732211470603943\n",
      "Epoch 13, Batch 115, Test Loss: 0.3732035756111145\n",
      "Epoch 13, Batch 116, Test Loss: 0.33842507004737854\n",
      "Epoch 13, Batch 117, Test Loss: 0.39691466093063354\n",
      "Epoch 13, Batch 118, Test Loss: 0.38414567708969116\n",
      "Epoch 13, Batch 119, Test Loss: 0.35679835081100464\n",
      "Epoch 13, Batch 120, Test Loss: 0.46554359793663025\n",
      "Epoch 13, Batch 121, Test Loss: 0.252939909696579\n",
      "Epoch 13, Batch 122, Test Loss: 0.3421047329902649\n",
      "Epoch 13, Batch 123, Test Loss: 0.4031203091144562\n",
      "Epoch 13, Batch 124, Test Loss: 0.47964227199554443\n",
      "Epoch 13, Batch 125, Test Loss: 0.5366053581237793\n",
      "Epoch 13, Batch 126, Test Loss: 0.40842488408088684\n",
      "Epoch 13, Batch 127, Test Loss: 0.4870786666870117\n",
      "Epoch 13, Batch 128, Test Loss: 0.5206398963928223\n",
      "Epoch 13, Batch 129, Test Loss: 0.32036536931991577\n",
      "Epoch 13, Batch 130, Test Loss: 0.3705379366874695\n",
      "Epoch 13, Batch 131, Test Loss: 0.29927486181259155\n",
      "Epoch 13, Batch 132, Test Loss: 0.42605894804000854\n",
      "Epoch 13, Batch 133, Test Loss: 0.3754933774471283\n",
      "Epoch 13, Batch 134, Test Loss: 0.5199590921401978\n",
      "Epoch 13, Batch 135, Test Loss: 0.41581016778945923\n",
      "Epoch 13, Batch 136, Test Loss: 0.501113772392273\n",
      "Epoch 13, Batch 137, Test Loss: 0.6172576546669006\n",
      "Epoch 13, Batch 138, Test Loss: 0.5179855227470398\n",
      "Epoch 13, Batch 139, Test Loss: 0.3553648591041565\n",
      "Epoch 13, Batch 140, Test Loss: 0.47577932476997375\n",
      "Epoch 13, Batch 141, Test Loss: 0.4399530291557312\n",
      "Epoch 13, Batch 142, Test Loss: 0.4579819142818451\n",
      "Epoch 13, Batch 143, Test Loss: 0.5874720811843872\n",
      "Epoch 13, Batch 144, Test Loss: 0.4963921010494232\n",
      "Epoch 13, Batch 145, Test Loss: 0.4517067074775696\n",
      "Epoch 13, Batch 146, Test Loss: 0.2767413556575775\n",
      "Epoch 13, Batch 147, Test Loss: 0.3552812337875366\n",
      "Epoch 13, Batch 148, Test Loss: 0.48744654655456543\n",
      "Epoch 13, Batch 149, Test Loss: 0.4151415228843689\n",
      "Epoch 13, Batch 150, Test Loss: 0.814096212387085\n",
      "Epoch 13, Batch 151, Test Loss: 0.5009294748306274\n",
      "Epoch 13, Batch 152, Test Loss: 0.5126871466636658\n",
      "Epoch 13, Batch 153, Test Loss: 0.6546356678009033\n",
      "Epoch 13, Batch 154, Test Loss: 0.33184221386909485\n",
      "Epoch 13, Batch 155, Test Loss: 0.2748885154724121\n",
      "Epoch 13, Batch 156, Test Loss: 0.4785977303981781\n",
      "Epoch 13, Batch 157, Test Loss: 0.37692350149154663\n",
      "Epoch 13, Batch 158, Test Loss: 0.30376172065734863\n",
      "Epoch 13, Batch 159, Test Loss: 0.28398212790489197\n",
      "Epoch 13, Batch 160, Test Loss: 0.5599276423454285\n",
      "Epoch 13, Batch 161, Test Loss: 0.41768980026245117\n",
      "Epoch 13, Batch 162, Test Loss: 0.6357594728469849\n",
      "Epoch 13, Batch 163, Test Loss: 0.4971221387386322\n",
      "Epoch 13, Batch 164, Test Loss: 0.5800102353096008\n",
      "Epoch 13, Batch 165, Test Loss: 0.44946804642677307\n",
      "Epoch 13, Batch 166, Test Loss: 0.5033828020095825\n",
      "Epoch 13, Batch 167, Test Loss: 0.5608944296836853\n",
      "Epoch 13, Batch 168, Test Loss: 0.5189603567123413\n",
      "Epoch 13, Batch 169, Test Loss: 0.6582786440849304\n",
      "Epoch 13, Batch 170, Test Loss: 0.6043649911880493\n",
      "Epoch 13, Batch 171, Test Loss: 0.45865753293037415\n",
      "Epoch 13, Batch 172, Test Loss: 0.31211310625076294\n",
      "Epoch 13, Batch 173, Test Loss: 0.36471614241600037\n",
      "Epoch 13, Batch 174, Test Loss: 0.33785930275917053\n",
      "Epoch 13, Batch 175, Test Loss: 0.47049322724342346\n",
      "Epoch 13, Batch 176, Test Loss: 0.5555633902549744\n",
      "Epoch 13, Batch 177, Test Loss: 0.48278647661209106\n",
      "Epoch 13, Batch 178, Test Loss: 0.5347560048103333\n",
      "Epoch 13, Batch 179, Test Loss: 0.4343324601650238\n",
      "Epoch 13, Batch 180, Test Loss: 0.30302804708480835\n",
      "Epoch 13, Batch 181, Test Loss: 0.5847352147102356\n",
      "Epoch 13, Batch 182, Test Loss: 0.4569179117679596\n",
      "Epoch 13, Batch 183, Test Loss: 0.294589102268219\n",
      "Epoch 13, Batch 184, Test Loss: 0.3274981379508972\n",
      "Epoch 13, Batch 185, Test Loss: 0.4523581862449646\n",
      "Epoch 13, Batch 186, Test Loss: 0.48391973972320557\n",
      "Epoch 13, Batch 187, Test Loss: 0.28857219219207764\n",
      "Epoch 13, Batch 188, Test Loss: 0.3837543725967407\n",
      "Epoch 13, Batch 189, Test Loss: 0.4770548343658447\n",
      "Epoch 13, Batch 190, Test Loss: 0.42867690324783325\n",
      "Epoch 13, Batch 191, Test Loss: 0.33333587646484375\n",
      "Epoch 13, Batch 192, Test Loss: 0.640755295753479\n",
      "Epoch 13, Batch 193, Test Loss: 0.47031089663505554\n",
      "Epoch 13, Batch 194, Test Loss: 0.4248690605163574\n",
      "Epoch 13, Batch 195, Test Loss: 0.28870663046836853\n",
      "Epoch 13, Batch 196, Test Loss: 0.33184537291526794\n",
      "Epoch 13, Batch 197, Test Loss: 0.391751766204834\n",
      "Epoch 13, Batch 198, Test Loss: 0.5821201205253601\n",
      "Epoch 13, Batch 199, Test Loss: 0.7649785876274109\n",
      "Epoch 13, Batch 200, Test Loss: 0.46819597482681274\n",
      "Epoch 13, Batch 201, Test Loss: 0.43802547454833984\n",
      "Epoch 13, Batch 202, Test Loss: 0.3960939645767212\n",
      "Epoch 13, Batch 203, Test Loss: 0.5371915102005005\n",
      "Epoch 13, Batch 204, Test Loss: 0.4276100993156433\n",
      "Epoch 13, Batch 205, Test Loss: 0.2993815243244171\n",
      "Epoch 13, Batch 206, Test Loss: 0.7006827592849731\n",
      "Epoch 13, Batch 207, Test Loss: 0.30466827750205994\n",
      "Epoch 13, Batch 208, Test Loss: 0.5635800957679749\n",
      "Epoch 13, Batch 209, Test Loss: 0.5252420902252197\n",
      "Epoch 13, Batch 210, Test Loss: 0.46375182271003723\n",
      "Epoch 13, Batch 211, Test Loss: 0.32933545112609863\n",
      "Epoch 13, Batch 212, Test Loss: 0.4843054711818695\n",
      "Epoch 13, Batch 213, Test Loss: 0.5419179797172546\n",
      "Epoch 13, Batch 214, Test Loss: 0.40235793590545654\n",
      "Epoch 13, Batch 215, Test Loss: 0.3922916352748871\n",
      "Epoch 13, Batch 216, Test Loss: 0.4810502529144287\n",
      "Epoch 13, Batch 217, Test Loss: 0.5319550037384033\n",
      "Epoch 13, Batch 218, Test Loss: 0.47751542925834656\n",
      "Epoch 13, Batch 219, Test Loss: 0.5122354626655579\n",
      "Epoch 13, Batch 220, Test Loss: 0.4441612958908081\n",
      "Epoch 13, Batch 221, Test Loss: 0.545996904373169\n",
      "Epoch 13, Batch 222, Test Loss: 0.4215836822986603\n",
      "Epoch 13, Batch 223, Test Loss: 0.5012641549110413\n",
      "Epoch 13, Batch 224, Test Loss: 0.5335243344306946\n",
      "Epoch 13, Batch 225, Test Loss: 0.3144707977771759\n",
      "Epoch 13, Batch 226, Test Loss: 0.5453869104385376\n",
      "Epoch 13, Batch 227, Test Loss: 0.5178592801094055\n",
      "Epoch 13, Batch 228, Test Loss: 0.7086292505264282\n",
      "Epoch 13, Batch 229, Test Loss: 0.4648985266685486\n",
      "Epoch 13, Batch 230, Test Loss: 0.3658698797225952\n",
      "Epoch 13, Batch 231, Test Loss: 0.47312259674072266\n",
      "Epoch 13, Batch 232, Test Loss: 0.3534359335899353\n",
      "Epoch 13, Batch 233, Test Loss: 0.7294024229049683\n",
      "Epoch 13, Batch 234, Test Loss: 0.3992990255355835\n",
      "Epoch 13, Batch 235, Test Loss: 0.4759839177131653\n",
      "Epoch 13, Batch 236, Test Loss: 0.44567158818244934\n",
      "Epoch 13, Batch 237, Test Loss: 0.38198602199554443\n",
      "Epoch 13, Batch 238, Test Loss: 0.30326932668685913\n",
      "Epoch 13, Batch 239, Test Loss: 0.5526981353759766\n",
      "Epoch 13, Batch 240, Test Loss: 0.40406882762908936\n",
      "Epoch 13, Batch 241, Test Loss: 0.44458115100860596\n",
      "Epoch 13, Batch 242, Test Loss: 0.3449120819568634\n",
      "Epoch 13, Batch 243, Test Loss: 0.6923092603683472\n",
      "Epoch 13, Batch 244, Test Loss: 0.6915942430496216\n",
      "Epoch 13, Batch 245, Test Loss: 0.39748167991638184\n",
      "Epoch 13, Batch 246, Test Loss: 0.39676377177238464\n",
      "Epoch 13, Batch 247, Test Loss: 0.4934914708137512\n",
      "Epoch 13, Batch 248, Test Loss: 0.39973270893096924\n",
      "Epoch 13, Batch 249, Test Loss: 0.43987682461738586\n",
      "Epoch 13, Batch 250, Test Loss: 0.44835060834884644\n",
      "Epoch 13, Batch 251, Test Loss: 0.5614200830459595\n",
      "Epoch 13, Batch 252, Test Loss: 0.4211525022983551\n",
      "Epoch 13, Batch 253, Test Loss: 0.4847140610218048\n",
      "Epoch 13, Batch 254, Test Loss: 0.4772140681743622\n",
      "Epoch 13, Batch 255, Test Loss: 0.48767200112342834\n",
      "Epoch 13, Batch 256, Test Loss: 0.5098203420639038\n",
      "Epoch 13, Batch 257, Test Loss: 0.4859645962715149\n",
      "Epoch 13, Batch 258, Test Loss: 0.4062270522117615\n",
      "Epoch 13, Batch 259, Test Loss: 0.39767760038375854\n",
      "Epoch 13, Batch 260, Test Loss: 0.2603711783885956\n",
      "Epoch 13, Batch 261, Test Loss: 0.4962376356124878\n",
      "Epoch 13, Batch 262, Test Loss: 0.6379011273384094\n",
      "Epoch 13, Batch 263, Test Loss: 0.4321301579475403\n",
      "Epoch 13, Batch 264, Test Loss: 0.4848964512348175\n",
      "Epoch 13, Batch 265, Test Loss: 0.7452586889266968\n",
      "Epoch 13, Batch 266, Test Loss: 0.4236752688884735\n",
      "Epoch 13, Batch 267, Test Loss: 0.42114463448524475\n",
      "Epoch 13, Batch 268, Test Loss: 0.4396129548549652\n",
      "Epoch 13, Batch 269, Test Loss: 0.6034148335456848\n",
      "Epoch 13, Batch 270, Test Loss: 0.33199936151504517\n",
      "Epoch 13, Batch 271, Test Loss: 0.5128801465034485\n",
      "Epoch 13, Batch 272, Test Loss: 0.4167644679546356\n",
      "Epoch 13, Batch 273, Test Loss: 0.40492138266563416\n",
      "Epoch 13, Batch 274, Test Loss: 0.6404891610145569\n",
      "Epoch 13, Batch 275, Test Loss: 0.2819485664367676\n",
      "Epoch 13, Batch 276, Test Loss: 0.44558364152908325\n",
      "Epoch 13, Batch 277, Test Loss: 0.4103325605392456\n",
      "Epoch 13, Batch 278, Test Loss: 0.57819664478302\n",
      "Epoch 13, Batch 279, Test Loss: 0.39561015367507935\n",
      "Epoch 13, Batch 280, Test Loss: 0.2770192325115204\n",
      "Epoch 13, Batch 281, Test Loss: 0.45553600788116455\n",
      "Epoch 13, Batch 282, Test Loss: 0.5272565484046936\n",
      "Epoch 13, Batch 283, Test Loss: 0.3290759027004242\n",
      "Epoch 13, Batch 284, Test Loss: 0.6586542129516602\n",
      "Epoch 13, Batch 285, Test Loss: 0.5959877967834473\n",
      "Epoch 13, Batch 286, Test Loss: 0.3598247468471527\n",
      "Epoch 13, Batch 287, Test Loss: 0.530198335647583\n",
      "Epoch 13, Batch 288, Test Loss: 0.5657392740249634\n",
      "Epoch 13, Batch 289, Test Loss: 0.38309261202812195\n",
      "Epoch 13, Batch 290, Test Loss: 0.3435845673084259\n",
      "Epoch 13, Batch 291, Test Loss: 0.47011253237724304\n",
      "Epoch 13, Batch 292, Test Loss: 0.5206896066665649\n",
      "Epoch 13, Batch 293, Test Loss: 0.3449399173259735\n",
      "Epoch 13, Batch 294, Test Loss: 0.517532229423523\n",
      "Epoch 13, Batch 295, Test Loss: 0.38152146339416504\n",
      "Epoch 13, Batch 296, Test Loss: 0.6473256945610046\n",
      "Epoch 13, Batch 297, Test Loss: 0.36650532484054565\n",
      "Epoch 13, Batch 298, Test Loss: 0.5784845352172852\n",
      "Epoch 13, Batch 299, Test Loss: 0.6039555072784424\n",
      "Epoch 13, Batch 300, Test Loss: 0.5903990268707275\n",
      "Epoch 13, Batch 301, Test Loss: 0.5022342801094055\n",
      "Epoch 13, Batch 302, Test Loss: 0.47468864917755127\n",
      "Epoch 13, Batch 303, Test Loss: 0.49583184719085693\n",
      "Epoch 13, Batch 304, Test Loss: 0.45014694333076477\n",
      "Epoch 13, Batch 305, Test Loss: 0.3103558123111725\n",
      "Epoch 13, Batch 306, Test Loss: 0.3476618230342865\n",
      "Epoch 13, Batch 307, Test Loss: 0.4155949652194977\n",
      "Epoch 13, Batch 308, Test Loss: 0.48829492926597595\n",
      "Epoch 13, Batch 309, Test Loss: 0.3534996509552002\n",
      "Epoch 13, Batch 310, Test Loss: 0.5956761837005615\n",
      "Epoch 13, Batch 311, Test Loss: 0.46678435802459717\n",
      "Epoch 13, Batch 312, Test Loss: 0.40161824226379395\n",
      "Epoch 13, Batch 313, Test Loss: 0.4353119134902954\n",
      "Epoch 13, Batch 314, Test Loss: 0.48611053824424744\n",
      "Epoch 13, Batch 315, Test Loss: 0.4205370545387268\n",
      "Epoch 13, Batch 316, Test Loss: 0.2749505937099457\n",
      "Epoch 13, Batch 317, Test Loss: 0.8141512870788574\n",
      "Epoch 13, Batch 318, Test Loss: 0.4578750431537628\n",
      "Epoch 13, Batch 319, Test Loss: 0.38120216131210327\n",
      "Epoch 13, Batch 320, Test Loss: 0.5200129151344299\n",
      "Epoch 13, Batch 321, Test Loss: 0.525232195854187\n",
      "Epoch 13, Batch 322, Test Loss: 0.580527663230896\n",
      "Epoch 13, Batch 323, Test Loss: 0.35143035650253296\n",
      "Epoch 13, Batch 324, Test Loss: 0.5882620215415955\n",
      "Epoch 13, Batch 325, Test Loss: 0.5249040722846985\n",
      "Epoch 13, Batch 326, Test Loss: 0.452665239572525\n",
      "Epoch 13, Batch 327, Test Loss: 0.3834810256958008\n",
      "Epoch 13, Batch 328, Test Loss: 0.490104079246521\n",
      "Epoch 13, Batch 329, Test Loss: 0.3314053416252136\n",
      "Epoch 13, Batch 330, Test Loss: 0.35737013816833496\n",
      "Epoch 13, Batch 331, Test Loss: 0.6295877695083618\n",
      "Epoch 13, Batch 332, Test Loss: 0.3982470631599426\n",
      "Epoch 13, Batch 333, Test Loss: 0.2838864028453827\n",
      "Epoch 13, Batch 334, Test Loss: 0.36374256014823914\n",
      "Epoch 13, Batch 335, Test Loss: 0.4376916289329529\n",
      "Epoch 13, Batch 336, Test Loss: 0.39161354303359985\n",
      "Epoch 13, Batch 337, Test Loss: 0.48979052901268005\n",
      "Epoch 13, Batch 338, Test Loss: 0.5410308837890625\n",
      "Epoch 13, Batch 339, Test Loss: 0.672205924987793\n",
      "Epoch 13, Batch 340, Test Loss: 0.4971717894077301\n",
      "Epoch 13, Batch 341, Test Loss: 0.5593926906585693\n",
      "Epoch 13, Batch 342, Test Loss: 0.634070098400116\n",
      "Epoch 13, Batch 343, Test Loss: 0.47232064604759216\n",
      "Epoch 13, Batch 344, Test Loss: 0.5933568477630615\n",
      "Epoch 13, Batch 345, Test Loss: 0.3587723672389984\n",
      "Epoch 13, Batch 346, Test Loss: 0.5520089864730835\n",
      "Epoch 13, Batch 347, Test Loss: 0.5519970655441284\n",
      "Epoch 13, Batch 348, Test Loss: 0.5426626801490784\n",
      "Epoch 13, Batch 349, Test Loss: 0.3028087615966797\n",
      "Epoch 13, Batch 350, Test Loss: 0.4540039002895355\n",
      "Epoch 13, Batch 351, Test Loss: 0.6127577424049377\n",
      "Epoch 13, Batch 352, Test Loss: 0.4463818669319153\n",
      "Epoch 13, Batch 353, Test Loss: 0.3700360059738159\n",
      "Epoch 13, Batch 354, Test Loss: 0.39565619826316833\n",
      "Epoch 13, Batch 355, Test Loss: 0.4584791660308838\n",
      "Epoch 13, Batch 356, Test Loss: 0.46141141653060913\n",
      "Epoch 13, Batch 357, Test Loss: 0.470560759305954\n",
      "Epoch 13, Batch 358, Test Loss: 0.45832979679107666\n",
      "Epoch 13, Batch 359, Test Loss: 0.49634283781051636\n",
      "Epoch 13, Batch 360, Test Loss: 0.5870223641395569\n",
      "Epoch 13, Batch 361, Test Loss: 0.5196696519851685\n",
      "Epoch 13, Batch 362, Test Loss: 0.33382463455200195\n",
      "Epoch 13, Batch 363, Test Loss: 0.3487732708454132\n",
      "Epoch 13, Batch 364, Test Loss: 0.40545207262039185\n",
      "Epoch 13, Batch 365, Test Loss: 0.5109615921974182\n",
      "Epoch 13, Batch 366, Test Loss: 0.5138478875160217\n",
      "Epoch 13, Batch 367, Test Loss: 0.3901478946208954\n",
      "Epoch 13, Batch 368, Test Loss: 0.4225583076477051\n",
      "Epoch 13, Batch 369, Test Loss: 0.4290015399456024\n",
      "Epoch 13, Batch 370, Test Loss: 0.5163961052894592\n",
      "Epoch 13, Batch 371, Test Loss: 0.5164083242416382\n",
      "Epoch 13, Batch 372, Test Loss: 0.414162814617157\n",
      "Epoch 13, Batch 373, Test Loss: 0.5194830298423767\n",
      "Epoch 13, Batch 374, Test Loss: 0.5941967964172363\n",
      "Epoch 13, Batch 375, Test Loss: 0.2228880524635315\n",
      "Epoch 13, Batch 376, Test Loss: 0.5494169592857361\n",
      "Epoch 13, Batch 377, Test Loss: 0.5007675886154175\n",
      "Epoch 13, Batch 378, Test Loss: 0.3796079158782959\n",
      "Epoch 13, Batch 379, Test Loss: 0.5594390034675598\n",
      "Epoch 13, Batch 380, Test Loss: 0.6478372812271118\n",
      "Epoch 13, Batch 381, Test Loss: 0.3556702733039856\n",
      "Epoch 13, Batch 382, Test Loss: 0.5830345749855042\n",
      "Epoch 13, Batch 383, Test Loss: 0.6554597020149231\n",
      "Epoch 13, Batch 384, Test Loss: 0.40592846274375916\n",
      "Epoch 13, Batch 385, Test Loss: 0.6431395411491394\n",
      "Epoch 13, Batch 386, Test Loss: 0.29039084911346436\n",
      "Epoch 13, Batch 387, Test Loss: 0.5982300639152527\n",
      "Epoch 13, Batch 388, Test Loss: 0.3876265585422516\n",
      "Epoch 13, Batch 389, Test Loss: 0.4095696210861206\n",
      "Epoch 13, Batch 390, Test Loss: 0.5061160326004028\n",
      "Epoch 13, Batch 391, Test Loss: 0.4753420948982239\n",
      "Epoch 13, Batch 392, Test Loss: 0.5887271165847778\n",
      "Epoch 13, Batch 393, Test Loss: 0.5061089396476746\n",
      "Epoch 13, Batch 394, Test Loss: 0.3451818525791168\n",
      "Epoch 13, Batch 395, Test Loss: 0.46674010157585144\n",
      "Epoch 13, Batch 396, Test Loss: 0.3404695987701416\n",
      "Epoch 13, Batch 397, Test Loss: 0.49767306447029114\n",
      "Epoch 13, Batch 398, Test Loss: 0.40050286054611206\n",
      "Epoch 13, Batch 399, Test Loss: 0.4449043869972229\n",
      "Epoch 13, Batch 400, Test Loss: 0.3823573887348175\n",
      "Epoch 13, Batch 401, Test Loss: 0.7490454912185669\n",
      "Epoch 13, Batch 402, Test Loss: 0.4199473261833191\n",
      "Epoch 13, Batch 403, Test Loss: 0.46354156732559204\n",
      "Epoch 13, Batch 404, Test Loss: 0.585554838180542\n",
      "Epoch 13, Batch 405, Test Loss: 0.3375866711139679\n",
      "Epoch 13, Batch 406, Test Loss: 0.3486180603504181\n",
      "Epoch 13, Batch 407, Test Loss: 0.6328674554824829\n",
      "Epoch 13, Batch 408, Test Loss: 0.4546010196208954\n",
      "Epoch 13, Batch 409, Test Loss: 0.49466484785079956\n",
      "Epoch 13, Batch 410, Test Loss: 0.5220804810523987\n",
      "Epoch 13, Batch 411, Test Loss: 0.42025572061538696\n",
      "Epoch 13, Batch 412, Test Loss: 0.4802706837654114\n",
      "Epoch 13, Batch 413, Test Loss: 0.39840734004974365\n",
      "Epoch 13, Batch 414, Test Loss: 0.38532209396362305\n",
      "Epoch 13, Batch 415, Test Loss: 0.4812288284301758\n",
      "Epoch 13, Batch 416, Test Loss: 0.4949573278427124\n",
      "Epoch 13, Batch 417, Test Loss: 0.39727354049682617\n",
      "Epoch 13, Batch 418, Test Loss: 0.6487681269645691\n",
      "Epoch 13, Batch 419, Test Loss: 0.8226461410522461\n",
      "Epoch 13, Batch 420, Test Loss: 0.5270031690597534\n",
      "Epoch 13, Batch 421, Test Loss: 0.4843837320804596\n",
      "Epoch 13, Batch 422, Test Loss: 0.559614896774292\n",
      "Epoch 13, Batch 423, Test Loss: 0.5057192444801331\n",
      "Epoch 13, Batch 424, Test Loss: 0.5635901689529419\n",
      "Epoch 13, Batch 425, Test Loss: 0.6779394149780273\n",
      "Epoch 13, Batch 426, Test Loss: 0.43257442116737366\n",
      "Epoch 13, Batch 427, Test Loss: 0.5258797407150269\n",
      "Epoch 13, Batch 428, Test Loss: 0.5687063932418823\n",
      "Epoch 13, Batch 429, Test Loss: 0.4732667803764343\n",
      "Epoch 13, Batch 430, Test Loss: 0.4172462821006775\n",
      "Epoch 13, Batch 431, Test Loss: 0.4155290126800537\n",
      "Epoch 13, Batch 432, Test Loss: 0.5402337312698364\n",
      "Epoch 13, Batch 433, Test Loss: 0.5533626079559326\n",
      "Epoch 13, Batch 434, Test Loss: 0.4962935745716095\n",
      "Epoch 13, Batch 435, Test Loss: 0.49385884404182434\n",
      "Epoch 13, Batch 436, Test Loss: 0.5664591789245605\n",
      "Epoch 13, Batch 437, Test Loss: 0.4363858103752136\n",
      "Epoch 13, Batch 438, Test Loss: 0.45886683464050293\n",
      "Epoch 13, Batch 439, Test Loss: 0.4691992700099945\n",
      "Epoch 13, Batch 440, Test Loss: 0.42940855026245117\n",
      "Epoch 13, Batch 441, Test Loss: 0.37586772441864014\n",
      "Epoch 13, Batch 442, Test Loss: 0.40051621198654175\n",
      "Epoch 13, Batch 443, Test Loss: 0.30645591020584106\n",
      "Epoch 13, Batch 444, Test Loss: 0.45270201563835144\n",
      "Epoch 13, Batch 445, Test Loss: 0.697941780090332\n",
      "Epoch 13, Batch 446, Test Loss: 0.3013014495372772\n",
      "Epoch 13, Batch 447, Test Loss: 0.3945346176624298\n",
      "Epoch 13, Batch 448, Test Loss: 0.35959872603416443\n",
      "Epoch 13, Batch 449, Test Loss: 0.6184588074684143\n",
      "Epoch 13, Batch 450, Test Loss: 0.5821825861930847\n",
      "Epoch 13, Batch 451, Test Loss: 0.47906091809272766\n",
      "Epoch 13, Batch 452, Test Loss: 0.3936227262020111\n",
      "Epoch 13, Batch 453, Test Loss: 0.4390987753868103\n",
      "Epoch 13, Batch 454, Test Loss: 0.5709043145179749\n",
      "Epoch 13, Batch 455, Test Loss: 0.2546495497226715\n",
      "Epoch 13, Batch 456, Test Loss: 0.679049551486969\n",
      "Epoch 13, Batch 457, Test Loss: 0.4701685905456543\n",
      "Epoch 13, Batch 458, Test Loss: 0.3489806652069092\n",
      "Epoch 13, Batch 459, Test Loss: 0.4570114016532898\n",
      "Epoch 13, Batch 460, Test Loss: 0.5331298112869263\n",
      "Epoch 13, Batch 461, Test Loss: 0.5036423802375793\n",
      "Epoch 13, Batch 462, Test Loss: 0.6802818179130554\n",
      "Epoch 13, Batch 463, Test Loss: 0.48595210909843445\n",
      "Epoch 13, Batch 464, Test Loss: 0.5557928681373596\n",
      "Epoch 13, Batch 465, Test Loss: 0.4056287407875061\n",
      "Epoch 13, Batch 466, Test Loss: 0.41918015480041504\n",
      "Epoch 13, Batch 467, Test Loss: 0.6311935186386108\n",
      "Epoch 13, Batch 468, Test Loss: 0.5114634037017822\n",
      "Epoch 13, Batch 469, Test Loss: 0.4479884207248688\n",
      "Epoch 13, Batch 470, Test Loss: 0.5375286340713501\n",
      "Epoch 13, Batch 471, Test Loss: 0.2714686989784241\n",
      "Epoch 13, Batch 472, Test Loss: 0.27056512236595154\n",
      "Epoch 13, Batch 473, Test Loss: 0.5869408845901489\n",
      "Epoch 13, Batch 474, Test Loss: 0.40102559328079224\n",
      "Epoch 13, Batch 475, Test Loss: 0.2127746194601059\n",
      "Epoch 13, Batch 476, Test Loss: 0.45197972655296326\n",
      "Epoch 13, Batch 477, Test Loss: 0.35241714119911194\n",
      "Epoch 13, Batch 478, Test Loss: 0.39855626225471497\n",
      "Epoch 13, Batch 479, Test Loss: 0.3821216821670532\n",
      "Epoch 13, Batch 480, Test Loss: 0.7172970771789551\n",
      "Epoch 13, Batch 481, Test Loss: 0.5022876262664795\n",
      "Epoch 13, Batch 482, Test Loss: 0.38800081610679626\n",
      "Epoch 13, Batch 483, Test Loss: 0.4270014464855194\n",
      "Epoch 13, Batch 484, Test Loss: 0.5657778382301331\n",
      "Epoch 13, Batch 485, Test Loss: 0.45870789885520935\n",
      "Epoch 13, Batch 486, Test Loss: 0.5224539637565613\n",
      "Epoch 13, Batch 487, Test Loss: 0.43696901202201843\n",
      "Epoch 13, Batch 488, Test Loss: 0.3985195755958557\n",
      "Epoch 13, Batch 489, Test Loss: 0.44428664445877075\n",
      "Epoch 13, Batch 490, Test Loss: 0.46793365478515625\n",
      "Epoch 13, Batch 491, Test Loss: 0.37648141384124756\n",
      "Epoch 13, Batch 492, Test Loss: 0.34136003255844116\n",
      "Epoch 13, Batch 493, Test Loss: 0.42225807905197144\n",
      "Epoch 13, Batch 494, Test Loss: 0.4501134157180786\n",
      "Epoch 13, Batch 495, Test Loss: 0.33116069436073303\n",
      "Epoch 13, Batch 496, Test Loss: 0.49030253291130066\n",
      "Epoch 13, Batch 497, Test Loss: 0.33554068207740784\n",
      "Epoch 13, Batch 498, Test Loss: 0.6662943363189697\n",
      "Epoch 13, Batch 499, Test Loss: 0.5878927111625671\n",
      "Epoch 13, Batch 500, Test Loss: 0.4724634289741516\n",
      "Epoch 13, Batch 501, Test Loss: 0.40339767932891846\n",
      "Epoch 13, Batch 502, Test Loss: 0.4098261296749115\n",
      "Epoch 13, Batch 503, Test Loss: 0.402738094329834\n",
      "Epoch 13, Batch 504, Test Loss: 0.48865795135498047\n",
      "Epoch 13, Batch 505, Test Loss: 0.5571330189704895\n",
      "Epoch 13, Batch 506, Test Loss: 0.6930155754089355\n",
      "Epoch 13, Batch 507, Test Loss: 0.45556455850601196\n",
      "Epoch 13, Batch 508, Test Loss: 0.43277812004089355\n",
      "Epoch 13, Batch 509, Test Loss: 0.3789418935775757\n",
      "Epoch 13, Batch 510, Test Loss: 0.4896678626537323\n",
      "Epoch 13, Batch 511, Test Loss: 0.4505603611469269\n",
      "Epoch 13, Batch 512, Test Loss: 0.3571507930755615\n",
      "Epoch 13, Batch 513, Test Loss: 0.4323531687259674\n",
      "Epoch 13, Batch 514, Test Loss: 0.3396299481391907\n",
      "Epoch 13, Batch 515, Test Loss: 0.4865017235279083\n",
      "Epoch 13, Batch 516, Test Loss: 0.32485806941986084\n",
      "Epoch 13, Batch 517, Test Loss: 0.2791462540626526\n",
      "Epoch 13, Batch 518, Test Loss: 0.42673492431640625\n",
      "Epoch 13, Batch 519, Test Loss: 0.6573271751403809\n",
      "Epoch 13, Batch 520, Test Loss: 0.31087619066238403\n",
      "Epoch 13, Batch 521, Test Loss: 0.39169248938560486\n",
      "Epoch 13, Batch 522, Test Loss: 0.5226099491119385\n",
      "Epoch 13, Batch 523, Test Loss: 0.3807457983493805\n",
      "Epoch 13, Batch 524, Test Loss: 0.6062310338020325\n",
      "Epoch 13, Batch 525, Test Loss: 0.3730309009552002\n",
      "Epoch 13, Batch 526, Test Loss: 0.4608137905597687\n",
      "Epoch 13, Batch 527, Test Loss: 0.43756023049354553\n",
      "Epoch 13, Batch 528, Test Loss: 0.43696022033691406\n",
      "Epoch 13, Batch 529, Test Loss: 0.48902058601379395\n",
      "Epoch 13, Batch 530, Test Loss: 0.5522915720939636\n",
      "Epoch 13, Batch 531, Test Loss: 0.5002850294113159\n",
      "Epoch 13, Batch 532, Test Loss: 0.42506325244903564\n",
      "Epoch 13, Batch 533, Test Loss: 0.4455174207687378\n",
      "Epoch 13, Batch 534, Test Loss: 0.6360759735107422\n",
      "Epoch 13, Batch 535, Test Loss: 0.6125487685203552\n",
      "Epoch 13, Batch 536, Test Loss: 0.427812397480011\n",
      "Epoch 13, Batch 537, Test Loss: 0.6611085534095764\n",
      "Epoch 13, Batch 538, Test Loss: 0.47630223631858826\n",
      "Epoch 13, Batch 539, Test Loss: 0.3171211779117584\n",
      "Epoch 13, Batch 540, Test Loss: 0.4951413869857788\n",
      "Epoch 13, Batch 541, Test Loss: 0.47864848375320435\n",
      "Epoch 13, Batch 542, Test Loss: 0.47649991512298584\n",
      "Epoch 13, Batch 543, Test Loss: 0.6256639957427979\n",
      "Epoch 13, Batch 544, Test Loss: 0.5321361422538757\n",
      "Epoch 13, Batch 545, Test Loss: 0.5548126697540283\n",
      "Epoch 13, Batch 546, Test Loss: 0.6174715757369995\n",
      "Epoch 13, Batch 547, Test Loss: 0.37513288855552673\n",
      "Epoch 13, Batch 548, Test Loss: 0.47017765045166016\n",
      "Epoch 13, Batch 549, Test Loss: 0.4668227434158325\n",
      "Epoch 13, Batch 550, Test Loss: 0.45206519961357117\n",
      "Epoch 13, Batch 551, Test Loss: 0.5734447240829468\n",
      "Epoch 13, Batch 552, Test Loss: 0.40010514855384827\n",
      "Epoch 13, Batch 553, Test Loss: 0.46756353974342346\n",
      "Epoch 13, Batch 554, Test Loss: 0.422061026096344\n",
      "Epoch 13, Batch 555, Test Loss: 0.5388166904449463\n",
      "Epoch 13, Batch 556, Test Loss: 0.45968514680862427\n",
      "Epoch 13, Batch 557, Test Loss: 0.4488808810710907\n",
      "Epoch 13, Batch 558, Test Loss: 0.45577120780944824\n",
      "Epoch 13, Batch 559, Test Loss: 0.3452761769294739\n",
      "Epoch 13, Batch 560, Test Loss: 0.4820535480976105\n",
      "Epoch 13, Batch 561, Test Loss: 0.337621808052063\n",
      "Epoch 13, Batch 562, Test Loss: 0.569920539855957\n",
      "Epoch 13, Batch 563, Test Loss: 0.5120784044265747\n",
      "Epoch 13, Batch 564, Test Loss: 0.5050627589225769\n",
      "Epoch 13, Batch 565, Test Loss: 0.23306423425674438\n",
      "Epoch 13, Batch 566, Test Loss: 0.4597472846508026\n",
      "Epoch 13, Batch 567, Test Loss: 0.5030038952827454\n",
      "Epoch 13, Batch 568, Test Loss: 0.4415094554424286\n",
      "Epoch 13, Batch 569, Test Loss: 0.6629911661148071\n",
      "Epoch 13, Batch 570, Test Loss: 0.48316460847854614\n",
      "Epoch 13, Batch 571, Test Loss: 0.516091525554657\n",
      "Epoch 13, Batch 572, Test Loss: 0.42029324173927307\n",
      "Epoch 13, Batch 573, Test Loss: 0.38221144676208496\n",
      "Epoch 13, Batch 574, Test Loss: 0.46288344264030457\n",
      "Epoch 13, Batch 575, Test Loss: 0.29034948348999023\n",
      "Epoch 13, Batch 576, Test Loss: 0.3339860439300537\n",
      "Epoch 13, Batch 577, Test Loss: 0.3720633387565613\n",
      "Epoch 13, Batch 578, Test Loss: 0.4675123691558838\n",
      "Epoch 13, Batch 579, Test Loss: 0.4706052839756012\n",
      "Epoch 13, Batch 580, Test Loss: 0.32694751024246216\n",
      "Epoch 13, Batch 581, Test Loss: 0.49605509638786316\n",
      "Epoch 13, Batch 582, Test Loss: 0.37931323051452637\n",
      "Epoch 13, Batch 583, Test Loss: 0.5150547027587891\n",
      "Epoch 13, Batch 584, Test Loss: 0.4312518537044525\n",
      "Epoch 13, Batch 585, Test Loss: 0.5076984167098999\n",
      "Epoch 13, Batch 586, Test Loss: 0.5613240003585815\n",
      "Epoch 13, Batch 587, Test Loss: 0.4673193097114563\n",
      "Epoch 13, Batch 588, Test Loss: 0.7242822051048279\n",
      "Epoch 13, Batch 589, Test Loss: 0.46293383836746216\n",
      "Epoch 13, Batch 590, Test Loss: 0.2725522220134735\n",
      "Epoch 13, Batch 591, Test Loss: 0.41710445284843445\n",
      "Epoch 13, Batch 592, Test Loss: 0.6284046173095703\n",
      "Epoch 13, Batch 593, Test Loss: 0.4186754822731018\n",
      "Epoch 13, Batch 594, Test Loss: 0.5771230459213257\n",
      "Epoch 13, Batch 595, Test Loss: 0.44057607650756836\n",
      "Epoch 13, Batch 596, Test Loss: 0.4722311496734619\n",
      "Epoch 13, Batch 597, Test Loss: 0.4255078434944153\n",
      "Epoch 13, Batch 598, Test Loss: 0.7432724237442017\n",
      "Epoch 13, Batch 599, Test Loss: 0.3832332491874695\n",
      "Epoch 13, Batch 600, Test Loss: 0.34520262479782104\n",
      "Epoch 13, Batch 601, Test Loss: 0.4137342870235443\n",
      "Epoch 13, Batch 602, Test Loss: 0.4948095679283142\n",
      "Epoch 13, Batch 603, Test Loss: 0.5390128493309021\n",
      "Epoch 13, Batch 604, Test Loss: 0.32428908348083496\n",
      "Epoch 13, Batch 605, Test Loss: 0.3628042936325073\n",
      "Epoch 13, Batch 606, Test Loss: 0.5166266560554504\n",
      "Epoch 13, Batch 607, Test Loss: 0.3241189122200012\n",
      "Epoch 13, Batch 608, Test Loss: 0.2787531316280365\n",
      "Epoch 13, Batch 609, Test Loss: 0.23343080282211304\n",
      "Epoch 13, Batch 610, Test Loss: 0.37295085191726685\n",
      "Epoch 13, Batch 611, Test Loss: 0.4493390917778015\n",
      "Epoch 13, Batch 612, Test Loss: 0.528649091720581\n",
      "Epoch 13, Batch 613, Test Loss: 0.3791184723377228\n",
      "Epoch 13, Batch 614, Test Loss: 0.5437859296798706\n",
      "Epoch 13, Batch 615, Test Loss: 0.5346328616142273\n",
      "Epoch 13, Batch 616, Test Loss: 0.39931195974349976\n",
      "Epoch 13, Batch 617, Test Loss: 0.39442622661590576\n",
      "Epoch 13, Batch 618, Test Loss: 0.5335769653320312\n",
      "Epoch 13, Batch 619, Test Loss: 0.656947910785675\n",
      "Epoch 13, Batch 620, Test Loss: 0.4031769931316376\n",
      "Epoch 13, Batch 621, Test Loss: 0.6326990723609924\n",
      "Epoch 13, Batch 622, Test Loss: 0.49238651990890503\n",
      "Epoch 13, Batch 623, Test Loss: 0.3794096112251282\n",
      "Epoch 13, Batch 624, Test Loss: 0.42191457748413086\n",
      "Epoch 13, Batch 625, Test Loss: 0.5919932126998901\n",
      "Epoch 13, Batch 626, Test Loss: 0.8151763677597046\n",
      "Epoch 13, Batch 627, Test Loss: 0.7033129930496216\n",
      "Epoch 13, Batch 628, Test Loss: 0.5115442872047424\n",
      "Epoch 13, Batch 629, Test Loss: 0.4159219563007355\n",
      "Epoch 13, Batch 630, Test Loss: 0.3592336177825928\n",
      "Epoch 13, Batch 631, Test Loss: 0.47267037630081177\n",
      "Epoch 13, Batch 632, Test Loss: 0.33405473828315735\n",
      "Epoch 13, Batch 633, Test Loss: 0.3605495095252991\n",
      "Epoch 13, Batch 634, Test Loss: 0.4391130805015564\n",
      "Epoch 13, Batch 635, Test Loss: 0.47201409935951233\n",
      "Epoch 13, Batch 636, Test Loss: 0.3799901306629181\n",
      "Epoch 13, Batch 637, Test Loss: 0.36353206634521484\n",
      "Epoch 13, Batch 638, Test Loss: 0.4387871325016022\n",
      "Epoch 13, Batch 639, Test Loss: 0.4853537082672119\n",
      "Epoch 13, Batch 640, Test Loss: 0.4374135732650757\n",
      "Epoch 13, Batch 641, Test Loss: 0.45006993412971497\n",
      "Epoch 13, Batch 642, Test Loss: 0.5592242479324341\n",
      "Epoch 13, Batch 643, Test Loss: 0.3728644847869873\n",
      "Epoch 13, Batch 644, Test Loss: 0.4359418749809265\n",
      "Epoch 13, Batch 645, Test Loss: 0.5585627555847168\n",
      "Epoch 13, Batch 646, Test Loss: 0.6094939708709717\n",
      "Epoch 13, Batch 647, Test Loss: 0.41716307401657104\n",
      "Epoch 13, Batch 648, Test Loss: 0.5390526652336121\n",
      "Epoch 13, Batch 649, Test Loss: 0.3613470792770386\n",
      "Epoch 13, Batch 650, Test Loss: 0.4796763062477112\n",
      "Epoch 13, Batch 651, Test Loss: 0.42655301094055176\n",
      "Epoch 13, Batch 652, Test Loss: 0.33395901322364807\n",
      "Epoch 13, Batch 653, Test Loss: 0.47537413239479065\n",
      "Epoch 13, Batch 654, Test Loss: 0.35821568965911865\n",
      "Epoch 13, Batch 655, Test Loss: 0.42782384157180786\n",
      "Epoch 13, Batch 656, Test Loss: 0.5195669531822205\n",
      "Epoch 13, Batch 657, Test Loss: 0.4870845377445221\n",
      "Epoch 13, Batch 658, Test Loss: 0.5049210786819458\n",
      "Epoch 13, Batch 659, Test Loss: 0.6745660901069641\n",
      "Epoch 13, Batch 660, Test Loss: 0.5962198972702026\n",
      "Epoch 13, Batch 661, Test Loss: 0.41953426599502563\n",
      "Epoch 13, Batch 662, Test Loss: 0.44076868891716003\n",
      "Epoch 13, Batch 663, Test Loss: 0.3919335603713989\n",
      "Epoch 13, Batch 664, Test Loss: 0.2281808853149414\n",
      "Epoch 13, Batch 665, Test Loss: 0.376635879278183\n",
      "Epoch 13, Batch 666, Test Loss: 0.5066677331924438\n",
      "Epoch 13, Batch 667, Test Loss: 0.38727280497550964\n",
      "Epoch 13, Batch 668, Test Loss: 0.4204961061477661\n",
      "Epoch 13, Batch 669, Test Loss: 0.47215479612350464\n",
      "Epoch 13, Batch 670, Test Loss: 0.808031439781189\n",
      "Epoch 13, Batch 671, Test Loss: 0.444599449634552\n",
      "Epoch 13, Batch 672, Test Loss: 0.4163888096809387\n",
      "Epoch 13, Batch 673, Test Loss: 0.4570322632789612\n",
      "Epoch 13, Batch 674, Test Loss: 0.5892677903175354\n",
      "Epoch 13, Batch 675, Test Loss: 0.48276686668395996\n",
      "Epoch 13, Batch 676, Test Loss: 0.39449891448020935\n",
      "Epoch 13, Batch 677, Test Loss: 0.5336799025535583\n",
      "Epoch 13, Batch 678, Test Loss: 0.6690388917922974\n",
      "Epoch 13, Batch 679, Test Loss: 0.5926530957221985\n",
      "Epoch 13, Batch 680, Test Loss: 0.4412273168563843\n",
      "Epoch 13, Batch 681, Test Loss: 0.38919928669929504\n",
      "Epoch 13, Batch 682, Test Loss: 0.5561230778694153\n",
      "Epoch 13, Batch 683, Test Loss: 0.47328001260757446\n",
      "Epoch 13, Batch 684, Test Loss: 0.39904606342315674\n",
      "Epoch 13, Batch 685, Test Loss: 0.5589176416397095\n",
      "Epoch 13, Batch 686, Test Loss: 0.37853753566741943\n",
      "Epoch 13, Batch 687, Test Loss: 0.5503677129745483\n",
      "Epoch 13, Batch 688, Test Loss: 0.31470417976379395\n",
      "Epoch 13, Batch 689, Test Loss: 0.481538861989975\n",
      "Epoch 13, Batch 690, Test Loss: 0.40632665157318115\n",
      "Epoch 13, Batch 691, Test Loss: 0.44460833072662354\n",
      "Epoch 13, Batch 692, Test Loss: 0.47088196873664856\n",
      "Epoch 13, Batch 693, Test Loss: 0.35191068053245544\n",
      "Epoch 13, Batch 694, Test Loss: 0.3283146023750305\n",
      "Epoch 13, Batch 695, Test Loss: 0.4500751197338104\n",
      "Epoch 13, Batch 696, Test Loss: 0.4726388454437256\n",
      "Epoch 13, Batch 697, Test Loss: 0.32673975825309753\n",
      "Epoch 13, Batch 698, Test Loss: 0.4035018980503082\n",
      "Epoch 13, Batch 699, Test Loss: 0.5261285305023193\n",
      "Epoch 13, Batch 700, Test Loss: 0.6154801845550537\n",
      "Epoch 13, Batch 701, Test Loss: 0.5230295658111572\n",
      "Epoch 13, Batch 702, Test Loss: 0.43246492743492126\n",
      "Epoch 13, Batch 703, Test Loss: 0.42645230889320374\n",
      "Epoch 13, Batch 704, Test Loss: 0.3562391698360443\n",
      "Epoch 13, Batch 705, Test Loss: 0.5096338987350464\n",
      "Epoch 13, Batch 706, Test Loss: 0.5037830471992493\n",
      "Epoch 13, Batch 707, Test Loss: 0.4418647289276123\n",
      "Epoch 13, Batch 708, Test Loss: 0.39051568508148193\n",
      "Epoch 13, Batch 709, Test Loss: 0.3831457197666168\n",
      "Epoch 13, Batch 710, Test Loss: 0.46749812364578247\n",
      "Epoch 13, Batch 711, Test Loss: 0.225847065448761\n",
      "Epoch 13, Batch 712, Test Loss: 0.24358966946601868\n",
      "Epoch 13, Batch 713, Test Loss: 0.7900418639183044\n",
      "Epoch 13, Batch 714, Test Loss: 0.5965850353240967\n",
      "Epoch 13, Batch 715, Test Loss: 0.5563152432441711\n",
      "Epoch 13, Batch 716, Test Loss: 0.4405727684497833\n",
      "Epoch 13, Batch 717, Test Loss: 0.2741239070892334\n",
      "Epoch 13, Batch 718, Test Loss: 0.45007938146591187\n",
      "Epoch 13, Batch 719, Test Loss: 0.49292829632759094\n",
      "Epoch 13, Batch 720, Test Loss: 0.4240230321884155\n",
      "Epoch 13, Batch 721, Test Loss: 0.505011796951294\n",
      "Epoch 13, Batch 722, Test Loss: 0.5385307669639587\n",
      "Epoch 13, Batch 723, Test Loss: 0.36967942118644714\n",
      "Epoch 13, Batch 724, Test Loss: 0.591503918170929\n",
      "Epoch 13, Batch 725, Test Loss: 0.34306955337524414\n",
      "Epoch 13, Batch 726, Test Loss: 0.37901315093040466\n",
      "Epoch 13, Batch 727, Test Loss: 0.6913672089576721\n",
      "Epoch 13, Batch 728, Test Loss: 0.4504053294658661\n",
      "Epoch 13, Batch 729, Test Loss: 0.5460689067840576\n",
      "Epoch 13, Batch 730, Test Loss: 0.5270604491233826\n",
      "Epoch 13, Batch 731, Test Loss: 0.335537850856781\n",
      "Epoch 13, Batch 732, Test Loss: 0.5995954871177673\n",
      "Epoch 13, Batch 733, Test Loss: 0.46127405762672424\n",
      "Epoch 13, Batch 734, Test Loss: 0.42209842801094055\n",
      "Epoch 13, Batch 735, Test Loss: 0.331511914730072\n",
      "Epoch 13, Batch 736, Test Loss: 0.6524529457092285\n",
      "Epoch 13, Batch 737, Test Loss: 0.43426012992858887\n",
      "Epoch 13, Batch 738, Test Loss: 0.6081493496894836\n",
      "Epoch 13, Batch 739, Test Loss: 0.46889692544937134\n",
      "Epoch 13, Batch 740, Test Loss: 0.35757017135620117\n",
      "Epoch 13, Batch 741, Test Loss: 0.4911699593067169\n",
      "Epoch 13, Batch 742, Test Loss: 0.5017600655555725\n",
      "Epoch 13, Batch 743, Test Loss: 0.6539504528045654\n",
      "Epoch 13, Batch 744, Test Loss: 0.39881187677383423\n",
      "Epoch 13, Batch 745, Test Loss: 0.40372174978256226\n",
      "Epoch 13, Batch 746, Test Loss: 0.6908073425292969\n",
      "Epoch 13, Batch 747, Test Loss: 0.5998308658599854\n",
      "Epoch 13, Batch 748, Test Loss: 0.4046095013618469\n",
      "Epoch 13, Batch 749, Test Loss: 0.45228591561317444\n",
      "Epoch 13, Batch 750, Test Loss: 0.5399930477142334\n",
      "Epoch 13, Batch 751, Test Loss: 0.42320123314857483\n",
      "Epoch 13, Batch 752, Test Loss: 0.4015349745750427\n",
      "Epoch 13, Batch 753, Test Loss: 0.4019012153148651\n",
      "Epoch 13, Batch 754, Test Loss: 0.4047427475452423\n",
      "Epoch 13, Batch 755, Test Loss: 0.3463174104690552\n",
      "Epoch 13, Batch 756, Test Loss: 0.3580300509929657\n",
      "Epoch 13, Batch 757, Test Loss: 0.4083082377910614\n",
      "Epoch 13, Batch 758, Test Loss: 0.3510383367538452\n",
      "Epoch 13, Batch 759, Test Loss: 0.575983464717865\n",
      "Epoch 13, Batch 760, Test Loss: 0.518760085105896\n",
      "Epoch 13, Batch 761, Test Loss: 0.5882765650749207\n",
      "Epoch 13, Batch 762, Test Loss: 0.4995412528514862\n",
      "Epoch 13, Batch 763, Test Loss: 0.6204149127006531\n",
      "Epoch 13, Batch 764, Test Loss: 0.41813352704048157\n",
      "Epoch 13, Batch 765, Test Loss: 0.45204463601112366\n",
      "Epoch 13, Batch 766, Test Loss: 0.6101463437080383\n",
      "Epoch 13, Batch 767, Test Loss: 0.4878334403038025\n",
      "Epoch 13, Batch 768, Test Loss: 0.47737571597099304\n",
      "Epoch 13, Batch 769, Test Loss: 0.9187450408935547\n",
      "Epoch 13, Batch 770, Test Loss: 0.4587675631046295\n",
      "Epoch 13, Batch 771, Test Loss: 0.25850367546081543\n",
      "Epoch 13, Batch 772, Test Loss: 0.6639304757118225\n",
      "Epoch 13, Batch 773, Test Loss: 0.8010109066963196\n",
      "Epoch 13, Batch 774, Test Loss: 0.2989393472671509\n",
      "Epoch 13, Batch 775, Test Loss: 0.42460164427757263\n",
      "Epoch 13, Batch 776, Test Loss: 0.44330546259880066\n",
      "Epoch 13, Batch 777, Test Loss: 0.520183801651001\n",
      "Epoch 13, Batch 778, Test Loss: 0.4210764169692993\n",
      "Epoch 13, Batch 779, Test Loss: 0.2760186791419983\n",
      "Epoch 13, Batch 780, Test Loss: 0.5530072450637817\n",
      "Epoch 13, Batch 781, Test Loss: 0.5671351552009583\n",
      "Epoch 13, Batch 782, Test Loss: 0.47207555174827576\n",
      "Epoch 13, Batch 783, Test Loss: 0.6244255900382996\n",
      "Epoch 13, Batch 784, Test Loss: 0.42793843150138855\n",
      "Epoch 13, Batch 785, Test Loss: 0.4211972951889038\n",
      "Epoch 13, Batch 786, Test Loss: 0.4691019654273987\n",
      "Epoch 13, Batch 787, Test Loss: 0.38583555817604065\n",
      "Epoch 13, Batch 788, Test Loss: 0.45218488574028015\n",
      "Epoch 13, Batch 789, Test Loss: 0.37150970101356506\n",
      "Epoch 13, Batch 790, Test Loss: 0.3911493122577667\n",
      "Epoch 13, Batch 791, Test Loss: 0.4951935410499573\n",
      "Epoch 13, Batch 792, Test Loss: 0.614370584487915\n",
      "Epoch 13, Batch 793, Test Loss: 0.585675835609436\n",
      "Epoch 13, Batch 794, Test Loss: 0.5700579285621643\n",
      "Epoch 13, Batch 795, Test Loss: 0.23690161108970642\n",
      "Epoch 13, Batch 796, Test Loss: 0.44155415892601013\n",
      "Epoch 13, Batch 797, Test Loss: 0.565313458442688\n",
      "Epoch 13, Batch 798, Test Loss: 0.6808218955993652\n",
      "Epoch 13, Batch 799, Test Loss: 0.43773895502090454\n",
      "Epoch 13, Batch 800, Test Loss: 0.6718071699142456\n",
      "Epoch 13, Batch 801, Test Loss: 0.3503231406211853\n",
      "Epoch 13, Batch 802, Test Loss: 0.4516308903694153\n",
      "Epoch 13, Batch 803, Test Loss: 0.27259406447410583\n",
      "Epoch 13, Batch 804, Test Loss: 0.547332763671875\n",
      "Epoch 13, Batch 805, Test Loss: 0.6885975003242493\n",
      "Epoch 13, Batch 806, Test Loss: 0.6013959646224976\n",
      "Epoch 13, Batch 807, Test Loss: 0.5618187785148621\n",
      "Epoch 13, Batch 808, Test Loss: 0.45179492235183716\n",
      "Epoch 13, Batch 809, Test Loss: 0.36539405584335327\n",
      "Epoch 13, Batch 810, Test Loss: 0.38088178634643555\n",
      "Epoch 13, Batch 811, Test Loss: 0.3544483780860901\n",
      "Epoch 13, Batch 812, Test Loss: 0.5929583311080933\n",
      "Epoch 13, Batch 813, Test Loss: 0.5729122161865234\n",
      "Epoch 13, Batch 814, Test Loss: 0.44539549946784973\n",
      "Epoch 13, Batch 815, Test Loss: 0.5125526189804077\n",
      "Epoch 13, Batch 816, Test Loss: 0.7806388139724731\n",
      "Epoch 13, Batch 817, Test Loss: 0.5262150764465332\n",
      "Epoch 13, Batch 818, Test Loss: 0.3968662917613983\n",
      "Epoch 13, Batch 819, Test Loss: 0.5123152732849121\n",
      "Epoch 13, Batch 820, Test Loss: 0.4681611657142639\n",
      "Epoch 13, Batch 821, Test Loss: 0.326805979013443\n",
      "Epoch 13, Batch 822, Test Loss: 0.4452695846557617\n",
      "Epoch 13, Batch 823, Test Loss: 0.5080277919769287\n",
      "Epoch 13, Batch 824, Test Loss: 0.5072199702262878\n",
      "Epoch 13, Batch 825, Test Loss: 0.3950066864490509\n",
      "Epoch 13, Batch 826, Test Loss: 0.37535688281059265\n",
      "Epoch 13, Batch 827, Test Loss: 0.34255772829055786\n",
      "Epoch 13, Batch 828, Test Loss: 0.5254654884338379\n",
      "Epoch 13, Batch 829, Test Loss: 0.3962608873844147\n",
      "Epoch 13, Batch 830, Test Loss: 0.4717608690261841\n",
      "Epoch 13, Batch 831, Test Loss: 0.4434923529624939\n",
      "Epoch 13, Batch 832, Test Loss: 0.3699549734592438\n",
      "Epoch 13, Batch 833, Test Loss: 0.5198526382446289\n",
      "Epoch 13, Batch 834, Test Loss: 0.5769979953765869\n",
      "Epoch 13, Batch 835, Test Loss: 0.398499459028244\n",
      "Epoch 13, Batch 836, Test Loss: 0.33292263746261597\n",
      "Epoch 13, Batch 837, Test Loss: 0.6574393510818481\n",
      "Epoch 13, Batch 838, Test Loss: 0.44945576786994934\n",
      "Epoch 13, Batch 839, Test Loss: 0.47346168756484985\n",
      "Epoch 13, Batch 840, Test Loss: 0.5187561511993408\n",
      "Epoch 13, Batch 841, Test Loss: 0.5976623296737671\n",
      "Epoch 13, Batch 842, Test Loss: 0.2712579369544983\n",
      "Epoch 13, Batch 843, Test Loss: 0.45131808519363403\n",
      "Epoch 13, Batch 844, Test Loss: 0.45708298683166504\n",
      "Epoch 13, Batch 845, Test Loss: 0.4582783579826355\n",
      "Epoch 13, Batch 846, Test Loss: 0.36719274520874023\n",
      "Epoch 13, Batch 847, Test Loss: 0.3984861373901367\n",
      "Epoch 13, Batch 848, Test Loss: 0.4444487690925598\n",
      "Epoch 13, Batch 849, Test Loss: 0.4580090641975403\n",
      "Epoch 13, Batch 850, Test Loss: 0.283682256937027\n",
      "Epoch 13, Batch 851, Test Loss: 0.5086193084716797\n",
      "Epoch 13, Batch 852, Test Loss: 0.2782736122608185\n",
      "Epoch 13, Batch 853, Test Loss: 0.5054617524147034\n",
      "Epoch 13, Batch 854, Test Loss: 0.6384803056716919\n",
      "Epoch 13, Batch 855, Test Loss: 0.516648530960083\n",
      "Epoch 13, Batch 856, Test Loss: 0.3721117079257965\n",
      "Epoch 13, Batch 857, Test Loss: 0.437567800283432\n",
      "Epoch 13, Batch 858, Test Loss: 0.5019499063491821\n",
      "Epoch 13, Batch 859, Test Loss: 0.3403301239013672\n",
      "Epoch 13, Batch 860, Test Loss: 0.41144734621047974\n",
      "Epoch 13, Batch 861, Test Loss: 0.3350018262863159\n",
      "Epoch 13, Batch 862, Test Loss: 0.30834317207336426\n",
      "Epoch 13, Batch 863, Test Loss: 0.341403603553772\n",
      "Epoch 13, Batch 864, Test Loss: 0.3943331837654114\n",
      "Epoch 13, Batch 865, Test Loss: 0.41289204359054565\n",
      "Epoch 13, Batch 866, Test Loss: 0.5760813355445862\n",
      "Epoch 13, Batch 867, Test Loss: 0.4416167140007019\n",
      "Epoch 13, Batch 868, Test Loss: 0.4858083128929138\n",
      "Epoch 13, Batch 869, Test Loss: 0.6522789001464844\n",
      "Epoch 13, Batch 870, Test Loss: 0.5163630843162537\n",
      "Epoch 13, Batch 871, Test Loss: 0.38769713044166565\n",
      "Epoch 13, Batch 872, Test Loss: 0.5079625844955444\n",
      "Epoch 13, Batch 873, Test Loss: 0.47963517904281616\n",
      "Epoch 13, Batch 874, Test Loss: 0.5067524909973145\n",
      "Epoch 13, Batch 875, Test Loss: 0.6221746206283569\n",
      "Epoch 13, Batch 876, Test Loss: 0.3954499065876007\n",
      "Epoch 13, Batch 877, Test Loss: 0.35716792941093445\n",
      "Epoch 13, Batch 878, Test Loss: 0.4056427776813507\n",
      "Epoch 13, Batch 879, Test Loss: 0.3607166111469269\n",
      "Epoch 13, Batch 880, Test Loss: 0.606309175491333\n",
      "Epoch 13, Batch 881, Test Loss: 0.592573344707489\n",
      "Epoch 13, Batch 882, Test Loss: 0.43463730812072754\n",
      "Epoch 13, Batch 883, Test Loss: 0.41077327728271484\n",
      "Epoch 13, Batch 884, Test Loss: 0.4104267954826355\n",
      "Epoch 13, Batch 885, Test Loss: 0.42825019359588623\n",
      "Epoch 13, Batch 886, Test Loss: 0.3816932439804077\n",
      "Epoch 13, Batch 887, Test Loss: 0.2982299327850342\n",
      "Epoch 13, Batch 888, Test Loss: 0.4478536546230316\n",
      "Epoch 13, Batch 889, Test Loss: 0.3911077082157135\n",
      "Epoch 13, Batch 890, Test Loss: 0.44181492924690247\n",
      "Epoch 13, Batch 891, Test Loss: 0.3416423499584198\n",
      "Epoch 13, Batch 892, Test Loss: 0.38438254594802856\n",
      "Epoch 13, Batch 893, Test Loss: 0.4580957293510437\n",
      "Epoch 13, Batch 894, Test Loss: 0.5406980514526367\n",
      "Epoch 13, Batch 895, Test Loss: 0.37110692262649536\n",
      "Epoch 13, Batch 896, Test Loss: 0.5591098666191101\n",
      "Epoch 13, Batch 897, Test Loss: 0.7384601831436157\n",
      "Epoch 13, Batch 898, Test Loss: 0.5997625589370728\n",
      "Epoch 13, Batch 899, Test Loss: 0.5777377486228943\n",
      "Epoch 13, Batch 900, Test Loss: 0.6134136319160461\n",
      "Epoch 13, Batch 901, Test Loss: 0.34144383668899536\n",
      "Epoch 13, Batch 902, Test Loss: 0.4828251600265503\n",
      "Epoch 13, Batch 903, Test Loss: 0.504047691822052\n",
      "Epoch 13, Batch 904, Test Loss: 0.549199104309082\n",
      "Epoch 13, Batch 905, Test Loss: 0.37029752135276794\n",
      "Epoch 13, Batch 906, Test Loss: 0.4006677567958832\n",
      "Epoch 13, Batch 907, Test Loss: 0.5620014667510986\n",
      "Epoch 13, Batch 908, Test Loss: 0.5376491546630859\n",
      "Epoch 13, Batch 909, Test Loss: 0.3528996706008911\n",
      "Epoch 13, Batch 910, Test Loss: 0.6084637641906738\n",
      "Epoch 13, Batch 911, Test Loss: 0.36173585057258606\n",
      "Epoch 13, Batch 912, Test Loss: 0.40721702575683594\n",
      "Epoch 13, Batch 913, Test Loss: 0.42563575506210327\n",
      "Epoch 13, Batch 914, Test Loss: 0.435416042804718\n",
      "Epoch 13, Batch 915, Test Loss: 0.5497572422027588\n",
      "Epoch 13, Batch 916, Test Loss: 0.7851397395133972\n",
      "Epoch 13, Batch 917, Test Loss: 0.43672946095466614\n",
      "Epoch 13, Batch 918, Test Loss: 0.3400301933288574\n",
      "Epoch 13, Batch 919, Test Loss: 0.5886496901512146\n",
      "Epoch 13, Batch 920, Test Loss: 0.3757816553115845\n",
      "Epoch 13, Batch 921, Test Loss: 0.41789236664772034\n",
      "Epoch 13, Batch 922, Test Loss: 0.68210369348526\n",
      "Epoch 13, Batch 923, Test Loss: 0.46912431716918945\n",
      "Epoch 13, Batch 924, Test Loss: 0.49684152007102966\n",
      "Epoch 13, Batch 925, Test Loss: 0.5310378074645996\n",
      "Epoch 13, Batch 926, Test Loss: 0.3728232979774475\n",
      "Epoch 13, Batch 927, Test Loss: 0.4479139447212219\n",
      "Epoch 13, Batch 928, Test Loss: 0.3147396147251129\n",
      "Epoch 13, Batch 929, Test Loss: 0.5493384599685669\n",
      "Epoch 13, Batch 930, Test Loss: 0.5987222790718079\n",
      "Epoch 13, Batch 931, Test Loss: 0.4485204517841339\n",
      "Epoch 13, Batch 932, Test Loss: 0.320472776889801\n",
      "Epoch 13, Batch 933, Test Loss: 0.37824511528015137\n",
      "Epoch 13, Batch 934, Test Loss: 0.33316749334335327\n",
      "Epoch 13, Batch 935, Test Loss: 0.48118746280670166\n",
      "Epoch 13, Batch 936, Test Loss: 0.6241266131401062\n",
      "Epoch 13, Batch 937, Test Loss: 0.541683554649353\n",
      "Epoch 13, Batch 938, Test Loss: 0.35331541299819946\n",
      "Accuracy of Test set: 0.83735\n",
      "Epoch 14, Batch 1, Loss: 0.41973161697387695\n",
      "Epoch 14, Batch 2, Loss: 0.4880343973636627\n",
      "Epoch 14, Batch 3, Loss: 0.5804807543754578\n",
      "Epoch 14, Batch 4, Loss: 0.4261929392814636\n",
      "Epoch 14, Batch 5, Loss: 0.42275404930114746\n",
      "Epoch 14, Batch 6, Loss: 0.4323943555355072\n",
      "Epoch 14, Batch 7, Loss: 0.4194609522819519\n",
      "Epoch 14, Batch 8, Loss: 0.45717567205429077\n",
      "Epoch 14, Batch 9, Loss: 0.4061378538608551\n",
      "Epoch 14, Batch 10, Loss: 0.43232592940330505\n",
      "Epoch 14, Batch 11, Loss: 0.5704955458641052\n",
      "Epoch 14, Batch 12, Loss: 0.47575369477272034\n",
      "Epoch 14, Batch 13, Loss: 0.560749351978302\n",
      "Epoch 14, Batch 14, Loss: 0.5686256885528564\n",
      "Epoch 14, Batch 15, Loss: 0.5469064712524414\n",
      "Epoch 14, Batch 16, Loss: 0.3362851142883301\n",
      "Epoch 14, Batch 17, Loss: 0.3975169360637665\n",
      "Epoch 14, Batch 18, Loss: 0.6623548269271851\n",
      "Epoch 14, Batch 19, Loss: 0.35340189933776855\n",
      "Epoch 14, Batch 20, Loss: 0.5047608613967896\n",
      "Epoch 14, Batch 21, Loss: 0.4947296977043152\n",
      "Epoch 14, Batch 22, Loss: 0.29852673411369324\n",
      "Epoch 14, Batch 23, Loss: 0.4756656885147095\n",
      "Epoch 14, Batch 24, Loss: 0.5140151381492615\n",
      "Epoch 14, Batch 25, Loss: 0.5022611618041992\n",
      "Epoch 14, Batch 26, Loss: 0.38013955950737\n",
      "Epoch 14, Batch 27, Loss: 0.5714176893234253\n",
      "Epoch 14, Batch 28, Loss: 0.5072716474533081\n",
      "Epoch 14, Batch 29, Loss: 0.4974585771560669\n",
      "Epoch 14, Batch 30, Loss: 0.4842458665370941\n",
      "Epoch 14, Batch 31, Loss: 0.5206464529037476\n",
      "Epoch 14, Batch 32, Loss: 0.5448830127716064\n",
      "Epoch 14, Batch 33, Loss: 0.3369898796081543\n",
      "Epoch 14, Batch 34, Loss: 0.4008473753929138\n",
      "Epoch 14, Batch 35, Loss: 0.38136887550354004\n",
      "Epoch 14, Batch 36, Loss: 0.46675774455070496\n",
      "Epoch 14, Batch 37, Loss: 0.7074450850486755\n",
      "Epoch 14, Batch 38, Loss: 0.4707139730453491\n",
      "Epoch 14, Batch 39, Loss: 0.4570341110229492\n",
      "Epoch 14, Batch 40, Loss: 0.4813711643218994\n",
      "Epoch 14, Batch 41, Loss: 0.4516902267932892\n",
      "Epoch 14, Batch 42, Loss: 0.5383186936378479\n",
      "Epoch 14, Batch 43, Loss: 0.7080938816070557\n",
      "Epoch 14, Batch 44, Loss: 0.56210857629776\n",
      "Epoch 14, Batch 45, Loss: 0.5896226763725281\n",
      "Epoch 14, Batch 46, Loss: 0.3440498411655426\n",
      "Epoch 14, Batch 47, Loss: 0.5111703276634216\n",
      "Epoch 14, Batch 48, Loss: 0.4256576895713806\n",
      "Epoch 14, Batch 49, Loss: 0.59538733959198\n",
      "Epoch 14, Batch 50, Loss: 0.40571242570877075\n",
      "Epoch 14, Batch 51, Loss: 0.4944668710231781\n",
      "Epoch 14, Batch 52, Loss: 0.49976301193237305\n",
      "Epoch 14, Batch 53, Loss: 0.5338396430015564\n",
      "Epoch 14, Batch 54, Loss: 0.30590763688087463\n",
      "Epoch 14, Batch 55, Loss: 0.49676811695098877\n",
      "Epoch 14, Batch 56, Loss: 0.2906724512577057\n",
      "Epoch 14, Batch 57, Loss: 0.42869165539741516\n",
      "Epoch 14, Batch 58, Loss: 0.47146010398864746\n",
      "Epoch 14, Batch 59, Loss: 0.5206083059310913\n",
      "Epoch 14, Batch 60, Loss: 0.46093037724494934\n",
      "Epoch 14, Batch 61, Loss: 0.42051368951797485\n",
      "Epoch 14, Batch 62, Loss: 0.3521537184715271\n",
      "Epoch 14, Batch 63, Loss: 0.4432986378669739\n",
      "Epoch 14, Batch 64, Loss: 0.44729727506637573\n",
      "Epoch 14, Batch 65, Loss: 0.47227129340171814\n",
      "Epoch 14, Batch 66, Loss: 0.5195154547691345\n",
      "Epoch 14, Batch 67, Loss: 0.5199965834617615\n",
      "Epoch 14, Batch 68, Loss: 0.39241355657577515\n",
      "Epoch 14, Batch 69, Loss: 0.5988562107086182\n",
      "Epoch 14, Batch 70, Loss: 0.6216439008712769\n",
      "Epoch 14, Batch 71, Loss: 0.39426907896995544\n",
      "Epoch 14, Batch 72, Loss: 0.5536148548126221\n",
      "Epoch 14, Batch 73, Loss: 0.3799038231372833\n",
      "Epoch 14, Batch 74, Loss: 0.5423340201377869\n",
      "Epoch 14, Batch 75, Loss: 0.5992746949195862\n",
      "Epoch 14, Batch 76, Loss: 0.3562893867492676\n",
      "Epoch 14, Batch 77, Loss: 0.432574987411499\n",
      "Epoch 14, Batch 78, Loss: 0.42694905400276184\n",
      "Epoch 14, Batch 79, Loss: 0.4613107442855835\n",
      "Epoch 14, Batch 80, Loss: 0.5901874303817749\n",
      "Epoch 14, Batch 81, Loss: 0.591407060623169\n",
      "Epoch 14, Batch 82, Loss: 0.46843647956848145\n",
      "Epoch 14, Batch 83, Loss: 0.48177069425582886\n",
      "Epoch 14, Batch 84, Loss: 0.7376207113265991\n",
      "Epoch 14, Batch 85, Loss: 0.5785725116729736\n",
      "Epoch 14, Batch 86, Loss: 0.37489813566207886\n",
      "Epoch 14, Batch 87, Loss: 0.4318075180053711\n",
      "Epoch 14, Batch 88, Loss: 0.395400732755661\n",
      "Epoch 14, Batch 89, Loss: 0.408368319272995\n",
      "Epoch 14, Batch 90, Loss: 0.36021846532821655\n",
      "Epoch 14, Batch 91, Loss: 0.48568254709243774\n",
      "Epoch 14, Batch 92, Loss: 0.526267945766449\n",
      "Epoch 14, Batch 93, Loss: 0.7872069478034973\n",
      "Epoch 14, Batch 94, Loss: 0.4632371664047241\n",
      "Epoch 14, Batch 95, Loss: 0.5127406120300293\n",
      "Epoch 14, Batch 96, Loss: 0.3712611198425293\n",
      "Epoch 14, Batch 97, Loss: 0.5727652907371521\n",
      "Epoch 14, Batch 98, Loss: 0.42684608697891235\n",
      "Epoch 14, Batch 99, Loss: 0.48705393075942993\n",
      "Epoch 14, Batch 100, Loss: 0.4686908721923828\n",
      "Epoch 14, Batch 101, Loss: 0.46053051948547363\n",
      "Epoch 14, Batch 102, Loss: 0.4935588836669922\n",
      "Epoch 14, Batch 103, Loss: 0.48843228816986084\n",
      "Epoch 14, Batch 104, Loss: 0.507349967956543\n",
      "Epoch 14, Batch 105, Loss: 0.5663168430328369\n",
      "Epoch 14, Batch 106, Loss: 0.5271963477134705\n",
      "Epoch 14, Batch 107, Loss: 0.4839140474796295\n",
      "Epoch 14, Batch 108, Loss: 0.5173803567886353\n",
      "Epoch 14, Batch 109, Loss: 0.41677990555763245\n",
      "Epoch 14, Batch 110, Loss: 0.388322114944458\n",
      "Epoch 14, Batch 111, Loss: 0.6505031585693359\n",
      "Epoch 14, Batch 112, Loss: 0.6025817394256592\n",
      "Epoch 14, Batch 113, Loss: 0.39080843329429626\n",
      "Epoch 14, Batch 114, Loss: 0.6435636878013611\n",
      "Epoch 14, Batch 115, Loss: 0.6499492526054382\n",
      "Epoch 14, Batch 116, Loss: 0.3040541112422943\n",
      "Epoch 14, Batch 117, Loss: 0.4358155429363251\n",
      "Epoch 14, Batch 118, Loss: 0.6271905303001404\n",
      "Epoch 14, Batch 119, Loss: 0.5336095094680786\n",
      "Epoch 14, Batch 120, Loss: 0.3298993408679962\n",
      "Epoch 14, Batch 121, Loss: 0.6121104955673218\n",
      "Epoch 14, Batch 122, Loss: 0.5057780146598816\n",
      "Epoch 14, Batch 123, Loss: 0.33612608909606934\n",
      "Epoch 14, Batch 124, Loss: 0.4029996991157532\n",
      "Epoch 14, Batch 125, Loss: 0.6268290877342224\n",
      "Epoch 14, Batch 126, Loss: 0.6223223209381104\n",
      "Epoch 14, Batch 127, Loss: 0.5970681309700012\n",
      "Epoch 14, Batch 128, Loss: 0.5820842385292053\n",
      "Epoch 14, Batch 129, Loss: 0.3942511975765228\n",
      "Epoch 14, Batch 130, Loss: 0.5232406854629517\n",
      "Epoch 14, Batch 131, Loss: 0.5517358183860779\n",
      "Epoch 14, Batch 132, Loss: 0.5188698768615723\n",
      "Epoch 14, Batch 133, Loss: 0.5023008584976196\n",
      "Epoch 14, Batch 134, Loss: 0.5142824649810791\n",
      "Epoch 14, Batch 135, Loss: 0.32657575607299805\n",
      "Epoch 14, Batch 136, Loss: 0.4175254702568054\n",
      "Epoch 14, Batch 137, Loss: 0.4796849489212036\n",
      "Epoch 14, Batch 138, Loss: 0.2581286132335663\n",
      "Epoch 14, Batch 139, Loss: 0.46158817410469055\n",
      "Epoch 14, Batch 140, Loss: 0.4935078024864197\n",
      "Epoch 14, Batch 141, Loss: 0.34633010625839233\n",
      "Epoch 14, Batch 142, Loss: 0.5678535103797913\n",
      "Epoch 14, Batch 143, Loss: 0.33161047101020813\n",
      "Epoch 14, Batch 144, Loss: 0.5478466749191284\n",
      "Epoch 14, Batch 145, Loss: 0.39915555715560913\n",
      "Epoch 14, Batch 146, Loss: 0.5284891128540039\n",
      "Epoch 14, Batch 147, Loss: 0.32549890875816345\n",
      "Epoch 14, Batch 148, Loss: 0.4494401514530182\n",
      "Epoch 14, Batch 149, Loss: 0.5378907918930054\n",
      "Epoch 14, Batch 150, Loss: 0.498918354511261\n",
      "Epoch 14, Batch 151, Loss: 0.4613117575645447\n",
      "Epoch 14, Batch 152, Loss: 0.40531253814697266\n",
      "Epoch 14, Batch 153, Loss: 0.4753705561161041\n",
      "Epoch 14, Batch 154, Loss: 0.582503080368042\n",
      "Epoch 14, Batch 155, Loss: 0.37689608335494995\n",
      "Epoch 14, Batch 156, Loss: 0.3507469594478607\n",
      "Epoch 14, Batch 157, Loss: 0.49576082825660706\n",
      "Epoch 14, Batch 158, Loss: 0.3486987352371216\n",
      "Epoch 14, Batch 159, Loss: 0.34968164563179016\n",
      "Epoch 14, Batch 160, Loss: 0.6127111911773682\n",
      "Epoch 14, Batch 161, Loss: 0.3364570438861847\n",
      "Epoch 14, Batch 162, Loss: 0.4232109785079956\n",
      "Epoch 14, Batch 163, Loss: 0.3673457205295563\n",
      "Epoch 14, Batch 164, Loss: 0.6488215327262878\n",
      "Epoch 14, Batch 165, Loss: 0.3472822904586792\n",
      "Epoch 14, Batch 166, Loss: 0.5659419894218445\n",
      "Epoch 14, Batch 167, Loss: 0.29810404777526855\n",
      "Epoch 14, Batch 168, Loss: 0.6412001848220825\n",
      "Epoch 14, Batch 169, Loss: 0.5414033532142639\n",
      "Epoch 14, Batch 170, Loss: 0.24842055141925812\n",
      "Epoch 14, Batch 171, Loss: 0.5128602981567383\n",
      "Epoch 14, Batch 172, Loss: 0.3146863579750061\n",
      "Epoch 14, Batch 173, Loss: 0.5689021348953247\n",
      "Epoch 14, Batch 174, Loss: 0.6166250705718994\n",
      "Epoch 14, Batch 175, Loss: 0.6459991931915283\n",
      "Epoch 14, Batch 176, Loss: 0.5553111433982849\n",
      "Epoch 14, Batch 177, Loss: 0.38998228311538696\n",
      "Epoch 14, Batch 178, Loss: 0.5062833428382874\n",
      "Epoch 14, Batch 179, Loss: 0.6610255241394043\n",
      "Epoch 14, Batch 180, Loss: 0.41652894020080566\n",
      "Epoch 14, Batch 181, Loss: 0.417659193277359\n",
      "Epoch 14, Batch 182, Loss: 0.45437678694725037\n",
      "Epoch 14, Batch 183, Loss: 0.522620677947998\n",
      "Epoch 14, Batch 184, Loss: 0.3458980917930603\n",
      "Epoch 14, Batch 185, Loss: 0.4652566909790039\n",
      "Epoch 14, Batch 186, Loss: 0.35862237215042114\n",
      "Epoch 14, Batch 187, Loss: 0.36653420329093933\n",
      "Epoch 14, Batch 188, Loss: 0.5005692839622498\n",
      "Epoch 14, Batch 189, Loss: 0.523666262626648\n",
      "Epoch 14, Batch 190, Loss: 0.5034731030464172\n",
      "Epoch 14, Batch 191, Loss: 0.6127647161483765\n",
      "Epoch 14, Batch 192, Loss: 0.48642462491989136\n",
      "Epoch 14, Batch 193, Loss: 0.588589072227478\n",
      "Epoch 14, Batch 194, Loss: 0.588162899017334\n",
      "Epoch 14, Batch 195, Loss: 0.5055785179138184\n",
      "Epoch 14, Batch 196, Loss: 0.5442171096801758\n",
      "Epoch 14, Batch 197, Loss: 0.5330612659454346\n",
      "Epoch 14, Batch 198, Loss: 0.33112040162086487\n",
      "Epoch 14, Batch 199, Loss: 0.30606722831726074\n",
      "Epoch 14, Batch 200, Loss: 0.4524293839931488\n",
      "Epoch 14, Batch 201, Loss: 0.4457798898220062\n",
      "Epoch 14, Batch 202, Loss: 0.5394577980041504\n",
      "Epoch 14, Batch 203, Loss: 0.5837833285331726\n",
      "Epoch 14, Batch 204, Loss: 0.6950536370277405\n",
      "Epoch 14, Batch 205, Loss: 0.6628539562225342\n",
      "Epoch 14, Batch 206, Loss: 0.5095927715301514\n",
      "Epoch 14, Batch 207, Loss: 0.3857943117618561\n",
      "Epoch 14, Batch 208, Loss: 0.4013493061065674\n",
      "Epoch 14, Batch 209, Loss: 0.6519085168838501\n",
      "Epoch 14, Batch 210, Loss: 0.4982355833053589\n",
      "Epoch 14, Batch 211, Loss: 0.5252396464347839\n",
      "Epoch 14, Batch 212, Loss: 0.5394623875617981\n",
      "Epoch 14, Batch 213, Loss: 0.677593469619751\n",
      "Epoch 14, Batch 214, Loss: 0.4152640402317047\n",
      "Epoch 14, Batch 215, Loss: 0.6425648331642151\n",
      "Epoch 14, Batch 216, Loss: 0.3940136730670929\n",
      "Epoch 14, Batch 217, Loss: 0.48260441422462463\n",
      "Epoch 14, Batch 218, Loss: 0.6091796159744263\n",
      "Epoch 14, Batch 219, Loss: 0.5507277250289917\n",
      "Epoch 14, Batch 220, Loss: 0.45352891087532043\n",
      "Epoch 14, Batch 221, Loss: 0.4581741690635681\n",
      "Epoch 14, Batch 222, Loss: 0.4440249502658844\n",
      "Epoch 14, Batch 223, Loss: 0.45603758096694946\n",
      "Epoch 14, Batch 224, Loss: 0.3872319459915161\n",
      "Epoch 14, Batch 225, Loss: 0.555677592754364\n",
      "Epoch 14, Batch 226, Loss: 0.5179489850997925\n",
      "Epoch 14, Batch 227, Loss: 0.4782812297344208\n",
      "Epoch 14, Batch 228, Loss: 0.3378969728946686\n",
      "Epoch 14, Batch 229, Loss: 0.47123265266418457\n",
      "Epoch 14, Batch 230, Loss: 0.6018069386482239\n",
      "Epoch 14, Batch 231, Loss: 0.4101355969905853\n",
      "Epoch 14, Batch 232, Loss: 0.398147851228714\n",
      "Epoch 14, Batch 233, Loss: 0.47964605689048767\n",
      "Epoch 14, Batch 234, Loss: 0.6880897283554077\n",
      "Epoch 14, Batch 235, Loss: 0.3344627320766449\n",
      "Epoch 14, Batch 236, Loss: 0.4656638205051422\n",
      "Epoch 14, Batch 237, Loss: 0.6229409575462341\n",
      "Epoch 14, Batch 238, Loss: 0.5530221462249756\n",
      "Epoch 14, Batch 239, Loss: 0.5123488903045654\n",
      "Epoch 14, Batch 240, Loss: 0.4666822552680969\n",
      "Epoch 14, Batch 241, Loss: 0.538999080657959\n",
      "Epoch 14, Batch 242, Loss: 0.26411592960357666\n",
      "Epoch 14, Batch 243, Loss: 0.3594399392604828\n",
      "Epoch 14, Batch 244, Loss: 0.43043994903564453\n",
      "Epoch 14, Batch 245, Loss: 0.4409099519252777\n",
      "Epoch 14, Batch 246, Loss: 0.3386143445968628\n",
      "Epoch 14, Batch 247, Loss: 0.5884720087051392\n",
      "Epoch 14, Batch 248, Loss: 0.5033599734306335\n",
      "Epoch 14, Batch 249, Loss: 0.4932531416416168\n",
      "Epoch 14, Batch 250, Loss: 0.3822973966598511\n",
      "Epoch 14, Batch 251, Loss: 0.37176594138145447\n",
      "Epoch 14, Batch 252, Loss: 0.5884220004081726\n",
      "Epoch 14, Batch 253, Loss: 0.38288414478302\n",
      "Epoch 14, Batch 254, Loss: 0.29133665561676025\n",
      "Epoch 14, Batch 255, Loss: 0.40054866671562195\n",
      "Epoch 14, Batch 256, Loss: 0.41343069076538086\n",
      "Epoch 14, Batch 257, Loss: 0.41661325097084045\n",
      "Epoch 14, Batch 258, Loss: 0.469727486371994\n",
      "Epoch 14, Batch 259, Loss: 0.43033644556999207\n",
      "Epoch 14, Batch 260, Loss: 0.3105021119117737\n",
      "Epoch 14, Batch 261, Loss: 0.560188889503479\n",
      "Epoch 14, Batch 262, Loss: 0.5512357950210571\n",
      "Epoch 14, Batch 263, Loss: 0.4868396520614624\n",
      "Epoch 14, Batch 264, Loss: 0.34007152915000916\n",
      "Epoch 14, Batch 265, Loss: 0.7203736901283264\n",
      "Epoch 14, Batch 266, Loss: 0.6236701011657715\n",
      "Epoch 14, Batch 267, Loss: 0.5679090619087219\n",
      "Epoch 14, Batch 268, Loss: 0.4262714982032776\n",
      "Epoch 14, Batch 269, Loss: 0.4576359987258911\n",
      "Epoch 14, Batch 270, Loss: 0.7240606546401978\n",
      "Epoch 14, Batch 271, Loss: 0.45550477504730225\n",
      "Epoch 14, Batch 272, Loss: 0.45560452342033386\n",
      "Epoch 14, Batch 273, Loss: 0.381022185087204\n",
      "Epoch 14, Batch 274, Loss: 0.6525004506111145\n",
      "Epoch 14, Batch 275, Loss: 0.42191022634506226\n",
      "Epoch 14, Batch 276, Loss: 0.4517838954925537\n",
      "Epoch 14, Batch 277, Loss: 0.6037546396255493\n",
      "Epoch 14, Batch 278, Loss: 0.4997860789299011\n",
      "Epoch 14, Batch 279, Loss: 0.4075089693069458\n",
      "Epoch 14, Batch 280, Loss: 0.5430797934532166\n",
      "Epoch 14, Batch 281, Loss: 0.4286443591117859\n",
      "Epoch 14, Batch 282, Loss: 0.29290497303009033\n",
      "Epoch 14, Batch 283, Loss: 0.4737323820590973\n",
      "Epoch 14, Batch 284, Loss: 0.43495509028434753\n",
      "Epoch 14, Batch 285, Loss: 0.5148547887802124\n",
      "Epoch 14, Batch 286, Loss: 0.30708593130111694\n",
      "Epoch 14, Batch 287, Loss: 0.521327018737793\n",
      "Epoch 14, Batch 288, Loss: 0.42558160424232483\n",
      "Epoch 14, Batch 289, Loss: 0.47224780917167664\n",
      "Epoch 14, Batch 290, Loss: 0.3750563859939575\n",
      "Epoch 14, Batch 291, Loss: 0.527290940284729\n",
      "Epoch 14, Batch 292, Loss: 0.36823341250419617\n",
      "Epoch 14, Batch 293, Loss: 0.560569167137146\n",
      "Epoch 14, Batch 294, Loss: 0.5490610003471375\n",
      "Epoch 14, Batch 295, Loss: 0.2850608229637146\n",
      "Epoch 14, Batch 296, Loss: 0.5373036861419678\n",
      "Epoch 14, Batch 297, Loss: 0.5827096700668335\n",
      "Epoch 14, Batch 298, Loss: 0.30526912212371826\n",
      "Epoch 14, Batch 299, Loss: 0.32901573181152344\n",
      "Epoch 14, Batch 300, Loss: 0.5510705709457397\n",
      "Epoch 14, Batch 301, Loss: 0.515222430229187\n",
      "Epoch 14, Batch 302, Loss: 0.496650755405426\n",
      "Epoch 14, Batch 303, Loss: 0.27690157294273376\n",
      "Epoch 14, Batch 304, Loss: 0.6602858304977417\n",
      "Epoch 14, Batch 305, Loss: 0.5537753701210022\n",
      "Epoch 14, Batch 306, Loss: 0.5778167843818665\n",
      "Epoch 14, Batch 307, Loss: 0.40338537096977234\n",
      "Epoch 14, Batch 308, Loss: 0.46677157282829285\n",
      "Epoch 14, Batch 309, Loss: 0.5342246294021606\n",
      "Epoch 14, Batch 310, Loss: 0.5770264863967896\n",
      "Epoch 14, Batch 311, Loss: 0.35415318608283997\n",
      "Epoch 14, Batch 312, Loss: 0.4205789566040039\n",
      "Epoch 14, Batch 313, Loss: 0.5232251286506653\n",
      "Epoch 14, Batch 314, Loss: 0.4216380715370178\n",
      "Epoch 14, Batch 315, Loss: 0.6955024600028992\n",
      "Epoch 14, Batch 316, Loss: 0.44531553983688354\n",
      "Epoch 14, Batch 317, Loss: 0.5189176797866821\n",
      "Epoch 14, Batch 318, Loss: 0.5049564838409424\n",
      "Epoch 14, Batch 319, Loss: 0.6835742592811584\n",
      "Epoch 14, Batch 320, Loss: 0.5594730377197266\n",
      "Epoch 14, Batch 321, Loss: 0.3992363214492798\n",
      "Epoch 14, Batch 322, Loss: 0.4588920474052429\n",
      "Epoch 14, Batch 323, Loss: 0.3864891827106476\n",
      "Epoch 14, Batch 324, Loss: 0.5023248195648193\n",
      "Epoch 14, Batch 325, Loss: 0.48474812507629395\n",
      "Epoch 14, Batch 326, Loss: 0.2992890775203705\n",
      "Epoch 14, Batch 327, Loss: 0.421440988779068\n",
      "Epoch 14, Batch 328, Loss: 0.43000924587249756\n",
      "Epoch 14, Batch 329, Loss: 0.5393368601799011\n",
      "Epoch 14, Batch 330, Loss: 0.3416050672531128\n",
      "Epoch 14, Batch 331, Loss: 0.3214256465435028\n",
      "Epoch 14, Batch 332, Loss: 0.35027003288269043\n",
      "Epoch 14, Batch 333, Loss: 0.3375358581542969\n",
      "Epoch 14, Batch 334, Loss: 0.42755547165870667\n",
      "Epoch 14, Batch 335, Loss: 0.5026339292526245\n",
      "Epoch 14, Batch 336, Loss: 0.4020612835884094\n",
      "Epoch 14, Batch 337, Loss: 0.3698062300682068\n",
      "Epoch 14, Batch 338, Loss: 0.2911109924316406\n",
      "Epoch 14, Batch 339, Loss: 0.46109411120414734\n",
      "Epoch 14, Batch 340, Loss: 0.4426177740097046\n",
      "Epoch 14, Batch 341, Loss: 0.45134711265563965\n",
      "Epoch 14, Batch 342, Loss: 0.6592212319374084\n",
      "Epoch 14, Batch 343, Loss: 0.5692477822303772\n",
      "Epoch 14, Batch 344, Loss: 0.5689865946769714\n",
      "Epoch 14, Batch 345, Loss: 0.36756181716918945\n",
      "Epoch 14, Batch 346, Loss: 0.40005379915237427\n",
      "Epoch 14, Batch 347, Loss: 0.3795431852340698\n",
      "Epoch 14, Batch 348, Loss: 0.5238311886787415\n",
      "Epoch 14, Batch 349, Loss: 0.4537220895290375\n",
      "Epoch 14, Batch 350, Loss: 0.38409096002578735\n",
      "Epoch 14, Batch 351, Loss: 0.5087186694145203\n",
      "Epoch 14, Batch 352, Loss: 0.4475584924221039\n",
      "Epoch 14, Batch 353, Loss: 0.47221219539642334\n",
      "Epoch 14, Batch 354, Loss: 0.47156375646591187\n",
      "Epoch 14, Batch 355, Loss: 0.3799385130405426\n",
      "Epoch 14, Batch 356, Loss: 0.3068000078201294\n",
      "Epoch 14, Batch 357, Loss: 0.34684622287750244\n",
      "Epoch 14, Batch 358, Loss: 0.336143434047699\n",
      "Epoch 14, Batch 359, Loss: 0.5834004878997803\n",
      "Epoch 14, Batch 360, Loss: 0.5363880395889282\n",
      "Epoch 14, Batch 361, Loss: 0.5760659575462341\n",
      "Epoch 14, Batch 362, Loss: 0.5376343727111816\n",
      "Epoch 14, Batch 363, Loss: 0.434547483921051\n",
      "Epoch 14, Batch 364, Loss: 0.4129103422164917\n",
      "Epoch 14, Batch 365, Loss: 0.50294029712677\n",
      "Epoch 14, Batch 366, Loss: 0.4481092393398285\n",
      "Epoch 14, Batch 367, Loss: 0.6080121397972107\n",
      "Epoch 14, Batch 368, Loss: 0.3648388087749481\n",
      "Epoch 14, Batch 369, Loss: 0.42584681510925293\n",
      "Epoch 14, Batch 370, Loss: 0.4775497019290924\n",
      "Epoch 14, Batch 371, Loss: 0.4396548569202423\n",
      "Epoch 14, Batch 372, Loss: 0.601647675037384\n",
      "Epoch 14, Batch 373, Loss: 0.27220791578292847\n",
      "Epoch 14, Batch 374, Loss: 0.32626113295555115\n",
      "Epoch 14, Batch 375, Loss: 0.5039336085319519\n",
      "Epoch 14, Batch 376, Loss: 0.30477410554885864\n",
      "Epoch 14, Batch 377, Loss: 0.49950844049453735\n",
      "Epoch 14, Batch 378, Loss: 0.48835238814353943\n",
      "Epoch 14, Batch 379, Loss: 0.4552127420902252\n",
      "Epoch 14, Batch 380, Loss: 0.3725515604019165\n",
      "Epoch 14, Batch 381, Loss: 0.6485807299613953\n",
      "Epoch 14, Batch 382, Loss: 0.4148978590965271\n",
      "Epoch 14, Batch 383, Loss: 0.38558000326156616\n",
      "Epoch 14, Batch 384, Loss: 0.5211724638938904\n",
      "Epoch 14, Batch 385, Loss: 0.46714672446250916\n",
      "Epoch 14, Batch 386, Loss: 0.5106632113456726\n",
      "Epoch 14, Batch 387, Loss: 0.4188466966152191\n",
      "Epoch 14, Batch 388, Loss: 0.2541540563106537\n",
      "Epoch 14, Batch 389, Loss: 0.4791543483734131\n",
      "Epoch 14, Batch 390, Loss: 0.4277380704879761\n",
      "Epoch 14, Batch 391, Loss: 0.6018074750900269\n",
      "Epoch 14, Batch 392, Loss: 0.35575416684150696\n",
      "Epoch 14, Batch 393, Loss: 0.30628079175949097\n",
      "Epoch 14, Batch 394, Loss: 0.49591970443725586\n",
      "Epoch 14, Batch 395, Loss: 0.5081956386566162\n",
      "Epoch 14, Batch 396, Loss: 0.47118526697158813\n",
      "Epoch 14, Batch 397, Loss: 0.28732022643089294\n",
      "Epoch 14, Batch 398, Loss: 0.5892691612243652\n",
      "Epoch 14, Batch 399, Loss: 0.4146806001663208\n",
      "Epoch 14, Batch 400, Loss: 0.6655669212341309\n",
      "Epoch 14, Batch 401, Loss: 0.5523487329483032\n",
      "Epoch 14, Batch 402, Loss: 0.7221311330795288\n",
      "Epoch 14, Batch 403, Loss: 0.42834073305130005\n",
      "Epoch 14, Batch 404, Loss: 0.35847559571266174\n",
      "Epoch 14, Batch 405, Loss: 0.30872735381126404\n",
      "Epoch 14, Batch 406, Loss: 0.4225435256958008\n",
      "Epoch 14, Batch 407, Loss: 0.3046426475048065\n",
      "Epoch 14, Batch 408, Loss: 0.3625875413417816\n",
      "Epoch 14, Batch 409, Loss: 0.4919063150882721\n",
      "Epoch 14, Batch 410, Loss: 0.6131599545478821\n",
      "Epoch 14, Batch 411, Loss: 0.3499215245246887\n",
      "Epoch 14, Batch 412, Loss: 0.4736362397670746\n",
      "Epoch 14, Batch 413, Loss: 0.39847779273986816\n",
      "Epoch 14, Batch 414, Loss: 0.24312326312065125\n",
      "Epoch 14, Batch 415, Loss: 0.5151092410087585\n",
      "Epoch 14, Batch 416, Loss: 0.41970035433769226\n",
      "Epoch 14, Batch 417, Loss: 0.3816108703613281\n",
      "Epoch 14, Batch 418, Loss: 0.2533019185066223\n",
      "Epoch 14, Batch 419, Loss: 0.43261727690696716\n",
      "Epoch 14, Batch 420, Loss: 0.36007100343704224\n",
      "Epoch 14, Batch 421, Loss: 0.3527357578277588\n",
      "Epoch 14, Batch 422, Loss: 0.3904019892215729\n",
      "Epoch 14, Batch 423, Loss: 0.5479727983474731\n",
      "Epoch 14, Batch 424, Loss: 0.39908942580223083\n",
      "Epoch 14, Batch 425, Loss: 0.560823380947113\n",
      "Epoch 14, Batch 426, Loss: 0.32691967487335205\n",
      "Epoch 14, Batch 427, Loss: 1.0561268329620361\n",
      "Epoch 14, Batch 428, Loss: 0.43812569975852966\n",
      "Epoch 14, Batch 429, Loss: 0.38826173543930054\n",
      "Epoch 14, Batch 430, Loss: 0.44673994183540344\n",
      "Epoch 14, Batch 431, Loss: 0.4878973066806793\n",
      "Epoch 14, Batch 432, Loss: 0.3940753936767578\n",
      "Epoch 14, Batch 433, Loss: 0.3917767405509949\n",
      "Epoch 14, Batch 434, Loss: 0.4184291958808899\n",
      "Epoch 14, Batch 435, Loss: 0.3767501711845398\n",
      "Epoch 14, Batch 436, Loss: 0.49352049827575684\n",
      "Epoch 14, Batch 437, Loss: 0.2753022611141205\n",
      "Epoch 14, Batch 438, Loss: 0.5402622818946838\n",
      "Epoch 14, Batch 439, Loss: 0.4167511463165283\n",
      "Epoch 14, Batch 440, Loss: 0.4915306866168976\n",
      "Epoch 14, Batch 441, Loss: 0.5232223868370056\n",
      "Epoch 14, Batch 442, Loss: 0.3689460754394531\n",
      "Epoch 14, Batch 443, Loss: 0.5439236760139465\n",
      "Epoch 14, Batch 444, Loss: 0.38991227746009827\n",
      "Epoch 14, Batch 445, Loss: 0.35441237688064575\n",
      "Epoch 14, Batch 446, Loss: 0.473887175321579\n",
      "Epoch 14, Batch 447, Loss: 0.5571467280387878\n",
      "Epoch 14, Batch 448, Loss: 0.37882110476493835\n",
      "Epoch 14, Batch 449, Loss: 0.493025541305542\n",
      "Epoch 14, Batch 450, Loss: 0.4513908624649048\n",
      "Epoch 14, Batch 451, Loss: 0.5105171203613281\n",
      "Epoch 14, Batch 452, Loss: 0.33731135725975037\n",
      "Epoch 14, Batch 453, Loss: 0.47078442573547363\n",
      "Epoch 14, Batch 454, Loss: 0.3784175217151642\n",
      "Epoch 14, Batch 455, Loss: 0.44639578461647034\n",
      "Epoch 14, Batch 456, Loss: 0.5251399278640747\n",
      "Epoch 14, Batch 457, Loss: 0.43263816833496094\n",
      "Epoch 14, Batch 458, Loss: 0.5133830904960632\n",
      "Epoch 14, Batch 459, Loss: 0.4298073649406433\n",
      "Epoch 14, Batch 460, Loss: 0.6030492186546326\n",
      "Epoch 14, Batch 461, Loss: 0.592669665813446\n",
      "Epoch 14, Batch 462, Loss: 0.5363176465034485\n",
      "Epoch 14, Batch 463, Loss: 0.3982473611831665\n",
      "Epoch 14, Batch 464, Loss: 0.6019632816314697\n",
      "Epoch 14, Batch 465, Loss: 0.43729978799819946\n",
      "Epoch 14, Batch 466, Loss: 0.40046125650405884\n",
      "Epoch 14, Batch 467, Loss: 0.6055311560630798\n",
      "Epoch 14, Batch 468, Loss: 0.6439070105552673\n",
      "Epoch 14, Batch 469, Loss: 0.40540146827697754\n",
      "Epoch 14, Batch 470, Loss: 0.5040612816810608\n",
      "Epoch 14, Batch 471, Loss: 0.5322127342224121\n",
      "Epoch 14, Batch 472, Loss: 0.824627697467804\n",
      "Epoch 14, Batch 473, Loss: 0.66310715675354\n",
      "Epoch 14, Batch 474, Loss: 0.40055525302886963\n",
      "Epoch 14, Batch 475, Loss: 0.4405025541782379\n",
      "Epoch 14, Batch 476, Loss: 0.43830209970474243\n",
      "Epoch 14, Batch 477, Loss: 0.4656763970851898\n",
      "Epoch 14, Batch 478, Loss: 0.4552392363548279\n",
      "Epoch 14, Batch 479, Loss: 0.5177898406982422\n",
      "Epoch 14, Batch 480, Loss: 0.6047659516334534\n",
      "Epoch 14, Batch 481, Loss: 0.7206741571426392\n",
      "Epoch 14, Batch 482, Loss: 0.34951213002204895\n",
      "Epoch 14, Batch 483, Loss: 0.4588254392147064\n",
      "Epoch 14, Batch 484, Loss: 0.4712521433830261\n",
      "Epoch 14, Batch 485, Loss: 0.4837229251861572\n",
      "Epoch 14, Batch 486, Loss: 0.39953577518463135\n",
      "Epoch 14, Batch 487, Loss: 0.646415650844574\n",
      "Epoch 14, Batch 488, Loss: 0.7129152417182922\n",
      "Epoch 14, Batch 489, Loss: 0.4679618775844574\n",
      "Epoch 14, Batch 490, Loss: 0.4680314064025879\n",
      "Epoch 14, Batch 491, Loss: 0.38160693645477295\n",
      "Epoch 14, Batch 492, Loss: 0.4157368242740631\n",
      "Epoch 14, Batch 493, Loss: 0.532082736492157\n",
      "Epoch 14, Batch 494, Loss: 0.3151138126850128\n",
      "Epoch 14, Batch 495, Loss: 0.5475784540176392\n",
      "Epoch 14, Batch 496, Loss: 0.5216289758682251\n",
      "Epoch 14, Batch 497, Loss: 0.5879366993904114\n",
      "Epoch 14, Batch 498, Loss: 0.5672997832298279\n",
      "Epoch 14, Batch 499, Loss: 0.4076978266239166\n",
      "Epoch 14, Batch 500, Loss: 0.427290678024292\n",
      "Epoch 14, Batch 501, Loss: 0.5717369914054871\n",
      "Epoch 14, Batch 502, Loss: 0.5451045036315918\n",
      "Epoch 14, Batch 503, Loss: 0.5149186849594116\n",
      "Epoch 14, Batch 504, Loss: 0.41579192876815796\n",
      "Epoch 14, Batch 505, Loss: 0.4166099429130554\n",
      "Epoch 14, Batch 506, Loss: 0.7051845788955688\n",
      "Epoch 14, Batch 507, Loss: 0.44826847314834595\n",
      "Epoch 14, Batch 508, Loss: 0.5504934787750244\n",
      "Epoch 14, Batch 509, Loss: 0.5033366680145264\n",
      "Epoch 14, Batch 510, Loss: 0.4179686903953552\n",
      "Epoch 14, Batch 511, Loss: 0.4615475833415985\n",
      "Epoch 14, Batch 512, Loss: 0.8957650065422058\n",
      "Epoch 14, Batch 513, Loss: 0.3723348379135132\n",
      "Epoch 14, Batch 514, Loss: 0.3151879906654358\n",
      "Epoch 14, Batch 515, Loss: 0.5338228940963745\n",
      "Epoch 14, Batch 516, Loss: 0.47211602330207825\n",
      "Epoch 14, Batch 517, Loss: 0.4265655279159546\n",
      "Epoch 14, Batch 518, Loss: 0.45248767733573914\n",
      "Epoch 14, Batch 519, Loss: 0.4237504005432129\n",
      "Epoch 14, Batch 520, Loss: 0.3054496645927429\n",
      "Epoch 14, Batch 521, Loss: 0.41860342025756836\n",
      "Epoch 14, Batch 522, Loss: 0.2866652309894562\n",
      "Epoch 14, Batch 523, Loss: 0.4739600419998169\n",
      "Epoch 14, Batch 524, Loss: 0.47272947430610657\n",
      "Epoch 14, Batch 525, Loss: 0.5856828689575195\n",
      "Epoch 14, Batch 526, Loss: 0.512575626373291\n",
      "Epoch 14, Batch 527, Loss: 0.48903483152389526\n",
      "Epoch 14, Batch 528, Loss: 0.5741913318634033\n",
      "Epoch 14, Batch 529, Loss: 0.43730783462524414\n",
      "Epoch 14, Batch 530, Loss: 0.3957114517688751\n",
      "Epoch 14, Batch 531, Loss: 0.33408454060554504\n",
      "Epoch 14, Batch 532, Loss: 0.5015180706977844\n",
      "Epoch 14, Batch 533, Loss: 0.34292319416999817\n",
      "Epoch 14, Batch 534, Loss: 0.6965538263320923\n",
      "Epoch 14, Batch 535, Loss: 0.5363928079605103\n",
      "Epoch 14, Batch 536, Loss: 0.46039384603500366\n",
      "Epoch 14, Batch 537, Loss: 0.3189319968223572\n",
      "Epoch 14, Batch 538, Loss: 0.4664541780948639\n",
      "Epoch 14, Batch 539, Loss: 0.5977709293365479\n",
      "Epoch 14, Batch 540, Loss: 0.4949254095554352\n",
      "Epoch 14, Batch 541, Loss: 0.42629778385162354\n",
      "Epoch 14, Batch 542, Loss: 0.3065146505832672\n",
      "Epoch 14, Batch 543, Loss: 0.48035603761672974\n",
      "Epoch 14, Batch 544, Loss: 0.3975161910057068\n",
      "Epoch 14, Batch 545, Loss: 0.638824999332428\n",
      "Epoch 14, Batch 546, Loss: 0.5136869549751282\n",
      "Epoch 14, Batch 547, Loss: 0.41740483045578003\n",
      "Epoch 14, Batch 548, Loss: 0.4178045392036438\n",
      "Epoch 14, Batch 549, Loss: 0.40547001361846924\n",
      "Epoch 14, Batch 550, Loss: 0.29306507110595703\n",
      "Epoch 14, Batch 551, Loss: 0.35746249556541443\n",
      "Epoch 14, Batch 552, Loss: 0.420305460691452\n",
      "Epoch 14, Batch 553, Loss: 0.6579475402832031\n",
      "Epoch 14, Batch 554, Loss: 0.39874404668807983\n",
      "Epoch 14, Batch 555, Loss: 0.37247538566589355\n",
      "Epoch 14, Batch 556, Loss: 0.607927143573761\n",
      "Epoch 14, Batch 557, Loss: 0.5091066956520081\n",
      "Epoch 14, Batch 558, Loss: 0.7274095416069031\n",
      "Epoch 14, Batch 559, Loss: 0.4928850531578064\n",
      "Epoch 14, Batch 560, Loss: 0.48585015535354614\n",
      "Epoch 14, Batch 561, Loss: 0.5308175086975098\n",
      "Epoch 14, Batch 562, Loss: 0.5596267580986023\n",
      "Epoch 14, Batch 563, Loss: 0.3635513186454773\n",
      "Epoch 14, Batch 564, Loss: 0.5470412969589233\n",
      "Epoch 14, Batch 565, Loss: 0.49389418959617615\n",
      "Epoch 14, Batch 566, Loss: 0.38376614451408386\n",
      "Epoch 14, Batch 567, Loss: 0.6756572127342224\n",
      "Epoch 14, Batch 568, Loss: 0.2977805435657501\n",
      "Epoch 14, Batch 569, Loss: 0.3753659427165985\n",
      "Epoch 14, Batch 570, Loss: 0.4884316325187683\n",
      "Epoch 14, Batch 571, Loss: 0.6142416596412659\n",
      "Epoch 14, Batch 572, Loss: 0.4409227967262268\n",
      "Epoch 14, Batch 573, Loss: 0.24331094324588776\n",
      "Epoch 14, Batch 574, Loss: 0.6635597348213196\n",
      "Epoch 14, Batch 575, Loss: 0.5076614618301392\n",
      "Epoch 14, Batch 576, Loss: 0.7359835505485535\n",
      "Epoch 14, Batch 577, Loss: 0.6100147366523743\n",
      "Epoch 14, Batch 578, Loss: 0.4444967806339264\n",
      "Epoch 14, Batch 579, Loss: 0.5520228147506714\n",
      "Epoch 14, Batch 580, Loss: 0.6532886624336243\n",
      "Epoch 14, Batch 581, Loss: 0.39641690254211426\n",
      "Epoch 14, Batch 582, Loss: 0.5691315531730652\n",
      "Epoch 14, Batch 583, Loss: 0.41427966952323914\n",
      "Epoch 14, Batch 584, Loss: 0.3853457272052765\n",
      "Epoch 14, Batch 585, Loss: 0.6521628499031067\n",
      "Epoch 14, Batch 586, Loss: 0.4292563199996948\n",
      "Epoch 14, Batch 587, Loss: 0.3693816363811493\n",
      "Epoch 14, Batch 588, Loss: 0.37605583667755127\n",
      "Epoch 14, Batch 589, Loss: 0.429782509803772\n",
      "Epoch 14, Batch 590, Loss: 0.5077334046363831\n",
      "Epoch 14, Batch 591, Loss: 0.5196219086647034\n",
      "Epoch 14, Batch 592, Loss: 0.34857118129730225\n",
      "Epoch 14, Batch 593, Loss: 0.536129355430603\n",
      "Epoch 14, Batch 594, Loss: 0.3038932681083679\n",
      "Epoch 14, Batch 595, Loss: 0.33738642930984497\n",
      "Epoch 14, Batch 596, Loss: 0.3023679256439209\n",
      "Epoch 14, Batch 597, Loss: 0.48906099796295166\n",
      "Epoch 14, Batch 598, Loss: 0.41141629219055176\n",
      "Epoch 14, Batch 599, Loss: 0.5439580678939819\n",
      "Epoch 14, Batch 600, Loss: 0.5261105895042419\n",
      "Epoch 14, Batch 601, Loss: 0.42411792278289795\n",
      "Epoch 14, Batch 602, Loss: 0.4878183603286743\n",
      "Epoch 14, Batch 603, Loss: 0.5913172960281372\n",
      "Epoch 14, Batch 604, Loss: 0.517724335193634\n",
      "Epoch 14, Batch 605, Loss: 0.45096248388290405\n",
      "Epoch 14, Batch 606, Loss: 0.40356940031051636\n",
      "Epoch 14, Batch 607, Loss: 0.4558757543563843\n",
      "Epoch 14, Batch 608, Loss: 0.4884308874607086\n",
      "Epoch 14, Batch 609, Loss: 0.4912031590938568\n",
      "Epoch 14, Batch 610, Loss: 0.7818996906280518\n",
      "Epoch 14, Batch 611, Loss: 0.4143660366535187\n",
      "Epoch 14, Batch 612, Loss: 0.2823195159435272\n",
      "Epoch 14, Batch 613, Loss: 0.6286037564277649\n",
      "Epoch 14, Batch 614, Loss: 0.46987244486808777\n",
      "Epoch 14, Batch 615, Loss: 0.42734548449516296\n",
      "Epoch 14, Batch 616, Loss: 0.37149012088775635\n",
      "Epoch 14, Batch 617, Loss: 0.5471367239952087\n",
      "Epoch 14, Batch 618, Loss: 0.5131348371505737\n",
      "Epoch 14, Batch 619, Loss: 0.5716127753257751\n",
      "Epoch 14, Batch 620, Loss: 0.33734244108200073\n",
      "Epoch 14, Batch 621, Loss: 0.3670327663421631\n",
      "Epoch 14, Batch 622, Loss: 0.3994337022304535\n",
      "Epoch 14, Batch 623, Loss: 0.34280675649642944\n",
      "Epoch 14, Batch 624, Loss: 0.5349740982055664\n",
      "Epoch 14, Batch 625, Loss: 0.5412429571151733\n",
      "Epoch 14, Batch 626, Loss: 0.47142452001571655\n",
      "Epoch 14, Batch 627, Loss: 0.4889572858810425\n",
      "Epoch 14, Batch 628, Loss: 0.49137333035469055\n",
      "Epoch 14, Batch 629, Loss: 0.4828794598579407\n",
      "Epoch 14, Batch 630, Loss: 0.3833710551261902\n",
      "Epoch 14, Batch 631, Loss: 0.4603048861026764\n",
      "Epoch 14, Batch 632, Loss: 0.38751354813575745\n",
      "Epoch 14, Batch 633, Loss: 0.36037129163742065\n",
      "Epoch 14, Batch 634, Loss: 0.5263696908950806\n",
      "Epoch 14, Batch 635, Loss: 0.4273219108581543\n",
      "Epoch 14, Batch 636, Loss: 0.5508041381835938\n",
      "Epoch 14, Batch 637, Loss: 0.4392388164997101\n",
      "Epoch 14, Batch 638, Loss: 0.4787072539329529\n",
      "Epoch 14, Batch 639, Loss: 0.4778773784637451\n",
      "Epoch 14, Batch 640, Loss: 0.5011466145515442\n",
      "Epoch 14, Batch 641, Loss: 0.38276103138923645\n",
      "Epoch 14, Batch 642, Loss: 0.6283506751060486\n",
      "Epoch 14, Batch 643, Loss: 0.3242262601852417\n",
      "Epoch 14, Batch 644, Loss: 0.6441906690597534\n",
      "Epoch 14, Batch 645, Loss: 0.26576855778694153\n",
      "Epoch 14, Batch 646, Loss: 0.40762951970100403\n",
      "Epoch 14, Batch 647, Loss: 0.48775649070739746\n",
      "Epoch 14, Batch 648, Loss: 0.384158194065094\n",
      "Epoch 14, Batch 649, Loss: 0.4675687849521637\n",
      "Epoch 14, Batch 650, Loss: 0.43247491121292114\n",
      "Epoch 14, Batch 651, Loss: 0.501558780670166\n",
      "Epoch 14, Batch 652, Loss: 0.4202863276004791\n",
      "Epoch 14, Batch 653, Loss: 0.6160504221916199\n",
      "Epoch 14, Batch 654, Loss: 0.47277480363845825\n",
      "Epoch 14, Batch 655, Loss: 0.5602755546569824\n",
      "Epoch 14, Batch 656, Loss: 0.49665743112564087\n",
      "Epoch 14, Batch 657, Loss: 0.35799679160118103\n",
      "Epoch 14, Batch 658, Loss: 0.530044674873352\n",
      "Epoch 14, Batch 659, Loss: 0.5635226368904114\n",
      "Epoch 14, Batch 660, Loss: 0.3586678206920624\n",
      "Epoch 14, Batch 661, Loss: 0.4195089340209961\n",
      "Epoch 14, Batch 662, Loss: 0.4103301465511322\n",
      "Epoch 14, Batch 663, Loss: 0.6513080596923828\n",
      "Epoch 14, Batch 664, Loss: 0.5862869620323181\n",
      "Epoch 14, Batch 665, Loss: 0.37131938338279724\n",
      "Epoch 14, Batch 666, Loss: 0.49792858958244324\n",
      "Epoch 14, Batch 667, Loss: 0.32838693261146545\n",
      "Epoch 14, Batch 668, Loss: 0.37939587235450745\n",
      "Epoch 14, Batch 669, Loss: 0.5839482545852661\n",
      "Epoch 14, Batch 670, Loss: 0.5081843733787537\n",
      "Epoch 14, Batch 671, Loss: 0.3203548789024353\n",
      "Epoch 14, Batch 672, Loss: 0.5188067555427551\n",
      "Epoch 14, Batch 673, Loss: 0.39456886053085327\n",
      "Epoch 14, Batch 674, Loss: 0.48078542947769165\n",
      "Epoch 14, Batch 675, Loss: 0.4590177834033966\n",
      "Epoch 14, Batch 676, Loss: 0.46759065985679626\n",
      "Epoch 14, Batch 677, Loss: 0.3794795274734497\n",
      "Epoch 14, Batch 678, Loss: 0.568932831287384\n",
      "Epoch 14, Batch 679, Loss: 0.32896754145622253\n",
      "Epoch 14, Batch 680, Loss: 0.6817429661750793\n",
      "Epoch 14, Batch 681, Loss: 0.5125550627708435\n",
      "Epoch 14, Batch 682, Loss: 0.33102506399154663\n",
      "Epoch 14, Batch 683, Loss: 0.3364148736000061\n",
      "Epoch 14, Batch 684, Loss: 0.462581604719162\n",
      "Epoch 14, Batch 685, Loss: 0.6930076479911804\n",
      "Epoch 14, Batch 686, Loss: 0.409923791885376\n",
      "Epoch 14, Batch 687, Loss: 0.3748723864555359\n",
      "Epoch 14, Batch 688, Loss: 0.35541534423828125\n",
      "Epoch 14, Batch 689, Loss: 0.5724141001701355\n",
      "Epoch 14, Batch 690, Loss: 0.3429502248764038\n",
      "Epoch 14, Batch 691, Loss: 0.35159939527511597\n",
      "Epoch 14, Batch 692, Loss: 0.4243870675563812\n",
      "Epoch 14, Batch 693, Loss: 0.5013653635978699\n",
      "Epoch 14, Batch 694, Loss: 0.5652904510498047\n",
      "Epoch 14, Batch 695, Loss: 0.46766194701194763\n",
      "Epoch 14, Batch 696, Loss: 0.6157852411270142\n",
      "Epoch 14, Batch 697, Loss: 0.580986738204956\n",
      "Epoch 14, Batch 698, Loss: 0.6585896015167236\n",
      "Epoch 14, Batch 699, Loss: 0.4737987220287323\n",
      "Epoch 14, Batch 700, Loss: 0.6439872980117798\n",
      "Epoch 14, Batch 701, Loss: 0.42833754420280457\n",
      "Epoch 14, Batch 702, Loss: 0.5332912802696228\n",
      "Epoch 14, Batch 703, Loss: 0.6211636662483215\n",
      "Epoch 14, Batch 704, Loss: 0.30311694741249084\n",
      "Epoch 14, Batch 705, Loss: 0.43973177671432495\n",
      "Epoch 14, Batch 706, Loss: 0.5133119821548462\n",
      "Epoch 14, Batch 707, Loss: 0.5725893974304199\n",
      "Epoch 14, Batch 708, Loss: 0.5737031698226929\n",
      "Epoch 14, Batch 709, Loss: 0.5940625667572021\n",
      "Epoch 14, Batch 710, Loss: 0.6366879940032959\n",
      "Epoch 14, Batch 711, Loss: 0.4743635952472687\n",
      "Epoch 14, Batch 712, Loss: 0.5243480205535889\n",
      "Epoch 14, Batch 713, Loss: 0.46920138597488403\n",
      "Epoch 14, Batch 714, Loss: 0.5983603596687317\n",
      "Epoch 14, Batch 715, Loss: 0.6031798124313354\n",
      "Epoch 14, Batch 716, Loss: 0.5018561482429504\n",
      "Epoch 14, Batch 717, Loss: 0.5948690176010132\n",
      "Epoch 14, Batch 718, Loss: 0.4561597406864166\n",
      "Epoch 14, Batch 719, Loss: 0.46180200576782227\n",
      "Epoch 14, Batch 720, Loss: 0.5710607767105103\n",
      "Epoch 14, Batch 721, Loss: 0.4116491675376892\n",
      "Epoch 14, Batch 722, Loss: 0.4607643485069275\n",
      "Epoch 14, Batch 723, Loss: 0.3441528081893921\n",
      "Epoch 14, Batch 724, Loss: 0.5278173089027405\n",
      "Epoch 14, Batch 725, Loss: 0.595899760723114\n",
      "Epoch 14, Batch 726, Loss: 0.3990490734577179\n",
      "Epoch 14, Batch 727, Loss: 0.4483732283115387\n",
      "Epoch 14, Batch 728, Loss: 0.2801956534385681\n",
      "Epoch 14, Batch 729, Loss: 0.3863105773925781\n",
      "Epoch 14, Batch 730, Loss: 0.34043586254119873\n",
      "Epoch 14, Batch 731, Loss: 0.4508923292160034\n",
      "Epoch 14, Batch 732, Loss: 0.4833335280418396\n",
      "Epoch 14, Batch 733, Loss: 0.42811816930770874\n",
      "Epoch 14, Batch 734, Loss: 0.7146593928337097\n",
      "Epoch 14, Batch 735, Loss: 0.6231123208999634\n",
      "Epoch 14, Batch 736, Loss: 0.3497180938720703\n",
      "Epoch 14, Batch 737, Loss: 0.5146251916885376\n",
      "Epoch 14, Batch 738, Loss: 0.6441997289657593\n",
      "Epoch 14, Batch 739, Loss: 0.4127100110054016\n",
      "Epoch 14, Batch 740, Loss: 0.4861223101615906\n",
      "Epoch 14, Batch 741, Loss: 0.480269193649292\n",
      "Epoch 14, Batch 742, Loss: 0.4480595588684082\n",
      "Epoch 14, Batch 743, Loss: 0.4220987856388092\n",
      "Epoch 14, Batch 744, Loss: 0.4896937608718872\n",
      "Epoch 14, Batch 745, Loss: 0.41151508688926697\n",
      "Epoch 14, Batch 746, Loss: 0.36462241411209106\n",
      "Epoch 14, Batch 747, Loss: 0.37968146800994873\n",
      "Epoch 14, Batch 748, Loss: 0.6196973919868469\n",
      "Epoch 14, Batch 749, Loss: 0.6944542527198792\n",
      "Epoch 14, Batch 750, Loss: 0.4106287360191345\n",
      "Epoch 14, Batch 751, Loss: 0.40286684036254883\n",
      "Epoch 14, Batch 752, Loss: 0.5096080899238586\n",
      "Epoch 14, Batch 753, Loss: 0.4806289076805115\n",
      "Epoch 14, Batch 754, Loss: 0.453377366065979\n",
      "Epoch 14, Batch 755, Loss: 0.448256254196167\n",
      "Epoch 14, Batch 756, Loss: 0.5207377672195435\n",
      "Epoch 14, Batch 757, Loss: 0.3708932399749756\n",
      "Epoch 14, Batch 758, Loss: 0.5315547585487366\n",
      "Epoch 14, Batch 759, Loss: 0.4089791774749756\n",
      "Epoch 14, Batch 760, Loss: 0.6076931357383728\n",
      "Epoch 14, Batch 761, Loss: 0.5323272943496704\n",
      "Epoch 14, Batch 762, Loss: 0.3852614164352417\n",
      "Epoch 14, Batch 763, Loss: 0.38489431142807007\n",
      "Epoch 14, Batch 764, Loss: 0.31670916080474854\n",
      "Epoch 14, Batch 765, Loss: 0.535318911075592\n",
      "Epoch 14, Batch 766, Loss: 0.5180204510688782\n",
      "Epoch 14, Batch 767, Loss: 0.5268081426620483\n",
      "Epoch 14, Batch 768, Loss: 0.49124667048454285\n",
      "Epoch 14, Batch 769, Loss: 0.3499947488307953\n",
      "Epoch 14, Batch 770, Loss: 0.4972006678581238\n",
      "Epoch 14, Batch 771, Loss: 0.489935040473938\n",
      "Epoch 14, Batch 772, Loss: 0.35465145111083984\n",
      "Epoch 14, Batch 773, Loss: 0.5432590246200562\n",
      "Epoch 14, Batch 774, Loss: 0.3234159052371979\n",
      "Epoch 14, Batch 775, Loss: 0.43130192160606384\n",
      "Epoch 14, Batch 776, Loss: 0.4191027283668518\n",
      "Epoch 14, Batch 777, Loss: 0.6659366488456726\n",
      "Epoch 14, Batch 778, Loss: 0.5406675338745117\n",
      "Epoch 14, Batch 779, Loss: 0.410078227519989\n",
      "Epoch 14, Batch 780, Loss: 0.40819206833839417\n",
      "Epoch 14, Batch 781, Loss: 0.5895869731903076\n",
      "Epoch 14, Batch 782, Loss: 0.47143715620040894\n",
      "Epoch 14, Batch 783, Loss: 0.6972662806510925\n",
      "Epoch 14, Batch 784, Loss: 0.2884514629840851\n",
      "Epoch 14, Batch 785, Loss: 0.3941468298435211\n",
      "Epoch 14, Batch 786, Loss: 0.4819819927215576\n",
      "Epoch 14, Batch 787, Loss: 0.5683871507644653\n",
      "Epoch 14, Batch 788, Loss: 0.4501432180404663\n",
      "Epoch 14, Batch 789, Loss: 0.7249976396560669\n",
      "Epoch 14, Batch 790, Loss: 0.5244258642196655\n",
      "Epoch 14, Batch 791, Loss: 0.2761443555355072\n",
      "Epoch 14, Batch 792, Loss: 0.3819725215435028\n",
      "Epoch 14, Batch 793, Loss: 0.3247360289096832\n",
      "Epoch 14, Batch 794, Loss: 0.4285661280155182\n",
      "Epoch 14, Batch 795, Loss: 0.6714365482330322\n",
      "Epoch 14, Batch 796, Loss: 0.5486817359924316\n",
      "Epoch 14, Batch 797, Loss: 0.3461327850818634\n",
      "Epoch 14, Batch 798, Loss: 0.48632165789604187\n",
      "Epoch 14, Batch 799, Loss: 0.5605208873748779\n",
      "Epoch 14, Batch 800, Loss: 0.40721991658210754\n",
      "Epoch 14, Batch 801, Loss: 0.4214704632759094\n",
      "Epoch 14, Batch 802, Loss: 0.5115110874176025\n",
      "Epoch 14, Batch 803, Loss: 0.4746875464916229\n",
      "Epoch 14, Batch 804, Loss: 0.3784499168395996\n",
      "Epoch 14, Batch 805, Loss: 0.505779504776001\n",
      "Epoch 14, Batch 806, Loss: 0.3277038037776947\n",
      "Epoch 14, Batch 807, Loss: 0.5523179769515991\n",
      "Epoch 14, Batch 808, Loss: 0.5204063653945923\n",
      "Epoch 14, Batch 809, Loss: 0.39481040835380554\n",
      "Epoch 14, Batch 810, Loss: 0.44290614128112793\n",
      "Epoch 14, Batch 811, Loss: 0.34735584259033203\n",
      "Epoch 14, Batch 812, Loss: 0.4079306125640869\n",
      "Epoch 14, Batch 813, Loss: 0.4020691215991974\n",
      "Epoch 14, Batch 814, Loss: 0.6777708530426025\n",
      "Epoch 14, Batch 815, Loss: 0.5575349926948547\n",
      "Epoch 14, Batch 816, Loss: 0.4266292452812195\n",
      "Epoch 14, Batch 817, Loss: 0.48510921001434326\n",
      "Epoch 14, Batch 818, Loss: 0.44876015186309814\n",
      "Epoch 14, Batch 819, Loss: 0.6373014450073242\n",
      "Epoch 14, Batch 820, Loss: 0.5048534274101257\n",
      "Epoch 14, Batch 821, Loss: 0.6125479936599731\n",
      "Epoch 14, Batch 822, Loss: 0.344146728515625\n",
      "Epoch 14, Batch 823, Loss: 0.5560871362686157\n",
      "Epoch 14, Batch 824, Loss: 0.5108252167701721\n",
      "Epoch 14, Batch 825, Loss: 0.5820237994194031\n",
      "Epoch 14, Batch 826, Loss: 0.40281665325164795\n",
      "Epoch 14, Batch 827, Loss: 0.3377252519130707\n",
      "Epoch 14, Batch 828, Loss: 0.3995240330696106\n",
      "Epoch 14, Batch 829, Loss: 0.38138046860694885\n",
      "Epoch 14, Batch 830, Loss: 0.4795674681663513\n",
      "Epoch 14, Batch 831, Loss: 0.4049415588378906\n",
      "Epoch 14, Batch 832, Loss: 0.4218221604824066\n",
      "Epoch 14, Batch 833, Loss: 0.39581534266471863\n",
      "Epoch 14, Batch 834, Loss: 0.30186954140663147\n",
      "Epoch 14, Batch 835, Loss: 0.5941298604011536\n",
      "Epoch 14, Batch 836, Loss: 0.415960431098938\n",
      "Epoch 14, Batch 837, Loss: 0.48393380641937256\n",
      "Epoch 14, Batch 838, Loss: 0.6385123133659363\n",
      "Epoch 14, Batch 839, Loss: 0.45510590076446533\n",
      "Epoch 14, Batch 840, Loss: 0.5417860746383667\n",
      "Epoch 14, Batch 841, Loss: 0.46673718094825745\n",
      "Epoch 14, Batch 842, Loss: 0.3524746894836426\n",
      "Epoch 14, Batch 843, Loss: 0.5964609384536743\n",
      "Epoch 14, Batch 844, Loss: 0.4140695035457611\n",
      "Epoch 14, Batch 845, Loss: 0.357756108045578\n",
      "Epoch 14, Batch 846, Loss: 0.31419137120246887\n",
      "Epoch 14, Batch 847, Loss: 0.45272910594940186\n",
      "Epoch 14, Batch 848, Loss: 0.3688100278377533\n",
      "Epoch 14, Batch 849, Loss: 0.5698509216308594\n",
      "Epoch 14, Batch 850, Loss: 0.47617173194885254\n",
      "Epoch 14, Batch 851, Loss: 0.3031110167503357\n",
      "Epoch 14, Batch 852, Loss: 0.7542824745178223\n",
      "Epoch 14, Batch 853, Loss: 0.6218592524528503\n",
      "Epoch 14, Batch 854, Loss: 0.42902234196662903\n",
      "Epoch 14, Batch 855, Loss: 0.4264799654483795\n",
      "Epoch 14, Batch 856, Loss: 0.27982479333877563\n",
      "Epoch 14, Batch 857, Loss: 0.5161539316177368\n",
      "Epoch 14, Batch 858, Loss: 0.4827619194984436\n",
      "Epoch 14, Batch 859, Loss: 0.4578694999217987\n",
      "Epoch 14, Batch 860, Loss: 0.32265815138816833\n",
      "Epoch 14, Batch 861, Loss: 0.4079517722129822\n",
      "Epoch 14, Batch 862, Loss: 0.46704185009002686\n",
      "Epoch 14, Batch 863, Loss: 0.42532283067703247\n",
      "Epoch 14, Batch 864, Loss: 0.5053500533103943\n",
      "Epoch 14, Batch 865, Loss: 0.5981192588806152\n",
      "Epoch 14, Batch 866, Loss: 0.5764460563659668\n",
      "Epoch 14, Batch 867, Loss: 0.5933215618133545\n",
      "Epoch 14, Batch 868, Loss: 0.40854546427726746\n",
      "Epoch 14, Batch 869, Loss: 0.3811466097831726\n",
      "Epoch 14, Batch 870, Loss: 0.47377562522888184\n",
      "Epoch 14, Batch 871, Loss: 0.4911283552646637\n",
      "Epoch 14, Batch 872, Loss: 0.6637766361236572\n",
      "Epoch 14, Batch 873, Loss: 0.4256342649459839\n",
      "Epoch 14, Batch 874, Loss: 0.497755765914917\n",
      "Epoch 14, Batch 875, Loss: 0.6165703535079956\n",
      "Epoch 14, Batch 876, Loss: 0.40306609869003296\n",
      "Epoch 14, Batch 877, Loss: 0.5925896763801575\n",
      "Epoch 14, Batch 878, Loss: 0.4916185438632965\n",
      "Epoch 14, Batch 879, Loss: 0.5934358239173889\n",
      "Epoch 14, Batch 880, Loss: 0.5613023042678833\n",
      "Epoch 14, Batch 881, Loss: 0.3995263874530792\n",
      "Epoch 14, Batch 882, Loss: 0.3092191219329834\n",
      "Epoch 14, Batch 883, Loss: 0.4313271641731262\n",
      "Epoch 14, Batch 884, Loss: 0.5399739742279053\n",
      "Epoch 14, Batch 885, Loss: 0.2887432873249054\n",
      "Epoch 14, Batch 886, Loss: 0.576062798500061\n",
      "Epoch 14, Batch 887, Loss: 0.4572364389896393\n",
      "Epoch 14, Batch 888, Loss: 0.46107277274131775\n",
      "Epoch 14, Batch 889, Loss: 0.3309348225593567\n",
      "Epoch 14, Batch 890, Loss: 0.44240227341651917\n",
      "Epoch 14, Batch 891, Loss: 0.48298564553260803\n",
      "Epoch 14, Batch 892, Loss: 0.47869133949279785\n",
      "Epoch 14, Batch 893, Loss: 0.5296728014945984\n",
      "Epoch 14, Batch 894, Loss: 0.5920592546463013\n",
      "Epoch 14, Batch 895, Loss: 0.3394395112991333\n",
      "Epoch 14, Batch 896, Loss: 0.3283459544181824\n",
      "Epoch 14, Batch 897, Loss: 0.48214268684387207\n",
      "Epoch 14, Batch 898, Loss: 0.48690265417099\n",
      "Epoch 14, Batch 899, Loss: 0.4422086179256439\n",
      "Epoch 14, Batch 900, Loss: 0.3818786144256592\n",
      "Epoch 14, Batch 901, Loss: 0.36274415254592896\n",
      "Epoch 14, Batch 902, Loss: 0.45696786046028137\n",
      "Epoch 14, Batch 903, Loss: 0.38430097699165344\n",
      "Epoch 14, Batch 904, Loss: 0.5418222546577454\n",
      "Epoch 14, Batch 905, Loss: 0.34519487619400024\n",
      "Epoch 14, Batch 906, Loss: 0.5383986830711365\n",
      "Epoch 14, Batch 907, Loss: 0.46095478534698486\n",
      "Epoch 14, Batch 908, Loss: 0.46735548973083496\n",
      "Epoch 14, Batch 909, Loss: 0.38021397590637207\n",
      "Epoch 14, Batch 910, Loss: 0.5265251994132996\n",
      "Epoch 14, Batch 911, Loss: 0.5143464207649231\n",
      "Epoch 14, Batch 912, Loss: 0.588100016117096\n",
      "Epoch 14, Batch 913, Loss: 0.40152809023857117\n",
      "Epoch 14, Batch 914, Loss: 0.3860882520675659\n",
      "Epoch 14, Batch 915, Loss: 0.36749887466430664\n",
      "Epoch 14, Batch 916, Loss: 0.4349789321422577\n",
      "Epoch 14, Batch 917, Loss: 0.43696072697639465\n",
      "Epoch 14, Batch 918, Loss: 0.40460991859436035\n",
      "Epoch 14, Batch 919, Loss: 0.41569453477859497\n",
      "Epoch 14, Batch 920, Loss: 0.2904186248779297\n",
      "Epoch 14, Batch 921, Loss: 0.6037206649780273\n",
      "Epoch 14, Batch 922, Loss: 0.6653903722763062\n",
      "Epoch 14, Batch 923, Loss: 0.5333518981933594\n",
      "Epoch 14, Batch 924, Loss: 0.6245109438896179\n",
      "Epoch 14, Batch 925, Loss: 0.5334400534629822\n",
      "Epoch 14, Batch 926, Loss: 0.8138657808303833\n",
      "Epoch 14, Batch 927, Loss: 0.47497719526290894\n",
      "Epoch 14, Batch 928, Loss: 0.4791005253791809\n",
      "Epoch 14, Batch 929, Loss: 0.7278448343276978\n",
      "Epoch 14, Batch 930, Loss: 0.4240860044956207\n",
      "Epoch 14, Batch 931, Loss: 0.6608364582061768\n",
      "Epoch 14, Batch 932, Loss: 0.3662603497505188\n",
      "Epoch 14, Batch 933, Loss: 0.42065757513046265\n",
      "Epoch 14, Batch 934, Loss: 0.4355853796005249\n",
      "Epoch 14, Batch 935, Loss: 0.6497316956520081\n",
      "Epoch 14, Batch 936, Loss: 0.47751063108444214\n",
      "Epoch 14, Batch 937, Loss: 0.38948550820350647\n",
      "Epoch 14, Batch 938, Loss: 0.29724013805389404\n",
      "Accuracy of train set: 0.8352833333333334\n",
      "Epoch 14, Batch 1, Test Loss: 0.4592142701148987\n",
      "Epoch 14, Batch 2, Test Loss: 0.43685632944107056\n",
      "Epoch 14, Batch 3, Test Loss: 0.4525679349899292\n",
      "Epoch 14, Batch 4, Test Loss: 0.3209584355354309\n",
      "Epoch 14, Batch 5, Test Loss: 0.2994305193424225\n",
      "Epoch 14, Batch 6, Test Loss: 0.5554758310317993\n",
      "Epoch 14, Batch 7, Test Loss: 0.33899950981140137\n",
      "Epoch 14, Batch 8, Test Loss: 0.33988144993782043\n",
      "Epoch 14, Batch 9, Test Loss: 0.5752015709877014\n",
      "Epoch 14, Batch 10, Test Loss: 0.5407727360725403\n",
      "Epoch 14, Batch 11, Test Loss: 0.4395950734615326\n",
      "Epoch 14, Batch 12, Test Loss: 0.43751415610313416\n",
      "Epoch 14, Batch 13, Test Loss: 0.4211849868297577\n",
      "Epoch 14, Batch 14, Test Loss: 0.39872467517852783\n",
      "Epoch 14, Batch 15, Test Loss: 0.6693986654281616\n",
      "Epoch 14, Batch 16, Test Loss: 0.44526350498199463\n",
      "Epoch 14, Batch 17, Test Loss: 0.43902331590652466\n",
      "Epoch 14, Batch 18, Test Loss: 0.4096544682979584\n",
      "Epoch 14, Batch 19, Test Loss: 0.6453889012336731\n",
      "Epoch 14, Batch 20, Test Loss: 0.5014511942863464\n",
      "Epoch 14, Batch 21, Test Loss: 0.42288708686828613\n",
      "Epoch 14, Batch 22, Test Loss: 0.42176151275634766\n",
      "Epoch 14, Batch 23, Test Loss: 0.4131573438644409\n",
      "Epoch 14, Batch 24, Test Loss: 0.37748289108276367\n",
      "Epoch 14, Batch 25, Test Loss: 0.5948089957237244\n",
      "Epoch 14, Batch 26, Test Loss: 0.4851115047931671\n",
      "Epoch 14, Batch 27, Test Loss: 0.3150569200515747\n",
      "Epoch 14, Batch 28, Test Loss: 0.5901212096214294\n",
      "Epoch 14, Batch 29, Test Loss: 0.5656684041023254\n",
      "Epoch 14, Batch 30, Test Loss: 0.48679500818252563\n",
      "Epoch 14, Batch 31, Test Loss: 0.4284714460372925\n",
      "Epoch 14, Batch 32, Test Loss: 0.37394917011260986\n",
      "Epoch 14, Batch 33, Test Loss: 0.4265969693660736\n",
      "Epoch 14, Batch 34, Test Loss: 0.42968320846557617\n",
      "Epoch 14, Batch 35, Test Loss: 0.6198509931564331\n",
      "Epoch 14, Batch 36, Test Loss: 0.46171921491622925\n",
      "Epoch 14, Batch 37, Test Loss: 0.5489912033081055\n",
      "Epoch 14, Batch 38, Test Loss: 0.39177921414375305\n",
      "Epoch 14, Batch 39, Test Loss: 0.4619845449924469\n",
      "Epoch 14, Batch 40, Test Loss: 0.4362967014312744\n",
      "Epoch 14, Batch 41, Test Loss: 0.7244616150856018\n",
      "Epoch 14, Batch 42, Test Loss: 0.3787100613117218\n",
      "Epoch 14, Batch 43, Test Loss: 0.37786245346069336\n",
      "Epoch 14, Batch 44, Test Loss: 0.44636663794517517\n",
      "Epoch 14, Batch 45, Test Loss: 0.47673216462135315\n",
      "Epoch 14, Batch 46, Test Loss: 0.44839078187942505\n",
      "Epoch 14, Batch 47, Test Loss: 0.4661963880062103\n",
      "Epoch 14, Batch 48, Test Loss: 0.47778505086898804\n",
      "Epoch 14, Batch 49, Test Loss: 0.6971527934074402\n",
      "Epoch 14, Batch 50, Test Loss: 0.42381829023361206\n",
      "Epoch 14, Batch 51, Test Loss: 0.38312017917633057\n",
      "Epoch 14, Batch 52, Test Loss: 0.39005258679389954\n",
      "Epoch 14, Batch 53, Test Loss: 0.4504316449165344\n",
      "Epoch 14, Batch 54, Test Loss: 0.5524719953536987\n",
      "Epoch 14, Batch 55, Test Loss: 0.5144691467285156\n",
      "Epoch 14, Batch 56, Test Loss: 0.44138240814208984\n",
      "Epoch 14, Batch 57, Test Loss: 0.27801358699798584\n",
      "Epoch 14, Batch 58, Test Loss: 0.6319143176078796\n",
      "Epoch 14, Batch 59, Test Loss: 0.4704866409301758\n",
      "Epoch 14, Batch 60, Test Loss: 0.48937344551086426\n",
      "Epoch 14, Batch 61, Test Loss: 0.5826325416564941\n",
      "Epoch 14, Batch 62, Test Loss: 0.3868511915206909\n",
      "Epoch 14, Batch 63, Test Loss: 0.3916131556034088\n",
      "Epoch 14, Batch 64, Test Loss: 0.4970240294933319\n",
      "Epoch 14, Batch 65, Test Loss: 0.4085216522216797\n",
      "Epoch 14, Batch 66, Test Loss: 0.6462888717651367\n",
      "Epoch 14, Batch 67, Test Loss: 0.4796302318572998\n",
      "Epoch 14, Batch 68, Test Loss: 0.5334518551826477\n",
      "Epoch 14, Batch 69, Test Loss: 0.4573628604412079\n",
      "Epoch 14, Batch 70, Test Loss: 0.4179651737213135\n",
      "Epoch 14, Batch 71, Test Loss: 0.3774072527885437\n",
      "Epoch 14, Batch 72, Test Loss: 0.44491466879844666\n",
      "Epoch 14, Batch 73, Test Loss: 0.5263180732727051\n",
      "Epoch 14, Batch 74, Test Loss: 0.468566358089447\n",
      "Epoch 14, Batch 75, Test Loss: 0.42762479186058044\n",
      "Epoch 14, Batch 76, Test Loss: 0.482460081577301\n",
      "Epoch 14, Batch 77, Test Loss: 0.4229790270328522\n",
      "Epoch 14, Batch 78, Test Loss: 0.45393630862236023\n",
      "Epoch 14, Batch 79, Test Loss: 0.4171866476535797\n",
      "Epoch 14, Batch 80, Test Loss: 0.364740788936615\n",
      "Epoch 14, Batch 81, Test Loss: 0.6538107395172119\n",
      "Epoch 14, Batch 82, Test Loss: 0.26459336280822754\n",
      "Epoch 14, Batch 83, Test Loss: 0.44525572657585144\n",
      "Epoch 14, Batch 84, Test Loss: 0.5901116728782654\n",
      "Epoch 14, Batch 85, Test Loss: 0.6343110203742981\n",
      "Epoch 14, Batch 86, Test Loss: 0.4597926735877991\n",
      "Epoch 14, Batch 87, Test Loss: 0.5058236718177795\n",
      "Epoch 14, Batch 88, Test Loss: 0.3256989121437073\n",
      "Epoch 14, Batch 89, Test Loss: 0.4129529595375061\n",
      "Epoch 14, Batch 90, Test Loss: 0.3515493869781494\n",
      "Epoch 14, Batch 91, Test Loss: 0.5820783376693726\n",
      "Epoch 14, Batch 92, Test Loss: 0.4780217409133911\n",
      "Epoch 14, Batch 93, Test Loss: 0.5821576118469238\n",
      "Epoch 14, Batch 94, Test Loss: 0.6889051795005798\n",
      "Epoch 14, Batch 95, Test Loss: 0.4451068043708801\n",
      "Epoch 14, Batch 96, Test Loss: 0.6002576947212219\n",
      "Epoch 14, Batch 97, Test Loss: 0.4543564021587372\n",
      "Epoch 14, Batch 98, Test Loss: 0.39937126636505127\n",
      "Epoch 14, Batch 99, Test Loss: 0.4202975332736969\n",
      "Epoch 14, Batch 100, Test Loss: 0.47023069858551025\n",
      "Epoch 14, Batch 101, Test Loss: 0.4018075466156006\n",
      "Epoch 14, Batch 102, Test Loss: 0.4079488515853882\n",
      "Epoch 14, Batch 103, Test Loss: 0.3811942934989929\n",
      "Epoch 14, Batch 104, Test Loss: 0.410308301448822\n",
      "Epoch 14, Batch 105, Test Loss: 0.7359833717346191\n",
      "Epoch 14, Batch 106, Test Loss: 0.7375286817550659\n",
      "Epoch 14, Batch 107, Test Loss: 0.5046932101249695\n",
      "Epoch 14, Batch 108, Test Loss: 0.32155001163482666\n",
      "Epoch 14, Batch 109, Test Loss: 0.49720078706741333\n",
      "Epoch 14, Batch 110, Test Loss: 0.42812544107437134\n",
      "Epoch 14, Batch 111, Test Loss: 0.40194618701934814\n",
      "Epoch 14, Batch 112, Test Loss: 0.4084813892841339\n",
      "Epoch 14, Batch 113, Test Loss: 0.5215145349502563\n",
      "Epoch 14, Batch 114, Test Loss: 0.38229304552078247\n",
      "Epoch 14, Batch 115, Test Loss: 0.5785664319992065\n",
      "Epoch 14, Batch 116, Test Loss: 0.37675827741622925\n",
      "Epoch 14, Batch 117, Test Loss: 0.42068737745285034\n",
      "Epoch 14, Batch 118, Test Loss: 0.4468819797039032\n",
      "Epoch 14, Batch 119, Test Loss: 0.41396865248680115\n",
      "Epoch 14, Batch 120, Test Loss: 0.4132315516471863\n",
      "Epoch 14, Batch 121, Test Loss: 0.49613723158836365\n",
      "Epoch 14, Batch 122, Test Loss: 0.395844042301178\n",
      "Epoch 14, Batch 123, Test Loss: 0.3897809386253357\n",
      "Epoch 14, Batch 124, Test Loss: 0.5133822560310364\n",
      "Epoch 14, Batch 125, Test Loss: 0.4496609568595886\n",
      "Epoch 14, Batch 126, Test Loss: 0.4367501139640808\n",
      "Epoch 14, Batch 127, Test Loss: 0.5060308575630188\n",
      "Epoch 14, Batch 128, Test Loss: 0.4882778227329254\n",
      "Epoch 14, Batch 129, Test Loss: 0.5021802186965942\n",
      "Epoch 14, Batch 130, Test Loss: 0.505553662776947\n",
      "Epoch 14, Batch 131, Test Loss: 0.39698678255081177\n",
      "Epoch 14, Batch 132, Test Loss: 0.4666040539741516\n",
      "Epoch 14, Batch 133, Test Loss: 0.4804707169532776\n",
      "Epoch 14, Batch 134, Test Loss: 0.48770877718925476\n",
      "Epoch 14, Batch 135, Test Loss: 0.44674310088157654\n",
      "Epoch 14, Batch 136, Test Loss: 0.5841425657272339\n",
      "Epoch 14, Batch 137, Test Loss: 0.5716325640678406\n",
      "Epoch 14, Batch 138, Test Loss: 0.6725347638130188\n",
      "Epoch 14, Batch 139, Test Loss: 0.46470779180526733\n",
      "Epoch 14, Batch 140, Test Loss: 0.36268627643585205\n",
      "Epoch 14, Batch 141, Test Loss: 0.5909656882286072\n",
      "Epoch 14, Batch 142, Test Loss: 0.4207257628440857\n",
      "Epoch 14, Batch 143, Test Loss: 0.52683025598526\n",
      "Epoch 14, Batch 144, Test Loss: 0.49963441491127014\n",
      "Epoch 14, Batch 145, Test Loss: 0.3343678414821625\n",
      "Epoch 14, Batch 146, Test Loss: 0.46875670552253723\n",
      "Epoch 14, Batch 147, Test Loss: 0.6197333931922913\n",
      "Epoch 14, Batch 148, Test Loss: 0.4614335894584656\n",
      "Epoch 14, Batch 149, Test Loss: 0.5472453236579895\n",
      "Epoch 14, Batch 150, Test Loss: 0.3746611773967743\n",
      "Epoch 14, Batch 151, Test Loss: 0.34373408555984497\n",
      "Epoch 14, Batch 152, Test Loss: 0.624140739440918\n",
      "Epoch 14, Batch 153, Test Loss: 0.33670303225517273\n",
      "Epoch 14, Batch 154, Test Loss: 0.6096867322921753\n",
      "Epoch 14, Batch 155, Test Loss: 0.5967997312545776\n",
      "Epoch 14, Batch 156, Test Loss: 0.5137977004051208\n",
      "Epoch 14, Batch 157, Test Loss: 0.31031402945518494\n",
      "Epoch 14, Batch 158, Test Loss: 0.802985429763794\n",
      "Epoch 14, Batch 159, Test Loss: 0.3564443290233612\n",
      "Epoch 14, Batch 160, Test Loss: 0.3804079294204712\n",
      "Epoch 14, Batch 161, Test Loss: 0.5808075070381165\n",
      "Epoch 14, Batch 162, Test Loss: 0.4258711636066437\n",
      "Epoch 14, Batch 163, Test Loss: 0.4978618621826172\n",
      "Epoch 14, Batch 164, Test Loss: 0.4638691842556\n",
      "Epoch 14, Batch 165, Test Loss: 0.5451598167419434\n",
      "Epoch 14, Batch 166, Test Loss: 0.5233848690986633\n",
      "Epoch 14, Batch 167, Test Loss: 0.48827818036079407\n",
      "Epoch 14, Batch 168, Test Loss: 0.5177785754203796\n",
      "Epoch 14, Batch 169, Test Loss: 0.5722694396972656\n",
      "Epoch 14, Batch 170, Test Loss: 0.5674787759780884\n",
      "Epoch 14, Batch 171, Test Loss: 0.4369770288467407\n",
      "Epoch 14, Batch 172, Test Loss: 0.46598196029663086\n",
      "Epoch 14, Batch 173, Test Loss: 0.5947662591934204\n",
      "Epoch 14, Batch 174, Test Loss: 0.5008286237716675\n",
      "Epoch 14, Batch 175, Test Loss: 0.4353558421134949\n",
      "Epoch 14, Batch 176, Test Loss: 0.6009525060653687\n",
      "Epoch 14, Batch 177, Test Loss: 0.5070305466651917\n",
      "Epoch 14, Batch 178, Test Loss: 0.4678923189640045\n",
      "Epoch 14, Batch 179, Test Loss: 0.3574880063533783\n",
      "Epoch 14, Batch 180, Test Loss: 0.31139665842056274\n",
      "Epoch 14, Batch 181, Test Loss: 0.388918399810791\n",
      "Epoch 14, Batch 182, Test Loss: 0.6064147353172302\n",
      "Epoch 14, Batch 183, Test Loss: 0.45388054847717285\n",
      "Epoch 14, Batch 184, Test Loss: 0.3776724934577942\n",
      "Epoch 14, Batch 185, Test Loss: 0.5708174109458923\n",
      "Epoch 14, Batch 186, Test Loss: 0.5976627469062805\n",
      "Epoch 14, Batch 187, Test Loss: 0.47913071513175964\n",
      "Epoch 14, Batch 188, Test Loss: 0.33148428797721863\n",
      "Epoch 14, Batch 189, Test Loss: 0.33930134773254395\n",
      "Epoch 14, Batch 190, Test Loss: 0.29010385274887085\n",
      "Epoch 14, Batch 191, Test Loss: 0.49189305305480957\n",
      "Epoch 14, Batch 192, Test Loss: 0.5270834565162659\n",
      "Epoch 14, Batch 193, Test Loss: 0.4051622152328491\n",
      "Epoch 14, Batch 194, Test Loss: 0.5094687938690186\n",
      "Epoch 14, Batch 195, Test Loss: 0.43288758397102356\n",
      "Epoch 14, Batch 196, Test Loss: 0.4477110505104065\n",
      "Epoch 14, Batch 197, Test Loss: 0.44269880652427673\n",
      "Epoch 14, Batch 198, Test Loss: 0.5127007365226746\n",
      "Epoch 14, Batch 199, Test Loss: 0.3920206129550934\n",
      "Epoch 14, Batch 200, Test Loss: 0.5103925466537476\n",
      "Epoch 14, Batch 201, Test Loss: 0.3953738212585449\n",
      "Epoch 14, Batch 202, Test Loss: 0.5054247379302979\n",
      "Epoch 14, Batch 203, Test Loss: 0.2921244502067566\n",
      "Epoch 14, Batch 204, Test Loss: 0.4531354606151581\n",
      "Epoch 14, Batch 205, Test Loss: 0.5279988050460815\n",
      "Epoch 14, Batch 206, Test Loss: 0.6910941004753113\n",
      "Epoch 14, Batch 207, Test Loss: 0.5734444260597229\n",
      "Epoch 14, Batch 208, Test Loss: 0.5025838017463684\n",
      "Epoch 14, Batch 209, Test Loss: 0.5112927556037903\n",
      "Epoch 14, Batch 210, Test Loss: 0.46706733107566833\n",
      "Epoch 14, Batch 211, Test Loss: 0.4112492501735687\n",
      "Epoch 14, Batch 212, Test Loss: 0.43261438608169556\n",
      "Epoch 14, Batch 213, Test Loss: 0.593150794506073\n",
      "Epoch 14, Batch 214, Test Loss: 0.4723353683948517\n",
      "Epoch 14, Batch 215, Test Loss: 0.5213832259178162\n",
      "Epoch 14, Batch 216, Test Loss: 0.5196297764778137\n",
      "Epoch 14, Batch 217, Test Loss: 0.5981824398040771\n",
      "Epoch 14, Batch 218, Test Loss: 0.3935440182685852\n",
      "Epoch 14, Batch 219, Test Loss: 0.4367663264274597\n",
      "Epoch 14, Batch 220, Test Loss: 0.48632147908210754\n",
      "Epoch 14, Batch 221, Test Loss: 0.351012259721756\n",
      "Epoch 14, Batch 222, Test Loss: 0.23667113482952118\n",
      "Epoch 14, Batch 223, Test Loss: 0.6568617820739746\n",
      "Epoch 14, Batch 224, Test Loss: 0.3938140869140625\n",
      "Epoch 14, Batch 225, Test Loss: 0.31536591053009033\n",
      "Epoch 14, Batch 226, Test Loss: 0.31738045811653137\n",
      "Epoch 14, Batch 227, Test Loss: 0.588989794254303\n",
      "Epoch 14, Batch 228, Test Loss: 0.49430039525032043\n",
      "Epoch 14, Batch 229, Test Loss: 0.3348972797393799\n",
      "Epoch 14, Batch 230, Test Loss: 0.5194053053855896\n",
      "Epoch 14, Batch 231, Test Loss: 0.44298166036605835\n",
      "Epoch 14, Batch 232, Test Loss: 0.49910426139831543\n",
      "Epoch 14, Batch 233, Test Loss: 0.4561648666858673\n",
      "Epoch 14, Batch 234, Test Loss: 0.6038161516189575\n",
      "Epoch 14, Batch 235, Test Loss: 0.41524386405944824\n",
      "Epoch 14, Batch 236, Test Loss: 0.46180033683776855\n",
      "Epoch 14, Batch 237, Test Loss: 0.7343703508377075\n",
      "Epoch 14, Batch 238, Test Loss: 0.5896179676055908\n",
      "Epoch 14, Batch 239, Test Loss: 0.5903663635253906\n",
      "Epoch 14, Batch 240, Test Loss: 0.467031866312027\n",
      "Epoch 14, Batch 241, Test Loss: 0.38220125436782837\n",
      "Epoch 14, Batch 242, Test Loss: 0.7112053632736206\n",
      "Epoch 14, Batch 243, Test Loss: 0.4900532364845276\n",
      "Epoch 14, Batch 244, Test Loss: 0.36378470063209534\n",
      "Epoch 14, Batch 245, Test Loss: 0.39294835925102234\n",
      "Epoch 14, Batch 246, Test Loss: 0.5968741774559021\n",
      "Epoch 14, Batch 247, Test Loss: 0.2960015535354614\n",
      "Epoch 14, Batch 248, Test Loss: 0.5278119444847107\n",
      "Epoch 14, Batch 249, Test Loss: 0.3471934199333191\n",
      "Epoch 14, Batch 250, Test Loss: 0.49160346388816833\n",
      "Epoch 14, Batch 251, Test Loss: 0.44123589992523193\n",
      "Epoch 14, Batch 252, Test Loss: 0.535642683506012\n",
      "Epoch 14, Batch 253, Test Loss: 0.5770364999771118\n",
      "Epoch 14, Batch 254, Test Loss: 0.5119212865829468\n",
      "Epoch 14, Batch 255, Test Loss: 0.6431505680084229\n",
      "Epoch 14, Batch 256, Test Loss: 0.512387216091156\n",
      "Epoch 14, Batch 257, Test Loss: 0.5500403046607971\n",
      "Epoch 14, Batch 258, Test Loss: 0.355217307806015\n",
      "Epoch 14, Batch 259, Test Loss: 0.3925274610519409\n",
      "Epoch 14, Batch 260, Test Loss: 0.44815176725387573\n",
      "Epoch 14, Batch 261, Test Loss: 0.5026036500930786\n",
      "Epoch 14, Batch 262, Test Loss: 0.35198551416397095\n",
      "Epoch 14, Batch 263, Test Loss: 0.4999006390571594\n",
      "Epoch 14, Batch 264, Test Loss: 0.4234013557434082\n",
      "Epoch 14, Batch 265, Test Loss: 0.3709668517112732\n",
      "Epoch 14, Batch 266, Test Loss: 0.35128331184387207\n",
      "Epoch 14, Batch 267, Test Loss: 0.5214330554008484\n",
      "Epoch 14, Batch 268, Test Loss: 0.574131965637207\n",
      "Epoch 14, Batch 269, Test Loss: 0.576826810836792\n",
      "Epoch 14, Batch 270, Test Loss: 0.47525012493133545\n",
      "Epoch 14, Batch 271, Test Loss: 0.3190266191959381\n",
      "Epoch 14, Batch 272, Test Loss: 0.4739028811454773\n",
      "Epoch 14, Batch 273, Test Loss: 0.5080320835113525\n",
      "Epoch 14, Batch 274, Test Loss: 0.6202946305274963\n",
      "Epoch 14, Batch 275, Test Loss: 0.3844262361526489\n",
      "Epoch 14, Batch 276, Test Loss: 0.4663180112838745\n",
      "Epoch 14, Batch 277, Test Loss: 0.3056698441505432\n",
      "Epoch 14, Batch 278, Test Loss: 0.5108598470687866\n",
      "Epoch 14, Batch 279, Test Loss: 0.4816517233848572\n",
      "Epoch 14, Batch 280, Test Loss: 0.4125155210494995\n",
      "Epoch 14, Batch 281, Test Loss: 0.47935882210731506\n",
      "Epoch 14, Batch 282, Test Loss: 0.5399229526519775\n",
      "Epoch 14, Batch 283, Test Loss: 0.4792206287384033\n",
      "Epoch 14, Batch 284, Test Loss: 0.5406525135040283\n",
      "Epoch 14, Batch 285, Test Loss: 0.2722015678882599\n",
      "Epoch 14, Batch 286, Test Loss: 0.5069816708564758\n",
      "Epoch 14, Batch 287, Test Loss: 0.38590121269226074\n",
      "Epoch 14, Batch 288, Test Loss: 0.5730476975440979\n",
      "Epoch 14, Batch 289, Test Loss: 0.4012661576271057\n",
      "Epoch 14, Batch 290, Test Loss: 0.483925461769104\n",
      "Epoch 14, Batch 291, Test Loss: 0.3759409785270691\n",
      "Epoch 14, Batch 292, Test Loss: 0.4379110634326935\n",
      "Epoch 14, Batch 293, Test Loss: 0.5004604458808899\n",
      "Epoch 14, Batch 294, Test Loss: 0.5240080952644348\n",
      "Epoch 14, Batch 295, Test Loss: 0.7146532535552979\n",
      "Epoch 14, Batch 296, Test Loss: 0.5149564743041992\n",
      "Epoch 14, Batch 297, Test Loss: 0.4641171097755432\n",
      "Epoch 14, Batch 298, Test Loss: 0.4088382124900818\n",
      "Epoch 14, Batch 299, Test Loss: 0.4046553373336792\n",
      "Epoch 14, Batch 300, Test Loss: 0.530402421951294\n",
      "Epoch 14, Batch 301, Test Loss: 0.6098965406417847\n",
      "Epoch 14, Batch 302, Test Loss: 0.42894884943962097\n",
      "Epoch 14, Batch 303, Test Loss: 0.39644697308540344\n",
      "Epoch 14, Batch 304, Test Loss: 0.4499613344669342\n",
      "Epoch 14, Batch 305, Test Loss: 0.6290489435195923\n",
      "Epoch 14, Batch 306, Test Loss: 0.6077523231506348\n",
      "Epoch 14, Batch 307, Test Loss: 0.42539626359939575\n",
      "Epoch 14, Batch 308, Test Loss: 0.5196699500083923\n",
      "Epoch 14, Batch 309, Test Loss: 0.5922375321388245\n",
      "Epoch 14, Batch 310, Test Loss: 0.5187308192253113\n",
      "Epoch 14, Batch 311, Test Loss: 0.5365231037139893\n",
      "Epoch 14, Batch 312, Test Loss: 0.27708855271339417\n",
      "Epoch 14, Batch 313, Test Loss: 0.5796456933021545\n",
      "Epoch 14, Batch 314, Test Loss: 0.44873732328414917\n",
      "Epoch 14, Batch 315, Test Loss: 0.3194770812988281\n",
      "Epoch 14, Batch 316, Test Loss: 0.4881676435470581\n",
      "Epoch 14, Batch 317, Test Loss: 0.38725200295448303\n",
      "Epoch 14, Batch 318, Test Loss: 0.44466498494148254\n",
      "Epoch 14, Batch 319, Test Loss: 0.4091283082962036\n",
      "Epoch 14, Batch 320, Test Loss: 0.5397529602050781\n",
      "Epoch 14, Batch 321, Test Loss: 0.438520610332489\n",
      "Epoch 14, Batch 322, Test Loss: 0.3148052394390106\n",
      "Epoch 14, Batch 323, Test Loss: 0.5474069714546204\n",
      "Epoch 14, Batch 324, Test Loss: 0.6264752149581909\n",
      "Epoch 14, Batch 325, Test Loss: 0.594616174697876\n",
      "Epoch 14, Batch 326, Test Loss: 0.37445375323295593\n",
      "Epoch 14, Batch 327, Test Loss: 0.36379554867744446\n",
      "Epoch 14, Batch 328, Test Loss: 0.5605912804603577\n",
      "Epoch 14, Batch 329, Test Loss: 0.503545343875885\n",
      "Epoch 14, Batch 330, Test Loss: 0.5497373938560486\n",
      "Epoch 14, Batch 331, Test Loss: 0.45126497745513916\n",
      "Epoch 14, Batch 332, Test Loss: 0.32899996638298035\n",
      "Epoch 14, Batch 333, Test Loss: 0.3726196587085724\n",
      "Epoch 14, Batch 334, Test Loss: 0.4643270969390869\n",
      "Epoch 14, Batch 335, Test Loss: 0.5536564588546753\n",
      "Epoch 14, Batch 336, Test Loss: 0.5607609152793884\n",
      "Epoch 14, Batch 337, Test Loss: 0.497967392206192\n",
      "Epoch 14, Batch 338, Test Loss: 0.5481061935424805\n",
      "Epoch 14, Batch 339, Test Loss: 0.5910396575927734\n",
      "Epoch 14, Batch 340, Test Loss: 0.7195971608161926\n",
      "Epoch 14, Batch 341, Test Loss: 0.4626631736755371\n",
      "Epoch 14, Batch 342, Test Loss: 0.41256117820739746\n",
      "Epoch 14, Batch 343, Test Loss: 0.4190163314342499\n",
      "Epoch 14, Batch 344, Test Loss: 0.48668330907821655\n",
      "Epoch 14, Batch 345, Test Loss: 0.40778154134750366\n",
      "Epoch 14, Batch 346, Test Loss: 0.4926954507827759\n",
      "Epoch 14, Batch 347, Test Loss: 0.38675636053085327\n",
      "Epoch 14, Batch 348, Test Loss: 0.2886148691177368\n",
      "Epoch 14, Batch 349, Test Loss: 0.6262468099594116\n",
      "Epoch 14, Batch 350, Test Loss: 0.4223572313785553\n",
      "Epoch 14, Batch 351, Test Loss: 0.4075384736061096\n",
      "Epoch 14, Batch 352, Test Loss: 0.4199928045272827\n",
      "Epoch 14, Batch 353, Test Loss: 0.4267462491989136\n",
      "Epoch 14, Batch 354, Test Loss: 0.64229416847229\n",
      "Epoch 14, Batch 355, Test Loss: 0.5726601481437683\n",
      "Epoch 14, Batch 356, Test Loss: 0.48800787329673767\n",
      "Epoch 14, Batch 357, Test Loss: 0.33215123414993286\n",
      "Epoch 14, Batch 358, Test Loss: 0.38102060556411743\n",
      "Epoch 14, Batch 359, Test Loss: 0.6101423501968384\n",
      "Epoch 14, Batch 360, Test Loss: 0.47781842947006226\n",
      "Epoch 14, Batch 361, Test Loss: 0.35337185859680176\n",
      "Epoch 14, Batch 362, Test Loss: 0.6372935175895691\n",
      "Epoch 14, Batch 363, Test Loss: 0.5299302339553833\n",
      "Epoch 14, Batch 364, Test Loss: 0.4277545213699341\n",
      "Epoch 14, Batch 365, Test Loss: 0.6308881640434265\n",
      "Epoch 14, Batch 366, Test Loss: 0.2948305904865265\n",
      "Epoch 14, Batch 367, Test Loss: 0.4647831916809082\n",
      "Epoch 14, Batch 368, Test Loss: 0.3474667966365814\n",
      "Epoch 14, Batch 369, Test Loss: 0.5674530267715454\n",
      "Epoch 14, Batch 370, Test Loss: 0.47869226336479187\n",
      "Epoch 14, Batch 371, Test Loss: 0.3559955060482025\n",
      "Epoch 14, Batch 372, Test Loss: 0.471779465675354\n",
      "Epoch 14, Batch 373, Test Loss: 0.4771406650543213\n",
      "Epoch 14, Batch 374, Test Loss: 0.3429839015007019\n",
      "Epoch 14, Batch 375, Test Loss: 0.4035392999649048\n",
      "Epoch 14, Batch 376, Test Loss: 0.336808443069458\n",
      "Epoch 14, Batch 377, Test Loss: 0.47755739092826843\n",
      "Epoch 14, Batch 378, Test Loss: 0.30998626351356506\n",
      "Epoch 14, Batch 379, Test Loss: 0.34028691053390503\n",
      "Epoch 14, Batch 380, Test Loss: 0.46310511231422424\n",
      "Epoch 14, Batch 381, Test Loss: 0.375135600566864\n",
      "Epoch 14, Batch 382, Test Loss: 0.3678441047668457\n",
      "Epoch 14, Batch 383, Test Loss: 0.3393618166446686\n",
      "Epoch 14, Batch 384, Test Loss: 0.4965072572231293\n",
      "Epoch 14, Batch 385, Test Loss: 0.5410328507423401\n",
      "Epoch 14, Batch 386, Test Loss: 0.852551281452179\n",
      "Epoch 14, Batch 387, Test Loss: 0.42140716314315796\n",
      "Epoch 14, Batch 388, Test Loss: 0.4588230848312378\n",
      "Epoch 14, Batch 389, Test Loss: 0.5361151099205017\n",
      "Epoch 14, Batch 390, Test Loss: 0.3651556372642517\n",
      "Epoch 14, Batch 391, Test Loss: 0.3681907057762146\n",
      "Epoch 14, Batch 392, Test Loss: 0.4812726676464081\n",
      "Epoch 14, Batch 393, Test Loss: 0.46455302834510803\n",
      "Epoch 14, Batch 394, Test Loss: 0.47277364134788513\n",
      "Epoch 14, Batch 395, Test Loss: 0.3071586787700653\n",
      "Epoch 14, Batch 396, Test Loss: 0.5806195735931396\n",
      "Epoch 14, Batch 397, Test Loss: 0.5266343951225281\n",
      "Epoch 14, Batch 398, Test Loss: 0.24736027419567108\n",
      "Epoch 14, Batch 399, Test Loss: 0.5709068179130554\n",
      "Epoch 14, Batch 400, Test Loss: 0.5419411659240723\n",
      "Epoch 14, Batch 401, Test Loss: 0.3738389313220978\n",
      "Epoch 14, Batch 402, Test Loss: 0.5811643600463867\n",
      "Epoch 14, Batch 403, Test Loss: 0.45491474866867065\n",
      "Epoch 14, Batch 404, Test Loss: 0.42805027961730957\n",
      "Epoch 14, Batch 405, Test Loss: 0.562039315700531\n",
      "Epoch 14, Batch 406, Test Loss: 0.4803130626678467\n",
      "Epoch 14, Batch 407, Test Loss: 0.5251967310905457\n",
      "Epoch 14, Batch 408, Test Loss: 0.48517096042633057\n",
      "Epoch 14, Batch 409, Test Loss: 0.3086187243461609\n",
      "Epoch 14, Batch 410, Test Loss: 0.47904977202415466\n",
      "Epoch 14, Batch 411, Test Loss: 0.6216598749160767\n",
      "Epoch 14, Batch 412, Test Loss: 0.3538541793823242\n",
      "Epoch 14, Batch 413, Test Loss: 0.6324092149734497\n",
      "Epoch 14, Batch 414, Test Loss: 0.4535135328769684\n",
      "Epoch 14, Batch 415, Test Loss: 0.30392035841941833\n",
      "Epoch 14, Batch 416, Test Loss: 0.33173948526382446\n",
      "Epoch 14, Batch 417, Test Loss: 0.515363872051239\n",
      "Epoch 14, Batch 418, Test Loss: 0.3932982087135315\n",
      "Epoch 14, Batch 419, Test Loss: 0.35185420513153076\n",
      "Epoch 14, Batch 420, Test Loss: 0.4346851110458374\n",
      "Epoch 14, Batch 421, Test Loss: 0.34507307410240173\n",
      "Epoch 14, Batch 422, Test Loss: 0.45918115973472595\n",
      "Epoch 14, Batch 423, Test Loss: 0.5826328992843628\n",
      "Epoch 14, Batch 424, Test Loss: 0.314094603061676\n",
      "Epoch 14, Batch 425, Test Loss: 0.4139496684074402\n",
      "Epoch 14, Batch 426, Test Loss: 0.46852800250053406\n",
      "Epoch 14, Batch 427, Test Loss: 0.4073016345500946\n",
      "Epoch 14, Batch 428, Test Loss: 0.3368719220161438\n",
      "Epoch 14, Batch 429, Test Loss: 0.5675109028816223\n",
      "Epoch 14, Batch 430, Test Loss: 0.597383975982666\n",
      "Epoch 14, Batch 431, Test Loss: 0.4661518335342407\n",
      "Epoch 14, Batch 432, Test Loss: 0.5154613852500916\n",
      "Epoch 14, Batch 433, Test Loss: 0.5161710381507874\n",
      "Epoch 14, Batch 434, Test Loss: 0.4628395140171051\n",
      "Epoch 14, Batch 435, Test Loss: 0.666662335395813\n",
      "Epoch 14, Batch 436, Test Loss: 0.5108171701431274\n",
      "Epoch 14, Batch 437, Test Loss: 0.607250452041626\n",
      "Epoch 14, Batch 438, Test Loss: 0.36281681060791016\n",
      "Epoch 14, Batch 439, Test Loss: 0.34829971194267273\n",
      "Epoch 14, Batch 440, Test Loss: 0.4717598855495453\n",
      "Epoch 14, Batch 441, Test Loss: 0.4600866138935089\n",
      "Epoch 14, Batch 442, Test Loss: 0.5857322216033936\n",
      "Epoch 14, Batch 443, Test Loss: 0.40443986654281616\n",
      "Epoch 14, Batch 444, Test Loss: 0.42283424735069275\n",
      "Epoch 14, Batch 445, Test Loss: 0.4541327655315399\n",
      "Epoch 14, Batch 446, Test Loss: 0.43831661343574524\n",
      "Epoch 14, Batch 447, Test Loss: 0.40534114837646484\n",
      "Epoch 14, Batch 448, Test Loss: 0.44099161028862\n",
      "Epoch 14, Batch 449, Test Loss: 0.5790460109710693\n",
      "Epoch 14, Batch 450, Test Loss: 0.29566338658332825\n",
      "Epoch 14, Batch 451, Test Loss: 0.49480152130126953\n",
      "Epoch 14, Batch 452, Test Loss: 0.3626232147216797\n",
      "Epoch 14, Batch 453, Test Loss: 0.41373777389526367\n",
      "Epoch 14, Batch 454, Test Loss: 0.45402991771698\n",
      "Epoch 14, Batch 455, Test Loss: 0.5692635774612427\n",
      "Epoch 14, Batch 456, Test Loss: 0.47913146018981934\n",
      "Epoch 14, Batch 457, Test Loss: 0.4764748811721802\n",
      "Epoch 14, Batch 458, Test Loss: 0.4652174711227417\n",
      "Epoch 14, Batch 459, Test Loss: 0.41508257389068604\n",
      "Epoch 14, Batch 460, Test Loss: 0.4341503381729126\n",
      "Epoch 14, Batch 461, Test Loss: 0.40579575300216675\n",
      "Epoch 14, Batch 462, Test Loss: 0.720807671546936\n",
      "Epoch 14, Batch 463, Test Loss: 0.5465468764305115\n",
      "Epoch 14, Batch 464, Test Loss: 0.43716463446617126\n",
      "Epoch 14, Batch 465, Test Loss: 0.4203813374042511\n",
      "Epoch 14, Batch 466, Test Loss: 0.372475266456604\n",
      "Epoch 14, Batch 467, Test Loss: 0.3213716745376587\n",
      "Epoch 14, Batch 468, Test Loss: 0.5114772915840149\n",
      "Epoch 14, Batch 469, Test Loss: 0.33443981409072876\n",
      "Epoch 14, Batch 470, Test Loss: 0.4834563136100769\n",
      "Epoch 14, Batch 471, Test Loss: 0.4981769919395447\n",
      "Epoch 14, Batch 472, Test Loss: 0.4780341386795044\n",
      "Epoch 14, Batch 473, Test Loss: 0.383639395236969\n",
      "Epoch 14, Batch 474, Test Loss: 0.5433627963066101\n",
      "Epoch 14, Batch 475, Test Loss: 0.3561194837093353\n",
      "Epoch 14, Batch 476, Test Loss: 0.591477632522583\n",
      "Epoch 14, Batch 477, Test Loss: 0.2984480857849121\n",
      "Epoch 14, Batch 478, Test Loss: 0.44436943531036377\n",
      "Epoch 14, Batch 479, Test Loss: 0.5657750368118286\n",
      "Epoch 14, Batch 480, Test Loss: 0.24819616973400116\n",
      "Epoch 14, Batch 481, Test Loss: 0.5070334672927856\n",
      "Epoch 14, Batch 482, Test Loss: 0.39067158102989197\n",
      "Epoch 14, Batch 483, Test Loss: 0.5138676762580872\n",
      "Epoch 14, Batch 484, Test Loss: 0.42001596093177795\n",
      "Epoch 14, Batch 485, Test Loss: 0.34705156087875366\n",
      "Epoch 14, Batch 486, Test Loss: 0.4240638315677643\n",
      "Epoch 14, Batch 487, Test Loss: 0.5581988096237183\n",
      "Epoch 14, Batch 488, Test Loss: 0.47153565287590027\n",
      "Epoch 14, Batch 489, Test Loss: 0.5036523938179016\n",
      "Epoch 14, Batch 490, Test Loss: 0.3801238536834717\n",
      "Epoch 14, Batch 491, Test Loss: 0.4189266860485077\n",
      "Epoch 14, Batch 492, Test Loss: 0.4890884757041931\n",
      "Epoch 14, Batch 493, Test Loss: 0.3488755524158478\n",
      "Epoch 14, Batch 494, Test Loss: 0.4972347617149353\n",
      "Epoch 14, Batch 495, Test Loss: 0.4528668522834778\n",
      "Epoch 14, Batch 496, Test Loss: 0.32138121128082275\n",
      "Epoch 14, Batch 497, Test Loss: 0.5252470970153809\n",
      "Epoch 14, Batch 498, Test Loss: 0.4508272409439087\n",
      "Epoch 14, Batch 499, Test Loss: 0.35429495573043823\n",
      "Epoch 14, Batch 500, Test Loss: 0.4314907491207123\n",
      "Epoch 14, Batch 501, Test Loss: 0.4849127233028412\n",
      "Epoch 14, Batch 502, Test Loss: 0.42430391907691956\n",
      "Epoch 14, Batch 503, Test Loss: 0.4864230155944824\n",
      "Epoch 14, Batch 504, Test Loss: 0.5228071212768555\n",
      "Epoch 14, Batch 505, Test Loss: 0.3839935064315796\n",
      "Epoch 14, Batch 506, Test Loss: 0.543960690498352\n",
      "Epoch 14, Batch 507, Test Loss: 0.45168012380599976\n",
      "Epoch 14, Batch 508, Test Loss: 0.5004700422286987\n",
      "Epoch 14, Batch 509, Test Loss: 0.43950122594833374\n",
      "Epoch 14, Batch 510, Test Loss: 0.43309152126312256\n",
      "Epoch 14, Batch 511, Test Loss: 0.3491680920124054\n",
      "Epoch 14, Batch 512, Test Loss: 0.4723750650882721\n",
      "Epoch 14, Batch 513, Test Loss: 0.3773384988307953\n",
      "Epoch 14, Batch 514, Test Loss: 0.45656293630599976\n",
      "Epoch 14, Batch 515, Test Loss: 0.39494776725769043\n",
      "Epoch 14, Batch 516, Test Loss: 0.4500126242637634\n",
      "Epoch 14, Batch 517, Test Loss: 0.4208867847919464\n",
      "Epoch 14, Batch 518, Test Loss: 0.38285091519355774\n",
      "Epoch 14, Batch 519, Test Loss: 0.4719812273979187\n",
      "Epoch 14, Batch 520, Test Loss: 0.5577430725097656\n",
      "Epoch 14, Batch 521, Test Loss: 0.6473802328109741\n",
      "Epoch 14, Batch 522, Test Loss: 0.2886570692062378\n",
      "Epoch 14, Batch 523, Test Loss: 0.31310951709747314\n",
      "Epoch 14, Batch 524, Test Loss: 0.5541898608207703\n",
      "Epoch 14, Batch 525, Test Loss: 0.3517917990684509\n",
      "Epoch 14, Batch 526, Test Loss: 0.4497455060482025\n",
      "Epoch 14, Batch 527, Test Loss: 0.3308177590370178\n",
      "Epoch 14, Batch 528, Test Loss: 0.5025312900543213\n",
      "Epoch 14, Batch 529, Test Loss: 0.5459144115447998\n",
      "Epoch 14, Batch 530, Test Loss: 0.3681991994380951\n",
      "Epoch 14, Batch 531, Test Loss: 0.383549302816391\n",
      "Epoch 14, Batch 532, Test Loss: 0.4127805531024933\n",
      "Epoch 14, Batch 533, Test Loss: 0.5143581032752991\n",
      "Epoch 14, Batch 534, Test Loss: 0.48308470845222473\n",
      "Epoch 14, Batch 535, Test Loss: 0.42138370871543884\n",
      "Epoch 14, Batch 536, Test Loss: 0.44975951313972473\n",
      "Epoch 14, Batch 537, Test Loss: 0.3524131774902344\n",
      "Epoch 14, Batch 538, Test Loss: 0.48696309328079224\n",
      "Epoch 14, Batch 539, Test Loss: 0.4859892427921295\n",
      "Epoch 14, Batch 540, Test Loss: 0.37123218178749084\n",
      "Epoch 14, Batch 541, Test Loss: 0.42799296975135803\n",
      "Epoch 14, Batch 542, Test Loss: 0.5790015459060669\n",
      "Epoch 14, Batch 543, Test Loss: 0.5290514826774597\n",
      "Epoch 14, Batch 544, Test Loss: 0.490556538105011\n",
      "Epoch 14, Batch 545, Test Loss: 0.5811905264854431\n",
      "Epoch 14, Batch 546, Test Loss: 0.5359967350959778\n",
      "Epoch 14, Batch 547, Test Loss: 0.4334062933921814\n",
      "Epoch 14, Batch 548, Test Loss: 0.40104761719703674\n",
      "Epoch 14, Batch 549, Test Loss: 0.45451319217681885\n",
      "Epoch 14, Batch 550, Test Loss: 0.4096437394618988\n",
      "Epoch 14, Batch 551, Test Loss: 0.4666003882884979\n",
      "Epoch 14, Batch 552, Test Loss: 0.7478954792022705\n",
      "Epoch 14, Batch 553, Test Loss: 0.5588441491127014\n",
      "Epoch 14, Batch 554, Test Loss: 0.7003344297409058\n",
      "Epoch 14, Batch 555, Test Loss: 0.3614862561225891\n",
      "Epoch 14, Batch 556, Test Loss: 0.4877696931362152\n",
      "Epoch 14, Batch 557, Test Loss: 0.3858364224433899\n",
      "Epoch 14, Batch 558, Test Loss: 0.47636619210243225\n",
      "Epoch 14, Batch 559, Test Loss: 0.8939488530158997\n",
      "Epoch 14, Batch 560, Test Loss: 0.4333032965660095\n",
      "Epoch 14, Batch 561, Test Loss: 0.5912158489227295\n",
      "Epoch 14, Batch 562, Test Loss: 0.3427560329437256\n",
      "Epoch 14, Batch 563, Test Loss: 0.3717660903930664\n",
      "Epoch 14, Batch 564, Test Loss: 0.3899011015892029\n",
      "Epoch 14, Batch 565, Test Loss: 0.3714805841445923\n",
      "Epoch 14, Batch 566, Test Loss: 0.5675572752952576\n",
      "Epoch 14, Batch 567, Test Loss: 0.4627628028392792\n",
      "Epoch 14, Batch 568, Test Loss: 0.49794507026672363\n",
      "Epoch 14, Batch 569, Test Loss: 0.34303387999534607\n",
      "Epoch 14, Batch 570, Test Loss: 0.48879313468933105\n",
      "Epoch 14, Batch 571, Test Loss: 0.4446238875389099\n",
      "Epoch 14, Batch 572, Test Loss: 0.4505190849304199\n",
      "Epoch 14, Batch 573, Test Loss: 0.5104711651802063\n",
      "Epoch 14, Batch 574, Test Loss: 0.4618339240550995\n",
      "Epoch 14, Batch 575, Test Loss: 0.43207475543022156\n",
      "Epoch 14, Batch 576, Test Loss: 0.5557402968406677\n",
      "Epoch 14, Batch 577, Test Loss: 0.23971720039844513\n",
      "Epoch 14, Batch 578, Test Loss: 0.43347638845443726\n",
      "Epoch 14, Batch 579, Test Loss: 0.6148081421852112\n",
      "Epoch 14, Batch 580, Test Loss: 0.32626065611839294\n",
      "Epoch 14, Batch 581, Test Loss: 0.49252045154571533\n",
      "Epoch 14, Batch 582, Test Loss: 0.46186211705207825\n",
      "Epoch 14, Batch 583, Test Loss: 0.2727472484111786\n",
      "Epoch 14, Batch 584, Test Loss: 0.47769367694854736\n",
      "Epoch 14, Batch 585, Test Loss: 0.4497653841972351\n",
      "Epoch 14, Batch 586, Test Loss: 0.7693202495574951\n",
      "Epoch 14, Batch 587, Test Loss: 0.5770906209945679\n",
      "Epoch 14, Batch 588, Test Loss: 0.4453970193862915\n",
      "Epoch 14, Batch 589, Test Loss: 0.4429274797439575\n",
      "Epoch 14, Batch 590, Test Loss: 0.42262113094329834\n",
      "Epoch 14, Batch 591, Test Loss: 0.6186187267303467\n",
      "Epoch 14, Batch 592, Test Loss: 0.5924450159072876\n",
      "Epoch 14, Batch 593, Test Loss: 0.6618008017539978\n",
      "Epoch 14, Batch 594, Test Loss: 0.46349385380744934\n",
      "Epoch 14, Batch 595, Test Loss: 0.5492522716522217\n",
      "Epoch 14, Batch 596, Test Loss: 0.4402158856391907\n",
      "Epoch 14, Batch 597, Test Loss: 0.6232079267501831\n",
      "Epoch 14, Batch 598, Test Loss: 0.4622478485107422\n",
      "Epoch 14, Batch 599, Test Loss: 0.3272104263305664\n",
      "Epoch 14, Batch 600, Test Loss: 0.4330047369003296\n",
      "Epoch 14, Batch 601, Test Loss: 0.6160512566566467\n",
      "Epoch 14, Batch 602, Test Loss: 0.28406035900115967\n",
      "Epoch 14, Batch 603, Test Loss: 0.47043082118034363\n",
      "Epoch 14, Batch 604, Test Loss: 0.49455684423446655\n",
      "Epoch 14, Batch 605, Test Loss: 0.2701188921928406\n",
      "Epoch 14, Batch 606, Test Loss: 0.36800673604011536\n",
      "Epoch 14, Batch 607, Test Loss: 0.46901556849479675\n",
      "Epoch 14, Batch 608, Test Loss: 0.35632339119911194\n",
      "Epoch 14, Batch 609, Test Loss: 0.5497580766677856\n",
      "Epoch 14, Batch 610, Test Loss: 0.53544020652771\n",
      "Epoch 14, Batch 611, Test Loss: 0.4170767366886139\n",
      "Epoch 14, Batch 612, Test Loss: 0.39722704887390137\n",
      "Epoch 14, Batch 613, Test Loss: 0.3885461091995239\n",
      "Epoch 14, Batch 614, Test Loss: 0.34567129611968994\n",
      "Epoch 14, Batch 615, Test Loss: 0.44155439734458923\n",
      "Epoch 14, Batch 616, Test Loss: 0.3916192650794983\n",
      "Epoch 14, Batch 617, Test Loss: 0.4080137014389038\n",
      "Epoch 14, Batch 618, Test Loss: 0.39359650015830994\n",
      "Epoch 14, Batch 619, Test Loss: 0.5291563272476196\n",
      "Epoch 14, Batch 620, Test Loss: 0.4974120259284973\n",
      "Epoch 14, Batch 621, Test Loss: 0.3987880349159241\n",
      "Epoch 14, Batch 622, Test Loss: 0.43696919083595276\n",
      "Epoch 14, Batch 623, Test Loss: 0.486794114112854\n",
      "Epoch 14, Batch 624, Test Loss: 0.20902785658836365\n",
      "Epoch 14, Batch 625, Test Loss: 0.3618098199367523\n",
      "Epoch 14, Batch 626, Test Loss: 0.5149234533309937\n",
      "Epoch 14, Batch 627, Test Loss: 0.4396927058696747\n",
      "Epoch 14, Batch 628, Test Loss: 0.7170419692993164\n",
      "Epoch 14, Batch 629, Test Loss: 0.684166431427002\n",
      "Epoch 14, Batch 630, Test Loss: 0.44781291484832764\n",
      "Epoch 14, Batch 631, Test Loss: 0.35027453303337097\n",
      "Epoch 14, Batch 632, Test Loss: 0.4188115894794464\n",
      "Epoch 14, Batch 633, Test Loss: 0.46461251378059387\n",
      "Epoch 14, Batch 634, Test Loss: 0.27431291341781616\n",
      "Epoch 14, Batch 635, Test Loss: 0.33276990056037903\n",
      "Epoch 14, Batch 636, Test Loss: 0.4791196286678314\n",
      "Epoch 14, Batch 637, Test Loss: 0.6008341908454895\n",
      "Epoch 14, Batch 638, Test Loss: 0.43151819705963135\n",
      "Epoch 14, Batch 639, Test Loss: 0.3571747839450836\n",
      "Epoch 14, Batch 640, Test Loss: 0.4466709792613983\n",
      "Epoch 14, Batch 641, Test Loss: 0.4142613708972931\n",
      "Epoch 14, Batch 642, Test Loss: 0.33722880482673645\n",
      "Epoch 14, Batch 643, Test Loss: 0.3916322886943817\n",
      "Epoch 14, Batch 644, Test Loss: 0.6524777412414551\n",
      "Epoch 14, Batch 645, Test Loss: 0.5079659819602966\n",
      "Epoch 14, Batch 646, Test Loss: 0.5109429359436035\n",
      "Epoch 14, Batch 647, Test Loss: 0.5302693247795105\n",
      "Epoch 14, Batch 648, Test Loss: 0.6568922400474548\n",
      "Epoch 14, Batch 649, Test Loss: 0.3810921311378479\n",
      "Epoch 14, Batch 650, Test Loss: 0.5005438327789307\n",
      "Epoch 14, Batch 651, Test Loss: 0.4512949287891388\n",
      "Epoch 14, Batch 652, Test Loss: 0.39163070917129517\n",
      "Epoch 14, Batch 653, Test Loss: 0.392470121383667\n",
      "Epoch 14, Batch 654, Test Loss: 0.43470096588134766\n",
      "Epoch 14, Batch 655, Test Loss: 0.5974817872047424\n",
      "Epoch 14, Batch 656, Test Loss: 0.3651182949542999\n",
      "Epoch 14, Batch 657, Test Loss: 0.6604426503181458\n",
      "Epoch 14, Batch 658, Test Loss: 0.5891950130462646\n",
      "Epoch 14, Batch 659, Test Loss: 0.5989173650741577\n",
      "Epoch 14, Batch 660, Test Loss: 0.30079907178878784\n",
      "Epoch 14, Batch 661, Test Loss: 0.4437345862388611\n",
      "Epoch 14, Batch 662, Test Loss: 0.5728374123573303\n",
      "Epoch 14, Batch 663, Test Loss: 0.37772026658058167\n",
      "Epoch 14, Batch 664, Test Loss: 0.5632087588310242\n",
      "Epoch 14, Batch 665, Test Loss: 0.46486568450927734\n",
      "Epoch 14, Batch 666, Test Loss: 0.2908991277217865\n",
      "Epoch 14, Batch 667, Test Loss: 0.2764436900615692\n",
      "Epoch 14, Batch 668, Test Loss: 0.3283323049545288\n",
      "Epoch 14, Batch 669, Test Loss: 0.4995746612548828\n",
      "Epoch 14, Batch 670, Test Loss: 0.7410147786140442\n",
      "Epoch 14, Batch 671, Test Loss: 0.3795534670352936\n",
      "Epoch 14, Batch 672, Test Loss: 0.49548888206481934\n",
      "Epoch 14, Batch 673, Test Loss: 0.4519432485103607\n",
      "Epoch 14, Batch 674, Test Loss: 0.6420398950576782\n",
      "Epoch 14, Batch 675, Test Loss: 0.3505404591560364\n",
      "Epoch 14, Batch 676, Test Loss: 0.5340702533721924\n",
      "Epoch 14, Batch 677, Test Loss: 0.5494470596313477\n",
      "Epoch 14, Batch 678, Test Loss: 0.32251325249671936\n",
      "Epoch 14, Batch 679, Test Loss: 0.4877997636795044\n",
      "Epoch 14, Batch 680, Test Loss: 0.4901934862136841\n",
      "Epoch 14, Batch 681, Test Loss: 0.5207443237304688\n",
      "Epoch 14, Batch 682, Test Loss: 0.4177567958831787\n",
      "Epoch 14, Batch 683, Test Loss: 0.6060853600502014\n",
      "Epoch 14, Batch 684, Test Loss: 0.5266462564468384\n",
      "Epoch 14, Batch 685, Test Loss: 0.2839921712875366\n",
      "Epoch 14, Batch 686, Test Loss: 0.5259766578674316\n",
      "Epoch 14, Batch 687, Test Loss: 0.3725239634513855\n",
      "Epoch 14, Batch 688, Test Loss: 0.3212429881095886\n",
      "Epoch 14, Batch 689, Test Loss: 0.572649359703064\n",
      "Epoch 14, Batch 690, Test Loss: 0.4521106481552124\n",
      "Epoch 14, Batch 691, Test Loss: 0.506081223487854\n",
      "Epoch 14, Batch 692, Test Loss: 0.478055477142334\n",
      "Epoch 14, Batch 693, Test Loss: 0.5475980639457703\n",
      "Epoch 14, Batch 694, Test Loss: 0.25058555603027344\n",
      "Epoch 14, Batch 695, Test Loss: 0.33240288496017456\n",
      "Epoch 14, Batch 696, Test Loss: 0.364330917596817\n",
      "Epoch 14, Batch 697, Test Loss: 0.5961092710494995\n",
      "Epoch 14, Batch 698, Test Loss: 0.3517240285873413\n",
      "Epoch 14, Batch 699, Test Loss: 0.3542158007621765\n",
      "Epoch 14, Batch 700, Test Loss: 0.5217275023460388\n",
      "Epoch 14, Batch 701, Test Loss: 0.5149572491645813\n",
      "Epoch 14, Batch 702, Test Loss: 0.36046504974365234\n",
      "Epoch 14, Batch 703, Test Loss: 0.3452804982662201\n",
      "Epoch 14, Batch 704, Test Loss: 0.4641452431678772\n",
      "Epoch 14, Batch 705, Test Loss: 0.5604374408721924\n",
      "Epoch 14, Batch 706, Test Loss: 0.41595473885536194\n",
      "Epoch 14, Batch 707, Test Loss: 0.4760878086090088\n",
      "Epoch 14, Batch 708, Test Loss: 0.4050844609737396\n",
      "Epoch 14, Batch 709, Test Loss: 0.36887234449386597\n",
      "Epoch 14, Batch 710, Test Loss: 0.46712666749954224\n",
      "Epoch 14, Batch 711, Test Loss: 0.483096718788147\n",
      "Epoch 14, Batch 712, Test Loss: 0.5073963403701782\n",
      "Epoch 14, Batch 713, Test Loss: 0.4246862828731537\n",
      "Epoch 14, Batch 714, Test Loss: 0.5050305128097534\n",
      "Epoch 14, Batch 715, Test Loss: 0.355008989572525\n",
      "Epoch 14, Batch 716, Test Loss: 0.43299609422683716\n",
      "Epoch 14, Batch 717, Test Loss: 0.5369603037834167\n",
      "Epoch 14, Batch 718, Test Loss: 0.48547929525375366\n",
      "Epoch 14, Batch 719, Test Loss: 0.41751813888549805\n",
      "Epoch 14, Batch 720, Test Loss: 0.3836870789527893\n",
      "Epoch 14, Batch 721, Test Loss: 0.34064698219299316\n",
      "Epoch 14, Batch 722, Test Loss: 0.4623970091342926\n",
      "Epoch 14, Batch 723, Test Loss: 0.4205540418624878\n",
      "Epoch 14, Batch 724, Test Loss: 0.49206918478012085\n",
      "Epoch 14, Batch 725, Test Loss: 0.46295636892318726\n",
      "Epoch 14, Batch 726, Test Loss: 0.4635062515735626\n",
      "Epoch 14, Batch 727, Test Loss: 0.36930084228515625\n",
      "Epoch 14, Batch 728, Test Loss: 0.4690280854701996\n",
      "Epoch 14, Batch 729, Test Loss: 0.5115000605583191\n",
      "Epoch 14, Batch 730, Test Loss: 0.334031879901886\n",
      "Epoch 14, Batch 731, Test Loss: 0.31169918179512024\n",
      "Epoch 14, Batch 732, Test Loss: 0.3352641463279724\n",
      "Epoch 14, Batch 733, Test Loss: 0.353639155626297\n",
      "Epoch 14, Batch 734, Test Loss: 0.533052921295166\n",
      "Epoch 14, Batch 735, Test Loss: 0.49883657693862915\n",
      "Epoch 14, Batch 736, Test Loss: 0.39840200543403625\n",
      "Epoch 14, Batch 737, Test Loss: 0.5046270489692688\n",
      "Epoch 14, Batch 738, Test Loss: 0.4861753582954407\n",
      "Epoch 14, Batch 739, Test Loss: 0.44609448313713074\n",
      "Epoch 14, Batch 740, Test Loss: 0.3773340582847595\n",
      "Epoch 14, Batch 741, Test Loss: 0.5154339075088501\n",
      "Epoch 14, Batch 742, Test Loss: 0.3965836465358734\n",
      "Epoch 14, Batch 743, Test Loss: 0.5646606087684631\n",
      "Epoch 14, Batch 744, Test Loss: 0.48797452449798584\n",
      "Epoch 14, Batch 745, Test Loss: 0.45054593682289124\n",
      "Epoch 14, Batch 746, Test Loss: 0.4065224826335907\n",
      "Epoch 14, Batch 747, Test Loss: 0.6323062181472778\n",
      "Epoch 14, Batch 748, Test Loss: 0.4011791944503784\n",
      "Epoch 14, Batch 749, Test Loss: 0.39163321256637573\n",
      "Epoch 14, Batch 750, Test Loss: 0.3694019913673401\n",
      "Epoch 14, Batch 751, Test Loss: 0.5527050495147705\n",
      "Epoch 14, Batch 752, Test Loss: 0.3164975941181183\n",
      "Epoch 14, Batch 753, Test Loss: 0.4166526794433594\n",
      "Epoch 14, Batch 754, Test Loss: 0.4905014634132385\n",
      "Epoch 14, Batch 755, Test Loss: 0.49302273988723755\n",
      "Epoch 14, Batch 756, Test Loss: 0.548009991645813\n",
      "Epoch 14, Batch 757, Test Loss: 0.44665050506591797\n",
      "Epoch 14, Batch 758, Test Loss: 0.5713093876838684\n",
      "Epoch 14, Batch 759, Test Loss: 0.4007028639316559\n",
      "Epoch 14, Batch 760, Test Loss: 0.6412268280982971\n",
      "Epoch 14, Batch 761, Test Loss: 0.5827308297157288\n",
      "Epoch 14, Batch 762, Test Loss: 0.4539675712585449\n",
      "Epoch 14, Batch 763, Test Loss: 0.39104411005973816\n",
      "Epoch 14, Batch 764, Test Loss: 0.268863320350647\n",
      "Epoch 14, Batch 765, Test Loss: 0.5420993566513062\n",
      "Epoch 14, Batch 766, Test Loss: 0.3836117684841156\n",
      "Epoch 14, Batch 767, Test Loss: 0.4887542128562927\n",
      "Epoch 14, Batch 768, Test Loss: 0.518377959728241\n",
      "Epoch 14, Batch 769, Test Loss: 0.4592934250831604\n",
      "Epoch 14, Batch 770, Test Loss: 0.5526570677757263\n",
      "Epoch 14, Batch 771, Test Loss: 0.48473304510116577\n",
      "Epoch 14, Batch 772, Test Loss: 0.44892409443855286\n",
      "Epoch 14, Batch 773, Test Loss: 0.4873605966567993\n",
      "Epoch 14, Batch 774, Test Loss: 0.26107949018478394\n",
      "Epoch 14, Batch 775, Test Loss: 0.3963715732097626\n",
      "Epoch 14, Batch 776, Test Loss: 0.37841179966926575\n",
      "Epoch 14, Batch 777, Test Loss: 0.4401625394821167\n",
      "Epoch 14, Batch 778, Test Loss: 0.6744900941848755\n",
      "Epoch 14, Batch 779, Test Loss: 0.45672252774238586\n",
      "Epoch 14, Batch 780, Test Loss: 0.7312008142471313\n",
      "Epoch 14, Batch 781, Test Loss: 0.43261682987213135\n",
      "Epoch 14, Batch 782, Test Loss: 0.4740995168685913\n",
      "Epoch 14, Batch 783, Test Loss: 0.5050222873687744\n",
      "Epoch 14, Batch 784, Test Loss: 0.6063469052314758\n",
      "Epoch 14, Batch 785, Test Loss: 0.581850528717041\n",
      "Epoch 14, Batch 786, Test Loss: 0.4597574770450592\n",
      "Epoch 14, Batch 787, Test Loss: 0.7916001677513123\n",
      "Epoch 14, Batch 788, Test Loss: 0.5455462336540222\n",
      "Epoch 14, Batch 789, Test Loss: 0.6353831887245178\n",
      "Epoch 14, Batch 790, Test Loss: 0.47650593519210815\n",
      "Epoch 14, Batch 791, Test Loss: 0.46450603008270264\n",
      "Epoch 14, Batch 792, Test Loss: 0.5069347023963928\n",
      "Epoch 14, Batch 793, Test Loss: 0.3819049596786499\n",
      "Epoch 14, Batch 794, Test Loss: 0.573306679725647\n",
      "Epoch 14, Batch 795, Test Loss: 0.3144204616546631\n",
      "Epoch 14, Batch 796, Test Loss: 0.41679325699806213\n",
      "Epoch 14, Batch 797, Test Loss: 0.6400169134140015\n",
      "Epoch 14, Batch 798, Test Loss: 0.377597838640213\n",
      "Epoch 14, Batch 799, Test Loss: 0.4401233196258545\n",
      "Epoch 14, Batch 800, Test Loss: 0.56836998462677\n",
      "Epoch 14, Batch 801, Test Loss: 0.5007513165473938\n",
      "Epoch 14, Batch 802, Test Loss: 0.38200873136520386\n",
      "Epoch 14, Batch 803, Test Loss: 0.675879180431366\n",
      "Epoch 14, Batch 804, Test Loss: 0.5945916771888733\n",
      "Epoch 14, Batch 805, Test Loss: 0.3952449858188629\n",
      "Epoch 14, Batch 806, Test Loss: 0.5367186665534973\n",
      "Epoch 14, Batch 807, Test Loss: 0.3921564221382141\n",
      "Epoch 14, Batch 808, Test Loss: 0.4995371401309967\n",
      "Epoch 14, Batch 809, Test Loss: 0.573415994644165\n",
      "Epoch 14, Batch 810, Test Loss: 0.4135669767856598\n",
      "Epoch 14, Batch 811, Test Loss: 0.49034130573272705\n",
      "Epoch 14, Batch 812, Test Loss: 0.3318045139312744\n",
      "Epoch 14, Batch 813, Test Loss: 0.33744174242019653\n",
      "Epoch 14, Batch 814, Test Loss: 0.4911215901374817\n",
      "Epoch 14, Batch 815, Test Loss: 0.6648088693618774\n",
      "Epoch 14, Batch 816, Test Loss: 0.5324274897575378\n",
      "Epoch 14, Batch 817, Test Loss: 0.592272937297821\n",
      "Epoch 14, Batch 818, Test Loss: 0.35436487197875977\n",
      "Epoch 14, Batch 819, Test Loss: 0.3876935839653015\n",
      "Epoch 14, Batch 820, Test Loss: 0.47895902395248413\n",
      "Epoch 14, Batch 821, Test Loss: 0.42943140864372253\n",
      "Epoch 14, Batch 822, Test Loss: 0.3618387281894684\n",
      "Epoch 14, Batch 823, Test Loss: 0.4458279013633728\n",
      "Epoch 14, Batch 824, Test Loss: 0.2920892536640167\n",
      "Epoch 14, Batch 825, Test Loss: 0.6117572784423828\n",
      "Epoch 14, Batch 826, Test Loss: 0.5614960193634033\n",
      "Epoch 14, Batch 827, Test Loss: 0.522635817527771\n",
      "Epoch 14, Batch 828, Test Loss: 0.39458033442497253\n",
      "Epoch 14, Batch 829, Test Loss: 0.4889329671859741\n",
      "Epoch 14, Batch 830, Test Loss: 0.3558318614959717\n",
      "Epoch 14, Batch 831, Test Loss: 0.2918975055217743\n",
      "Epoch 14, Batch 832, Test Loss: 0.326602578163147\n",
      "Epoch 14, Batch 833, Test Loss: 0.35446685552597046\n",
      "Epoch 14, Batch 834, Test Loss: 0.38019275665283203\n",
      "Epoch 14, Batch 835, Test Loss: 0.3828270137310028\n",
      "Epoch 14, Batch 836, Test Loss: 0.6019887328147888\n",
      "Epoch 14, Batch 837, Test Loss: 0.3642290532588959\n",
      "Epoch 14, Batch 838, Test Loss: 0.47794970870018005\n",
      "Epoch 14, Batch 839, Test Loss: 0.41687047481536865\n",
      "Epoch 14, Batch 840, Test Loss: 0.30801570415496826\n",
      "Epoch 14, Batch 841, Test Loss: 0.37373000383377075\n",
      "Epoch 14, Batch 842, Test Loss: 0.3596782386302948\n",
      "Epoch 14, Batch 843, Test Loss: 0.6620816588401794\n",
      "Epoch 14, Batch 844, Test Loss: 0.3962279260158539\n",
      "Epoch 14, Batch 845, Test Loss: 0.4936756193637848\n",
      "Epoch 14, Batch 846, Test Loss: 0.5564301013946533\n",
      "Epoch 14, Batch 847, Test Loss: 0.5680858492851257\n",
      "Epoch 14, Batch 848, Test Loss: 0.42876678705215454\n",
      "Epoch 14, Batch 849, Test Loss: 0.32268407940864563\n",
      "Epoch 14, Batch 850, Test Loss: 0.3691554367542267\n",
      "Epoch 14, Batch 851, Test Loss: 0.4476345181465149\n",
      "Epoch 14, Batch 852, Test Loss: 0.7264833450317383\n",
      "Epoch 14, Batch 853, Test Loss: 0.4345388412475586\n",
      "Epoch 14, Batch 854, Test Loss: 0.712680995464325\n",
      "Epoch 14, Batch 855, Test Loss: 0.36676713824272156\n",
      "Epoch 14, Batch 856, Test Loss: 0.3665878176689148\n",
      "Epoch 14, Batch 857, Test Loss: 0.39746299386024475\n",
      "Epoch 14, Batch 858, Test Loss: 0.3105139136314392\n",
      "Epoch 14, Batch 859, Test Loss: 0.2983594536781311\n",
      "Epoch 14, Batch 860, Test Loss: 0.5364444851875305\n",
      "Epoch 14, Batch 861, Test Loss: 0.48865291476249695\n",
      "Epoch 14, Batch 862, Test Loss: 0.2587994933128357\n",
      "Epoch 14, Batch 863, Test Loss: 0.6254457831382751\n",
      "Epoch 14, Batch 864, Test Loss: 0.46142005920410156\n",
      "Epoch 14, Batch 865, Test Loss: 0.5830874443054199\n",
      "Epoch 14, Batch 866, Test Loss: 0.42607030272483826\n",
      "Epoch 14, Batch 867, Test Loss: 0.7611216902732849\n",
      "Epoch 14, Batch 868, Test Loss: 0.3632833659648895\n",
      "Epoch 14, Batch 869, Test Loss: 0.3251665234565735\n",
      "Epoch 14, Batch 870, Test Loss: 0.4911506474018097\n",
      "Epoch 14, Batch 871, Test Loss: 0.5278134942054749\n",
      "Epoch 14, Batch 872, Test Loss: 0.6249288320541382\n",
      "Epoch 14, Batch 873, Test Loss: 0.4521215260028839\n",
      "Epoch 14, Batch 874, Test Loss: 0.413407564163208\n",
      "Epoch 14, Batch 875, Test Loss: 0.3693331182003021\n",
      "Epoch 14, Batch 876, Test Loss: 0.44548484683036804\n",
      "Epoch 14, Batch 877, Test Loss: 0.4207383990287781\n",
      "Epoch 14, Batch 878, Test Loss: 0.5852030515670776\n",
      "Epoch 14, Batch 879, Test Loss: 0.44654837250709534\n",
      "Epoch 14, Batch 880, Test Loss: 0.6933620572090149\n",
      "Epoch 14, Batch 881, Test Loss: 0.37742185592651367\n",
      "Epoch 14, Batch 882, Test Loss: 0.5480678677558899\n",
      "Epoch 14, Batch 883, Test Loss: 0.5667376518249512\n",
      "Epoch 14, Batch 884, Test Loss: 0.6057068109512329\n",
      "Epoch 14, Batch 885, Test Loss: 0.5909624695777893\n",
      "Epoch 14, Batch 886, Test Loss: 0.4410383999347687\n",
      "Epoch 14, Batch 887, Test Loss: 0.5815582275390625\n",
      "Epoch 14, Batch 888, Test Loss: 0.462937593460083\n",
      "Epoch 14, Batch 889, Test Loss: 0.47330978512763977\n",
      "Epoch 14, Batch 890, Test Loss: 0.4586808681488037\n",
      "Epoch 14, Batch 891, Test Loss: 0.40193039178848267\n",
      "Epoch 14, Batch 892, Test Loss: 0.5534455180168152\n",
      "Epoch 14, Batch 893, Test Loss: 0.5813149213790894\n",
      "Epoch 14, Batch 894, Test Loss: 0.4112391769886017\n",
      "Epoch 14, Batch 895, Test Loss: 0.5168176889419556\n",
      "Epoch 14, Batch 896, Test Loss: 0.4339592754840851\n",
      "Epoch 14, Batch 897, Test Loss: 0.48150888085365295\n",
      "Epoch 14, Batch 898, Test Loss: 0.3981495201587677\n",
      "Epoch 14, Batch 899, Test Loss: 0.37657153606414795\n",
      "Epoch 14, Batch 900, Test Loss: 0.3694399297237396\n",
      "Epoch 14, Batch 901, Test Loss: 0.5500989556312561\n",
      "Epoch 14, Batch 902, Test Loss: 0.4365937113761902\n",
      "Epoch 14, Batch 903, Test Loss: 0.4261120557785034\n",
      "Epoch 14, Batch 904, Test Loss: 0.28872594237327576\n",
      "Epoch 14, Batch 905, Test Loss: 0.3505398631095886\n",
      "Epoch 14, Batch 906, Test Loss: 0.3152994215488434\n",
      "Epoch 14, Batch 907, Test Loss: 0.5731129050254822\n",
      "Epoch 14, Batch 908, Test Loss: 0.22953082621097565\n",
      "Epoch 14, Batch 909, Test Loss: 0.5865360498428345\n",
      "Epoch 14, Batch 910, Test Loss: 0.4836673438549042\n",
      "Epoch 14, Batch 911, Test Loss: 0.5520610213279724\n",
      "Epoch 14, Batch 912, Test Loss: 0.35775822401046753\n",
      "Epoch 14, Batch 913, Test Loss: 0.2107134312391281\n",
      "Epoch 14, Batch 914, Test Loss: 0.5192047357559204\n",
      "Epoch 14, Batch 915, Test Loss: 0.3636721968650818\n",
      "Epoch 14, Batch 916, Test Loss: 0.433946818113327\n",
      "Epoch 14, Batch 917, Test Loss: 0.5551289319992065\n",
      "Epoch 14, Batch 918, Test Loss: 0.3581904172897339\n",
      "Epoch 14, Batch 919, Test Loss: 0.35915982723236084\n",
      "Epoch 14, Batch 920, Test Loss: 0.41254886984825134\n",
      "Epoch 14, Batch 921, Test Loss: 0.4048159420490265\n",
      "Epoch 14, Batch 922, Test Loss: 0.4505167603492737\n",
      "Epoch 14, Batch 923, Test Loss: 0.3510054647922516\n",
      "Epoch 14, Batch 924, Test Loss: 0.5165128707885742\n",
      "Epoch 14, Batch 925, Test Loss: 0.5761788487434387\n",
      "Epoch 14, Batch 926, Test Loss: 0.530877947807312\n",
      "Epoch 14, Batch 927, Test Loss: 0.47365304827690125\n",
      "Epoch 14, Batch 928, Test Loss: 0.4683314859867096\n",
      "Epoch 14, Batch 929, Test Loss: 0.41808241605758667\n",
      "Epoch 14, Batch 930, Test Loss: 0.3592933118343353\n",
      "Epoch 14, Batch 931, Test Loss: 0.35064125061035156\n",
      "Epoch 14, Batch 932, Test Loss: 0.5285677313804626\n",
      "Epoch 14, Batch 933, Test Loss: 0.4861568808555603\n",
      "Epoch 14, Batch 934, Test Loss: 0.49238401651382446\n",
      "Epoch 14, Batch 935, Test Loss: 0.3910738527774811\n",
      "Epoch 14, Batch 936, Test Loss: 0.3863386809825897\n",
      "Epoch 14, Batch 937, Test Loss: 0.6187894344329834\n",
      "Epoch 14, Batch 938, Test Loss: 0.2833029329776764\n",
      "Accuracy of Test set: 0.8385\n",
      "Epoch 15, Batch 1, Loss: 0.3915082812309265\n",
      "Epoch 15, Batch 2, Loss: 0.33151498436927795\n",
      "Epoch 15, Batch 3, Loss: 0.4106557071208954\n",
      "Epoch 15, Batch 4, Loss: 0.39449983835220337\n",
      "Epoch 15, Batch 5, Loss: 0.5055128335952759\n",
      "Epoch 15, Batch 6, Loss: 0.5135504007339478\n",
      "Epoch 15, Batch 7, Loss: 0.5768707394599915\n",
      "Epoch 15, Batch 8, Loss: 0.7073419094085693\n",
      "Epoch 15, Batch 9, Loss: 0.485897958278656\n",
      "Epoch 15, Batch 10, Loss: 0.3231468200683594\n",
      "Epoch 15, Batch 11, Loss: 0.4274158775806427\n",
      "Epoch 15, Batch 12, Loss: 0.42619502544403076\n",
      "Epoch 15, Batch 13, Loss: 0.3635038733482361\n",
      "Epoch 15, Batch 14, Loss: 0.46098315715789795\n",
      "Epoch 15, Batch 15, Loss: 0.5509862303733826\n",
      "Epoch 15, Batch 16, Loss: 0.4968930780887604\n",
      "Epoch 15, Batch 17, Loss: 0.5019291639328003\n",
      "Epoch 15, Batch 18, Loss: 0.34514594078063965\n",
      "Epoch 15, Batch 19, Loss: 0.4109151363372803\n",
      "Epoch 15, Batch 20, Loss: 0.3793303966522217\n",
      "Epoch 15, Batch 21, Loss: 0.6225241422653198\n",
      "Epoch 15, Batch 22, Loss: 0.3644564151763916\n",
      "Epoch 15, Batch 23, Loss: 0.5030308961868286\n",
      "Epoch 15, Batch 24, Loss: 0.37831443548202515\n",
      "Epoch 15, Batch 25, Loss: 0.4260070323944092\n",
      "Epoch 15, Batch 26, Loss: 0.5086694955825806\n",
      "Epoch 15, Batch 27, Loss: 0.4819771647453308\n",
      "Epoch 15, Batch 28, Loss: 0.7910300493240356\n",
      "Epoch 15, Batch 29, Loss: 0.36728203296661377\n",
      "Epoch 15, Batch 30, Loss: 0.5223316550254822\n",
      "Epoch 15, Batch 31, Loss: 0.5535786747932434\n",
      "Epoch 15, Batch 32, Loss: 0.5692128539085388\n",
      "Epoch 15, Batch 33, Loss: 0.6690145134925842\n",
      "Epoch 15, Batch 34, Loss: 0.30884718894958496\n",
      "Epoch 15, Batch 35, Loss: 0.5583031177520752\n",
      "Epoch 15, Batch 36, Loss: 0.4997400939464569\n",
      "Epoch 15, Batch 37, Loss: 0.5215411186218262\n",
      "Epoch 15, Batch 38, Loss: 0.4452905058860779\n",
      "Epoch 15, Batch 39, Loss: 0.4216214418411255\n",
      "Epoch 15, Batch 40, Loss: 0.3444007635116577\n",
      "Epoch 15, Batch 41, Loss: 0.6008725166320801\n",
      "Epoch 15, Batch 42, Loss: 0.410334050655365\n",
      "Epoch 15, Batch 43, Loss: 0.35496437549591064\n",
      "Epoch 15, Batch 44, Loss: 0.40222421288490295\n",
      "Epoch 15, Batch 45, Loss: 0.35032418370246887\n",
      "Epoch 15, Batch 46, Loss: 0.35759374499320984\n",
      "Epoch 15, Batch 47, Loss: 0.3738914728164673\n",
      "Epoch 15, Batch 48, Loss: 0.4389514625072479\n",
      "Epoch 15, Batch 49, Loss: 0.29568496346473694\n",
      "Epoch 15, Batch 50, Loss: 0.3694898188114166\n",
      "Epoch 15, Batch 51, Loss: 0.40153127908706665\n",
      "Epoch 15, Batch 52, Loss: 0.6633259057998657\n",
      "Epoch 15, Batch 53, Loss: 0.400030255317688\n",
      "Epoch 15, Batch 54, Loss: 0.3523876667022705\n",
      "Epoch 15, Batch 55, Loss: 0.531074583530426\n",
      "Epoch 15, Batch 56, Loss: 0.4349248707294464\n",
      "Epoch 15, Batch 57, Loss: 0.5681889057159424\n",
      "Epoch 15, Batch 58, Loss: 0.6282007694244385\n",
      "Epoch 15, Batch 59, Loss: 0.5069751143455505\n",
      "Epoch 15, Batch 60, Loss: 0.5418765544891357\n",
      "Epoch 15, Batch 61, Loss: 0.5771471261978149\n",
      "Epoch 15, Batch 62, Loss: 0.6505931615829468\n",
      "Epoch 15, Batch 63, Loss: 0.8074713945388794\n",
      "Epoch 15, Batch 64, Loss: 0.5060409307479858\n",
      "Epoch 15, Batch 65, Loss: 0.46017301082611084\n",
      "Epoch 15, Batch 66, Loss: 0.3532310724258423\n",
      "Epoch 15, Batch 67, Loss: 0.33670106530189514\n",
      "Epoch 15, Batch 68, Loss: 0.603900134563446\n",
      "Epoch 15, Batch 69, Loss: 0.5004405975341797\n",
      "Epoch 15, Batch 70, Loss: 0.46947938203811646\n",
      "Epoch 15, Batch 71, Loss: 0.4874769449234009\n",
      "Epoch 15, Batch 72, Loss: 0.5882437825202942\n",
      "Epoch 15, Batch 73, Loss: 0.5567866563796997\n",
      "Epoch 15, Batch 74, Loss: 0.39003047347068787\n",
      "Epoch 15, Batch 75, Loss: 0.3367100954055786\n",
      "Epoch 15, Batch 76, Loss: 0.43926307559013367\n",
      "Epoch 15, Batch 77, Loss: 0.5402613878250122\n",
      "Epoch 15, Batch 78, Loss: 0.4942946434020996\n",
      "Epoch 15, Batch 79, Loss: 0.5445230603218079\n",
      "Epoch 15, Batch 80, Loss: 0.7553465962409973\n",
      "Epoch 15, Batch 81, Loss: 0.5119285583496094\n",
      "Epoch 15, Batch 82, Loss: 0.6422927975654602\n",
      "Epoch 15, Batch 83, Loss: 0.5456715822219849\n",
      "Epoch 15, Batch 84, Loss: 0.5266656875610352\n",
      "Epoch 15, Batch 85, Loss: 0.5326071381568909\n",
      "Epoch 15, Batch 86, Loss: 0.6065722703933716\n",
      "Epoch 15, Batch 87, Loss: 0.6756300330162048\n",
      "Epoch 15, Batch 88, Loss: 0.47440701723098755\n",
      "Epoch 15, Batch 89, Loss: 0.6529572010040283\n",
      "Epoch 15, Batch 90, Loss: 0.4766358733177185\n",
      "Epoch 15, Batch 91, Loss: 0.5285450220108032\n",
      "Epoch 15, Batch 92, Loss: 0.3624652028083801\n",
      "Epoch 15, Batch 93, Loss: 0.3967084288597107\n",
      "Epoch 15, Batch 94, Loss: 0.48895251750946045\n",
      "Epoch 15, Batch 95, Loss: 0.41539692878723145\n",
      "Epoch 15, Batch 96, Loss: 0.4676228165626526\n",
      "Epoch 15, Batch 97, Loss: 0.48631715774536133\n",
      "Epoch 15, Batch 98, Loss: 0.4453178942203522\n",
      "Epoch 15, Batch 99, Loss: 0.4811512231826782\n",
      "Epoch 15, Batch 100, Loss: 0.5755838751792908\n",
      "Epoch 15, Batch 101, Loss: 0.6117693185806274\n",
      "Epoch 15, Batch 102, Loss: 0.4164818227291107\n",
      "Epoch 15, Batch 103, Loss: 0.377628356218338\n",
      "Epoch 15, Batch 104, Loss: 0.4851519465446472\n",
      "Epoch 15, Batch 105, Loss: 0.5998106598854065\n",
      "Epoch 15, Batch 106, Loss: 0.5981793999671936\n",
      "Epoch 15, Batch 107, Loss: 0.7598852515220642\n",
      "Epoch 15, Batch 108, Loss: 0.5010679364204407\n",
      "Epoch 15, Batch 109, Loss: 0.5378555059432983\n",
      "Epoch 15, Batch 110, Loss: 0.5385869145393372\n",
      "Epoch 15, Batch 111, Loss: 0.6298951506614685\n",
      "Epoch 15, Batch 112, Loss: 0.5106513500213623\n",
      "Epoch 15, Batch 113, Loss: 0.5822263360023499\n",
      "Epoch 15, Batch 114, Loss: 0.4726644456386566\n",
      "Epoch 15, Batch 115, Loss: 0.38870006799697876\n",
      "Epoch 15, Batch 116, Loss: 0.588308572769165\n",
      "Epoch 15, Batch 117, Loss: 0.3010035455226898\n",
      "Epoch 15, Batch 118, Loss: 0.5642768144607544\n",
      "Epoch 15, Batch 119, Loss: 0.39789634943008423\n",
      "Epoch 15, Batch 120, Loss: 0.43276023864746094\n",
      "Epoch 15, Batch 121, Loss: 0.5080868005752563\n",
      "Epoch 15, Batch 122, Loss: 0.4763016104698181\n",
      "Epoch 15, Batch 123, Loss: 0.5433300137519836\n",
      "Epoch 15, Batch 124, Loss: 0.5734823942184448\n",
      "Epoch 15, Batch 125, Loss: 0.4039178788661957\n",
      "Epoch 15, Batch 126, Loss: 0.5686600804328918\n",
      "Epoch 15, Batch 127, Loss: 0.36004748940467834\n",
      "Epoch 15, Batch 128, Loss: 0.562612771987915\n",
      "Epoch 15, Batch 129, Loss: 0.39153191447257996\n",
      "Epoch 15, Batch 130, Loss: 0.29011690616607666\n",
      "Epoch 15, Batch 131, Loss: 0.41666626930236816\n",
      "Epoch 15, Batch 132, Loss: 0.35905721783638\n",
      "Epoch 15, Batch 133, Loss: 0.33286914229393005\n",
      "Epoch 15, Batch 134, Loss: 0.33332017064094543\n",
      "Epoch 15, Batch 135, Loss: 0.4126824140548706\n",
      "Epoch 15, Batch 136, Loss: 0.26349496841430664\n",
      "Epoch 15, Batch 137, Loss: 0.34275248646736145\n",
      "Epoch 15, Batch 138, Loss: 0.6408379077911377\n",
      "Epoch 15, Batch 139, Loss: 0.36866626143455505\n",
      "Epoch 15, Batch 140, Loss: 0.5621628761291504\n",
      "Epoch 15, Batch 141, Loss: 0.4573579728603363\n",
      "Epoch 15, Batch 142, Loss: 0.5996987819671631\n",
      "Epoch 15, Batch 143, Loss: 0.5932761430740356\n",
      "Epoch 15, Batch 144, Loss: 0.2930583953857422\n",
      "Epoch 15, Batch 145, Loss: 0.6116712093353271\n",
      "Epoch 15, Batch 146, Loss: 0.5966314077377319\n",
      "Epoch 15, Batch 147, Loss: 0.49442368745803833\n",
      "Epoch 15, Batch 148, Loss: 0.41399210691452026\n",
      "Epoch 15, Batch 149, Loss: 0.5690126419067383\n",
      "Epoch 15, Batch 150, Loss: 0.3880705237388611\n",
      "Epoch 15, Batch 151, Loss: 0.46618130803108215\n",
      "Epoch 15, Batch 152, Loss: 0.6242914795875549\n",
      "Epoch 15, Batch 153, Loss: 0.43290773034095764\n",
      "Epoch 15, Batch 154, Loss: 0.5422850251197815\n",
      "Epoch 15, Batch 155, Loss: 0.5336042046546936\n",
      "Epoch 15, Batch 156, Loss: 0.4238036274909973\n",
      "Epoch 15, Batch 157, Loss: 0.5207654237747192\n",
      "Epoch 15, Batch 158, Loss: 0.3224495053291321\n",
      "Epoch 15, Batch 159, Loss: 0.2654707729816437\n",
      "Epoch 15, Batch 160, Loss: 0.5349198579788208\n",
      "Epoch 15, Batch 161, Loss: 0.2587728500366211\n",
      "Epoch 15, Batch 162, Loss: 0.3756478428840637\n",
      "Epoch 15, Batch 163, Loss: 0.37278029322624207\n",
      "Epoch 15, Batch 164, Loss: 0.24406307935714722\n",
      "Epoch 15, Batch 165, Loss: 0.43503814935684204\n",
      "Epoch 15, Batch 166, Loss: 0.42970016598701477\n",
      "Epoch 15, Batch 167, Loss: 0.4661952555179596\n",
      "Epoch 15, Batch 168, Loss: 0.4948664605617523\n",
      "Epoch 15, Batch 169, Loss: 0.5342586040496826\n",
      "Epoch 15, Batch 170, Loss: 0.8843924403190613\n",
      "Epoch 15, Batch 171, Loss: 0.468810498714447\n",
      "Epoch 15, Batch 172, Loss: 0.35346120595932007\n",
      "Epoch 15, Batch 173, Loss: 0.5421427488327026\n",
      "Epoch 15, Batch 174, Loss: 0.46527767181396484\n",
      "Epoch 15, Batch 175, Loss: 0.3549356758594513\n",
      "Epoch 15, Batch 176, Loss: 0.36344772577285767\n",
      "Epoch 15, Batch 177, Loss: 0.40078118443489075\n",
      "Epoch 15, Batch 178, Loss: 0.4690394401550293\n",
      "Epoch 15, Batch 179, Loss: 0.5202927589416504\n",
      "Epoch 15, Batch 180, Loss: 0.5028768181800842\n",
      "Epoch 15, Batch 181, Loss: 0.4671277701854706\n",
      "Epoch 15, Batch 182, Loss: 0.34812578558921814\n",
      "Epoch 15, Batch 183, Loss: 0.4544515311717987\n",
      "Epoch 15, Batch 184, Loss: 0.5387226939201355\n",
      "Epoch 15, Batch 185, Loss: 0.4107396602630615\n",
      "Epoch 15, Batch 186, Loss: 0.3731304705142975\n",
      "Epoch 15, Batch 187, Loss: 0.5051565766334534\n",
      "Epoch 15, Batch 188, Loss: 0.5144407749176025\n",
      "Epoch 15, Batch 189, Loss: 0.6058311462402344\n",
      "Epoch 15, Batch 190, Loss: 0.3580043613910675\n",
      "Epoch 15, Batch 191, Loss: 0.2991624176502228\n",
      "Epoch 15, Batch 192, Loss: 0.5091785192489624\n",
      "Epoch 15, Batch 193, Loss: 0.3749605417251587\n",
      "Epoch 15, Batch 194, Loss: 0.4861016273498535\n",
      "Epoch 15, Batch 195, Loss: 0.3766297698020935\n",
      "Epoch 15, Batch 196, Loss: 0.6580906510353088\n",
      "Epoch 15, Batch 197, Loss: 0.3474709689617157\n",
      "Epoch 15, Batch 198, Loss: 0.6448183059692383\n",
      "Epoch 15, Batch 199, Loss: 0.522956907749176\n",
      "Epoch 15, Batch 200, Loss: 0.23106534779071808\n",
      "Epoch 15, Batch 201, Loss: 0.4315597712993622\n",
      "Epoch 15, Batch 202, Loss: 0.4592352509498596\n",
      "Epoch 15, Batch 203, Loss: 0.4477086663246155\n",
      "Epoch 15, Batch 204, Loss: 0.38104724884033203\n",
      "Epoch 15, Batch 205, Loss: 0.4582054913043976\n",
      "Epoch 15, Batch 206, Loss: 0.3718545436859131\n",
      "Epoch 15, Batch 207, Loss: 0.535268247127533\n",
      "Epoch 15, Batch 208, Loss: 0.3076097369194031\n",
      "Epoch 15, Batch 209, Loss: 0.5192550420761108\n",
      "Epoch 15, Batch 210, Loss: 0.774132251739502\n",
      "Epoch 15, Batch 211, Loss: 0.35666534304618835\n",
      "Epoch 15, Batch 212, Loss: 0.2916859984397888\n",
      "Epoch 15, Batch 213, Loss: 0.40518316626548767\n",
      "Epoch 15, Batch 214, Loss: 0.36675766110420227\n",
      "Epoch 15, Batch 215, Loss: 0.44509196281433105\n",
      "Epoch 15, Batch 216, Loss: 0.4999464452266693\n",
      "Epoch 15, Batch 217, Loss: 0.3834863007068634\n",
      "Epoch 15, Batch 218, Loss: 0.5600975751876831\n",
      "Epoch 15, Batch 219, Loss: 0.3431077301502228\n",
      "Epoch 15, Batch 220, Loss: 0.41964542865753174\n",
      "Epoch 15, Batch 221, Loss: 0.38887861371040344\n",
      "Epoch 15, Batch 222, Loss: 0.474761039018631\n",
      "Epoch 15, Batch 223, Loss: 0.4893665015697479\n",
      "Epoch 15, Batch 224, Loss: 0.4743790328502655\n",
      "Epoch 15, Batch 225, Loss: 0.3371947705745697\n",
      "Epoch 15, Batch 226, Loss: 0.42063215374946594\n",
      "Epoch 15, Batch 227, Loss: 0.5403313636779785\n",
      "Epoch 15, Batch 228, Loss: 0.5929402709007263\n",
      "Epoch 15, Batch 229, Loss: 0.5259056687355042\n",
      "Epoch 15, Batch 230, Loss: 0.4723970890045166\n",
      "Epoch 15, Batch 231, Loss: 0.34885141253471375\n",
      "Epoch 15, Batch 232, Loss: 0.5794264674186707\n",
      "Epoch 15, Batch 233, Loss: 0.36648795008659363\n",
      "Epoch 15, Batch 234, Loss: 0.5811723470687866\n",
      "Epoch 15, Batch 235, Loss: 0.3335556983947754\n",
      "Epoch 15, Batch 236, Loss: 0.45557984709739685\n",
      "Epoch 15, Batch 237, Loss: 0.3308897018432617\n",
      "Epoch 15, Batch 238, Loss: 0.2794346213340759\n",
      "Epoch 15, Batch 239, Loss: 0.32542312145233154\n",
      "Epoch 15, Batch 240, Loss: 0.45342835783958435\n",
      "Epoch 15, Batch 241, Loss: 0.7525201439857483\n",
      "Epoch 15, Batch 242, Loss: 0.3451898396015167\n",
      "Epoch 15, Batch 243, Loss: 0.6049790382385254\n",
      "Epoch 15, Batch 244, Loss: 0.41801998019218445\n",
      "Epoch 15, Batch 245, Loss: 0.3016202449798584\n",
      "Epoch 15, Batch 246, Loss: 0.5658923983573914\n",
      "Epoch 15, Batch 247, Loss: 0.40866026282310486\n",
      "Epoch 15, Batch 248, Loss: 0.3945082426071167\n",
      "Epoch 15, Batch 249, Loss: 0.4501834213733673\n",
      "Epoch 15, Batch 250, Loss: 0.26353156566619873\n",
      "Epoch 15, Batch 251, Loss: 0.4952761232852936\n",
      "Epoch 15, Batch 252, Loss: 0.38504716753959656\n",
      "Epoch 15, Batch 253, Loss: 0.4304133355617523\n",
      "Epoch 15, Batch 254, Loss: 0.3544631004333496\n",
      "Epoch 15, Batch 255, Loss: 0.41170021891593933\n",
      "Epoch 15, Batch 256, Loss: 0.6150895953178406\n",
      "Epoch 15, Batch 257, Loss: 0.45591089129447937\n",
      "Epoch 15, Batch 258, Loss: 0.6453647613525391\n",
      "Epoch 15, Batch 259, Loss: 0.4166213274002075\n",
      "Epoch 15, Batch 260, Loss: 0.4949212670326233\n",
      "Epoch 15, Batch 261, Loss: 0.5683136582374573\n",
      "Epoch 15, Batch 262, Loss: 0.5768439173698425\n",
      "Epoch 15, Batch 263, Loss: 0.457915723323822\n",
      "Epoch 15, Batch 264, Loss: 0.5456571578979492\n",
      "Epoch 15, Batch 265, Loss: 0.4365217089653015\n",
      "Epoch 15, Batch 266, Loss: 0.36160948872566223\n",
      "Epoch 15, Batch 267, Loss: 0.2719702422618866\n",
      "Epoch 15, Batch 268, Loss: 0.5253800749778748\n",
      "Epoch 15, Batch 269, Loss: 0.48827695846557617\n",
      "Epoch 15, Batch 270, Loss: 0.6204935908317566\n",
      "Epoch 15, Batch 271, Loss: 0.5131926536560059\n",
      "Epoch 15, Batch 272, Loss: 0.4034499526023865\n",
      "Epoch 15, Batch 273, Loss: 0.7679160833358765\n",
      "Epoch 15, Batch 274, Loss: 0.44809281826019287\n",
      "Epoch 15, Batch 275, Loss: 0.30824196338653564\n",
      "Epoch 15, Batch 276, Loss: 0.4215712249279022\n",
      "Epoch 15, Batch 277, Loss: 0.6469475030899048\n",
      "Epoch 15, Batch 278, Loss: 0.5243076086044312\n",
      "Epoch 15, Batch 279, Loss: 0.5717232823371887\n",
      "Epoch 15, Batch 280, Loss: 0.4807699918746948\n",
      "Epoch 15, Batch 281, Loss: 0.29159295558929443\n",
      "Epoch 15, Batch 282, Loss: 0.503533124923706\n",
      "Epoch 15, Batch 283, Loss: 0.34701770544052124\n",
      "Epoch 15, Batch 284, Loss: 0.4106524586677551\n",
      "Epoch 15, Batch 285, Loss: 0.33425748348236084\n",
      "Epoch 15, Batch 286, Loss: 0.6516531109809875\n",
      "Epoch 15, Batch 287, Loss: 0.7381182909011841\n",
      "Epoch 15, Batch 288, Loss: 0.4734482169151306\n",
      "Epoch 15, Batch 289, Loss: 0.3394516110420227\n",
      "Epoch 15, Batch 290, Loss: 0.4550081193447113\n",
      "Epoch 15, Batch 291, Loss: 0.45196929574012756\n",
      "Epoch 15, Batch 292, Loss: 0.5061818361282349\n",
      "Epoch 15, Batch 293, Loss: 0.3148646354675293\n",
      "Epoch 15, Batch 294, Loss: 0.38932085037231445\n",
      "Epoch 15, Batch 295, Loss: 0.5229130983352661\n",
      "Epoch 15, Batch 296, Loss: 0.49609604477882385\n",
      "Epoch 15, Batch 297, Loss: 0.37811559438705444\n",
      "Epoch 15, Batch 298, Loss: 0.3119310736656189\n",
      "Epoch 15, Batch 299, Loss: 0.413668692111969\n",
      "Epoch 15, Batch 300, Loss: 0.41643625497817993\n",
      "Epoch 15, Batch 301, Loss: 0.5458956360816956\n",
      "Epoch 15, Batch 302, Loss: 0.4182790219783783\n",
      "Epoch 15, Batch 303, Loss: 0.48165684938430786\n",
      "Epoch 15, Batch 304, Loss: 0.30655479431152344\n",
      "Epoch 15, Batch 305, Loss: 0.49595630168914795\n",
      "Epoch 15, Batch 306, Loss: 0.4393351376056671\n",
      "Epoch 15, Batch 307, Loss: 0.4343869090080261\n",
      "Epoch 15, Batch 308, Loss: 0.445818692445755\n",
      "Epoch 15, Batch 309, Loss: 0.38096699118614197\n",
      "Epoch 15, Batch 310, Loss: 0.5986331105232239\n",
      "Epoch 15, Batch 311, Loss: 0.490665078163147\n",
      "Epoch 15, Batch 312, Loss: 0.4465881288051605\n",
      "Epoch 15, Batch 313, Loss: 0.4798494577407837\n",
      "Epoch 15, Batch 314, Loss: 0.4131729304790497\n",
      "Epoch 15, Batch 315, Loss: 0.3633245527744293\n",
      "Epoch 15, Batch 316, Loss: 0.3613244295120239\n",
      "Epoch 15, Batch 317, Loss: 0.486998051404953\n",
      "Epoch 15, Batch 318, Loss: 0.5212806463241577\n",
      "Epoch 15, Batch 319, Loss: 0.21993248164653778\n",
      "Epoch 15, Batch 320, Loss: 0.5643964409828186\n",
      "Epoch 15, Batch 321, Loss: 0.5644333958625793\n",
      "Epoch 15, Batch 322, Loss: 0.48631933331489563\n",
      "Epoch 15, Batch 323, Loss: 0.39334601163864136\n",
      "Epoch 15, Batch 324, Loss: 0.43958041071891785\n",
      "Epoch 15, Batch 325, Loss: 0.5057511925697327\n",
      "Epoch 15, Batch 326, Loss: 0.3950519561767578\n",
      "Epoch 15, Batch 327, Loss: 0.24517951905727386\n",
      "Epoch 15, Batch 328, Loss: 0.30518990755081177\n",
      "Epoch 15, Batch 329, Loss: 0.3589896261692047\n",
      "Epoch 15, Batch 330, Loss: 0.6362544894218445\n",
      "Epoch 15, Batch 331, Loss: 0.5349477529525757\n",
      "Epoch 15, Batch 332, Loss: 0.3879633843898773\n",
      "Epoch 15, Batch 333, Loss: 0.4692131280899048\n",
      "Epoch 15, Batch 334, Loss: 0.47635042667388916\n",
      "Epoch 15, Batch 335, Loss: 0.22937673330307007\n",
      "Epoch 15, Batch 336, Loss: 0.52191561460495\n",
      "Epoch 15, Batch 337, Loss: 0.6155096888542175\n",
      "Epoch 15, Batch 338, Loss: 0.7333027124404907\n",
      "Epoch 15, Batch 339, Loss: 0.5827593803405762\n",
      "Epoch 15, Batch 340, Loss: 0.5880512595176697\n",
      "Epoch 15, Batch 341, Loss: 0.5374571084976196\n",
      "Epoch 15, Batch 342, Loss: 0.42917749285697937\n",
      "Epoch 15, Batch 343, Loss: 0.559902548789978\n",
      "Epoch 15, Batch 344, Loss: 0.459520548582077\n",
      "Epoch 15, Batch 345, Loss: 0.4483616352081299\n",
      "Epoch 15, Batch 346, Loss: 0.5389682650566101\n",
      "Epoch 15, Batch 347, Loss: 0.6461883187294006\n",
      "Epoch 15, Batch 348, Loss: 0.5099415183067322\n",
      "Epoch 15, Batch 349, Loss: 0.5477569699287415\n",
      "Epoch 15, Batch 350, Loss: 0.3910762369632721\n",
      "Epoch 15, Batch 351, Loss: 0.43940478563308716\n",
      "Epoch 15, Batch 352, Loss: 0.4450445771217346\n",
      "Epoch 15, Batch 353, Loss: 0.47864341735839844\n",
      "Epoch 15, Batch 354, Loss: 0.7212561964988708\n",
      "Epoch 15, Batch 355, Loss: 0.5112916827201843\n",
      "Epoch 15, Batch 356, Loss: 0.4107697010040283\n",
      "Epoch 15, Batch 357, Loss: 0.26854631304740906\n",
      "Epoch 15, Batch 358, Loss: 0.5104870796203613\n",
      "Epoch 15, Batch 359, Loss: 0.4019085466861725\n",
      "Epoch 15, Batch 360, Loss: 0.3545510768890381\n",
      "Epoch 15, Batch 361, Loss: 0.4662030339241028\n",
      "Epoch 15, Batch 362, Loss: 0.4687584936618805\n",
      "Epoch 15, Batch 363, Loss: 0.4643993675708771\n",
      "Epoch 15, Batch 364, Loss: 0.4451075494289398\n",
      "Epoch 15, Batch 365, Loss: 0.39679595828056335\n",
      "Epoch 15, Batch 366, Loss: 0.706083357334137\n",
      "Epoch 15, Batch 367, Loss: 0.5983335375785828\n",
      "Epoch 15, Batch 368, Loss: 0.41895395517349243\n",
      "Epoch 15, Batch 369, Loss: 0.35733529925346375\n",
      "Epoch 15, Batch 370, Loss: 0.45783761143684387\n",
      "Epoch 15, Batch 371, Loss: 0.2927502989768982\n",
      "Epoch 15, Batch 372, Loss: 0.3269088864326477\n",
      "Epoch 15, Batch 373, Loss: 0.4240987300872803\n",
      "Epoch 15, Batch 374, Loss: 0.3234257102012634\n",
      "Epoch 15, Batch 375, Loss: 0.40470534563064575\n",
      "Epoch 15, Batch 376, Loss: 0.5037868022918701\n",
      "Epoch 15, Batch 377, Loss: 0.6265299916267395\n",
      "Epoch 15, Batch 378, Loss: 0.6476528644561768\n",
      "Epoch 15, Batch 379, Loss: 0.49903637170791626\n",
      "Epoch 15, Batch 380, Loss: 0.2924114167690277\n",
      "Epoch 15, Batch 381, Loss: 0.6403505802154541\n",
      "Epoch 15, Batch 382, Loss: 0.3008057475090027\n",
      "Epoch 15, Batch 383, Loss: 0.4335184693336487\n",
      "Epoch 15, Batch 384, Loss: 0.3804503083229065\n",
      "Epoch 15, Batch 385, Loss: 0.45153626799583435\n",
      "Epoch 15, Batch 386, Loss: 0.4584241211414337\n",
      "Epoch 15, Batch 387, Loss: 0.29813843965530396\n",
      "Epoch 15, Batch 388, Loss: 0.25717729330062866\n",
      "Epoch 15, Batch 389, Loss: 0.47371727228164673\n",
      "Epoch 15, Batch 390, Loss: 0.5076361298561096\n",
      "Epoch 15, Batch 391, Loss: 0.5189211368560791\n",
      "Epoch 15, Batch 392, Loss: 0.28704944252967834\n",
      "Epoch 15, Batch 393, Loss: 0.4189751148223877\n",
      "Epoch 15, Batch 394, Loss: 0.42358383536338806\n",
      "Epoch 15, Batch 395, Loss: 0.8271602392196655\n",
      "Epoch 15, Batch 396, Loss: 0.4952932596206665\n",
      "Epoch 15, Batch 397, Loss: 0.37601929903030396\n",
      "Epoch 15, Batch 398, Loss: 0.42744261026382446\n",
      "Epoch 15, Batch 399, Loss: 0.35035839676856995\n",
      "Epoch 15, Batch 400, Loss: 0.7646180987358093\n",
      "Epoch 15, Batch 401, Loss: 0.6064988970756531\n",
      "Epoch 15, Batch 402, Loss: 0.46701276302337646\n",
      "Epoch 15, Batch 403, Loss: 0.5266715288162231\n",
      "Epoch 15, Batch 404, Loss: 0.6048871278762817\n",
      "Epoch 15, Batch 405, Loss: 0.6334935426712036\n",
      "Epoch 15, Batch 406, Loss: 0.7290645837783813\n",
      "Epoch 15, Batch 407, Loss: 0.3651382029056549\n",
      "Epoch 15, Batch 408, Loss: 0.511896014213562\n",
      "Epoch 15, Batch 409, Loss: 0.4566889703273773\n",
      "Epoch 15, Batch 410, Loss: 0.39296650886535645\n",
      "Epoch 15, Batch 411, Loss: 0.39178529381752014\n",
      "Epoch 15, Batch 412, Loss: 0.36932528018951416\n",
      "Epoch 15, Batch 413, Loss: 0.5707222819328308\n",
      "Epoch 15, Batch 414, Loss: 0.39193421602249146\n",
      "Epoch 15, Batch 415, Loss: 0.4011152684688568\n",
      "Epoch 15, Batch 416, Loss: 0.3940887451171875\n",
      "Epoch 15, Batch 417, Loss: 0.4201710820198059\n",
      "Epoch 15, Batch 418, Loss: 0.645839273929596\n",
      "Epoch 15, Batch 419, Loss: 0.395720511674881\n",
      "Epoch 15, Batch 420, Loss: 0.5226920247077942\n",
      "Epoch 15, Batch 421, Loss: 0.3928302526473999\n",
      "Epoch 15, Batch 422, Loss: 0.3943787217140198\n",
      "Epoch 15, Batch 423, Loss: 0.30796486139297485\n",
      "Epoch 15, Batch 424, Loss: 0.3262921869754791\n",
      "Epoch 15, Batch 425, Loss: 0.34702494740486145\n",
      "Epoch 15, Batch 426, Loss: 0.6449306011199951\n",
      "Epoch 15, Batch 427, Loss: 0.40466901659965515\n",
      "Epoch 15, Batch 428, Loss: 0.589124858379364\n",
      "Epoch 15, Batch 429, Loss: 0.4636715054512024\n",
      "Epoch 15, Batch 430, Loss: 0.4555542767047882\n",
      "Epoch 15, Batch 431, Loss: 0.4812884032726288\n",
      "Epoch 15, Batch 432, Loss: 0.3346913158893585\n",
      "Epoch 15, Batch 433, Loss: 0.5429756045341492\n",
      "Epoch 15, Batch 434, Loss: 0.37557923793792725\n",
      "Epoch 15, Batch 435, Loss: 0.4050733149051666\n",
      "Epoch 15, Batch 436, Loss: 0.46346285939216614\n",
      "Epoch 15, Batch 437, Loss: 0.42544999718666077\n",
      "Epoch 15, Batch 438, Loss: 0.4814753234386444\n",
      "Epoch 15, Batch 439, Loss: 0.421703577041626\n",
      "Epoch 15, Batch 440, Loss: 0.36035627126693726\n",
      "Epoch 15, Batch 441, Loss: 0.6260153651237488\n",
      "Epoch 15, Batch 442, Loss: 0.4028255343437195\n",
      "Epoch 15, Batch 443, Loss: 0.6343300342559814\n",
      "Epoch 15, Batch 444, Loss: 0.5365164875984192\n",
      "Epoch 15, Batch 445, Loss: 0.6311739683151245\n",
      "Epoch 15, Batch 446, Loss: 0.3492099642753601\n",
      "Epoch 15, Batch 447, Loss: 0.41891658306121826\n",
      "Epoch 15, Batch 448, Loss: 0.40753886103630066\n",
      "Epoch 15, Batch 449, Loss: 0.4796086251735687\n",
      "Epoch 15, Batch 450, Loss: 0.2842312753200531\n",
      "Epoch 15, Batch 451, Loss: 0.5459803938865662\n",
      "Epoch 15, Batch 452, Loss: 0.5290884375572205\n",
      "Epoch 15, Batch 453, Loss: 0.626725435256958\n",
      "Epoch 15, Batch 454, Loss: 0.361319363117218\n",
      "Epoch 15, Batch 455, Loss: 0.4186711311340332\n",
      "Epoch 15, Batch 456, Loss: 0.3211725652217865\n",
      "Epoch 15, Batch 457, Loss: 0.3680950403213501\n",
      "Epoch 15, Batch 458, Loss: 0.5704903602600098\n",
      "Epoch 15, Batch 459, Loss: 0.31899768114089966\n",
      "Epoch 15, Batch 460, Loss: 0.6067155599594116\n",
      "Epoch 15, Batch 461, Loss: 0.3489138185977936\n",
      "Epoch 15, Batch 462, Loss: 0.47451940178871155\n",
      "Epoch 15, Batch 463, Loss: 0.20611847937107086\n",
      "Epoch 15, Batch 464, Loss: 0.46255385875701904\n",
      "Epoch 15, Batch 465, Loss: 0.6020997762680054\n",
      "Epoch 15, Batch 466, Loss: 0.5712982416152954\n",
      "Epoch 15, Batch 467, Loss: 0.5361908078193665\n",
      "Epoch 15, Batch 468, Loss: 0.6168032884597778\n",
      "Epoch 15, Batch 469, Loss: 0.526131272315979\n",
      "Epoch 15, Batch 470, Loss: 0.4893411099910736\n",
      "Epoch 15, Batch 471, Loss: 0.6104328632354736\n",
      "Epoch 15, Batch 472, Loss: 0.3379436731338501\n",
      "Epoch 15, Batch 473, Loss: 0.45170721411705017\n",
      "Epoch 15, Batch 474, Loss: 0.4419015645980835\n",
      "Epoch 15, Batch 475, Loss: 0.5262141227722168\n",
      "Epoch 15, Batch 476, Loss: 0.3120638132095337\n",
      "Epoch 15, Batch 477, Loss: 0.4690997004508972\n",
      "Epoch 15, Batch 478, Loss: 0.6561648845672607\n",
      "Epoch 15, Batch 479, Loss: 0.374837189912796\n",
      "Epoch 15, Batch 480, Loss: 0.4394499361515045\n",
      "Epoch 15, Batch 481, Loss: 0.5476499199867249\n",
      "Epoch 15, Batch 482, Loss: 0.43687474727630615\n",
      "Epoch 15, Batch 483, Loss: 0.5304965376853943\n",
      "Epoch 15, Batch 484, Loss: 0.3034250736236572\n",
      "Epoch 15, Batch 485, Loss: 0.6099298000335693\n",
      "Epoch 15, Batch 486, Loss: 0.4374326765537262\n",
      "Epoch 15, Batch 487, Loss: 0.40239548683166504\n",
      "Epoch 15, Batch 488, Loss: 0.5448049902915955\n",
      "Epoch 15, Batch 489, Loss: 0.3932110369205475\n",
      "Epoch 15, Batch 490, Loss: 0.5508606433868408\n",
      "Epoch 15, Batch 491, Loss: 0.35777485370635986\n",
      "Epoch 15, Batch 492, Loss: 0.44351160526275635\n",
      "Epoch 15, Batch 493, Loss: 0.5733168721199036\n",
      "Epoch 15, Batch 494, Loss: 0.5168882012367249\n",
      "Epoch 15, Batch 495, Loss: 0.4130830466747284\n",
      "Epoch 15, Batch 496, Loss: 0.33988240361213684\n",
      "Epoch 15, Batch 497, Loss: 0.3670153021812439\n",
      "Epoch 15, Batch 498, Loss: 0.37420654296875\n",
      "Epoch 15, Batch 499, Loss: 0.5806360840797424\n",
      "Epoch 15, Batch 500, Loss: 0.34224653244018555\n",
      "Epoch 15, Batch 501, Loss: 0.4057426154613495\n",
      "Epoch 15, Batch 502, Loss: 0.5132744312286377\n",
      "Epoch 15, Batch 503, Loss: 0.6077951192855835\n",
      "Epoch 15, Batch 504, Loss: 0.4562903940677643\n",
      "Epoch 15, Batch 505, Loss: 0.5029931664466858\n",
      "Epoch 15, Batch 506, Loss: 0.505577802658081\n",
      "Epoch 15, Batch 507, Loss: 0.44458112120628357\n",
      "Epoch 15, Batch 508, Loss: 0.3878327012062073\n",
      "Epoch 15, Batch 509, Loss: 0.51725172996521\n",
      "Epoch 15, Batch 510, Loss: 0.4445032775402069\n",
      "Epoch 15, Batch 511, Loss: 0.2954537868499756\n",
      "Epoch 15, Batch 512, Loss: 0.43856167793273926\n",
      "Epoch 15, Batch 513, Loss: 0.5321612358093262\n",
      "Epoch 15, Batch 514, Loss: 0.32877540588378906\n",
      "Epoch 15, Batch 515, Loss: 0.5502086281776428\n",
      "Epoch 15, Batch 516, Loss: 0.6702030897140503\n",
      "Epoch 15, Batch 517, Loss: 0.3905540406703949\n",
      "Epoch 15, Batch 518, Loss: 0.46103063225746155\n",
      "Epoch 15, Batch 519, Loss: 0.5656510591506958\n",
      "Epoch 15, Batch 520, Loss: 0.4449547529220581\n",
      "Epoch 15, Batch 521, Loss: 0.46564269065856934\n",
      "Epoch 15, Batch 522, Loss: 0.6116571426391602\n",
      "Epoch 15, Batch 523, Loss: 0.3023054599761963\n",
      "Epoch 15, Batch 524, Loss: 0.4752472937107086\n",
      "Epoch 15, Batch 525, Loss: 0.3471219837665558\n",
      "Epoch 15, Batch 526, Loss: 0.5129846334457397\n",
      "Epoch 15, Batch 527, Loss: 0.3718622028827667\n",
      "Epoch 15, Batch 528, Loss: 0.34672510623931885\n",
      "Epoch 15, Batch 529, Loss: 0.4609293043613434\n",
      "Epoch 15, Batch 530, Loss: 0.42361101508140564\n",
      "Epoch 15, Batch 531, Loss: 0.46213316917419434\n",
      "Epoch 15, Batch 532, Loss: 0.3158756196498871\n",
      "Epoch 15, Batch 533, Loss: 0.5400307178497314\n",
      "Epoch 15, Batch 534, Loss: 0.47342124581336975\n",
      "Epoch 15, Batch 535, Loss: 0.34095433354377747\n",
      "Epoch 15, Batch 536, Loss: 0.5711767077445984\n",
      "Epoch 15, Batch 537, Loss: 0.4146701693534851\n",
      "Epoch 15, Batch 538, Loss: 0.6265342831611633\n",
      "Epoch 15, Batch 539, Loss: 0.28071779012680054\n",
      "Epoch 15, Batch 540, Loss: 0.3381083309650421\n",
      "Epoch 15, Batch 541, Loss: 0.5089104771614075\n",
      "Epoch 15, Batch 542, Loss: 0.4229688346385956\n",
      "Epoch 15, Batch 543, Loss: 0.3577713668346405\n",
      "Epoch 15, Batch 544, Loss: 0.5489051342010498\n",
      "Epoch 15, Batch 545, Loss: 0.34524059295654297\n",
      "Epoch 15, Batch 546, Loss: 0.40636128187179565\n",
      "Epoch 15, Batch 547, Loss: 0.48948806524276733\n",
      "Epoch 15, Batch 548, Loss: 0.36390167474746704\n",
      "Epoch 15, Batch 549, Loss: 0.2315714955329895\n",
      "Epoch 15, Batch 550, Loss: 0.595110297203064\n",
      "Epoch 15, Batch 551, Loss: 0.37223777174949646\n",
      "Epoch 15, Batch 552, Loss: 0.40593183040618896\n",
      "Epoch 15, Batch 553, Loss: 0.35835134983062744\n",
      "Epoch 15, Batch 554, Loss: 0.5142791271209717\n",
      "Epoch 15, Batch 555, Loss: 0.43271276354789734\n",
      "Epoch 15, Batch 556, Loss: 0.4885993003845215\n",
      "Epoch 15, Batch 557, Loss: 0.4253990352153778\n",
      "Epoch 15, Batch 558, Loss: 0.3736523687839508\n",
      "Epoch 15, Batch 559, Loss: 0.3070496916770935\n",
      "Epoch 15, Batch 560, Loss: 0.32981929183006287\n",
      "Epoch 15, Batch 561, Loss: 0.3233658969402313\n",
      "Epoch 15, Batch 562, Loss: 0.4659905433654785\n",
      "Epoch 15, Batch 563, Loss: 0.5190415382385254\n",
      "Epoch 15, Batch 564, Loss: 0.5184811353683472\n",
      "Epoch 15, Batch 565, Loss: 0.3913634121417999\n",
      "Epoch 15, Batch 566, Loss: 0.6709816455841064\n",
      "Epoch 15, Batch 567, Loss: 0.4112931787967682\n",
      "Epoch 15, Batch 568, Loss: 0.4650348424911499\n",
      "Epoch 15, Batch 569, Loss: 0.5116622447967529\n",
      "Epoch 15, Batch 570, Loss: 0.5493118166923523\n",
      "Epoch 15, Batch 571, Loss: 0.2627221345901489\n",
      "Epoch 15, Batch 572, Loss: 0.4167490601539612\n",
      "Epoch 15, Batch 573, Loss: 0.4269316494464874\n",
      "Epoch 15, Batch 574, Loss: 0.43628278374671936\n",
      "Epoch 15, Batch 575, Loss: 0.5348456501960754\n",
      "Epoch 15, Batch 576, Loss: 0.22757777571678162\n",
      "Epoch 15, Batch 577, Loss: 0.4248238801956177\n",
      "Epoch 15, Batch 578, Loss: 0.4934157729148865\n",
      "Epoch 15, Batch 579, Loss: 0.3223590850830078\n",
      "Epoch 15, Batch 580, Loss: 0.5638220310211182\n",
      "Epoch 15, Batch 581, Loss: 0.6938125491142273\n",
      "Epoch 15, Batch 582, Loss: 0.5581202507019043\n",
      "Epoch 15, Batch 583, Loss: 0.43815064430236816\n",
      "Epoch 15, Batch 584, Loss: 0.5454891324043274\n",
      "Epoch 15, Batch 585, Loss: 0.43414783477783203\n",
      "Epoch 15, Batch 586, Loss: 0.3611947298049927\n",
      "Epoch 15, Batch 587, Loss: 0.28883567452430725\n",
      "Epoch 15, Batch 588, Loss: 0.349810928106308\n",
      "Epoch 15, Batch 589, Loss: 0.4154745936393738\n",
      "Epoch 15, Batch 590, Loss: 0.4164290130138397\n",
      "Epoch 15, Batch 591, Loss: 0.5068416595458984\n",
      "Epoch 15, Batch 592, Loss: 0.3959205746650696\n",
      "Epoch 15, Batch 593, Loss: 0.5184167623519897\n",
      "Epoch 15, Batch 594, Loss: 0.38166379928588867\n",
      "Epoch 15, Batch 595, Loss: 0.8442275524139404\n",
      "Epoch 15, Batch 596, Loss: 0.6339188814163208\n",
      "Epoch 15, Batch 597, Loss: 0.48253610730171204\n",
      "Epoch 15, Batch 598, Loss: 0.35820212960243225\n",
      "Epoch 15, Batch 599, Loss: 0.4000852108001709\n",
      "Epoch 15, Batch 600, Loss: 0.45839789509773254\n",
      "Epoch 15, Batch 601, Loss: 0.5016834735870361\n",
      "Epoch 15, Batch 602, Loss: 0.4667636454105377\n",
      "Epoch 15, Batch 603, Loss: 0.37950387597084045\n",
      "Epoch 15, Batch 604, Loss: 0.39091020822525024\n",
      "Epoch 15, Batch 605, Loss: 0.48984500765800476\n",
      "Epoch 15, Batch 606, Loss: 0.3627453148365021\n",
      "Epoch 15, Batch 607, Loss: 0.3059210181236267\n",
      "Epoch 15, Batch 608, Loss: 0.7114293575286865\n",
      "Epoch 15, Batch 609, Loss: 0.47660017013549805\n",
      "Epoch 15, Batch 610, Loss: 0.5296335220336914\n",
      "Epoch 15, Batch 611, Loss: 0.3050558567047119\n",
      "Epoch 15, Batch 612, Loss: 0.4711301624774933\n",
      "Epoch 15, Batch 613, Loss: 0.2716844081878662\n",
      "Epoch 15, Batch 614, Loss: 0.36127471923828125\n",
      "Epoch 15, Batch 615, Loss: 0.4438841938972473\n",
      "Epoch 15, Batch 616, Loss: 0.39190688729286194\n",
      "Epoch 15, Batch 617, Loss: 0.3402414321899414\n",
      "Epoch 15, Batch 618, Loss: 0.6294583678245544\n",
      "Epoch 15, Batch 619, Loss: 0.37088707089424133\n",
      "Epoch 15, Batch 620, Loss: 0.4146801829338074\n",
      "Epoch 15, Batch 621, Loss: 0.48532187938690186\n",
      "Epoch 15, Batch 622, Loss: 0.42813393473625183\n",
      "Epoch 15, Batch 623, Loss: 0.43879491090774536\n",
      "Epoch 15, Batch 624, Loss: 0.4736975431442261\n",
      "Epoch 15, Batch 625, Loss: 0.24293819069862366\n",
      "Epoch 15, Batch 626, Loss: 0.5362311601638794\n",
      "Epoch 15, Batch 627, Loss: 0.4377344250679016\n",
      "Epoch 15, Batch 628, Loss: 0.43332356214523315\n",
      "Epoch 15, Batch 629, Loss: 0.375998318195343\n",
      "Epoch 15, Batch 630, Loss: 0.4639776945114136\n",
      "Epoch 15, Batch 631, Loss: 0.5030772686004639\n",
      "Epoch 15, Batch 632, Loss: 0.48270976543426514\n",
      "Epoch 15, Batch 633, Loss: 0.5576075315475464\n",
      "Epoch 15, Batch 634, Loss: 0.695095419883728\n",
      "Epoch 15, Batch 635, Loss: 0.4201200008392334\n",
      "Epoch 15, Batch 636, Loss: 0.5411474704742432\n",
      "Epoch 15, Batch 637, Loss: 0.3402877449989319\n",
      "Epoch 15, Batch 638, Loss: 0.3739742934703827\n",
      "Epoch 15, Batch 639, Loss: 0.6038543581962585\n",
      "Epoch 15, Batch 640, Loss: 0.6420079469680786\n",
      "Epoch 15, Batch 641, Loss: 0.43243157863616943\n",
      "Epoch 15, Batch 642, Loss: 0.45844149589538574\n",
      "Epoch 15, Batch 643, Loss: 0.3109252154827118\n",
      "Epoch 15, Batch 644, Loss: 0.28317826986312866\n",
      "Epoch 15, Batch 645, Loss: 0.41325855255126953\n",
      "Epoch 15, Batch 646, Loss: 0.48719322681427\n",
      "Epoch 15, Batch 647, Loss: 0.2910100519657135\n",
      "Epoch 15, Batch 648, Loss: 0.4685799777507782\n",
      "Epoch 15, Batch 649, Loss: 0.4431595504283905\n",
      "Epoch 15, Batch 650, Loss: 0.3647361993789673\n",
      "Epoch 15, Batch 651, Loss: 0.36043763160705566\n",
      "Epoch 15, Batch 652, Loss: 0.3612072467803955\n",
      "Epoch 15, Batch 653, Loss: 0.45920518040657043\n",
      "Epoch 15, Batch 654, Loss: 0.27427878975868225\n",
      "Epoch 15, Batch 655, Loss: 0.615743100643158\n",
      "Epoch 15, Batch 656, Loss: 0.4226481020450592\n",
      "Epoch 15, Batch 657, Loss: 0.4036950170993805\n",
      "Epoch 15, Batch 658, Loss: 0.4529185891151428\n",
      "Epoch 15, Batch 659, Loss: 0.3482837975025177\n",
      "Epoch 15, Batch 660, Loss: 0.3835211396217346\n",
      "Epoch 15, Batch 661, Loss: 0.32781729102134705\n",
      "Epoch 15, Batch 662, Loss: 0.5401358008384705\n",
      "Epoch 15, Batch 663, Loss: 0.4866912364959717\n",
      "Epoch 15, Batch 664, Loss: 0.6706968545913696\n",
      "Epoch 15, Batch 665, Loss: 0.30358514189720154\n",
      "Epoch 15, Batch 666, Loss: 0.3787703514099121\n",
      "Epoch 15, Batch 667, Loss: 0.33974260091781616\n",
      "Epoch 15, Batch 668, Loss: 0.5511476397514343\n",
      "Epoch 15, Batch 669, Loss: 0.41541755199432373\n",
      "Epoch 15, Batch 670, Loss: 0.3930569887161255\n",
      "Epoch 15, Batch 671, Loss: 0.5271316766738892\n",
      "Epoch 15, Batch 672, Loss: 0.4267623722553253\n",
      "Epoch 15, Batch 673, Loss: 0.4949434697628021\n",
      "Epoch 15, Batch 674, Loss: 0.4450005292892456\n",
      "Epoch 15, Batch 675, Loss: 0.4545939266681671\n",
      "Epoch 15, Batch 676, Loss: 0.3067939281463623\n",
      "Epoch 15, Batch 677, Loss: 0.4504683017730713\n",
      "Epoch 15, Batch 678, Loss: 0.3059922456741333\n",
      "Epoch 15, Batch 679, Loss: 0.41506123542785645\n",
      "Epoch 15, Batch 680, Loss: 0.3034898638725281\n",
      "Epoch 15, Batch 681, Loss: 0.4008011221885681\n",
      "Epoch 15, Batch 682, Loss: 0.33496350049972534\n",
      "Epoch 15, Batch 683, Loss: 0.4283383786678314\n",
      "Epoch 15, Batch 684, Loss: 0.5187672972679138\n",
      "Epoch 15, Batch 685, Loss: 0.5251696109771729\n",
      "Epoch 15, Batch 686, Loss: 0.6268501877784729\n",
      "Epoch 15, Batch 687, Loss: 0.3414125442504883\n",
      "Epoch 15, Batch 688, Loss: 0.4361763894557953\n",
      "Epoch 15, Batch 689, Loss: 0.4057496190071106\n",
      "Epoch 15, Batch 690, Loss: 0.3974221646785736\n",
      "Epoch 15, Batch 691, Loss: 0.4275938868522644\n",
      "Epoch 15, Batch 692, Loss: 0.47752615809440613\n",
      "Epoch 15, Batch 693, Loss: 0.5423325300216675\n",
      "Epoch 15, Batch 694, Loss: 0.3613851070404053\n",
      "Epoch 15, Batch 695, Loss: 0.5525885224342346\n",
      "Epoch 15, Batch 696, Loss: 0.3585644066333771\n",
      "Epoch 15, Batch 697, Loss: 0.49181538820266724\n",
      "Epoch 15, Batch 698, Loss: 0.42167359590530396\n",
      "Epoch 15, Batch 699, Loss: 0.2903589904308319\n",
      "Epoch 15, Batch 700, Loss: 0.26494717597961426\n",
      "Epoch 15, Batch 701, Loss: 0.27434855699539185\n",
      "Epoch 15, Batch 702, Loss: 0.49806204438209534\n",
      "Epoch 15, Batch 703, Loss: 0.3735676109790802\n",
      "Epoch 15, Batch 704, Loss: 0.4828348755836487\n",
      "Epoch 15, Batch 705, Loss: 0.4542955756187439\n",
      "Epoch 15, Batch 706, Loss: 0.5968116521835327\n",
      "Epoch 15, Batch 707, Loss: 0.4606991410255432\n",
      "Epoch 15, Batch 708, Loss: 0.4700486660003662\n",
      "Epoch 15, Batch 709, Loss: 0.5612863898277283\n",
      "Epoch 15, Batch 710, Loss: 0.30298370122909546\n",
      "Epoch 15, Batch 711, Loss: 0.5545125603675842\n",
      "Epoch 15, Batch 712, Loss: 0.5755738019943237\n",
      "Epoch 15, Batch 713, Loss: 0.4069175720214844\n",
      "Epoch 15, Batch 714, Loss: 0.6084764003753662\n",
      "Epoch 15, Batch 715, Loss: 0.8214173913002014\n",
      "Epoch 15, Batch 716, Loss: 0.4579860270023346\n",
      "Epoch 15, Batch 717, Loss: 0.5475049614906311\n",
      "Epoch 15, Batch 718, Loss: 0.735370397567749\n",
      "Epoch 15, Batch 719, Loss: 0.5321298241615295\n",
      "Epoch 15, Batch 720, Loss: 0.4200623333454132\n",
      "Epoch 15, Batch 721, Loss: 0.5433698296546936\n",
      "Epoch 15, Batch 722, Loss: 0.5200462341308594\n",
      "Epoch 15, Batch 723, Loss: 0.519114077091217\n",
      "Epoch 15, Batch 724, Loss: 0.3413691818714142\n",
      "Epoch 15, Batch 725, Loss: 0.5491976737976074\n",
      "Epoch 15, Batch 726, Loss: 0.5608906149864197\n",
      "Epoch 15, Batch 727, Loss: 0.3649123013019562\n",
      "Epoch 15, Batch 728, Loss: 0.5207230448722839\n",
      "Epoch 15, Batch 729, Loss: 0.48865067958831787\n",
      "Epoch 15, Batch 730, Loss: 0.3890879154205322\n",
      "Epoch 15, Batch 731, Loss: 0.5963122844696045\n",
      "Epoch 15, Batch 732, Loss: 0.3685915172100067\n",
      "Epoch 15, Batch 733, Loss: 0.3963475823402405\n",
      "Epoch 15, Batch 734, Loss: 0.5114096403121948\n",
      "Epoch 15, Batch 735, Loss: 0.1850321888923645\n",
      "Epoch 15, Batch 736, Loss: 0.47866034507751465\n",
      "Epoch 15, Batch 737, Loss: 0.581531286239624\n",
      "Epoch 15, Batch 738, Loss: 0.69954514503479\n",
      "Epoch 15, Batch 739, Loss: 0.5501585006713867\n",
      "Epoch 15, Batch 740, Loss: 0.3605307936668396\n",
      "Epoch 15, Batch 741, Loss: 0.41200610995292664\n",
      "Epoch 15, Batch 742, Loss: 0.3615899384021759\n",
      "Epoch 15, Batch 743, Loss: 0.47706425189971924\n",
      "Epoch 15, Batch 744, Loss: 0.4851965308189392\n",
      "Epoch 15, Batch 745, Loss: 0.5065016746520996\n",
      "Epoch 15, Batch 746, Loss: 0.41168105602264404\n",
      "Epoch 15, Batch 747, Loss: 0.27513355016708374\n",
      "Epoch 15, Batch 748, Loss: 0.3495088219642639\n",
      "Epoch 15, Batch 749, Loss: 0.42665618658065796\n",
      "Epoch 15, Batch 750, Loss: 0.572178065776825\n",
      "Epoch 15, Batch 751, Loss: 0.43310609459877014\n",
      "Epoch 15, Batch 752, Loss: 0.5645579099655151\n",
      "Epoch 15, Batch 753, Loss: 0.5266793966293335\n",
      "Epoch 15, Batch 754, Loss: 0.6276096105575562\n",
      "Epoch 15, Batch 755, Loss: 0.3906477093696594\n",
      "Epoch 15, Batch 756, Loss: 0.4868851602077484\n",
      "Epoch 15, Batch 757, Loss: 0.5165441632270813\n",
      "Epoch 15, Batch 758, Loss: 0.38978052139282227\n",
      "Epoch 15, Batch 759, Loss: 0.5270415544509888\n",
      "Epoch 15, Batch 760, Loss: 0.6282832622528076\n",
      "Epoch 15, Batch 761, Loss: 0.47863051295280457\n",
      "Epoch 15, Batch 762, Loss: 0.36960649490356445\n",
      "Epoch 15, Batch 763, Loss: 0.3487503230571747\n",
      "Epoch 15, Batch 764, Loss: 0.4218851923942566\n",
      "Epoch 15, Batch 765, Loss: 0.5308188796043396\n",
      "Epoch 15, Batch 766, Loss: 0.5580496191978455\n",
      "Epoch 15, Batch 767, Loss: 0.3945997357368469\n",
      "Epoch 15, Batch 768, Loss: 0.6354038715362549\n",
      "Epoch 15, Batch 769, Loss: 0.45846810936927795\n",
      "Epoch 15, Batch 770, Loss: 0.32056373357772827\n",
      "Epoch 15, Batch 771, Loss: 0.5438230037689209\n",
      "Epoch 15, Batch 772, Loss: 0.5739255547523499\n",
      "Epoch 15, Batch 773, Loss: 0.48533931374549866\n",
      "Epoch 15, Batch 774, Loss: 0.39120134711265564\n",
      "Epoch 15, Batch 775, Loss: 0.4140259325504303\n",
      "Epoch 15, Batch 776, Loss: 0.7158292531967163\n",
      "Epoch 15, Batch 777, Loss: 0.46056780219078064\n",
      "Epoch 15, Batch 778, Loss: 0.5336768627166748\n",
      "Epoch 15, Batch 779, Loss: 0.36594295501708984\n",
      "Epoch 15, Batch 780, Loss: 0.5747552514076233\n",
      "Epoch 15, Batch 781, Loss: 0.39219731092453003\n",
      "Epoch 15, Batch 782, Loss: 0.5469866394996643\n",
      "Epoch 15, Batch 783, Loss: 0.46951824426651\n",
      "Epoch 15, Batch 784, Loss: 0.6172290444374084\n",
      "Epoch 15, Batch 785, Loss: 0.3337733745574951\n",
      "Epoch 15, Batch 786, Loss: 0.5648291110992432\n",
      "Epoch 15, Batch 787, Loss: 0.5895807147026062\n",
      "Epoch 15, Batch 788, Loss: 0.3809303045272827\n",
      "Epoch 15, Batch 789, Loss: 0.4088900089263916\n",
      "Epoch 15, Batch 790, Loss: 0.5756499767303467\n",
      "Epoch 15, Batch 791, Loss: 0.44852232933044434\n",
      "Epoch 15, Batch 792, Loss: 0.42047110199928284\n",
      "Epoch 15, Batch 793, Loss: 0.4679880142211914\n",
      "Epoch 15, Batch 794, Loss: 0.7749434113502502\n",
      "Epoch 15, Batch 795, Loss: 0.42439091205596924\n",
      "Epoch 15, Batch 796, Loss: 0.4926910400390625\n",
      "Epoch 15, Batch 797, Loss: 0.5038972496986389\n",
      "Epoch 15, Batch 798, Loss: 0.39463502168655396\n",
      "Epoch 15, Batch 799, Loss: 0.3214573860168457\n",
      "Epoch 15, Batch 800, Loss: 0.47501763701438904\n",
      "Epoch 15, Batch 801, Loss: 0.49297934770584106\n",
      "Epoch 15, Batch 802, Loss: 0.27270936965942383\n",
      "Epoch 15, Batch 803, Loss: 0.6189107298851013\n",
      "Epoch 15, Batch 804, Loss: 0.47940266132354736\n",
      "Epoch 15, Batch 805, Loss: 0.5273609161376953\n",
      "Epoch 15, Batch 806, Loss: 0.45079606771469116\n",
      "Epoch 15, Batch 807, Loss: 0.5100005865097046\n",
      "Epoch 15, Batch 808, Loss: 0.5907765626907349\n",
      "Epoch 15, Batch 809, Loss: 0.4948285222053528\n",
      "Epoch 15, Batch 810, Loss: 0.5389398336410522\n",
      "Epoch 15, Batch 811, Loss: 0.4628540575504303\n",
      "Epoch 15, Batch 812, Loss: 0.3967055380344391\n",
      "Epoch 15, Batch 813, Loss: 0.4970824718475342\n",
      "Epoch 15, Batch 814, Loss: 0.47647029161453247\n",
      "Epoch 15, Batch 815, Loss: 0.2654450833797455\n",
      "Epoch 15, Batch 816, Loss: 0.4510960280895233\n",
      "Epoch 15, Batch 817, Loss: 0.543276309967041\n",
      "Epoch 15, Batch 818, Loss: 0.4816477298736572\n",
      "Epoch 15, Batch 819, Loss: 0.5667563676834106\n",
      "Epoch 15, Batch 820, Loss: 0.45900362730026245\n",
      "Epoch 15, Batch 821, Loss: 0.33608436584472656\n",
      "Epoch 15, Batch 822, Loss: 0.6419000625610352\n",
      "Epoch 15, Batch 823, Loss: 0.5750424265861511\n",
      "Epoch 15, Batch 824, Loss: 0.5802380442619324\n",
      "Epoch 15, Batch 825, Loss: 0.45785579085350037\n",
      "Epoch 15, Batch 826, Loss: 0.4923337399959564\n",
      "Epoch 15, Batch 827, Loss: 0.5045964121818542\n",
      "Epoch 15, Batch 828, Loss: 0.3930166959762573\n",
      "Epoch 15, Batch 829, Loss: 0.3772056996822357\n",
      "Epoch 15, Batch 830, Loss: 0.41264933347702026\n",
      "Epoch 15, Batch 831, Loss: 0.4105013906955719\n",
      "Epoch 15, Batch 832, Loss: 0.49416354298591614\n",
      "Epoch 15, Batch 833, Loss: 0.5265031456947327\n",
      "Epoch 15, Batch 834, Loss: 0.3054071366786957\n",
      "Epoch 15, Batch 835, Loss: 0.5749133825302124\n",
      "Epoch 15, Batch 836, Loss: 0.561576783657074\n",
      "Epoch 15, Batch 837, Loss: 0.4599061608314514\n",
      "Epoch 15, Batch 838, Loss: 0.470872163772583\n",
      "Epoch 15, Batch 839, Loss: 0.39585649967193604\n",
      "Epoch 15, Batch 840, Loss: 0.4248771071434021\n",
      "Epoch 15, Batch 841, Loss: 0.5029970407485962\n",
      "Epoch 15, Batch 842, Loss: 0.3418486714363098\n",
      "Epoch 15, Batch 843, Loss: 0.4247089624404907\n",
      "Epoch 15, Batch 844, Loss: 0.38077810406684875\n",
      "Epoch 15, Batch 845, Loss: 0.39338940382003784\n",
      "Epoch 15, Batch 846, Loss: 0.46043962240219116\n",
      "Epoch 15, Batch 847, Loss: 0.5303869247436523\n",
      "Epoch 15, Batch 848, Loss: 0.27309349179267883\n",
      "Epoch 15, Batch 849, Loss: 0.5821859836578369\n",
      "Epoch 15, Batch 850, Loss: 0.5467482805252075\n",
      "Epoch 15, Batch 851, Loss: 0.578313946723938\n",
      "Epoch 15, Batch 852, Loss: 0.43664735555648804\n",
      "Epoch 15, Batch 853, Loss: 0.4189761281013489\n",
      "Epoch 15, Batch 854, Loss: 0.40245774388313293\n",
      "Epoch 15, Batch 855, Loss: 0.4480896294116974\n",
      "Epoch 15, Batch 856, Loss: 0.4198160767555237\n",
      "Epoch 15, Batch 857, Loss: 0.3965023159980774\n",
      "Epoch 15, Batch 858, Loss: 0.45612943172454834\n",
      "Epoch 15, Batch 859, Loss: 0.41545912623405457\n",
      "Epoch 15, Batch 860, Loss: 0.6250693202018738\n",
      "Epoch 15, Batch 861, Loss: 0.2958766520023346\n",
      "Epoch 15, Batch 862, Loss: 0.40824684500694275\n",
      "Epoch 15, Batch 863, Loss: 0.2948985695838928\n",
      "Epoch 15, Batch 864, Loss: 0.4329705536365509\n",
      "Epoch 15, Batch 865, Loss: 0.4075573682785034\n",
      "Epoch 15, Batch 866, Loss: 0.5644863247871399\n",
      "Epoch 15, Batch 867, Loss: 0.6680001616477966\n",
      "Epoch 15, Batch 868, Loss: 0.39708811044692993\n",
      "Epoch 15, Batch 869, Loss: 0.48172491788864136\n",
      "Epoch 15, Batch 870, Loss: 0.20540250837802887\n",
      "Epoch 15, Batch 871, Loss: 0.5331801176071167\n",
      "Epoch 15, Batch 872, Loss: 0.31972646713256836\n",
      "Epoch 15, Batch 873, Loss: 0.4948981702327728\n",
      "Epoch 15, Batch 874, Loss: 0.5528334975242615\n",
      "Epoch 15, Batch 875, Loss: 0.4302491843700409\n",
      "Epoch 15, Batch 876, Loss: 0.34951409697532654\n",
      "Epoch 15, Batch 877, Loss: 0.4903896450996399\n",
      "Epoch 15, Batch 878, Loss: 0.26874321699142456\n",
      "Epoch 15, Batch 879, Loss: 0.4470233619213104\n",
      "Epoch 15, Batch 880, Loss: 0.4531552791595459\n",
      "Epoch 15, Batch 881, Loss: 0.4231003522872925\n",
      "Epoch 15, Batch 882, Loss: 0.5796820521354675\n",
      "Epoch 15, Batch 883, Loss: 0.39632660150527954\n",
      "Epoch 15, Batch 884, Loss: 0.42985838651657104\n",
      "Epoch 15, Batch 885, Loss: 0.5281262993812561\n",
      "Epoch 15, Batch 886, Loss: 0.6022776961326599\n",
      "Epoch 15, Batch 887, Loss: 0.46176677942276\n",
      "Epoch 15, Batch 888, Loss: 0.47466766834259033\n",
      "Epoch 15, Batch 889, Loss: 0.5051100254058838\n",
      "Epoch 15, Batch 890, Loss: 0.5112954378128052\n",
      "Epoch 15, Batch 891, Loss: 0.6352741718292236\n",
      "Epoch 15, Batch 892, Loss: 0.5777684450149536\n",
      "Epoch 15, Batch 893, Loss: 0.42332160472869873\n",
      "Epoch 15, Batch 894, Loss: 0.49256545305252075\n",
      "Epoch 15, Batch 895, Loss: 0.6095729470252991\n",
      "Epoch 15, Batch 896, Loss: 0.48025524616241455\n",
      "Epoch 15, Batch 897, Loss: 0.4721485674381256\n",
      "Epoch 15, Batch 898, Loss: 0.6349932551383972\n",
      "Epoch 15, Batch 899, Loss: 0.4007604420185089\n",
      "Epoch 15, Batch 900, Loss: 0.5108059644699097\n",
      "Epoch 15, Batch 901, Loss: 0.27505066990852356\n",
      "Epoch 15, Batch 902, Loss: 0.4675872325897217\n",
      "Epoch 15, Batch 903, Loss: 0.5586629509925842\n",
      "Epoch 15, Batch 904, Loss: 0.6149709820747375\n",
      "Epoch 15, Batch 905, Loss: 0.40338990092277527\n",
      "Epoch 15, Batch 906, Loss: 0.44576966762542725\n",
      "Epoch 15, Batch 907, Loss: 0.6844592094421387\n",
      "Epoch 15, Batch 908, Loss: 0.5006099343299866\n",
      "Epoch 15, Batch 909, Loss: 0.5955455899238586\n",
      "Epoch 15, Batch 910, Loss: 0.38739216327667236\n",
      "Epoch 15, Batch 911, Loss: 0.34078988432884216\n",
      "Epoch 15, Batch 912, Loss: 0.23216192424297333\n",
      "Epoch 15, Batch 913, Loss: 0.4207831621170044\n",
      "Epoch 15, Batch 914, Loss: 0.40532398223876953\n",
      "Epoch 15, Batch 915, Loss: 0.4628409743309021\n",
      "Epoch 15, Batch 916, Loss: 0.6557626724243164\n",
      "Epoch 15, Batch 917, Loss: 0.4320051670074463\n",
      "Epoch 15, Batch 918, Loss: 0.6049079895019531\n",
      "Epoch 15, Batch 919, Loss: 0.35481929779052734\n",
      "Epoch 15, Batch 920, Loss: 0.43835288286209106\n",
      "Epoch 15, Batch 921, Loss: 0.434128075838089\n",
      "Epoch 15, Batch 922, Loss: 0.3760656416416168\n",
      "Epoch 15, Batch 923, Loss: 0.45274046063423157\n",
      "Epoch 15, Batch 924, Loss: 0.39281898736953735\n",
      "Epoch 15, Batch 925, Loss: 0.627519965171814\n",
      "Epoch 15, Batch 926, Loss: 0.3511732220649719\n",
      "Epoch 15, Batch 927, Loss: 0.32963594794273376\n",
      "Epoch 15, Batch 928, Loss: 0.37281420826911926\n",
      "Epoch 15, Batch 929, Loss: 0.5296523571014404\n",
      "Epoch 15, Batch 930, Loss: 0.41332560777664185\n",
      "Epoch 15, Batch 931, Loss: 0.41021743416786194\n",
      "Epoch 15, Batch 932, Loss: 0.32599785923957825\n",
      "Epoch 15, Batch 933, Loss: 0.4961041510105133\n",
      "Epoch 15, Batch 934, Loss: 0.31050071120262146\n",
      "Epoch 15, Batch 935, Loss: 0.41388192772865295\n",
      "Epoch 15, Batch 936, Loss: 0.34590625762939453\n",
      "Epoch 15, Batch 937, Loss: 0.9549517035484314\n",
      "Epoch 15, Batch 938, Loss: 0.36027273535728455\n",
      "Accuracy of train set: 0.8384666666666667\n",
      "Epoch 15, Batch 1, Test Loss: 0.5079160332679749\n",
      "Epoch 15, Batch 2, Test Loss: 0.41562679409980774\n",
      "Epoch 15, Batch 3, Test Loss: 0.4724562168121338\n",
      "Epoch 15, Batch 4, Test Loss: 0.40411555767059326\n",
      "Epoch 15, Batch 5, Test Loss: 0.4904727041721344\n",
      "Epoch 15, Batch 6, Test Loss: 0.571369469165802\n",
      "Epoch 15, Batch 7, Test Loss: 0.4832315742969513\n",
      "Epoch 15, Batch 8, Test Loss: 0.4772522449493408\n",
      "Epoch 15, Batch 9, Test Loss: 0.5163823366165161\n",
      "Epoch 15, Batch 10, Test Loss: 0.4219481647014618\n",
      "Epoch 15, Batch 11, Test Loss: 0.4108341336250305\n",
      "Epoch 15, Batch 12, Test Loss: 0.47455766797065735\n",
      "Epoch 15, Batch 13, Test Loss: 0.39633482694625854\n",
      "Epoch 15, Batch 14, Test Loss: 0.47834959626197815\n",
      "Epoch 15, Batch 15, Test Loss: 0.633289098739624\n",
      "Epoch 15, Batch 16, Test Loss: 0.35402336716651917\n",
      "Epoch 15, Batch 17, Test Loss: 0.44717544317245483\n",
      "Epoch 15, Batch 18, Test Loss: 0.3717386722564697\n",
      "Epoch 15, Batch 19, Test Loss: 0.36465832591056824\n",
      "Epoch 15, Batch 20, Test Loss: 0.4840719699859619\n",
      "Epoch 15, Batch 21, Test Loss: 0.4286300837993622\n",
      "Epoch 15, Batch 22, Test Loss: 0.5949289202690125\n",
      "Epoch 15, Batch 23, Test Loss: 0.3603278696537018\n",
      "Epoch 15, Batch 24, Test Loss: 0.37449243664741516\n",
      "Epoch 15, Batch 25, Test Loss: 0.4080081582069397\n",
      "Epoch 15, Batch 26, Test Loss: 0.3341965079307556\n",
      "Epoch 15, Batch 27, Test Loss: 0.44213178753852844\n",
      "Epoch 15, Batch 28, Test Loss: 0.5128073692321777\n",
      "Epoch 15, Batch 29, Test Loss: 0.4108160436153412\n",
      "Epoch 15, Batch 30, Test Loss: 0.5636364221572876\n",
      "Epoch 15, Batch 31, Test Loss: 0.6192138195037842\n",
      "Epoch 15, Batch 32, Test Loss: 0.4671899080276489\n",
      "Epoch 15, Batch 33, Test Loss: 0.42197340726852417\n",
      "Epoch 15, Batch 34, Test Loss: 0.3344888389110565\n",
      "Epoch 15, Batch 35, Test Loss: 0.542381763458252\n",
      "Epoch 15, Batch 36, Test Loss: 0.46135300397872925\n",
      "Epoch 15, Batch 37, Test Loss: 0.38837337493896484\n",
      "Epoch 15, Batch 38, Test Loss: 0.30044156312942505\n",
      "Epoch 15, Batch 39, Test Loss: 0.362494558095932\n",
      "Epoch 15, Batch 40, Test Loss: 0.4027286469936371\n",
      "Epoch 15, Batch 41, Test Loss: 0.5214987993240356\n",
      "Epoch 15, Batch 42, Test Loss: 0.4654309153556824\n",
      "Epoch 15, Batch 43, Test Loss: 0.4956037998199463\n",
      "Epoch 15, Batch 44, Test Loss: 0.7161943316459656\n",
      "Epoch 15, Batch 45, Test Loss: 0.36490947008132935\n",
      "Epoch 15, Batch 46, Test Loss: 0.38425320386886597\n",
      "Epoch 15, Batch 47, Test Loss: 0.4340798854827881\n",
      "Epoch 15, Batch 48, Test Loss: 0.30000317096710205\n",
      "Epoch 15, Batch 49, Test Loss: 0.3505004644393921\n",
      "Epoch 15, Batch 50, Test Loss: 0.34581509232521057\n",
      "Epoch 15, Batch 51, Test Loss: 0.36066603660583496\n",
      "Epoch 15, Batch 52, Test Loss: 0.44738537073135376\n",
      "Epoch 15, Batch 53, Test Loss: 0.5441027283668518\n",
      "Epoch 15, Batch 54, Test Loss: 0.44027993083000183\n",
      "Epoch 15, Batch 55, Test Loss: 0.635090708732605\n",
      "Epoch 15, Batch 56, Test Loss: 0.43964657187461853\n",
      "Epoch 15, Batch 57, Test Loss: 0.31587204337120056\n",
      "Epoch 15, Batch 58, Test Loss: 0.34356313943862915\n",
      "Epoch 15, Batch 59, Test Loss: 0.43705853819847107\n",
      "Epoch 15, Batch 60, Test Loss: 0.46626827120780945\n",
      "Epoch 15, Batch 61, Test Loss: 0.3792809247970581\n",
      "Epoch 15, Batch 62, Test Loss: 0.37619075179100037\n",
      "Epoch 15, Batch 63, Test Loss: 0.46095508337020874\n",
      "Epoch 15, Batch 64, Test Loss: 0.4770011305809021\n",
      "Epoch 15, Batch 65, Test Loss: 0.3358394205570221\n",
      "Epoch 15, Batch 66, Test Loss: 0.2703002691268921\n",
      "Epoch 15, Batch 67, Test Loss: 0.4458674192428589\n",
      "Epoch 15, Batch 68, Test Loss: 0.8656450510025024\n",
      "Epoch 15, Batch 69, Test Loss: 0.46195465326309204\n",
      "Epoch 15, Batch 70, Test Loss: 0.5703639388084412\n",
      "Epoch 15, Batch 71, Test Loss: 0.36473727226257324\n",
      "Epoch 15, Batch 72, Test Loss: 0.3781024217605591\n",
      "Epoch 15, Batch 73, Test Loss: 0.3497084081172943\n",
      "Epoch 15, Batch 74, Test Loss: 0.4575818181037903\n",
      "Epoch 15, Batch 75, Test Loss: 0.4984375834465027\n",
      "Epoch 15, Batch 76, Test Loss: 0.5293647050857544\n",
      "Epoch 15, Batch 77, Test Loss: 0.3348293900489807\n",
      "Epoch 15, Batch 78, Test Loss: 0.43468815088272095\n",
      "Epoch 15, Batch 79, Test Loss: 0.43061915040016174\n",
      "Epoch 15, Batch 80, Test Loss: 0.3711463212966919\n",
      "Epoch 15, Batch 81, Test Loss: 0.3064814507961273\n",
      "Epoch 15, Batch 82, Test Loss: 0.4532802700996399\n",
      "Epoch 15, Batch 83, Test Loss: 0.4118386507034302\n",
      "Epoch 15, Batch 84, Test Loss: 0.5558102130889893\n",
      "Epoch 15, Batch 85, Test Loss: 0.40121012926101685\n",
      "Epoch 15, Batch 86, Test Loss: 0.3089110851287842\n",
      "Epoch 15, Batch 87, Test Loss: 0.5246003866195679\n",
      "Epoch 15, Batch 88, Test Loss: 0.43010413646698\n",
      "Epoch 15, Batch 89, Test Loss: 0.4467364549636841\n",
      "Epoch 15, Batch 90, Test Loss: 0.2673718333244324\n",
      "Epoch 15, Batch 91, Test Loss: 0.6290736794471741\n",
      "Epoch 15, Batch 92, Test Loss: 0.594932496547699\n",
      "Epoch 15, Batch 93, Test Loss: 0.49755358695983887\n",
      "Epoch 15, Batch 94, Test Loss: 0.4616709351539612\n",
      "Epoch 15, Batch 95, Test Loss: 0.49382466077804565\n",
      "Epoch 15, Batch 96, Test Loss: 0.7040361166000366\n",
      "Epoch 15, Batch 97, Test Loss: 0.4093630015850067\n",
      "Epoch 15, Batch 98, Test Loss: 0.43536826968193054\n",
      "Epoch 15, Batch 99, Test Loss: 0.3512676954269409\n",
      "Epoch 15, Batch 100, Test Loss: 0.3152644634246826\n",
      "Epoch 15, Batch 101, Test Loss: 0.47234398126602173\n",
      "Epoch 15, Batch 102, Test Loss: 0.38465991616249084\n",
      "Epoch 15, Batch 103, Test Loss: 0.5364977121353149\n",
      "Epoch 15, Batch 104, Test Loss: 0.44466471672058105\n",
      "Epoch 15, Batch 105, Test Loss: 0.4097023010253906\n",
      "Epoch 15, Batch 106, Test Loss: 0.5800749659538269\n",
      "Epoch 15, Batch 107, Test Loss: 0.6177337169647217\n",
      "Epoch 15, Batch 108, Test Loss: 0.49307727813720703\n",
      "Epoch 15, Batch 109, Test Loss: 0.389206200838089\n",
      "Epoch 15, Batch 110, Test Loss: 0.5577038526535034\n",
      "Epoch 15, Batch 111, Test Loss: 0.39011332392692566\n",
      "Epoch 15, Batch 112, Test Loss: 0.5882202386856079\n",
      "Epoch 15, Batch 113, Test Loss: 0.4631204903125763\n",
      "Epoch 15, Batch 114, Test Loss: 0.4865386486053467\n",
      "Epoch 15, Batch 115, Test Loss: 0.43587493896484375\n",
      "Epoch 15, Batch 116, Test Loss: 0.4643414318561554\n",
      "Epoch 15, Batch 117, Test Loss: 0.5321309566497803\n",
      "Epoch 15, Batch 118, Test Loss: 0.5324879884719849\n",
      "Epoch 15, Batch 119, Test Loss: 0.5815727710723877\n",
      "Epoch 15, Batch 120, Test Loss: 0.6005290746688843\n",
      "Epoch 15, Batch 121, Test Loss: 0.41583192348480225\n",
      "Epoch 15, Batch 122, Test Loss: 0.5680875182151794\n",
      "Epoch 15, Batch 123, Test Loss: 0.31514692306518555\n",
      "Epoch 15, Batch 124, Test Loss: 0.4039264917373657\n",
      "Epoch 15, Batch 125, Test Loss: 0.4628453552722931\n",
      "Epoch 15, Batch 126, Test Loss: 0.4169289171695709\n",
      "Epoch 15, Batch 127, Test Loss: 0.45486873388290405\n",
      "Epoch 15, Batch 128, Test Loss: 0.5049815773963928\n",
      "Epoch 15, Batch 129, Test Loss: 0.3769387900829315\n",
      "Epoch 15, Batch 130, Test Loss: 0.5161467790603638\n",
      "Epoch 15, Batch 131, Test Loss: 0.5650464296340942\n",
      "Epoch 15, Batch 132, Test Loss: 0.38869959115982056\n",
      "Epoch 15, Batch 133, Test Loss: 0.4733330011367798\n",
      "Epoch 15, Batch 134, Test Loss: 0.4067748785018921\n",
      "Epoch 15, Batch 135, Test Loss: 0.3829870820045471\n",
      "Epoch 15, Batch 136, Test Loss: 0.7078276872634888\n",
      "Epoch 15, Batch 137, Test Loss: 0.47970083355903625\n",
      "Epoch 15, Batch 138, Test Loss: 0.4308212995529175\n",
      "Epoch 15, Batch 139, Test Loss: 0.37489575147628784\n",
      "Epoch 15, Batch 140, Test Loss: 0.4773126244544983\n",
      "Epoch 15, Batch 141, Test Loss: 0.4465938210487366\n",
      "Epoch 15, Batch 142, Test Loss: 0.5715146660804749\n",
      "Epoch 15, Batch 143, Test Loss: 0.44535428285598755\n",
      "Epoch 15, Batch 144, Test Loss: 0.3137904405593872\n",
      "Epoch 15, Batch 145, Test Loss: 0.3909124433994293\n",
      "Epoch 15, Batch 146, Test Loss: 0.4964417517185211\n",
      "Epoch 15, Batch 147, Test Loss: 0.4089960753917694\n",
      "Epoch 15, Batch 148, Test Loss: 0.5250711441040039\n",
      "Epoch 15, Batch 149, Test Loss: 0.5157901048660278\n",
      "Epoch 15, Batch 150, Test Loss: 0.4658159911632538\n",
      "Epoch 15, Batch 151, Test Loss: 0.5131153464317322\n",
      "Epoch 15, Batch 152, Test Loss: 0.3577363193035126\n",
      "Epoch 15, Batch 153, Test Loss: 0.41967132687568665\n",
      "Epoch 15, Batch 154, Test Loss: 0.3296205401420593\n",
      "Epoch 15, Batch 155, Test Loss: 0.6712532639503479\n",
      "Epoch 15, Batch 156, Test Loss: 0.3529127538204193\n",
      "Epoch 15, Batch 157, Test Loss: 0.502273440361023\n",
      "Epoch 15, Batch 158, Test Loss: 0.6466041803359985\n",
      "Epoch 15, Batch 159, Test Loss: 0.5484253168106079\n",
      "Epoch 15, Batch 160, Test Loss: 0.5699527859687805\n",
      "Epoch 15, Batch 161, Test Loss: 0.41375628113746643\n",
      "Epoch 15, Batch 162, Test Loss: 0.43732762336730957\n",
      "Epoch 15, Batch 163, Test Loss: 0.5143821835517883\n",
      "Epoch 15, Batch 164, Test Loss: 0.2704339623451233\n",
      "Epoch 15, Batch 165, Test Loss: 0.3044920861721039\n",
      "Epoch 15, Batch 166, Test Loss: 0.4032546281814575\n",
      "Epoch 15, Batch 167, Test Loss: 0.35412147641181946\n",
      "Epoch 15, Batch 168, Test Loss: 0.2880886197090149\n",
      "Epoch 15, Batch 169, Test Loss: 0.475139856338501\n",
      "Epoch 15, Batch 170, Test Loss: 0.7580292820930481\n",
      "Epoch 15, Batch 171, Test Loss: 0.5031263828277588\n",
      "Epoch 15, Batch 172, Test Loss: 0.4057632386684418\n",
      "Epoch 15, Batch 173, Test Loss: 0.4358178377151489\n",
      "Epoch 15, Batch 174, Test Loss: 0.47376400232315063\n",
      "Epoch 15, Batch 175, Test Loss: 0.615082323551178\n",
      "Epoch 15, Batch 176, Test Loss: 0.3969641923904419\n",
      "Epoch 15, Batch 177, Test Loss: 0.5005465149879456\n",
      "Epoch 15, Batch 178, Test Loss: 0.3797299861907959\n",
      "Epoch 15, Batch 179, Test Loss: 0.44930532574653625\n",
      "Epoch 15, Batch 180, Test Loss: 0.4365359842777252\n",
      "Epoch 15, Batch 181, Test Loss: 0.4163465201854706\n",
      "Epoch 15, Batch 182, Test Loss: 0.6863136887550354\n",
      "Epoch 15, Batch 183, Test Loss: 0.5310465097427368\n",
      "Epoch 15, Batch 184, Test Loss: 0.4005717933177948\n",
      "Epoch 15, Batch 185, Test Loss: 0.3995717167854309\n",
      "Epoch 15, Batch 186, Test Loss: 0.6294174194335938\n",
      "Epoch 15, Batch 187, Test Loss: 0.5437372922897339\n",
      "Epoch 15, Batch 188, Test Loss: 0.5184961557388306\n",
      "Epoch 15, Batch 189, Test Loss: 0.4647980332374573\n",
      "Epoch 15, Batch 190, Test Loss: 0.4681859612464905\n",
      "Epoch 15, Batch 191, Test Loss: 0.46985799074172974\n",
      "Epoch 15, Batch 192, Test Loss: 0.4829559326171875\n",
      "Epoch 15, Batch 193, Test Loss: 0.2868848443031311\n",
      "Epoch 15, Batch 194, Test Loss: 0.3727954924106598\n",
      "Epoch 15, Batch 195, Test Loss: 0.5473266243934631\n",
      "Epoch 15, Batch 196, Test Loss: 0.46049803495407104\n",
      "Epoch 15, Batch 197, Test Loss: 0.4605400264263153\n",
      "Epoch 15, Batch 198, Test Loss: 0.6845498085021973\n",
      "Epoch 15, Batch 199, Test Loss: 0.4916638135910034\n",
      "Epoch 15, Batch 200, Test Loss: 0.2965836226940155\n",
      "Epoch 15, Batch 201, Test Loss: 0.3920827805995941\n",
      "Epoch 15, Batch 202, Test Loss: 0.417524516582489\n",
      "Epoch 15, Batch 203, Test Loss: 0.40498748421669006\n",
      "Epoch 15, Batch 204, Test Loss: 0.5845104455947876\n",
      "Epoch 15, Batch 205, Test Loss: 0.2794482111930847\n",
      "Epoch 15, Batch 206, Test Loss: 0.4940245449542999\n",
      "Epoch 15, Batch 207, Test Loss: 0.6855626106262207\n",
      "Epoch 15, Batch 208, Test Loss: 0.35527610778808594\n",
      "Epoch 15, Batch 209, Test Loss: 0.6239136457443237\n",
      "Epoch 15, Batch 210, Test Loss: 0.5549314022064209\n",
      "Epoch 15, Batch 211, Test Loss: 0.6668972373008728\n",
      "Epoch 15, Batch 212, Test Loss: 0.49140092730522156\n",
      "Epoch 15, Batch 213, Test Loss: 0.4710466265678406\n",
      "Epoch 15, Batch 214, Test Loss: 0.4430163502693176\n",
      "Epoch 15, Batch 215, Test Loss: 0.5315293073654175\n",
      "Epoch 15, Batch 216, Test Loss: 0.45970454812049866\n",
      "Epoch 15, Batch 217, Test Loss: 0.38773107528686523\n",
      "Epoch 15, Batch 218, Test Loss: 0.5666227340698242\n",
      "Epoch 15, Batch 219, Test Loss: 0.4390883445739746\n",
      "Epoch 15, Batch 220, Test Loss: 0.4043568968772888\n",
      "Epoch 15, Batch 221, Test Loss: 0.466028094291687\n",
      "Epoch 15, Batch 222, Test Loss: 0.4296649098396301\n",
      "Epoch 15, Batch 223, Test Loss: 0.5958282351493835\n",
      "Epoch 15, Batch 224, Test Loss: 0.4192909300327301\n",
      "Epoch 15, Batch 225, Test Loss: 0.2747228741645813\n",
      "Epoch 15, Batch 226, Test Loss: 0.49395039677619934\n",
      "Epoch 15, Batch 227, Test Loss: 0.28119736909866333\n",
      "Epoch 15, Batch 228, Test Loss: 0.4418182671070099\n",
      "Epoch 15, Batch 229, Test Loss: 0.5199810862541199\n",
      "Epoch 15, Batch 230, Test Loss: 0.5412733554840088\n",
      "Epoch 15, Batch 231, Test Loss: 0.4477175176143646\n",
      "Epoch 15, Batch 232, Test Loss: 0.6417685151100159\n",
      "Epoch 15, Batch 233, Test Loss: 0.4797019958496094\n",
      "Epoch 15, Batch 234, Test Loss: 0.21518126130104065\n",
      "Epoch 15, Batch 235, Test Loss: 0.4597535729408264\n",
      "Epoch 15, Batch 236, Test Loss: 0.45710933208465576\n",
      "Epoch 15, Batch 237, Test Loss: 0.3688428997993469\n",
      "Epoch 15, Batch 238, Test Loss: 0.47626417875289917\n",
      "Epoch 15, Batch 239, Test Loss: 0.5096684098243713\n",
      "Epoch 15, Batch 240, Test Loss: 0.5008058547973633\n",
      "Epoch 15, Batch 241, Test Loss: 0.39542222023010254\n",
      "Epoch 15, Batch 242, Test Loss: 0.33189138770103455\n",
      "Epoch 15, Batch 243, Test Loss: 0.5345891118049622\n",
      "Epoch 15, Batch 244, Test Loss: 0.5274704694747925\n",
      "Epoch 15, Batch 245, Test Loss: 0.46510571241378784\n",
      "Epoch 15, Batch 246, Test Loss: 0.43993544578552246\n",
      "Epoch 15, Batch 247, Test Loss: 0.346477210521698\n",
      "Epoch 15, Batch 248, Test Loss: 0.49899283051490784\n",
      "Epoch 15, Batch 249, Test Loss: 0.37862908840179443\n",
      "Epoch 15, Batch 250, Test Loss: 0.45328718423843384\n",
      "Epoch 15, Batch 251, Test Loss: 0.4700498878955841\n",
      "Epoch 15, Batch 252, Test Loss: 0.5444402098655701\n",
      "Epoch 15, Batch 253, Test Loss: 0.4134728014469147\n",
      "Epoch 15, Batch 254, Test Loss: 0.6503684520721436\n",
      "Epoch 15, Batch 255, Test Loss: 0.4576398432254791\n",
      "Epoch 15, Batch 256, Test Loss: 0.5233418941497803\n",
      "Epoch 15, Batch 257, Test Loss: 0.3906427025794983\n",
      "Epoch 15, Batch 258, Test Loss: 0.4989965856075287\n",
      "Epoch 15, Batch 259, Test Loss: 0.5196381211280823\n",
      "Epoch 15, Batch 260, Test Loss: 0.4527301490306854\n",
      "Epoch 15, Batch 261, Test Loss: 0.8091503381729126\n",
      "Epoch 15, Batch 262, Test Loss: 0.5363392233848572\n",
      "Epoch 15, Batch 263, Test Loss: 0.33712637424468994\n",
      "Epoch 15, Batch 264, Test Loss: 0.5305542945861816\n",
      "Epoch 15, Batch 265, Test Loss: 0.49663668870925903\n",
      "Epoch 15, Batch 266, Test Loss: 0.561554491519928\n",
      "Epoch 15, Batch 267, Test Loss: 0.46426627039909363\n",
      "Epoch 15, Batch 268, Test Loss: 0.3441394567489624\n",
      "Epoch 15, Batch 269, Test Loss: 0.3889637887477875\n",
      "Epoch 15, Batch 270, Test Loss: 0.3494902551174164\n",
      "Epoch 15, Batch 271, Test Loss: 0.47032004594802856\n",
      "Epoch 15, Batch 272, Test Loss: 0.5844554901123047\n",
      "Epoch 15, Batch 273, Test Loss: 0.35197409987449646\n",
      "Epoch 15, Batch 274, Test Loss: 0.5434420108795166\n",
      "Epoch 15, Batch 275, Test Loss: 0.5971586108207703\n",
      "Epoch 15, Batch 276, Test Loss: 0.5014381408691406\n",
      "Epoch 15, Batch 277, Test Loss: 0.5162036418914795\n",
      "Epoch 15, Batch 278, Test Loss: 0.4278847277164459\n",
      "Epoch 15, Batch 279, Test Loss: 0.4954448640346527\n",
      "Epoch 15, Batch 280, Test Loss: 0.3630524277687073\n",
      "Epoch 15, Batch 281, Test Loss: 0.42886674404144287\n",
      "Epoch 15, Batch 282, Test Loss: 0.35425087809562683\n",
      "Epoch 15, Batch 283, Test Loss: 0.5170578956604004\n",
      "Epoch 15, Batch 284, Test Loss: 0.440888375043869\n",
      "Epoch 15, Batch 285, Test Loss: 0.3608854413032532\n",
      "Epoch 15, Batch 286, Test Loss: 0.5533584952354431\n",
      "Epoch 15, Batch 287, Test Loss: 0.4017185866832733\n",
      "Epoch 15, Batch 288, Test Loss: 0.3025740683078766\n",
      "Epoch 15, Batch 289, Test Loss: 0.6809485554695129\n",
      "Epoch 15, Batch 290, Test Loss: 0.39304277300834656\n",
      "Epoch 15, Batch 291, Test Loss: 0.6345471739768982\n",
      "Epoch 15, Batch 292, Test Loss: 0.5175391435623169\n",
      "Epoch 15, Batch 293, Test Loss: 0.6006807684898376\n",
      "Epoch 15, Batch 294, Test Loss: 0.5303754210472107\n",
      "Epoch 15, Batch 295, Test Loss: 0.4320010840892792\n",
      "Epoch 15, Batch 296, Test Loss: 0.6874632239341736\n",
      "Epoch 15, Batch 297, Test Loss: 0.4370710253715515\n",
      "Epoch 15, Batch 298, Test Loss: 0.40865010023117065\n",
      "Epoch 15, Batch 299, Test Loss: 0.368089497089386\n",
      "Epoch 15, Batch 300, Test Loss: 0.4587355852127075\n",
      "Epoch 15, Batch 301, Test Loss: 0.41369935870170593\n",
      "Epoch 15, Batch 302, Test Loss: 0.2914157807826996\n",
      "Epoch 15, Batch 303, Test Loss: 0.5154217481613159\n",
      "Epoch 15, Batch 304, Test Loss: 0.5979620814323425\n",
      "Epoch 15, Batch 305, Test Loss: 0.5129196047782898\n",
      "Epoch 15, Batch 306, Test Loss: 0.3452599346637726\n",
      "Epoch 15, Batch 307, Test Loss: 0.39985018968582153\n",
      "Epoch 15, Batch 308, Test Loss: 0.4685363173484802\n",
      "Epoch 15, Batch 309, Test Loss: 0.47849732637405396\n",
      "Epoch 15, Batch 310, Test Loss: 0.3720196783542633\n",
      "Epoch 15, Batch 311, Test Loss: 0.3257047235965729\n",
      "Epoch 15, Batch 312, Test Loss: 0.49858948588371277\n",
      "Epoch 15, Batch 313, Test Loss: 0.6568434238433838\n",
      "Epoch 15, Batch 314, Test Loss: 0.5668877959251404\n",
      "Epoch 15, Batch 315, Test Loss: 0.37681490182876587\n",
      "Epoch 15, Batch 316, Test Loss: 0.5062710642814636\n",
      "Epoch 15, Batch 317, Test Loss: 0.3965592682361603\n",
      "Epoch 15, Batch 318, Test Loss: 0.4933314323425293\n",
      "Epoch 15, Batch 319, Test Loss: 0.42701393365859985\n",
      "Epoch 15, Batch 320, Test Loss: 0.4501054286956787\n",
      "Epoch 15, Batch 321, Test Loss: 0.4491148293018341\n",
      "Epoch 15, Batch 322, Test Loss: 0.4866408705711365\n",
      "Epoch 15, Batch 323, Test Loss: 0.42363619804382324\n",
      "Epoch 15, Batch 324, Test Loss: 0.5413619875907898\n",
      "Epoch 15, Batch 325, Test Loss: 0.44948503375053406\n",
      "Epoch 15, Batch 326, Test Loss: 0.5366795659065247\n",
      "Epoch 15, Batch 327, Test Loss: 0.29325225949287415\n",
      "Epoch 15, Batch 328, Test Loss: 0.43816184997558594\n",
      "Epoch 15, Batch 329, Test Loss: 0.2623400092124939\n",
      "Epoch 15, Batch 330, Test Loss: 0.4667666554450989\n",
      "Epoch 15, Batch 331, Test Loss: 0.3683969974517822\n",
      "Epoch 15, Batch 332, Test Loss: 0.36944904923439026\n",
      "Epoch 15, Batch 333, Test Loss: 0.343936026096344\n",
      "Epoch 15, Batch 334, Test Loss: 0.46381819248199463\n",
      "Epoch 15, Batch 335, Test Loss: 0.36735865473747253\n",
      "Epoch 15, Batch 336, Test Loss: 0.36624807119369507\n",
      "Epoch 15, Batch 337, Test Loss: 0.5001922249794006\n",
      "Epoch 15, Batch 338, Test Loss: 0.45681655406951904\n",
      "Epoch 15, Batch 339, Test Loss: 0.37401455640792847\n",
      "Epoch 15, Batch 340, Test Loss: 0.33749720454216003\n",
      "Epoch 15, Batch 341, Test Loss: 0.4574998915195465\n",
      "Epoch 15, Batch 342, Test Loss: 0.5078825950622559\n",
      "Epoch 15, Batch 343, Test Loss: 0.42002230882644653\n",
      "Epoch 15, Batch 344, Test Loss: 0.3134692311286926\n",
      "Epoch 15, Batch 345, Test Loss: 0.644544780254364\n",
      "Epoch 15, Batch 346, Test Loss: 0.4902490973472595\n",
      "Epoch 15, Batch 347, Test Loss: 0.4770267605781555\n",
      "Epoch 15, Batch 348, Test Loss: 0.32150521874427795\n",
      "Epoch 15, Batch 349, Test Loss: 0.43721604347229004\n",
      "Epoch 15, Batch 350, Test Loss: 0.5591388940811157\n",
      "Epoch 15, Batch 351, Test Loss: 0.3525250256061554\n",
      "Epoch 15, Batch 352, Test Loss: 0.5433583855628967\n",
      "Epoch 15, Batch 353, Test Loss: 0.4329785406589508\n",
      "Epoch 15, Batch 354, Test Loss: 0.5413150191307068\n",
      "Epoch 15, Batch 355, Test Loss: 0.654971718788147\n",
      "Epoch 15, Batch 356, Test Loss: 0.27436763048171997\n",
      "Epoch 15, Batch 357, Test Loss: 0.4659675657749176\n",
      "Epoch 15, Batch 358, Test Loss: 0.4704086184501648\n",
      "Epoch 15, Batch 359, Test Loss: 0.31818002462387085\n",
      "Epoch 15, Batch 360, Test Loss: 0.4053841531276703\n",
      "Epoch 15, Batch 361, Test Loss: 0.5086141228675842\n",
      "Epoch 15, Batch 362, Test Loss: 0.4961012601852417\n",
      "Epoch 15, Batch 363, Test Loss: 0.39938709139823914\n",
      "Epoch 15, Batch 364, Test Loss: 0.5238645076751709\n",
      "Epoch 15, Batch 365, Test Loss: 0.42049798369407654\n",
      "Epoch 15, Batch 366, Test Loss: 0.44417011737823486\n",
      "Epoch 15, Batch 367, Test Loss: 0.3850543200969696\n",
      "Epoch 15, Batch 368, Test Loss: 0.4142288267612457\n",
      "Epoch 15, Batch 369, Test Loss: 0.37514954805374146\n",
      "Epoch 15, Batch 370, Test Loss: 0.4240122139453888\n",
      "Epoch 15, Batch 371, Test Loss: 0.7612859010696411\n",
      "Epoch 15, Batch 372, Test Loss: 0.43728160858154297\n",
      "Epoch 15, Batch 373, Test Loss: 0.29770055413246155\n",
      "Epoch 15, Batch 374, Test Loss: 0.3261479437351227\n",
      "Epoch 15, Batch 375, Test Loss: 0.4616052806377411\n",
      "Epoch 15, Batch 376, Test Loss: 0.4404035210609436\n",
      "Epoch 15, Batch 377, Test Loss: 0.5182608366012573\n",
      "Epoch 15, Batch 378, Test Loss: 0.45186877250671387\n",
      "Epoch 15, Batch 379, Test Loss: 0.5066325068473816\n",
      "Epoch 15, Batch 380, Test Loss: 0.3079302906990051\n",
      "Epoch 15, Batch 381, Test Loss: 0.426250696182251\n",
      "Epoch 15, Batch 382, Test Loss: 0.4412071406841278\n",
      "Epoch 15, Batch 383, Test Loss: 0.42163968086242676\n",
      "Epoch 15, Batch 384, Test Loss: 0.4233955144882202\n",
      "Epoch 15, Batch 385, Test Loss: 0.4465310275554657\n",
      "Epoch 15, Batch 386, Test Loss: 0.48432308435440063\n",
      "Epoch 15, Batch 387, Test Loss: 0.7548638582229614\n",
      "Epoch 15, Batch 388, Test Loss: 0.4546869397163391\n",
      "Epoch 15, Batch 389, Test Loss: 0.32586371898651123\n",
      "Epoch 15, Batch 390, Test Loss: 0.44017428159713745\n",
      "Epoch 15, Batch 391, Test Loss: 0.44281110167503357\n",
      "Epoch 15, Batch 392, Test Loss: 0.3971375823020935\n",
      "Epoch 15, Batch 393, Test Loss: 0.34500283002853394\n",
      "Epoch 15, Batch 394, Test Loss: 0.4398638606071472\n",
      "Epoch 15, Batch 395, Test Loss: 0.2983762323856354\n",
      "Epoch 15, Batch 396, Test Loss: 0.40312445163726807\n",
      "Epoch 15, Batch 397, Test Loss: 0.4960913360118866\n",
      "Epoch 15, Batch 398, Test Loss: 0.3004666864871979\n",
      "Epoch 15, Batch 399, Test Loss: 0.4249922037124634\n",
      "Epoch 15, Batch 400, Test Loss: 0.5231176614761353\n",
      "Epoch 15, Batch 401, Test Loss: 0.40428727865219116\n",
      "Epoch 15, Batch 402, Test Loss: 0.27705588936805725\n",
      "Epoch 15, Batch 403, Test Loss: 0.5741379857063293\n",
      "Epoch 15, Batch 404, Test Loss: 0.49165284633636475\n",
      "Epoch 15, Batch 405, Test Loss: 0.5550855398178101\n",
      "Epoch 15, Batch 406, Test Loss: 0.5010589957237244\n",
      "Epoch 15, Batch 407, Test Loss: 0.5800760984420776\n",
      "Epoch 15, Batch 408, Test Loss: 0.3798900544643402\n",
      "Epoch 15, Batch 409, Test Loss: 0.5824456214904785\n",
      "Epoch 15, Batch 410, Test Loss: 0.49881502985954285\n",
      "Epoch 15, Batch 411, Test Loss: 0.5068624019622803\n",
      "Epoch 15, Batch 412, Test Loss: 0.45713528990745544\n",
      "Epoch 15, Batch 413, Test Loss: 0.613548994064331\n",
      "Epoch 15, Batch 414, Test Loss: 0.5490891933441162\n",
      "Epoch 15, Batch 415, Test Loss: 0.6071522235870361\n",
      "Epoch 15, Batch 416, Test Loss: 0.46439608931541443\n",
      "Epoch 15, Batch 417, Test Loss: 0.48648566007614136\n",
      "Epoch 15, Batch 418, Test Loss: 0.284976601600647\n",
      "Epoch 15, Batch 419, Test Loss: 0.3743740916252136\n",
      "Epoch 15, Batch 420, Test Loss: 0.43091410398483276\n",
      "Epoch 15, Batch 421, Test Loss: 0.528416633605957\n",
      "Epoch 15, Batch 422, Test Loss: 0.42549067735671997\n",
      "Epoch 15, Batch 423, Test Loss: 0.36992448568344116\n",
      "Epoch 15, Batch 424, Test Loss: 0.5712741613388062\n",
      "Epoch 15, Batch 425, Test Loss: 0.43069109320640564\n",
      "Epoch 15, Batch 426, Test Loss: 0.3294713795185089\n",
      "Epoch 15, Batch 427, Test Loss: 0.3924175798892975\n",
      "Epoch 15, Batch 428, Test Loss: 0.3856413960456848\n",
      "Epoch 15, Batch 429, Test Loss: 0.38374900817871094\n",
      "Epoch 15, Batch 430, Test Loss: 0.46892088651657104\n",
      "Epoch 15, Batch 431, Test Loss: 0.3696224093437195\n",
      "Epoch 15, Batch 432, Test Loss: 0.3956373929977417\n",
      "Epoch 15, Batch 433, Test Loss: 0.36639735102653503\n",
      "Epoch 15, Batch 434, Test Loss: 0.4400135576725006\n",
      "Epoch 15, Batch 435, Test Loss: 0.3345046639442444\n",
      "Epoch 15, Batch 436, Test Loss: 0.1563788652420044\n",
      "Epoch 15, Batch 437, Test Loss: 0.380246639251709\n",
      "Epoch 15, Batch 438, Test Loss: 0.41103535890579224\n",
      "Epoch 15, Batch 439, Test Loss: 0.41779476404190063\n",
      "Epoch 15, Batch 440, Test Loss: 0.27888616919517517\n",
      "Epoch 15, Batch 441, Test Loss: 0.34614452719688416\n",
      "Epoch 15, Batch 442, Test Loss: 0.46917054057121277\n",
      "Epoch 15, Batch 443, Test Loss: 0.5338078737258911\n",
      "Epoch 15, Batch 444, Test Loss: 0.6164981722831726\n",
      "Epoch 15, Batch 445, Test Loss: 0.5766829252243042\n",
      "Epoch 15, Batch 446, Test Loss: 0.3927706182003021\n",
      "Epoch 15, Batch 447, Test Loss: 0.28091245889663696\n",
      "Epoch 15, Batch 448, Test Loss: 0.39224156737327576\n",
      "Epoch 15, Batch 449, Test Loss: 0.4398210644721985\n",
      "Epoch 15, Batch 450, Test Loss: 0.5544108152389526\n",
      "Epoch 15, Batch 451, Test Loss: 0.5168709754943848\n",
      "Epoch 15, Batch 452, Test Loss: 0.4942842125892639\n",
      "Epoch 15, Batch 453, Test Loss: 0.530704915523529\n",
      "Epoch 15, Batch 454, Test Loss: 0.465948224067688\n",
      "Epoch 15, Batch 455, Test Loss: 0.5036041736602783\n",
      "Epoch 15, Batch 456, Test Loss: 0.3571889400482178\n",
      "Epoch 15, Batch 457, Test Loss: 0.3390737771987915\n",
      "Epoch 15, Batch 458, Test Loss: 0.42162835597991943\n",
      "Epoch 15, Batch 459, Test Loss: 0.35658228397369385\n",
      "Epoch 15, Batch 460, Test Loss: 0.44087862968444824\n",
      "Epoch 15, Batch 461, Test Loss: 0.3595838248729706\n",
      "Epoch 15, Batch 462, Test Loss: 0.4423378109931946\n",
      "Epoch 15, Batch 463, Test Loss: 0.5018413662910461\n",
      "Epoch 15, Batch 464, Test Loss: 0.4193808138370514\n",
      "Epoch 15, Batch 465, Test Loss: 0.36249855160713196\n",
      "Epoch 15, Batch 466, Test Loss: 0.2600472569465637\n",
      "Epoch 15, Batch 467, Test Loss: 0.36789608001708984\n",
      "Epoch 15, Batch 468, Test Loss: 0.30317121744155884\n",
      "Epoch 15, Batch 469, Test Loss: 0.35430213809013367\n",
      "Epoch 15, Batch 470, Test Loss: 0.4253842830657959\n",
      "Epoch 15, Batch 471, Test Loss: 0.5949562191963196\n",
      "Epoch 15, Batch 472, Test Loss: 0.3395397365093231\n",
      "Epoch 15, Batch 473, Test Loss: 0.49734604358673096\n",
      "Epoch 15, Batch 474, Test Loss: 0.4851142168045044\n",
      "Epoch 15, Batch 475, Test Loss: 0.43116962909698486\n",
      "Epoch 15, Batch 476, Test Loss: 0.3052002787590027\n",
      "Epoch 15, Batch 477, Test Loss: 0.40221625566482544\n",
      "Epoch 15, Batch 478, Test Loss: 0.5269036293029785\n",
      "Epoch 15, Batch 479, Test Loss: 0.5556681156158447\n",
      "Epoch 15, Batch 480, Test Loss: 0.4264686703681946\n",
      "Epoch 15, Batch 481, Test Loss: 0.26482680439949036\n",
      "Epoch 15, Batch 482, Test Loss: 0.39231178164482117\n",
      "Epoch 15, Batch 483, Test Loss: 0.5327748656272888\n",
      "Epoch 15, Batch 484, Test Loss: 0.37342405319213867\n",
      "Epoch 15, Batch 485, Test Loss: 0.40871429443359375\n",
      "Epoch 15, Batch 486, Test Loss: 0.531740128993988\n",
      "Epoch 15, Batch 487, Test Loss: 0.5420622229576111\n",
      "Epoch 15, Batch 488, Test Loss: 0.4109145998954773\n",
      "Epoch 15, Batch 489, Test Loss: 0.5486495494842529\n",
      "Epoch 15, Batch 490, Test Loss: 0.4706663191318512\n",
      "Epoch 15, Batch 491, Test Loss: 0.3909033536911011\n",
      "Epoch 15, Batch 492, Test Loss: 0.564169704914093\n",
      "Epoch 15, Batch 493, Test Loss: 0.3981746435165405\n",
      "Epoch 15, Batch 494, Test Loss: 0.3426652252674103\n",
      "Epoch 15, Batch 495, Test Loss: 0.4998813271522522\n",
      "Epoch 15, Batch 496, Test Loss: 0.45683106780052185\n",
      "Epoch 15, Batch 497, Test Loss: 0.5142521262168884\n",
      "Epoch 15, Batch 498, Test Loss: 0.31132084131240845\n",
      "Epoch 15, Batch 499, Test Loss: 0.4716447591781616\n",
      "Epoch 15, Batch 500, Test Loss: 0.47420263290405273\n",
      "Epoch 15, Batch 501, Test Loss: 0.32527220249176025\n",
      "Epoch 15, Batch 502, Test Loss: 0.3706958591938019\n",
      "Epoch 15, Batch 503, Test Loss: 0.5661752223968506\n",
      "Epoch 15, Batch 504, Test Loss: 0.50063157081604\n",
      "Epoch 15, Batch 505, Test Loss: 0.2465718686580658\n",
      "Epoch 15, Batch 506, Test Loss: 0.5082538723945618\n",
      "Epoch 15, Batch 507, Test Loss: 0.53044593334198\n",
      "Epoch 15, Batch 508, Test Loss: 0.41352325677871704\n",
      "Epoch 15, Batch 509, Test Loss: 0.3827449381351471\n",
      "Epoch 15, Batch 510, Test Loss: 0.42832428216934204\n",
      "Epoch 15, Batch 511, Test Loss: 0.5692607164382935\n",
      "Epoch 15, Batch 512, Test Loss: 0.23042228817939758\n",
      "Epoch 15, Batch 513, Test Loss: 0.4212613105773926\n",
      "Epoch 15, Batch 514, Test Loss: 0.29068079590797424\n",
      "Epoch 15, Batch 515, Test Loss: 0.5980128049850464\n",
      "Epoch 15, Batch 516, Test Loss: 0.3893163800239563\n",
      "Epoch 15, Batch 517, Test Loss: 0.6080567836761475\n",
      "Epoch 15, Batch 518, Test Loss: 0.4859392046928406\n",
      "Epoch 15, Batch 519, Test Loss: 0.4791380763053894\n",
      "Epoch 15, Batch 520, Test Loss: 0.33741316199302673\n",
      "Epoch 15, Batch 521, Test Loss: 0.534088671207428\n",
      "Epoch 15, Batch 522, Test Loss: 0.5262956619262695\n",
      "Epoch 15, Batch 523, Test Loss: 0.4900587201118469\n",
      "Epoch 15, Batch 524, Test Loss: 0.6011757254600525\n",
      "Epoch 15, Batch 525, Test Loss: 0.5167577266693115\n",
      "Epoch 15, Batch 526, Test Loss: 0.5119882225990295\n",
      "Epoch 15, Batch 527, Test Loss: 0.529008686542511\n",
      "Epoch 15, Batch 528, Test Loss: 0.5455783009529114\n",
      "Epoch 15, Batch 529, Test Loss: 0.32333070039749146\n",
      "Epoch 15, Batch 530, Test Loss: 0.43330442905426025\n",
      "Epoch 15, Batch 531, Test Loss: 0.4654330611228943\n",
      "Epoch 15, Batch 532, Test Loss: 0.4899280071258545\n",
      "Epoch 15, Batch 533, Test Loss: 0.4648319482803345\n",
      "Epoch 15, Batch 534, Test Loss: 0.6928744316101074\n",
      "Epoch 15, Batch 535, Test Loss: 0.38167643547058105\n",
      "Epoch 15, Batch 536, Test Loss: 0.3082422614097595\n",
      "Epoch 15, Batch 537, Test Loss: 0.4140433967113495\n",
      "Epoch 15, Batch 538, Test Loss: 0.35966774821281433\n",
      "Epoch 15, Batch 539, Test Loss: 0.5476512908935547\n",
      "Epoch 15, Batch 540, Test Loss: 0.3635151982307434\n",
      "Epoch 15, Batch 541, Test Loss: 0.3881463408470154\n",
      "Epoch 15, Batch 542, Test Loss: 0.3804737627506256\n",
      "Epoch 15, Batch 543, Test Loss: 0.371956467628479\n",
      "Epoch 15, Batch 544, Test Loss: 0.44775068759918213\n",
      "Epoch 15, Batch 545, Test Loss: 0.55661940574646\n",
      "Epoch 15, Batch 546, Test Loss: 0.5106979012489319\n",
      "Epoch 15, Batch 547, Test Loss: 0.40145841240882874\n",
      "Epoch 15, Batch 548, Test Loss: 0.4633971154689789\n",
      "Epoch 15, Batch 549, Test Loss: 0.42428895831108093\n",
      "Epoch 15, Batch 550, Test Loss: 0.4209924042224884\n",
      "Epoch 15, Batch 551, Test Loss: 0.5147647857666016\n",
      "Epoch 15, Batch 552, Test Loss: 0.42809292674064636\n",
      "Epoch 15, Batch 553, Test Loss: 0.3124540150165558\n",
      "Epoch 15, Batch 554, Test Loss: 0.47427651286125183\n",
      "Epoch 15, Batch 555, Test Loss: 0.41208553314208984\n",
      "Epoch 15, Batch 556, Test Loss: 0.274078369140625\n",
      "Epoch 15, Batch 557, Test Loss: 0.35977208614349365\n",
      "Epoch 15, Batch 558, Test Loss: 0.599562406539917\n",
      "Epoch 15, Batch 559, Test Loss: 0.569561243057251\n",
      "Epoch 15, Batch 560, Test Loss: 0.5222687721252441\n",
      "Epoch 15, Batch 561, Test Loss: 0.4469074010848999\n",
      "Epoch 15, Batch 562, Test Loss: 0.2693648934364319\n",
      "Epoch 15, Batch 563, Test Loss: 0.2431107759475708\n",
      "Epoch 15, Batch 564, Test Loss: 0.5318516492843628\n",
      "Epoch 15, Batch 565, Test Loss: 0.5138213038444519\n",
      "Epoch 15, Batch 566, Test Loss: 0.6793721914291382\n",
      "Epoch 15, Batch 567, Test Loss: 0.4189227223396301\n",
      "Epoch 15, Batch 568, Test Loss: 0.35005369782447815\n",
      "Epoch 15, Batch 569, Test Loss: 0.5879698991775513\n",
      "Epoch 15, Batch 570, Test Loss: 0.47693300247192383\n",
      "Epoch 15, Batch 571, Test Loss: 0.48173007369041443\n",
      "Epoch 15, Batch 572, Test Loss: 0.47557955980300903\n",
      "Epoch 15, Batch 573, Test Loss: 0.4202452600002289\n",
      "Epoch 15, Batch 574, Test Loss: 0.379146009683609\n",
      "Epoch 15, Batch 575, Test Loss: 0.42617854475975037\n",
      "Epoch 15, Batch 576, Test Loss: 0.47311142086982727\n",
      "Epoch 15, Batch 577, Test Loss: 0.4412664473056793\n",
      "Epoch 15, Batch 578, Test Loss: 0.5061777234077454\n",
      "Epoch 15, Batch 579, Test Loss: 0.6254462599754333\n",
      "Epoch 15, Batch 580, Test Loss: 0.5939846038818359\n",
      "Epoch 15, Batch 581, Test Loss: 0.5185309052467346\n",
      "Epoch 15, Batch 582, Test Loss: 0.5870095491409302\n",
      "Epoch 15, Batch 583, Test Loss: 0.4189773499965668\n",
      "Epoch 15, Batch 584, Test Loss: 0.6259278059005737\n",
      "Epoch 15, Batch 585, Test Loss: 0.36396217346191406\n",
      "Epoch 15, Batch 586, Test Loss: 0.483602374792099\n",
      "Epoch 15, Batch 587, Test Loss: 0.447820246219635\n",
      "Epoch 15, Batch 588, Test Loss: 0.5217376947402954\n",
      "Epoch 15, Batch 589, Test Loss: 0.7667943239212036\n",
      "Epoch 15, Batch 590, Test Loss: 0.41272175312042236\n",
      "Epoch 15, Batch 591, Test Loss: 0.5132619142532349\n",
      "Epoch 15, Batch 592, Test Loss: 0.33639422059059143\n",
      "Epoch 15, Batch 593, Test Loss: 0.329009473323822\n",
      "Epoch 15, Batch 594, Test Loss: 0.35198289155960083\n",
      "Epoch 15, Batch 595, Test Loss: 0.254630446434021\n",
      "Epoch 15, Batch 596, Test Loss: 0.34194889664649963\n",
      "Epoch 15, Batch 597, Test Loss: 0.49738550186157227\n",
      "Epoch 15, Batch 598, Test Loss: 0.4723033010959625\n",
      "Epoch 15, Batch 599, Test Loss: 0.47341209650039673\n",
      "Epoch 15, Batch 600, Test Loss: 0.5802680253982544\n",
      "Epoch 15, Batch 601, Test Loss: 0.49029579758644104\n",
      "Epoch 15, Batch 602, Test Loss: 0.5712788105010986\n",
      "Epoch 15, Batch 603, Test Loss: 0.37158262729644775\n",
      "Epoch 15, Batch 604, Test Loss: 0.4156602621078491\n",
      "Epoch 15, Batch 605, Test Loss: 0.43171608448028564\n",
      "Epoch 15, Batch 606, Test Loss: 0.4637435972690582\n",
      "Epoch 15, Batch 607, Test Loss: 0.588814914226532\n",
      "Epoch 15, Batch 608, Test Loss: 0.522125244140625\n",
      "Epoch 15, Batch 609, Test Loss: 0.3227349519729614\n",
      "Epoch 15, Batch 610, Test Loss: 0.3596538305282593\n",
      "Epoch 15, Batch 611, Test Loss: 0.4228120744228363\n",
      "Epoch 15, Batch 612, Test Loss: 0.5135882496833801\n",
      "Epoch 15, Batch 613, Test Loss: 0.3767321705818176\n",
      "Epoch 15, Batch 614, Test Loss: 0.22922706604003906\n",
      "Epoch 15, Batch 615, Test Loss: 0.3625895380973816\n",
      "Epoch 15, Batch 616, Test Loss: 0.30827492475509644\n",
      "Epoch 15, Batch 617, Test Loss: 0.48730647563934326\n",
      "Epoch 15, Batch 618, Test Loss: 0.3606718182563782\n",
      "Epoch 15, Batch 619, Test Loss: 0.567356526851654\n",
      "Epoch 15, Batch 620, Test Loss: 0.5879773497581482\n",
      "Epoch 15, Batch 621, Test Loss: 0.7135351896286011\n",
      "Epoch 15, Batch 622, Test Loss: 0.4945005774497986\n",
      "Epoch 15, Batch 623, Test Loss: 0.5133963823318481\n",
      "Epoch 15, Batch 624, Test Loss: 0.4551704525947571\n",
      "Epoch 15, Batch 625, Test Loss: 0.3368159532546997\n",
      "Epoch 15, Batch 626, Test Loss: 0.3728964924812317\n",
      "Epoch 15, Batch 627, Test Loss: 0.4215122163295746\n",
      "Epoch 15, Batch 628, Test Loss: 0.5751104354858398\n",
      "Epoch 15, Batch 629, Test Loss: 0.27243807911872864\n",
      "Epoch 15, Batch 630, Test Loss: 0.42732614278793335\n",
      "Epoch 15, Batch 631, Test Loss: 0.5458065867424011\n",
      "Epoch 15, Batch 632, Test Loss: 0.3091624081134796\n",
      "Epoch 15, Batch 633, Test Loss: 0.4180556535720825\n",
      "Epoch 15, Batch 634, Test Loss: 0.44099563360214233\n",
      "Epoch 15, Batch 635, Test Loss: 0.38300520181655884\n",
      "Epoch 15, Batch 636, Test Loss: 0.6461692452430725\n",
      "Epoch 15, Batch 637, Test Loss: 0.30108433961868286\n",
      "Epoch 15, Batch 638, Test Loss: 0.4901420474052429\n",
      "Epoch 15, Batch 639, Test Loss: 0.5238650441169739\n",
      "Epoch 15, Batch 640, Test Loss: 0.6133504509925842\n",
      "Epoch 15, Batch 641, Test Loss: 0.4798189699649811\n",
      "Epoch 15, Batch 642, Test Loss: 0.38630005717277527\n",
      "Epoch 15, Batch 643, Test Loss: 0.5728829503059387\n",
      "Epoch 15, Batch 644, Test Loss: 0.38632404804229736\n",
      "Epoch 15, Batch 645, Test Loss: 0.4685787856578827\n",
      "Epoch 15, Batch 646, Test Loss: 0.6751917600631714\n",
      "Epoch 15, Batch 647, Test Loss: 0.46342992782592773\n",
      "Epoch 15, Batch 648, Test Loss: 0.43826964497566223\n",
      "Epoch 15, Batch 649, Test Loss: 0.40322697162628174\n",
      "Epoch 15, Batch 650, Test Loss: 0.29556363821029663\n",
      "Epoch 15, Batch 651, Test Loss: 0.5194502472877502\n",
      "Epoch 15, Batch 652, Test Loss: 0.4068583846092224\n",
      "Epoch 15, Batch 653, Test Loss: 0.512370765209198\n",
      "Epoch 15, Batch 654, Test Loss: 0.4151378571987152\n",
      "Epoch 15, Batch 655, Test Loss: 0.41591355204582214\n",
      "Epoch 15, Batch 656, Test Loss: 0.42367252707481384\n",
      "Epoch 15, Batch 657, Test Loss: 0.4933626353740692\n",
      "Epoch 15, Batch 658, Test Loss: 0.44389522075653076\n",
      "Epoch 15, Batch 659, Test Loss: 0.7619943022727966\n",
      "Epoch 15, Batch 660, Test Loss: 0.4533252418041229\n",
      "Epoch 15, Batch 661, Test Loss: 0.46525782346725464\n",
      "Epoch 15, Batch 662, Test Loss: 0.40715134143829346\n",
      "Epoch 15, Batch 663, Test Loss: 0.5635213255882263\n",
      "Epoch 15, Batch 664, Test Loss: 0.4896831512451172\n",
      "Epoch 15, Batch 665, Test Loss: 0.4176746904850006\n",
      "Epoch 15, Batch 666, Test Loss: 0.6428007483482361\n",
      "Epoch 15, Batch 667, Test Loss: 0.4211732745170593\n",
      "Epoch 15, Batch 668, Test Loss: 0.3474560081958771\n",
      "Epoch 15, Batch 669, Test Loss: 0.5099695324897766\n",
      "Epoch 15, Batch 670, Test Loss: 0.40896445512771606\n",
      "Epoch 15, Batch 671, Test Loss: 0.4579083025455475\n",
      "Epoch 15, Batch 672, Test Loss: 0.5763728022575378\n",
      "Epoch 15, Batch 673, Test Loss: 0.33470189571380615\n",
      "Epoch 15, Batch 674, Test Loss: 0.3284028172492981\n",
      "Epoch 15, Batch 675, Test Loss: 0.2537752091884613\n",
      "Epoch 15, Batch 676, Test Loss: 0.4304116368293762\n",
      "Epoch 15, Batch 677, Test Loss: 0.542747437953949\n",
      "Epoch 15, Batch 678, Test Loss: 0.488363116979599\n",
      "Epoch 15, Batch 679, Test Loss: 0.5624682903289795\n",
      "Epoch 15, Batch 680, Test Loss: 0.521365761756897\n",
      "Epoch 15, Batch 681, Test Loss: 0.477077841758728\n",
      "Epoch 15, Batch 682, Test Loss: 0.42581111192703247\n",
      "Epoch 15, Batch 683, Test Loss: 0.49774491786956787\n",
      "Epoch 15, Batch 684, Test Loss: 0.36567914485931396\n",
      "Epoch 15, Batch 685, Test Loss: 0.42831873893737793\n",
      "Epoch 15, Batch 686, Test Loss: 0.6161173582077026\n",
      "Epoch 15, Batch 687, Test Loss: 0.5108734369277954\n",
      "Epoch 15, Batch 688, Test Loss: 0.5237959623336792\n",
      "Epoch 15, Batch 689, Test Loss: 0.5788570046424866\n",
      "Epoch 15, Batch 690, Test Loss: 0.45670661330223083\n",
      "Epoch 15, Batch 691, Test Loss: 0.5867999792098999\n",
      "Epoch 15, Batch 692, Test Loss: 0.5629947781562805\n",
      "Epoch 15, Batch 693, Test Loss: 0.4413455128669739\n",
      "Epoch 15, Batch 694, Test Loss: 0.3394612967967987\n",
      "Epoch 15, Batch 695, Test Loss: 0.36039453744888306\n",
      "Epoch 15, Batch 696, Test Loss: 0.3901084065437317\n",
      "Epoch 15, Batch 697, Test Loss: 0.5804147124290466\n",
      "Epoch 15, Batch 698, Test Loss: 0.442649781703949\n",
      "Epoch 15, Batch 699, Test Loss: 0.5499565005302429\n",
      "Epoch 15, Batch 700, Test Loss: 0.49354490637779236\n",
      "Epoch 15, Batch 701, Test Loss: 0.357994019985199\n",
      "Epoch 15, Batch 702, Test Loss: 0.3430798649787903\n",
      "Epoch 15, Batch 703, Test Loss: 0.7274608612060547\n",
      "Epoch 15, Batch 704, Test Loss: 0.4384516477584839\n",
      "Epoch 15, Batch 705, Test Loss: 0.33871030807495117\n",
      "Epoch 15, Batch 706, Test Loss: 0.47263604402542114\n",
      "Epoch 15, Batch 707, Test Loss: 0.31568002700805664\n",
      "Epoch 15, Batch 708, Test Loss: 0.483187735080719\n",
      "Epoch 15, Batch 709, Test Loss: 0.3625922203063965\n",
      "Epoch 15, Batch 710, Test Loss: 0.3716577887535095\n",
      "Epoch 15, Batch 711, Test Loss: 0.5453791618347168\n",
      "Epoch 15, Batch 712, Test Loss: 0.5697571039199829\n",
      "Epoch 15, Batch 713, Test Loss: 0.6894781589508057\n",
      "Epoch 15, Batch 714, Test Loss: 0.41925498843193054\n",
      "Epoch 15, Batch 715, Test Loss: 0.3737764060497284\n",
      "Epoch 15, Batch 716, Test Loss: 0.4520847797393799\n",
      "Epoch 15, Batch 717, Test Loss: 0.5867435932159424\n",
      "Epoch 15, Batch 718, Test Loss: 0.39835840463638306\n",
      "Epoch 15, Batch 719, Test Loss: 0.3905807435512543\n",
      "Epoch 15, Batch 720, Test Loss: 0.47965556383132935\n",
      "Epoch 15, Batch 721, Test Loss: 0.7049883008003235\n",
      "Epoch 15, Batch 722, Test Loss: 0.4910575747489929\n",
      "Epoch 15, Batch 723, Test Loss: 0.29468896985054016\n",
      "Epoch 15, Batch 724, Test Loss: 0.4998650550842285\n",
      "Epoch 15, Batch 725, Test Loss: 0.5084532499313354\n",
      "Epoch 15, Batch 726, Test Loss: 0.2594560384750366\n",
      "Epoch 15, Batch 727, Test Loss: 0.44159939885139465\n",
      "Epoch 15, Batch 728, Test Loss: 0.636906623840332\n",
      "Epoch 15, Batch 729, Test Loss: 0.46680206060409546\n",
      "Epoch 15, Batch 730, Test Loss: 0.2399032860994339\n",
      "Epoch 15, Batch 731, Test Loss: 0.4339629113674164\n",
      "Epoch 15, Batch 732, Test Loss: 0.6173309683799744\n",
      "Epoch 15, Batch 733, Test Loss: 0.5246787667274475\n",
      "Epoch 15, Batch 734, Test Loss: 0.45158880949020386\n",
      "Epoch 15, Batch 735, Test Loss: 0.36979854106903076\n",
      "Epoch 15, Batch 736, Test Loss: 0.39649856090545654\n",
      "Epoch 15, Batch 737, Test Loss: 0.4569360017776489\n",
      "Epoch 15, Batch 738, Test Loss: 0.4154838025569916\n",
      "Epoch 15, Batch 739, Test Loss: 0.3285375237464905\n",
      "Epoch 15, Batch 740, Test Loss: 0.559402585029602\n",
      "Epoch 15, Batch 741, Test Loss: 0.3750912845134735\n",
      "Epoch 15, Batch 742, Test Loss: 0.34101805090904236\n",
      "Epoch 15, Batch 743, Test Loss: 0.5172964930534363\n",
      "Epoch 15, Batch 744, Test Loss: 0.3727640211582184\n",
      "Epoch 15, Batch 745, Test Loss: 0.58867347240448\n",
      "Epoch 15, Batch 746, Test Loss: 0.33683082461357117\n",
      "Epoch 15, Batch 747, Test Loss: 0.38886386156082153\n",
      "Epoch 15, Batch 748, Test Loss: 0.6663821935653687\n",
      "Epoch 15, Batch 749, Test Loss: 0.417945921421051\n",
      "Epoch 15, Batch 750, Test Loss: 0.5012231469154358\n",
      "Epoch 15, Batch 751, Test Loss: 0.5850508809089661\n",
      "Epoch 15, Batch 752, Test Loss: 0.4679723083972931\n",
      "Epoch 15, Batch 753, Test Loss: 0.4116153120994568\n",
      "Epoch 15, Batch 754, Test Loss: 0.4604136645793915\n",
      "Epoch 15, Batch 755, Test Loss: 0.5092672109603882\n",
      "Epoch 15, Batch 756, Test Loss: 0.3669368326663971\n",
      "Epoch 15, Batch 757, Test Loss: 0.4522603154182434\n",
      "Epoch 15, Batch 758, Test Loss: 0.4645324647426605\n",
      "Epoch 15, Batch 759, Test Loss: 0.3638010621070862\n",
      "Epoch 15, Batch 760, Test Loss: 0.47141337394714355\n",
      "Epoch 15, Batch 761, Test Loss: 0.4088597893714905\n",
      "Epoch 15, Batch 762, Test Loss: 0.280867338180542\n",
      "Epoch 15, Batch 763, Test Loss: 0.38042640686035156\n",
      "Epoch 15, Batch 764, Test Loss: 0.6784662008285522\n",
      "Epoch 15, Batch 765, Test Loss: 0.5059868693351746\n",
      "Epoch 15, Batch 766, Test Loss: 0.42562925815582275\n",
      "Epoch 15, Batch 767, Test Loss: 0.5885463953018188\n",
      "Epoch 15, Batch 768, Test Loss: 0.36663809418678284\n",
      "Epoch 15, Batch 769, Test Loss: 0.43082156777381897\n",
      "Epoch 15, Batch 770, Test Loss: 0.5013123750686646\n",
      "Epoch 15, Batch 771, Test Loss: 0.329831600189209\n",
      "Epoch 15, Batch 772, Test Loss: 0.6086498498916626\n",
      "Epoch 15, Batch 773, Test Loss: 0.5118240118026733\n",
      "Epoch 15, Batch 774, Test Loss: 0.2965738773345947\n",
      "Epoch 15, Batch 775, Test Loss: 0.4818000793457031\n",
      "Epoch 15, Batch 776, Test Loss: 0.4989016354084015\n",
      "Epoch 15, Batch 777, Test Loss: 0.35821533203125\n",
      "Epoch 15, Batch 778, Test Loss: 0.6081550121307373\n",
      "Epoch 15, Batch 779, Test Loss: 0.45763906836509705\n",
      "Epoch 15, Batch 780, Test Loss: 0.36050236225128174\n",
      "Epoch 15, Batch 781, Test Loss: 0.45979252457618713\n",
      "Epoch 15, Batch 782, Test Loss: 0.3276335597038269\n",
      "Epoch 15, Batch 783, Test Loss: 0.6028575301170349\n",
      "Epoch 15, Batch 784, Test Loss: 0.32338783144950867\n",
      "Epoch 15, Batch 785, Test Loss: 0.6053985953330994\n",
      "Epoch 15, Batch 786, Test Loss: 0.3177652955055237\n",
      "Epoch 15, Batch 787, Test Loss: 0.41047507524490356\n",
      "Epoch 15, Batch 788, Test Loss: 0.42007866501808167\n",
      "Epoch 15, Batch 789, Test Loss: 0.4180062413215637\n",
      "Epoch 15, Batch 790, Test Loss: 0.4232902228832245\n",
      "Epoch 15, Batch 791, Test Loss: 0.5712896585464478\n",
      "Epoch 15, Batch 792, Test Loss: 0.38627511262893677\n",
      "Epoch 15, Batch 793, Test Loss: 0.3736613690853119\n",
      "Epoch 15, Batch 794, Test Loss: 0.4450729489326477\n",
      "Epoch 15, Batch 795, Test Loss: 0.5431892275810242\n",
      "Epoch 15, Batch 796, Test Loss: 0.32463890314102173\n",
      "Epoch 15, Batch 797, Test Loss: 0.45042750239372253\n",
      "Epoch 15, Batch 798, Test Loss: 0.44773373007774353\n",
      "Epoch 15, Batch 799, Test Loss: 0.47437363862991333\n",
      "Epoch 15, Batch 800, Test Loss: 0.44547039270401\n",
      "Epoch 15, Batch 801, Test Loss: 0.49617624282836914\n",
      "Epoch 15, Batch 802, Test Loss: 0.4836891293525696\n",
      "Epoch 15, Batch 803, Test Loss: 0.5703173279762268\n",
      "Epoch 15, Batch 804, Test Loss: 0.3286650478839874\n",
      "Epoch 15, Batch 805, Test Loss: 0.46974343061447144\n",
      "Epoch 15, Batch 806, Test Loss: 0.43187272548675537\n",
      "Epoch 15, Batch 807, Test Loss: 0.3722352087497711\n",
      "Epoch 15, Batch 808, Test Loss: 0.2976885437965393\n",
      "Epoch 15, Batch 809, Test Loss: 0.5995320677757263\n",
      "Epoch 15, Batch 810, Test Loss: 0.4956919550895691\n",
      "Epoch 15, Batch 811, Test Loss: 0.4560909867286682\n",
      "Epoch 15, Batch 812, Test Loss: 0.44598233699798584\n",
      "Epoch 15, Batch 813, Test Loss: 0.4351375699043274\n",
      "Epoch 15, Batch 814, Test Loss: 0.47453856468200684\n",
      "Epoch 15, Batch 815, Test Loss: 0.4174448251724243\n",
      "Epoch 15, Batch 816, Test Loss: 0.5028731822967529\n",
      "Epoch 15, Batch 817, Test Loss: 0.5556592345237732\n",
      "Epoch 15, Batch 818, Test Loss: 0.46277743577957153\n",
      "Epoch 15, Batch 819, Test Loss: 0.6033375859260559\n",
      "Epoch 15, Batch 820, Test Loss: 0.3038215637207031\n",
      "Epoch 15, Batch 821, Test Loss: 0.45821183919906616\n",
      "Epoch 15, Batch 822, Test Loss: 0.40766167640686035\n",
      "Epoch 15, Batch 823, Test Loss: 0.5452260971069336\n",
      "Epoch 15, Batch 824, Test Loss: 0.5738809108734131\n",
      "Epoch 15, Batch 825, Test Loss: 0.41903069615364075\n",
      "Epoch 15, Batch 826, Test Loss: 0.478373646736145\n",
      "Epoch 15, Batch 827, Test Loss: 0.5193368792533875\n",
      "Epoch 15, Batch 828, Test Loss: 0.3701692819595337\n",
      "Epoch 15, Batch 829, Test Loss: 0.4506851136684418\n",
      "Epoch 15, Batch 830, Test Loss: 0.49657952785491943\n",
      "Epoch 15, Batch 831, Test Loss: 0.4884604215621948\n",
      "Epoch 15, Batch 832, Test Loss: 0.4317241609096527\n",
      "Epoch 15, Batch 833, Test Loss: 0.4633576273918152\n",
      "Epoch 15, Batch 834, Test Loss: 0.5437889099121094\n",
      "Epoch 15, Batch 835, Test Loss: 0.41991347074508667\n",
      "Epoch 15, Batch 836, Test Loss: 0.4503054618835449\n",
      "Epoch 15, Batch 837, Test Loss: 0.5924192070960999\n",
      "Epoch 15, Batch 838, Test Loss: 0.43003082275390625\n",
      "Epoch 15, Batch 839, Test Loss: 0.5229102969169617\n",
      "Epoch 15, Batch 840, Test Loss: 0.3685639500617981\n",
      "Epoch 15, Batch 841, Test Loss: 0.39497360587120056\n",
      "Epoch 15, Batch 842, Test Loss: 0.36001044511795044\n",
      "Epoch 15, Batch 843, Test Loss: 0.3730746805667877\n",
      "Epoch 15, Batch 844, Test Loss: 0.4656394422054291\n",
      "Epoch 15, Batch 845, Test Loss: 0.39864426851272583\n",
      "Epoch 15, Batch 846, Test Loss: 0.4437187612056732\n",
      "Epoch 15, Batch 847, Test Loss: 0.3477981388568878\n",
      "Epoch 15, Batch 848, Test Loss: 0.47890231013298035\n",
      "Epoch 15, Batch 849, Test Loss: 0.579351544380188\n",
      "Epoch 15, Batch 850, Test Loss: 0.3376348912715912\n",
      "Epoch 15, Batch 851, Test Loss: 0.4796600043773651\n",
      "Epoch 15, Batch 852, Test Loss: 0.43923521041870117\n",
      "Epoch 15, Batch 853, Test Loss: 0.34828290343284607\n",
      "Epoch 15, Batch 854, Test Loss: 0.336306095123291\n",
      "Epoch 15, Batch 855, Test Loss: 0.44320058822631836\n",
      "Epoch 15, Batch 856, Test Loss: 0.4227837324142456\n",
      "Epoch 15, Batch 857, Test Loss: 0.5084375143051147\n",
      "Epoch 15, Batch 858, Test Loss: 0.36798059940338135\n",
      "Epoch 15, Batch 859, Test Loss: 0.4894748032093048\n",
      "Epoch 15, Batch 860, Test Loss: 0.45638546347618103\n",
      "Epoch 15, Batch 861, Test Loss: 0.7026330232620239\n",
      "Epoch 15, Batch 862, Test Loss: 0.4532339572906494\n",
      "Epoch 15, Batch 863, Test Loss: 0.2938154339790344\n",
      "Epoch 15, Batch 864, Test Loss: 0.44358688592910767\n",
      "Epoch 15, Batch 865, Test Loss: 0.4779852330684662\n",
      "Epoch 15, Batch 866, Test Loss: 0.4717327058315277\n",
      "Epoch 15, Batch 867, Test Loss: 0.39636364579200745\n",
      "Epoch 15, Batch 868, Test Loss: 0.46429136395454407\n",
      "Epoch 15, Batch 869, Test Loss: 0.3600693345069885\n",
      "Epoch 15, Batch 870, Test Loss: 0.2614825367927551\n",
      "Epoch 15, Batch 871, Test Loss: 0.4074987471103668\n",
      "Epoch 15, Batch 872, Test Loss: 0.4728783965110779\n",
      "Epoch 15, Batch 873, Test Loss: 0.5537459850311279\n",
      "Epoch 15, Batch 874, Test Loss: 0.5684939622879028\n",
      "Epoch 15, Batch 875, Test Loss: 0.34561610221862793\n",
      "Epoch 15, Batch 876, Test Loss: 0.4137771427631378\n",
      "Epoch 15, Batch 877, Test Loss: 0.3825203478336334\n",
      "Epoch 15, Batch 878, Test Loss: 0.5472921133041382\n",
      "Epoch 15, Batch 879, Test Loss: 0.42076796293258667\n",
      "Epoch 15, Batch 880, Test Loss: 0.27095016837120056\n",
      "Epoch 15, Batch 881, Test Loss: 0.4800119400024414\n",
      "Epoch 15, Batch 882, Test Loss: 0.3994438946247101\n",
      "Epoch 15, Batch 883, Test Loss: 0.4558287262916565\n",
      "Epoch 15, Batch 884, Test Loss: 0.3708711266517639\n",
      "Epoch 15, Batch 885, Test Loss: 0.5779032111167908\n",
      "Epoch 15, Batch 886, Test Loss: 0.5766699314117432\n",
      "Epoch 15, Batch 887, Test Loss: 0.6047558784484863\n",
      "Epoch 15, Batch 888, Test Loss: 0.4650059938430786\n",
      "Epoch 15, Batch 889, Test Loss: 0.493071585893631\n",
      "Epoch 15, Batch 890, Test Loss: 0.296220988035202\n",
      "Epoch 15, Batch 891, Test Loss: 0.5905032753944397\n",
      "Epoch 15, Batch 892, Test Loss: 0.3963278532028198\n",
      "Epoch 15, Batch 893, Test Loss: 0.5432554483413696\n",
      "Epoch 15, Batch 894, Test Loss: 0.5829508304595947\n",
      "Epoch 15, Batch 895, Test Loss: 0.44174426794052124\n",
      "Epoch 15, Batch 896, Test Loss: 0.5042169094085693\n",
      "Epoch 15, Batch 897, Test Loss: 0.2551480531692505\n",
      "Epoch 15, Batch 898, Test Loss: 0.6322553753852844\n",
      "Epoch 15, Batch 899, Test Loss: 0.4361621141433716\n",
      "Epoch 15, Batch 900, Test Loss: 0.2910911738872528\n",
      "Epoch 15, Batch 901, Test Loss: 0.4952744245529175\n",
      "Epoch 15, Batch 902, Test Loss: 0.5865782499313354\n",
      "Epoch 15, Batch 903, Test Loss: 0.5101344585418701\n",
      "Epoch 15, Batch 904, Test Loss: 0.6294162273406982\n",
      "Epoch 15, Batch 905, Test Loss: 0.5283130407333374\n",
      "Epoch 15, Batch 906, Test Loss: 0.40768176317214966\n",
      "Epoch 15, Batch 907, Test Loss: 0.49498897790908813\n",
      "Epoch 15, Batch 908, Test Loss: 0.4109618067741394\n",
      "Epoch 15, Batch 909, Test Loss: 0.5055869221687317\n",
      "Epoch 15, Batch 910, Test Loss: 0.4523734450340271\n",
      "Epoch 15, Batch 911, Test Loss: 0.38969263434410095\n",
      "Epoch 15, Batch 912, Test Loss: 0.5663383603096008\n",
      "Epoch 15, Batch 913, Test Loss: 0.4338880181312561\n",
      "Epoch 15, Batch 914, Test Loss: 0.40476131439208984\n",
      "Epoch 15, Batch 915, Test Loss: 0.5193331837654114\n",
      "Epoch 15, Batch 916, Test Loss: 0.5716018676757812\n",
      "Epoch 15, Batch 917, Test Loss: 0.35314157605171204\n",
      "Epoch 15, Batch 918, Test Loss: 0.443659245967865\n",
      "Epoch 15, Batch 919, Test Loss: 0.49595507979393005\n",
      "Epoch 15, Batch 920, Test Loss: 0.3370741605758667\n",
      "Epoch 15, Batch 921, Test Loss: 0.49278804659843445\n",
      "Epoch 15, Batch 922, Test Loss: 0.6228306889533997\n",
      "Epoch 15, Batch 923, Test Loss: 0.592734158039093\n",
      "Epoch 15, Batch 924, Test Loss: 0.45629164576530457\n",
      "Epoch 15, Batch 925, Test Loss: 0.3731747567653656\n",
      "Epoch 15, Batch 926, Test Loss: 0.5433254837989807\n",
      "Epoch 15, Batch 927, Test Loss: 0.4221353828907013\n",
      "Epoch 15, Batch 928, Test Loss: 0.46413230895996094\n",
      "Epoch 15, Batch 929, Test Loss: 0.47311079502105713\n",
      "Epoch 15, Batch 930, Test Loss: 0.3546181321144104\n",
      "Epoch 15, Batch 931, Test Loss: 0.5121371746063232\n",
      "Epoch 15, Batch 932, Test Loss: 0.49004584550857544\n",
      "Epoch 15, Batch 933, Test Loss: 0.38672688603401184\n",
      "Epoch 15, Batch 934, Test Loss: 0.41092196106910706\n",
      "Epoch 15, Batch 935, Test Loss: 0.5979935526847839\n",
      "Epoch 15, Batch 936, Test Loss: 0.2972702383995056\n",
      "Epoch 15, Batch 937, Test Loss: 0.45535364747047424\n",
      "Epoch 15, Batch 938, Test Loss: 0.5645761489868164\n",
      "Accuracy of Test set: 0.8404\n",
      "Epoch 16, Batch 1, Loss: 0.37404942512512207\n",
      "Epoch 16, Batch 2, Loss: 0.3626859784126282\n",
      "Epoch 16, Batch 3, Loss: 0.26029253005981445\n",
      "Epoch 16, Batch 4, Loss: 0.3838194012641907\n",
      "Epoch 16, Batch 5, Loss: 0.6421058177947998\n",
      "Epoch 16, Batch 6, Loss: 0.5176279544830322\n",
      "Epoch 16, Batch 7, Loss: 0.43182265758514404\n",
      "Epoch 16, Batch 8, Loss: 0.4656493663787842\n",
      "Epoch 16, Batch 9, Loss: 0.4005354344844818\n",
      "Epoch 16, Batch 10, Loss: 0.46581074595451355\n",
      "Epoch 16, Batch 11, Loss: 0.5927789807319641\n",
      "Epoch 16, Batch 12, Loss: 0.3771287798881531\n",
      "Epoch 16, Batch 13, Loss: 0.3144182860851288\n",
      "Epoch 16, Batch 14, Loss: 0.3665870130062103\n",
      "Epoch 16, Batch 15, Loss: 0.469115674495697\n",
      "Epoch 16, Batch 16, Loss: 0.42925599217414856\n",
      "Epoch 16, Batch 17, Loss: 0.5739586353302002\n",
      "Epoch 16, Batch 18, Loss: 0.5471866726875305\n",
      "Epoch 16, Batch 19, Loss: 0.3601589798927307\n",
      "Epoch 16, Batch 20, Loss: 0.3371643126010895\n",
      "Epoch 16, Batch 21, Loss: 0.4192526936531067\n",
      "Epoch 16, Batch 22, Loss: 0.35187622904777527\n",
      "Epoch 16, Batch 23, Loss: 0.551608681678772\n",
      "Epoch 16, Batch 24, Loss: 0.3884199261665344\n",
      "Epoch 16, Batch 25, Loss: 0.3026854991912842\n",
      "Epoch 16, Batch 26, Loss: 0.6489396095275879\n",
      "Epoch 16, Batch 27, Loss: 0.7090227007865906\n",
      "Epoch 16, Batch 28, Loss: 0.3862581253051758\n",
      "Epoch 16, Batch 29, Loss: 0.4338322877883911\n",
      "Epoch 16, Batch 30, Loss: 0.6732295155525208\n",
      "Epoch 16, Batch 31, Loss: 0.3149169683456421\n",
      "Epoch 16, Batch 32, Loss: 0.41497328877449036\n",
      "Epoch 16, Batch 33, Loss: 0.45825523138046265\n",
      "Epoch 16, Batch 34, Loss: 0.4459580183029175\n",
      "Epoch 16, Batch 35, Loss: 0.3090636432170868\n",
      "Epoch 16, Batch 36, Loss: 0.5060954093933105\n",
      "Epoch 16, Batch 37, Loss: 0.22886362671852112\n",
      "Epoch 16, Batch 38, Loss: 0.42514389753341675\n",
      "Epoch 16, Batch 39, Loss: 0.4356639087200165\n",
      "Epoch 16, Batch 40, Loss: 0.5886190533638\n",
      "Epoch 16, Batch 41, Loss: 0.625618040561676\n",
      "Epoch 16, Batch 42, Loss: 0.37298375368118286\n",
      "Epoch 16, Batch 43, Loss: 0.30894699692726135\n",
      "Epoch 16, Batch 44, Loss: 0.6421838998794556\n",
      "Epoch 16, Batch 45, Loss: 0.5918484926223755\n",
      "Epoch 16, Batch 46, Loss: 0.36962297558784485\n",
      "Epoch 16, Batch 47, Loss: 0.369149386882782\n",
      "Epoch 16, Batch 48, Loss: 0.5046175122261047\n",
      "Epoch 16, Batch 49, Loss: 0.41786667704582214\n",
      "Epoch 16, Batch 50, Loss: 0.6456665992736816\n",
      "Epoch 16, Batch 51, Loss: 0.3402808904647827\n",
      "Epoch 16, Batch 52, Loss: 0.4498806893825531\n",
      "Epoch 16, Batch 53, Loss: 0.3636721968650818\n",
      "Epoch 16, Batch 54, Loss: 0.2761939465999603\n",
      "Epoch 16, Batch 55, Loss: 0.3847738802433014\n",
      "Epoch 16, Batch 56, Loss: 0.3598804175853729\n",
      "Epoch 16, Batch 57, Loss: 0.501122772693634\n",
      "Epoch 16, Batch 58, Loss: 0.4236372113227844\n",
      "Epoch 16, Batch 59, Loss: 0.5301041603088379\n",
      "Epoch 16, Batch 60, Loss: 0.48693835735321045\n",
      "Epoch 16, Batch 61, Loss: 0.4695150852203369\n",
      "Epoch 16, Batch 62, Loss: 0.45537179708480835\n",
      "Epoch 16, Batch 63, Loss: 0.40009889006614685\n",
      "Epoch 16, Batch 64, Loss: 0.38965117931365967\n",
      "Epoch 16, Batch 65, Loss: 0.38657674193382263\n",
      "Epoch 16, Batch 66, Loss: 0.40710780024528503\n",
      "Epoch 16, Batch 67, Loss: 0.3651743233203888\n",
      "Epoch 16, Batch 68, Loss: 0.4586540162563324\n",
      "Epoch 16, Batch 69, Loss: 0.4155493676662445\n",
      "Epoch 16, Batch 70, Loss: 0.6610930562019348\n",
      "Epoch 16, Batch 71, Loss: 0.5568931698799133\n",
      "Epoch 16, Batch 72, Loss: 0.36942437291145325\n",
      "Epoch 16, Batch 73, Loss: 0.45819392800331116\n",
      "Epoch 16, Batch 74, Loss: 0.490138977766037\n",
      "Epoch 16, Batch 75, Loss: 0.4646908938884735\n",
      "Epoch 16, Batch 76, Loss: 0.4860784113407135\n",
      "Epoch 16, Batch 77, Loss: 0.288220077753067\n",
      "Epoch 16, Batch 78, Loss: 0.5452114939689636\n",
      "Epoch 16, Batch 79, Loss: 0.36775118112564087\n",
      "Epoch 16, Batch 80, Loss: 0.4371824860572815\n",
      "Epoch 16, Batch 81, Loss: 0.43652763962745667\n",
      "Epoch 16, Batch 82, Loss: 0.42531880736351013\n",
      "Epoch 16, Batch 83, Loss: 0.26715004444122314\n",
      "Epoch 16, Batch 84, Loss: 0.4778079390525818\n",
      "Epoch 16, Batch 85, Loss: 0.47685912251472473\n",
      "Epoch 16, Batch 86, Loss: 0.29481881856918335\n",
      "Epoch 16, Batch 87, Loss: 0.3477762043476105\n",
      "Epoch 16, Batch 88, Loss: 0.5033376216888428\n",
      "Epoch 16, Batch 89, Loss: 0.35702139139175415\n",
      "Epoch 16, Batch 90, Loss: 0.37833210825920105\n",
      "Epoch 16, Batch 91, Loss: 0.594760537147522\n",
      "Epoch 16, Batch 92, Loss: 0.44147807359695435\n",
      "Epoch 16, Batch 93, Loss: 0.3490302860736847\n",
      "Epoch 16, Batch 94, Loss: 0.41639041900634766\n",
      "Epoch 16, Batch 95, Loss: 0.31088006496429443\n",
      "Epoch 16, Batch 96, Loss: 0.35770708322525024\n",
      "Epoch 16, Batch 97, Loss: 0.4198835492134094\n",
      "Epoch 16, Batch 98, Loss: 0.3007095754146576\n",
      "Epoch 16, Batch 99, Loss: 0.431902140378952\n",
      "Epoch 16, Batch 100, Loss: 0.5690497159957886\n",
      "Epoch 16, Batch 101, Loss: 0.5874168872833252\n",
      "Epoch 16, Batch 102, Loss: 0.8519121408462524\n",
      "Epoch 16, Batch 103, Loss: 0.5175151228904724\n",
      "Epoch 16, Batch 104, Loss: 0.4324726462364197\n",
      "Epoch 16, Batch 105, Loss: 0.4782370924949646\n",
      "Epoch 16, Batch 106, Loss: 0.3583833575248718\n",
      "Epoch 16, Batch 107, Loss: 0.3901834189891815\n",
      "Epoch 16, Batch 108, Loss: 0.3858560621738434\n",
      "Epoch 16, Batch 109, Loss: 0.6317143440246582\n",
      "Epoch 16, Batch 110, Loss: 0.4178304374217987\n",
      "Epoch 16, Batch 111, Loss: 0.5039304494857788\n",
      "Epoch 16, Batch 112, Loss: 0.20451204478740692\n",
      "Epoch 16, Batch 113, Loss: 0.48754259943962097\n",
      "Epoch 16, Batch 114, Loss: 0.49250033497810364\n",
      "Epoch 16, Batch 115, Loss: 0.30933254957199097\n",
      "Epoch 16, Batch 116, Loss: 0.3387477695941925\n",
      "Epoch 16, Batch 117, Loss: 0.6346573829650879\n",
      "Epoch 16, Batch 118, Loss: 0.49046388268470764\n",
      "Epoch 16, Batch 119, Loss: 0.2726154327392578\n",
      "Epoch 16, Batch 120, Loss: 0.4398801922798157\n",
      "Epoch 16, Batch 121, Loss: 0.3500262200832367\n",
      "Epoch 16, Batch 122, Loss: 0.5960928201675415\n",
      "Epoch 16, Batch 123, Loss: 0.36026477813720703\n",
      "Epoch 16, Batch 124, Loss: 0.3194694519042969\n",
      "Epoch 16, Batch 125, Loss: 0.4992603659629822\n",
      "Epoch 16, Batch 126, Loss: 0.7804856300354004\n",
      "Epoch 16, Batch 127, Loss: 0.3518180847167969\n",
      "Epoch 16, Batch 128, Loss: 0.5006709098815918\n",
      "Epoch 16, Batch 129, Loss: 0.4247722923755646\n",
      "Epoch 16, Batch 130, Loss: 0.342748761177063\n",
      "Epoch 16, Batch 131, Loss: 0.4613972306251526\n",
      "Epoch 16, Batch 132, Loss: 0.3981553912162781\n",
      "Epoch 16, Batch 133, Loss: 0.24996350705623627\n",
      "Epoch 16, Batch 134, Loss: 0.48305222392082214\n",
      "Epoch 16, Batch 135, Loss: 0.5456681847572327\n",
      "Epoch 16, Batch 136, Loss: 0.2950989603996277\n",
      "Epoch 16, Batch 137, Loss: 0.27734696865081787\n",
      "Epoch 16, Batch 138, Loss: 0.44091475009918213\n",
      "Epoch 16, Batch 139, Loss: 0.40043047070503235\n",
      "Epoch 16, Batch 140, Loss: 0.7157127261161804\n",
      "Epoch 16, Batch 141, Loss: 0.47935718297958374\n",
      "Epoch 16, Batch 142, Loss: 0.5498249530792236\n",
      "Epoch 16, Batch 143, Loss: 0.5376671552658081\n",
      "Epoch 16, Batch 144, Loss: 0.584178626537323\n",
      "Epoch 16, Batch 145, Loss: 0.45265862345695496\n",
      "Epoch 16, Batch 146, Loss: 0.42165407538414\n",
      "Epoch 16, Batch 147, Loss: 0.592604398727417\n",
      "Epoch 16, Batch 148, Loss: 0.35038936138153076\n",
      "Epoch 16, Batch 149, Loss: 0.5367969274520874\n",
      "Epoch 16, Batch 150, Loss: 0.42329755425453186\n",
      "Epoch 16, Batch 151, Loss: 0.3949529528617859\n",
      "Epoch 16, Batch 152, Loss: 0.6061399579048157\n",
      "Epoch 16, Batch 153, Loss: 0.35807907581329346\n",
      "Epoch 16, Batch 154, Loss: 0.5744237899780273\n",
      "Epoch 16, Batch 155, Loss: 0.3511585593223572\n",
      "Epoch 16, Batch 156, Loss: 0.7096538543701172\n",
      "Epoch 16, Batch 157, Loss: 0.39687463641166687\n",
      "Epoch 16, Batch 158, Loss: 0.32424497604370117\n",
      "Epoch 16, Batch 159, Loss: 0.3346318304538727\n",
      "Epoch 16, Batch 160, Loss: 0.3915148675441742\n",
      "Epoch 16, Batch 161, Loss: 0.40114325284957886\n",
      "Epoch 16, Batch 162, Loss: 0.4347313940525055\n",
      "Epoch 16, Batch 163, Loss: 0.5326361656188965\n",
      "Epoch 16, Batch 164, Loss: 0.48568060994148254\n",
      "Epoch 16, Batch 165, Loss: 0.3259888291358948\n",
      "Epoch 16, Batch 166, Loss: 0.3204318881034851\n",
      "Epoch 16, Batch 167, Loss: 0.533228874206543\n",
      "Epoch 16, Batch 168, Loss: 0.4633241295814514\n",
      "Epoch 16, Batch 169, Loss: 0.5301818251609802\n",
      "Epoch 16, Batch 170, Loss: 0.3400050401687622\n",
      "Epoch 16, Batch 171, Loss: 0.5609885454177856\n",
      "Epoch 16, Batch 172, Loss: 0.53791743516922\n",
      "Epoch 16, Batch 173, Loss: 0.5414308905601501\n",
      "Epoch 16, Batch 174, Loss: 0.3879905641078949\n",
      "Epoch 16, Batch 175, Loss: 0.43177855014801025\n",
      "Epoch 16, Batch 176, Loss: 0.48693931102752686\n",
      "Epoch 16, Batch 177, Loss: 0.38402241468429565\n",
      "Epoch 16, Batch 178, Loss: 0.7424027323722839\n",
      "Epoch 16, Batch 179, Loss: 0.4553321301937103\n",
      "Epoch 16, Batch 180, Loss: 0.370549738407135\n",
      "Epoch 16, Batch 181, Loss: 0.31918764114379883\n",
      "Epoch 16, Batch 182, Loss: 0.5616422891616821\n",
      "Epoch 16, Batch 183, Loss: 0.4029644727706909\n",
      "Epoch 16, Batch 184, Loss: 0.4423814117908478\n",
      "Epoch 16, Batch 185, Loss: 0.30229419469833374\n",
      "Epoch 16, Batch 186, Loss: 0.6217651963233948\n",
      "Epoch 16, Batch 187, Loss: 0.466928631067276\n",
      "Epoch 16, Batch 188, Loss: 0.28304925560951233\n",
      "Epoch 16, Batch 189, Loss: 0.5050368309020996\n",
      "Epoch 16, Batch 190, Loss: 0.35429367423057556\n",
      "Epoch 16, Batch 191, Loss: 0.30526965856552124\n",
      "Epoch 16, Batch 192, Loss: 0.5555516481399536\n",
      "Epoch 16, Batch 193, Loss: 0.5520051717758179\n",
      "Epoch 16, Batch 194, Loss: 0.5187925100326538\n",
      "Epoch 16, Batch 195, Loss: 0.4209631681442261\n",
      "Epoch 16, Batch 196, Loss: 0.38213908672332764\n",
      "Epoch 16, Batch 197, Loss: 0.433467298746109\n",
      "Epoch 16, Batch 198, Loss: 0.3656294047832489\n",
      "Epoch 16, Batch 199, Loss: 0.3647516965866089\n",
      "Epoch 16, Batch 200, Loss: 0.40303391218185425\n",
      "Epoch 16, Batch 201, Loss: 0.5115706324577332\n",
      "Epoch 16, Batch 202, Loss: 0.33458229899406433\n",
      "Epoch 16, Batch 203, Loss: 0.5589814186096191\n",
      "Epoch 16, Batch 204, Loss: 0.6291080713272095\n",
      "Epoch 16, Batch 205, Loss: 0.48379525542259216\n",
      "Epoch 16, Batch 206, Loss: 0.3574315309524536\n",
      "Epoch 16, Batch 207, Loss: 0.4436410069465637\n",
      "Epoch 16, Batch 208, Loss: 0.5827493071556091\n",
      "Epoch 16, Batch 209, Loss: 0.39714911580085754\n",
      "Epoch 16, Batch 210, Loss: 0.3916025459766388\n",
      "Epoch 16, Batch 211, Loss: 0.5518736243247986\n",
      "Epoch 16, Batch 212, Loss: 0.5106423497200012\n",
      "Epoch 16, Batch 213, Loss: 0.4463609457015991\n",
      "Epoch 16, Batch 214, Loss: 0.5191901922225952\n",
      "Epoch 16, Batch 215, Loss: 0.6513067483901978\n",
      "Epoch 16, Batch 216, Loss: 0.4988977313041687\n",
      "Epoch 16, Batch 217, Loss: 0.46807482838630676\n",
      "Epoch 16, Batch 218, Loss: 0.40407949686050415\n",
      "Epoch 16, Batch 219, Loss: 0.5744442343711853\n",
      "Epoch 16, Batch 220, Loss: 0.5871191024780273\n",
      "Epoch 16, Batch 221, Loss: 0.3885522484779358\n",
      "Epoch 16, Batch 222, Loss: 0.5447031855583191\n",
      "Epoch 16, Batch 223, Loss: 0.3428620398044586\n",
      "Epoch 16, Batch 224, Loss: 0.5760151147842407\n",
      "Epoch 16, Batch 225, Loss: 0.39942848682403564\n",
      "Epoch 16, Batch 226, Loss: 0.40259379148483276\n",
      "Epoch 16, Batch 227, Loss: 0.4521489143371582\n",
      "Epoch 16, Batch 228, Loss: 0.48065757751464844\n",
      "Epoch 16, Batch 229, Loss: 0.5532033443450928\n",
      "Epoch 16, Batch 230, Loss: 0.6743152737617493\n",
      "Epoch 16, Batch 231, Loss: 0.3466733396053314\n",
      "Epoch 16, Batch 232, Loss: 0.3424246311187744\n",
      "Epoch 16, Batch 233, Loss: 0.5696974396705627\n",
      "Epoch 16, Batch 234, Loss: 0.8112690448760986\n",
      "Epoch 16, Batch 235, Loss: 0.5061811208724976\n",
      "Epoch 16, Batch 236, Loss: 0.5908597707748413\n",
      "Epoch 16, Batch 237, Loss: 0.3511997163295746\n",
      "Epoch 16, Batch 238, Loss: 0.5339040756225586\n",
      "Epoch 16, Batch 239, Loss: 0.29789239168167114\n",
      "Epoch 16, Batch 240, Loss: 0.5062534213066101\n",
      "Epoch 16, Batch 241, Loss: 0.4067426025867462\n",
      "Epoch 16, Batch 242, Loss: 0.5630292892456055\n",
      "Epoch 16, Batch 243, Loss: 0.5184809565544128\n",
      "Epoch 16, Batch 244, Loss: 0.4677581191062927\n",
      "Epoch 16, Batch 245, Loss: 0.26604047417640686\n",
      "Epoch 16, Batch 246, Loss: 0.39928174018859863\n",
      "Epoch 16, Batch 247, Loss: 0.3526347875595093\n",
      "Epoch 16, Batch 248, Loss: 0.5058445930480957\n",
      "Epoch 16, Batch 249, Loss: 0.5368391871452332\n",
      "Epoch 16, Batch 250, Loss: 0.24168287217617035\n",
      "Epoch 16, Batch 251, Loss: 0.6548133492469788\n",
      "Epoch 16, Batch 252, Loss: 0.2798767685890198\n",
      "Epoch 16, Batch 253, Loss: 0.3933625817298889\n",
      "Epoch 16, Batch 254, Loss: 0.304254949092865\n",
      "Epoch 16, Batch 255, Loss: 0.4385582208633423\n",
      "Epoch 16, Batch 256, Loss: 0.5122249722480774\n",
      "Epoch 16, Batch 257, Loss: 0.37012147903442383\n",
      "Epoch 16, Batch 258, Loss: 0.4256460964679718\n",
      "Epoch 16, Batch 259, Loss: 0.6417908072471619\n",
      "Epoch 16, Batch 260, Loss: 0.37503179907798767\n",
      "Epoch 16, Batch 261, Loss: 0.5276811122894287\n",
      "Epoch 16, Batch 262, Loss: 0.429766446352005\n",
      "Epoch 16, Batch 263, Loss: 0.5336794257164001\n",
      "Epoch 16, Batch 264, Loss: 0.32762062549591064\n",
      "Epoch 16, Batch 265, Loss: 0.4809202551841736\n",
      "Epoch 16, Batch 266, Loss: 0.2676275372505188\n",
      "Epoch 16, Batch 267, Loss: 0.5341870188713074\n",
      "Epoch 16, Batch 268, Loss: 0.4857172966003418\n",
      "Epoch 16, Batch 269, Loss: 0.4882291257381439\n",
      "Epoch 16, Batch 270, Loss: 0.41914162039756775\n",
      "Epoch 16, Batch 271, Loss: 0.34952592849731445\n",
      "Epoch 16, Batch 272, Loss: 0.3804842233657837\n",
      "Epoch 16, Batch 273, Loss: 0.3780427575111389\n",
      "Epoch 16, Batch 274, Loss: 0.7046418190002441\n",
      "Epoch 16, Batch 275, Loss: 0.545153796672821\n",
      "Epoch 16, Batch 276, Loss: 0.3326113522052765\n",
      "Epoch 16, Batch 277, Loss: 0.3291471600532532\n",
      "Epoch 16, Batch 278, Loss: 0.4716028571128845\n",
      "Epoch 16, Batch 279, Loss: 0.38589784502983093\n",
      "Epoch 16, Batch 280, Loss: 0.3337216377258301\n",
      "Epoch 16, Batch 281, Loss: 0.39116171002388\n",
      "Epoch 16, Batch 282, Loss: 0.3999025821685791\n",
      "Epoch 16, Batch 283, Loss: 0.6122958660125732\n",
      "Epoch 16, Batch 284, Loss: 0.2547261416912079\n",
      "Epoch 16, Batch 285, Loss: 0.4468114972114563\n",
      "Epoch 16, Batch 286, Loss: 0.3293321132659912\n",
      "Epoch 16, Batch 287, Loss: 0.543126106262207\n",
      "Epoch 16, Batch 288, Loss: 0.3068474531173706\n",
      "Epoch 16, Batch 289, Loss: 0.3774372339248657\n",
      "Epoch 16, Batch 290, Loss: 0.3510556221008301\n",
      "Epoch 16, Batch 291, Loss: 0.4536399245262146\n",
      "Epoch 16, Batch 292, Loss: 0.3876892626285553\n",
      "Epoch 16, Batch 293, Loss: 0.27170538902282715\n",
      "Epoch 16, Batch 294, Loss: 0.4943493902683258\n",
      "Epoch 16, Batch 295, Loss: 0.27804696559906006\n",
      "Epoch 16, Batch 296, Loss: 0.7161787748336792\n",
      "Epoch 16, Batch 297, Loss: 0.47851336002349854\n",
      "Epoch 16, Batch 298, Loss: 0.49015361070632935\n",
      "Epoch 16, Batch 299, Loss: 0.45620450377464294\n",
      "Epoch 16, Batch 300, Loss: 0.28928878903388977\n",
      "Epoch 16, Batch 301, Loss: 0.5127065181732178\n",
      "Epoch 16, Batch 302, Loss: 0.5106709003448486\n",
      "Epoch 16, Batch 303, Loss: 0.35641351342201233\n",
      "Epoch 16, Batch 304, Loss: 0.5693159103393555\n",
      "Epoch 16, Batch 305, Loss: 0.6588391065597534\n",
      "Epoch 16, Batch 306, Loss: 0.4231102168560028\n",
      "Epoch 16, Batch 307, Loss: 0.36540111899375916\n",
      "Epoch 16, Batch 308, Loss: 0.4457662105560303\n",
      "Epoch 16, Batch 309, Loss: 0.49479472637176514\n",
      "Epoch 16, Batch 310, Loss: 0.4611312747001648\n",
      "Epoch 16, Batch 311, Loss: 0.6411808729171753\n",
      "Epoch 16, Batch 312, Loss: 0.3330649733543396\n",
      "Epoch 16, Batch 313, Loss: 0.36562854051589966\n",
      "Epoch 16, Batch 314, Loss: 0.33140701055526733\n",
      "Epoch 16, Batch 315, Loss: 0.4319607615470886\n",
      "Epoch 16, Batch 316, Loss: 0.5333754420280457\n",
      "Epoch 16, Batch 317, Loss: 0.4073926508426666\n",
      "Epoch 16, Batch 318, Loss: 0.5543262958526611\n",
      "Epoch 16, Batch 319, Loss: 0.824856698513031\n",
      "Epoch 16, Batch 320, Loss: 0.3839457035064697\n",
      "Epoch 16, Batch 321, Loss: 0.34368595480918884\n",
      "Epoch 16, Batch 322, Loss: 0.392102986574173\n",
      "Epoch 16, Batch 323, Loss: 0.3684138059616089\n",
      "Epoch 16, Batch 324, Loss: 0.39449578523635864\n",
      "Epoch 16, Batch 325, Loss: 0.32213887572288513\n",
      "Epoch 16, Batch 326, Loss: 0.41894060373306274\n",
      "Epoch 16, Batch 327, Loss: 0.4065735340118408\n",
      "Epoch 16, Batch 328, Loss: 0.42643195390701294\n",
      "Epoch 16, Batch 329, Loss: 0.5012364983558655\n",
      "Epoch 16, Batch 330, Loss: 0.3173879086971283\n",
      "Epoch 16, Batch 331, Loss: 0.504836916923523\n",
      "Epoch 16, Batch 332, Loss: 0.2893417775630951\n",
      "Epoch 16, Batch 333, Loss: 0.425741046667099\n",
      "Epoch 16, Batch 334, Loss: 0.5450866222381592\n",
      "Epoch 16, Batch 335, Loss: 0.3111417889595032\n",
      "Epoch 16, Batch 336, Loss: 0.4888402819633484\n",
      "Epoch 16, Batch 337, Loss: 0.5260133743286133\n",
      "Epoch 16, Batch 338, Loss: 0.36866647005081177\n",
      "Epoch 16, Batch 339, Loss: 0.3929823637008667\n",
      "Epoch 16, Batch 340, Loss: 0.3769896328449249\n",
      "Epoch 16, Batch 341, Loss: 0.2884707748889923\n",
      "Epoch 16, Batch 342, Loss: 0.4692531228065491\n",
      "Epoch 16, Batch 343, Loss: 0.61475670337677\n",
      "Epoch 16, Batch 344, Loss: 0.36481690406799316\n",
      "Epoch 16, Batch 345, Loss: 0.5124586820602417\n",
      "Epoch 16, Batch 346, Loss: 0.4351740777492523\n",
      "Epoch 16, Batch 347, Loss: 0.40114396810531616\n",
      "Epoch 16, Batch 348, Loss: 0.6987723112106323\n",
      "Epoch 16, Batch 349, Loss: 0.5691157579421997\n",
      "Epoch 16, Batch 350, Loss: 0.593650221824646\n",
      "Epoch 16, Batch 351, Loss: 0.4145241379737854\n",
      "Epoch 16, Batch 352, Loss: 0.36150655150413513\n",
      "Epoch 16, Batch 353, Loss: 0.4261343479156494\n",
      "Epoch 16, Batch 354, Loss: 0.3716619610786438\n",
      "Epoch 16, Batch 355, Loss: 0.5508291125297546\n",
      "Epoch 16, Batch 356, Loss: 0.24973592162132263\n",
      "Epoch 16, Batch 357, Loss: 0.5329765677452087\n",
      "Epoch 16, Batch 358, Loss: 0.548898458480835\n",
      "Epoch 16, Batch 359, Loss: 0.435568630695343\n",
      "Epoch 16, Batch 360, Loss: 0.4427697956562042\n",
      "Epoch 16, Batch 361, Loss: 0.5672402381896973\n",
      "Epoch 16, Batch 362, Loss: 0.536645770072937\n",
      "Epoch 16, Batch 363, Loss: 0.3474096655845642\n",
      "Epoch 16, Batch 364, Loss: 0.4848991930484772\n",
      "Epoch 16, Batch 365, Loss: 0.40621471405029297\n",
      "Epoch 16, Batch 366, Loss: 0.5793097615242004\n",
      "Epoch 16, Batch 367, Loss: 0.5649929642677307\n",
      "Epoch 16, Batch 368, Loss: 0.6512967944145203\n",
      "Epoch 16, Batch 369, Loss: 0.4778788685798645\n",
      "Epoch 16, Batch 370, Loss: 0.3627126216888428\n",
      "Epoch 16, Batch 371, Loss: 0.45767921209335327\n",
      "Epoch 16, Batch 372, Loss: 0.5480621457099915\n",
      "Epoch 16, Batch 373, Loss: 0.3718990981578827\n",
      "Epoch 16, Batch 374, Loss: 0.5692810416221619\n",
      "Epoch 16, Batch 375, Loss: 0.5125352740287781\n",
      "Epoch 16, Batch 376, Loss: 0.5505890846252441\n",
      "Epoch 16, Batch 377, Loss: 0.5215544700622559\n",
      "Epoch 16, Batch 378, Loss: 0.5661483407020569\n",
      "Epoch 16, Batch 379, Loss: 0.4321795105934143\n",
      "Epoch 16, Batch 380, Loss: 0.3777528703212738\n",
      "Epoch 16, Batch 381, Loss: 0.4803309440612793\n",
      "Epoch 16, Batch 382, Loss: 0.3057664930820465\n",
      "Epoch 16, Batch 383, Loss: 0.3128405809402466\n",
      "Epoch 16, Batch 384, Loss: 0.4645816385746002\n",
      "Epoch 16, Batch 385, Loss: 0.3446064293384552\n",
      "Epoch 16, Batch 386, Loss: 0.5423362255096436\n",
      "Epoch 16, Batch 387, Loss: 0.7283384203910828\n",
      "Epoch 16, Batch 388, Loss: 0.5100535154342651\n",
      "Epoch 16, Batch 389, Loss: 0.5425590872764587\n",
      "Epoch 16, Batch 390, Loss: 0.46361470222473145\n",
      "Epoch 16, Batch 391, Loss: 0.654488205909729\n",
      "Epoch 16, Batch 392, Loss: 0.4973481297492981\n",
      "Epoch 16, Batch 393, Loss: 0.5621417164802551\n",
      "Epoch 16, Batch 394, Loss: 0.4139880836009979\n",
      "Epoch 16, Batch 395, Loss: 0.554887592792511\n",
      "Epoch 16, Batch 396, Loss: 0.5713822245597839\n",
      "Epoch 16, Batch 397, Loss: 0.4179278314113617\n",
      "Epoch 16, Batch 398, Loss: 0.6364665031433105\n",
      "Epoch 16, Batch 399, Loss: 0.46606433391571045\n",
      "Epoch 16, Batch 400, Loss: 0.3605661988258362\n",
      "Epoch 16, Batch 401, Loss: 0.35373133420944214\n",
      "Epoch 16, Batch 402, Loss: 0.5562324523925781\n",
      "Epoch 16, Batch 403, Loss: 0.6060730814933777\n",
      "Epoch 16, Batch 404, Loss: 0.4687696397304535\n",
      "Epoch 16, Batch 405, Loss: 0.48886531591415405\n",
      "Epoch 16, Batch 406, Loss: 0.3629271686077118\n",
      "Epoch 16, Batch 407, Loss: 0.44520097970962524\n",
      "Epoch 16, Batch 408, Loss: 0.7090365886688232\n",
      "Epoch 16, Batch 409, Loss: 0.307529479265213\n",
      "Epoch 16, Batch 410, Loss: 0.41530483961105347\n",
      "Epoch 16, Batch 411, Loss: 0.29105615615844727\n",
      "Epoch 16, Batch 412, Loss: 0.3973945379257202\n",
      "Epoch 16, Batch 413, Loss: 0.38356462121009827\n",
      "Epoch 16, Batch 414, Loss: 0.5127701759338379\n",
      "Epoch 16, Batch 415, Loss: 0.39540895819664\n",
      "Epoch 16, Batch 416, Loss: 0.47259172797203064\n",
      "Epoch 16, Batch 417, Loss: 0.6195560693740845\n",
      "Epoch 16, Batch 418, Loss: 0.5373443365097046\n",
      "Epoch 16, Batch 419, Loss: 0.3140406906604767\n",
      "Epoch 16, Batch 420, Loss: 0.3763483166694641\n",
      "Epoch 16, Batch 421, Loss: 0.4726298153400421\n",
      "Epoch 16, Batch 422, Loss: 0.6501889228820801\n",
      "Epoch 16, Batch 423, Loss: 0.31419479846954346\n",
      "Epoch 16, Batch 424, Loss: 0.5404878854751587\n",
      "Epoch 16, Batch 425, Loss: 0.29176774621009827\n",
      "Epoch 16, Batch 426, Loss: 0.3823380470275879\n",
      "Epoch 16, Batch 427, Loss: 0.36820778250694275\n",
      "Epoch 16, Batch 428, Loss: 0.44748619198799133\n",
      "Epoch 16, Batch 429, Loss: 0.2802215814590454\n",
      "Epoch 16, Batch 430, Loss: 0.5414877533912659\n",
      "Epoch 16, Batch 431, Loss: 0.4212402105331421\n",
      "Epoch 16, Batch 432, Loss: 0.33653438091278076\n",
      "Epoch 16, Batch 433, Loss: 0.4357943534851074\n",
      "Epoch 16, Batch 434, Loss: 0.7511293292045593\n",
      "Epoch 16, Batch 435, Loss: 0.6945369243621826\n",
      "Epoch 16, Batch 436, Loss: 0.5735408663749695\n",
      "Epoch 16, Batch 437, Loss: 0.2821267545223236\n",
      "Epoch 16, Batch 438, Loss: 0.6029577255249023\n",
      "Epoch 16, Batch 439, Loss: 0.33582162857055664\n",
      "Epoch 16, Batch 440, Loss: 0.31991153955459595\n",
      "Epoch 16, Batch 441, Loss: 0.5011031031608582\n",
      "Epoch 16, Batch 442, Loss: 0.4703117907047272\n",
      "Epoch 16, Batch 443, Loss: 0.40288442373275757\n",
      "Epoch 16, Batch 444, Loss: 0.5864223837852478\n",
      "Epoch 16, Batch 445, Loss: 0.30632075667381287\n",
      "Epoch 16, Batch 446, Loss: 0.2629868686199188\n",
      "Epoch 16, Batch 447, Loss: 0.5731069445610046\n",
      "Epoch 16, Batch 448, Loss: 0.23995718359947205\n",
      "Epoch 16, Batch 449, Loss: 0.3280274271965027\n",
      "Epoch 16, Batch 450, Loss: 0.42483633756637573\n",
      "Epoch 16, Batch 451, Loss: 0.30131006240844727\n",
      "Epoch 16, Batch 452, Loss: 0.38656413555145264\n",
      "Epoch 16, Batch 453, Loss: 0.5012665390968323\n",
      "Epoch 16, Batch 454, Loss: 0.35812610387802124\n",
      "Epoch 16, Batch 455, Loss: 0.4251481890678406\n",
      "Epoch 16, Batch 456, Loss: 0.4049558937549591\n",
      "Epoch 16, Batch 457, Loss: 0.3332183063030243\n",
      "Epoch 16, Batch 458, Loss: 0.4044794738292694\n",
      "Epoch 16, Batch 459, Loss: 0.4035089612007141\n",
      "Epoch 16, Batch 460, Loss: 0.3581833243370056\n",
      "Epoch 16, Batch 461, Loss: 0.31348487734794617\n",
      "Epoch 16, Batch 462, Loss: 0.30668461322784424\n",
      "Epoch 16, Batch 463, Loss: 0.3839043080806732\n",
      "Epoch 16, Batch 464, Loss: 0.4336855709552765\n",
      "Epoch 16, Batch 465, Loss: 0.5310400724411011\n",
      "Epoch 16, Batch 466, Loss: 0.46433380246162415\n",
      "Epoch 16, Batch 467, Loss: 0.4969395101070404\n",
      "Epoch 16, Batch 468, Loss: 0.4285249412059784\n",
      "Epoch 16, Batch 469, Loss: 0.41081514954566956\n",
      "Epoch 16, Batch 470, Loss: 0.332086980342865\n",
      "Epoch 16, Batch 471, Loss: 0.2843332886695862\n",
      "Epoch 16, Batch 472, Loss: 0.4564189314842224\n",
      "Epoch 16, Batch 473, Loss: 0.5874683260917664\n",
      "Epoch 16, Batch 474, Loss: 0.4533179700374603\n",
      "Epoch 16, Batch 475, Loss: 0.7131832838058472\n",
      "Epoch 16, Batch 476, Loss: 0.6195106506347656\n",
      "Epoch 16, Batch 477, Loss: 0.47395288944244385\n",
      "Epoch 16, Batch 478, Loss: 0.38717931509017944\n",
      "Epoch 16, Batch 479, Loss: 0.44974973797798157\n",
      "Epoch 16, Batch 480, Loss: 0.623757004737854\n",
      "Epoch 16, Batch 481, Loss: 0.6684426069259644\n",
      "Epoch 16, Batch 482, Loss: 0.3500920236110687\n",
      "Epoch 16, Batch 483, Loss: 0.6179646253585815\n",
      "Epoch 16, Batch 484, Loss: 0.5333182215690613\n",
      "Epoch 16, Batch 485, Loss: 0.5250778794288635\n",
      "Epoch 16, Batch 486, Loss: 0.4867614507675171\n",
      "Epoch 16, Batch 487, Loss: 0.452532559633255\n",
      "Epoch 16, Batch 488, Loss: 0.3706912696361542\n",
      "Epoch 16, Batch 489, Loss: 0.6464716196060181\n",
      "Epoch 16, Batch 490, Loss: 0.5491775870323181\n",
      "Epoch 16, Batch 491, Loss: 0.4488064646720886\n",
      "Epoch 16, Batch 492, Loss: 0.49403059482574463\n",
      "Epoch 16, Batch 493, Loss: 0.4760812520980835\n",
      "Epoch 16, Batch 494, Loss: 0.3601386845111847\n",
      "Epoch 16, Batch 495, Loss: 0.5388026833534241\n",
      "Epoch 16, Batch 496, Loss: 0.2940978407859802\n",
      "Epoch 16, Batch 497, Loss: 0.4787093997001648\n",
      "Epoch 16, Batch 498, Loss: 0.43469512462615967\n",
      "Epoch 16, Batch 499, Loss: 0.4471363425254822\n",
      "Epoch 16, Batch 500, Loss: 0.3470690846443176\n",
      "Epoch 16, Batch 501, Loss: 0.5060371160507202\n",
      "Epoch 16, Batch 502, Loss: 0.23733919858932495\n",
      "Epoch 16, Batch 503, Loss: 0.5158876776695251\n",
      "Epoch 16, Batch 504, Loss: 0.5350538492202759\n",
      "Epoch 16, Batch 505, Loss: 0.3409092426300049\n",
      "Epoch 16, Batch 506, Loss: 0.2873573303222656\n",
      "Epoch 16, Batch 507, Loss: 0.48829397559165955\n",
      "Epoch 16, Batch 508, Loss: 0.3552999496459961\n",
      "Epoch 16, Batch 509, Loss: 0.475584477186203\n",
      "Epoch 16, Batch 510, Loss: 0.38796496391296387\n",
      "Epoch 16, Batch 511, Loss: 0.49560225009918213\n",
      "Epoch 16, Batch 512, Loss: 0.5984412431716919\n",
      "Epoch 16, Batch 513, Loss: 0.2535044252872467\n",
      "Epoch 16, Batch 514, Loss: 0.2132987529039383\n",
      "Epoch 16, Batch 515, Loss: 0.4312402307987213\n",
      "Epoch 16, Batch 516, Loss: 0.3394807279109955\n",
      "Epoch 16, Batch 517, Loss: 0.4550870656967163\n",
      "Epoch 16, Batch 518, Loss: 0.6348060369491577\n",
      "Epoch 16, Batch 519, Loss: 0.3368375301361084\n",
      "Epoch 16, Batch 520, Loss: 0.31497809290885925\n",
      "Epoch 16, Batch 521, Loss: 0.5500572323799133\n",
      "Epoch 16, Batch 522, Loss: 0.7021563053131104\n",
      "Epoch 16, Batch 523, Loss: 0.5914889574050903\n",
      "Epoch 16, Batch 524, Loss: 0.38565200567245483\n",
      "Epoch 16, Batch 525, Loss: 0.40156465768814087\n",
      "Epoch 16, Batch 526, Loss: 0.5414350032806396\n",
      "Epoch 16, Batch 527, Loss: 0.3974933624267578\n",
      "Epoch 16, Batch 528, Loss: 0.3965783715248108\n",
      "Epoch 16, Batch 529, Loss: 0.4975152015686035\n",
      "Epoch 16, Batch 530, Loss: 0.2772534489631653\n",
      "Epoch 16, Batch 531, Loss: 0.4204888343811035\n",
      "Epoch 16, Batch 532, Loss: 0.4182552695274353\n",
      "Epoch 16, Batch 533, Loss: 0.43986010551452637\n",
      "Epoch 16, Batch 534, Loss: 0.5350066423416138\n",
      "Epoch 16, Batch 535, Loss: 0.4545839726924896\n",
      "Epoch 16, Batch 536, Loss: 0.3623064458370209\n",
      "Epoch 16, Batch 537, Loss: 0.3566852807998657\n",
      "Epoch 16, Batch 538, Loss: 0.3853822350502014\n",
      "Epoch 16, Batch 539, Loss: 0.49938535690307617\n",
      "Epoch 16, Batch 540, Loss: 0.4543191194534302\n",
      "Epoch 16, Batch 541, Loss: 0.601017951965332\n",
      "Epoch 16, Batch 542, Loss: 0.5083292126655579\n",
      "Epoch 16, Batch 543, Loss: 0.43388035893440247\n",
      "Epoch 16, Batch 544, Loss: 0.3426637053489685\n",
      "Epoch 16, Batch 545, Loss: 0.4362650513648987\n",
      "Epoch 16, Batch 546, Loss: 0.5509863495826721\n",
      "Epoch 16, Batch 547, Loss: 0.3746178150177002\n",
      "Epoch 16, Batch 548, Loss: 0.40927058458328247\n",
      "Epoch 16, Batch 549, Loss: 0.3533785045146942\n",
      "Epoch 16, Batch 550, Loss: 0.34231284260749817\n",
      "Epoch 16, Batch 551, Loss: 0.29208630323410034\n",
      "Epoch 16, Batch 552, Loss: 0.34277939796447754\n",
      "Epoch 16, Batch 553, Loss: 0.3629509508609772\n",
      "Epoch 16, Batch 554, Loss: 0.39195239543914795\n",
      "Epoch 16, Batch 555, Loss: 0.4026722013950348\n",
      "Epoch 16, Batch 556, Loss: 0.35380542278289795\n",
      "Epoch 16, Batch 557, Loss: 0.419577032327652\n",
      "Epoch 16, Batch 558, Loss: 0.41614654660224915\n",
      "Epoch 16, Batch 559, Loss: 0.5932289361953735\n",
      "Epoch 16, Batch 560, Loss: 0.4273703098297119\n",
      "Epoch 16, Batch 561, Loss: 0.46625202894210815\n",
      "Epoch 16, Batch 562, Loss: 0.6156441569328308\n",
      "Epoch 16, Batch 563, Loss: 0.43703493475914\n",
      "Epoch 16, Batch 564, Loss: 0.6917930841445923\n",
      "Epoch 16, Batch 565, Loss: 0.42853492498397827\n",
      "Epoch 16, Batch 566, Loss: 0.5942458510398865\n",
      "Epoch 16, Batch 567, Loss: 0.3667965531349182\n",
      "Epoch 16, Batch 568, Loss: 0.47586172819137573\n",
      "Epoch 16, Batch 569, Loss: 0.3249305784702301\n",
      "Epoch 16, Batch 570, Loss: 0.48618653416633606\n",
      "Epoch 16, Batch 571, Loss: 0.36725494265556335\n",
      "Epoch 16, Batch 572, Loss: 0.4626270830631256\n",
      "Epoch 16, Batch 573, Loss: 0.4610259532928467\n",
      "Epoch 16, Batch 574, Loss: 0.419362336397171\n",
      "Epoch 16, Batch 575, Loss: 0.3099978268146515\n",
      "Epoch 16, Batch 576, Loss: 0.2855617105960846\n",
      "Epoch 16, Batch 577, Loss: 0.6143293976783752\n",
      "Epoch 16, Batch 578, Loss: 0.3532268702983856\n",
      "Epoch 16, Batch 579, Loss: 0.6482117772102356\n",
      "Epoch 16, Batch 580, Loss: 0.4157327115535736\n",
      "Epoch 16, Batch 581, Loss: 0.5015144944190979\n",
      "Epoch 16, Batch 582, Loss: 0.3672472834587097\n",
      "Epoch 16, Batch 583, Loss: 0.726351261138916\n",
      "Epoch 16, Batch 584, Loss: 0.589053750038147\n",
      "Epoch 16, Batch 585, Loss: 0.5365018844604492\n",
      "Epoch 16, Batch 586, Loss: 0.3509857654571533\n",
      "Epoch 16, Batch 587, Loss: 0.3940846920013428\n",
      "Epoch 16, Batch 588, Loss: 0.33253419399261475\n",
      "Epoch 16, Batch 589, Loss: 0.8139760494232178\n",
      "Epoch 16, Batch 590, Loss: 0.417646199464798\n",
      "Epoch 16, Batch 591, Loss: 0.4267030358314514\n",
      "Epoch 16, Batch 592, Loss: 0.46716952323913574\n",
      "Epoch 16, Batch 593, Loss: 0.3691358268260956\n",
      "Epoch 16, Batch 594, Loss: 0.32294756174087524\n",
      "Epoch 16, Batch 595, Loss: 0.3914937973022461\n",
      "Epoch 16, Batch 596, Loss: 0.444031298160553\n",
      "Epoch 16, Batch 597, Loss: 0.46822479367256165\n",
      "Epoch 16, Batch 598, Loss: 0.3682386875152588\n",
      "Epoch 16, Batch 599, Loss: 0.38237082958221436\n",
      "Epoch 16, Batch 600, Loss: 0.40122902393341064\n",
      "Epoch 16, Batch 601, Loss: 0.4889087677001953\n",
      "Epoch 16, Batch 602, Loss: 0.5813027620315552\n",
      "Epoch 16, Batch 603, Loss: 0.43897223472595215\n",
      "Epoch 16, Batch 604, Loss: 0.5914908647537231\n",
      "Epoch 16, Batch 605, Loss: 0.4286682903766632\n",
      "Epoch 16, Batch 606, Loss: 0.6161108613014221\n",
      "Epoch 16, Batch 607, Loss: 0.29083552956581116\n",
      "Epoch 16, Batch 608, Loss: 0.29555365443229675\n",
      "Epoch 16, Batch 609, Loss: 0.3806022107601166\n",
      "Epoch 16, Batch 610, Loss: 0.3483423590660095\n",
      "Epoch 16, Batch 611, Loss: 0.48957228660583496\n",
      "Epoch 16, Batch 612, Loss: 0.45526039600372314\n",
      "Epoch 16, Batch 613, Loss: 0.33937904238700867\n",
      "Epoch 16, Batch 614, Loss: 0.48289698362350464\n",
      "Epoch 16, Batch 615, Loss: 0.2975813150405884\n",
      "Epoch 16, Batch 616, Loss: 0.5319435000419617\n",
      "Epoch 16, Batch 617, Loss: 0.5037763714790344\n",
      "Epoch 16, Batch 618, Loss: 0.5353148579597473\n",
      "Epoch 16, Batch 619, Loss: 0.4210411012172699\n",
      "Epoch 16, Batch 620, Loss: 0.339142382144928\n",
      "Epoch 16, Batch 621, Loss: 0.40722665190696716\n",
      "Epoch 16, Batch 622, Loss: 0.435107558965683\n",
      "Epoch 16, Batch 623, Loss: 0.5867618918418884\n",
      "Epoch 16, Batch 624, Loss: 0.5280031561851501\n",
      "Epoch 16, Batch 625, Loss: 0.4580165445804596\n",
      "Epoch 16, Batch 626, Loss: 0.5222738981246948\n",
      "Epoch 16, Batch 627, Loss: 0.3245723247528076\n",
      "Epoch 16, Batch 628, Loss: 0.41179099678993225\n",
      "Epoch 16, Batch 629, Loss: 0.37693431973457336\n",
      "Epoch 16, Batch 630, Loss: 0.34863853454589844\n",
      "Epoch 16, Batch 631, Loss: 0.3956112861633301\n",
      "Epoch 16, Batch 632, Loss: 0.4306522011756897\n",
      "Epoch 16, Batch 633, Loss: 0.490373432636261\n",
      "Epoch 16, Batch 634, Loss: 0.4066002368927002\n",
      "Epoch 16, Batch 635, Loss: 0.3554682731628418\n",
      "Epoch 16, Batch 636, Loss: 0.3637621998786926\n",
      "Epoch 16, Batch 637, Loss: 0.359579473733902\n",
      "Epoch 16, Batch 638, Loss: 0.46157824993133545\n",
      "Epoch 16, Batch 639, Loss: 0.5443999767303467\n",
      "Epoch 16, Batch 640, Loss: 0.4345835745334625\n",
      "Epoch 16, Batch 641, Loss: 0.4805036187171936\n",
      "Epoch 16, Batch 642, Loss: 0.47560811042785645\n",
      "Epoch 16, Batch 643, Loss: 0.7096436619758606\n",
      "Epoch 16, Batch 644, Loss: 0.4603460729122162\n",
      "Epoch 16, Batch 645, Loss: 0.6899120211601257\n",
      "Epoch 16, Batch 646, Loss: 0.4238435924053192\n",
      "Epoch 16, Batch 647, Loss: 0.3739742934703827\n",
      "Epoch 16, Batch 648, Loss: 0.4421824812889099\n",
      "Epoch 16, Batch 649, Loss: 0.43561163544654846\n",
      "Epoch 16, Batch 650, Loss: 0.2896374464035034\n",
      "Epoch 16, Batch 651, Loss: 0.39424607157707214\n",
      "Epoch 16, Batch 652, Loss: 0.40563875436782837\n",
      "Epoch 16, Batch 653, Loss: 0.4030627906322479\n",
      "Epoch 16, Batch 654, Loss: 0.3790457546710968\n",
      "Epoch 16, Batch 655, Loss: 0.4942293167114258\n",
      "Epoch 16, Batch 656, Loss: 0.49601373076438904\n",
      "Epoch 16, Batch 657, Loss: 0.4154239296913147\n",
      "Epoch 16, Batch 658, Loss: 0.4356341063976288\n",
      "Epoch 16, Batch 659, Loss: 0.48035377264022827\n",
      "Epoch 16, Batch 660, Loss: 0.26443538069725037\n",
      "Epoch 16, Batch 661, Loss: 0.3455156087875366\n",
      "Epoch 16, Batch 662, Loss: 0.3842964768409729\n",
      "Epoch 16, Batch 663, Loss: 0.4976435601711273\n",
      "Epoch 16, Batch 664, Loss: 0.3066815435886383\n",
      "Epoch 16, Batch 665, Loss: 0.5283724069595337\n",
      "Epoch 16, Batch 666, Loss: 0.5726105570793152\n",
      "Epoch 16, Batch 667, Loss: 0.5018482804298401\n",
      "Epoch 16, Batch 668, Loss: 0.6088463068008423\n",
      "Epoch 16, Batch 669, Loss: 0.39639347791671753\n",
      "Epoch 16, Batch 670, Loss: 0.6279202103614807\n",
      "Epoch 16, Batch 671, Loss: 0.6194993853569031\n",
      "Epoch 16, Batch 672, Loss: 0.2998988628387451\n",
      "Epoch 16, Batch 673, Loss: 0.3648506999015808\n",
      "Epoch 16, Batch 674, Loss: 0.44492045044898987\n",
      "Epoch 16, Batch 675, Loss: 0.5112977623939514\n",
      "Epoch 16, Batch 676, Loss: 0.37275806069374084\n",
      "Epoch 16, Batch 677, Loss: 0.40857529640197754\n",
      "Epoch 16, Batch 678, Loss: 0.316089928150177\n",
      "Epoch 16, Batch 679, Loss: 0.3879397511482239\n",
      "Epoch 16, Batch 680, Loss: 0.34637290239334106\n",
      "Epoch 16, Batch 681, Loss: 0.4184882938861847\n",
      "Epoch 16, Batch 682, Loss: 0.4705435037612915\n",
      "Epoch 16, Batch 683, Loss: 0.4833604693412781\n",
      "Epoch 16, Batch 684, Loss: 0.32622677087783813\n",
      "Epoch 16, Batch 685, Loss: 0.3156915605068207\n",
      "Epoch 16, Batch 686, Loss: 0.3518463969230652\n",
      "Epoch 16, Batch 687, Loss: 0.4794457256793976\n",
      "Epoch 16, Batch 688, Loss: 0.4836905598640442\n",
      "Epoch 16, Batch 689, Loss: 0.5949265956878662\n",
      "Epoch 16, Batch 690, Loss: 0.374747097492218\n",
      "Epoch 16, Batch 691, Loss: 0.4493506848812103\n",
      "Epoch 16, Batch 692, Loss: 0.3342667520046234\n",
      "Epoch 16, Batch 693, Loss: 0.4948062300682068\n",
      "Epoch 16, Batch 694, Loss: 0.3555753231048584\n",
      "Epoch 16, Batch 695, Loss: 0.5284160375595093\n",
      "Epoch 16, Batch 696, Loss: 0.5643816590309143\n",
      "Epoch 16, Batch 697, Loss: 0.5168895125389099\n",
      "Epoch 16, Batch 698, Loss: 0.6226682662963867\n",
      "Epoch 16, Batch 699, Loss: 0.5034558176994324\n",
      "Epoch 16, Batch 700, Loss: 0.2574530243873596\n",
      "Epoch 16, Batch 701, Loss: 0.37228667736053467\n",
      "Epoch 16, Batch 702, Loss: 0.2960788905620575\n",
      "Epoch 16, Batch 703, Loss: 0.37890055775642395\n",
      "Epoch 16, Batch 704, Loss: 0.5270793437957764\n",
      "Epoch 16, Batch 705, Loss: 0.5231536626815796\n",
      "Epoch 16, Batch 706, Loss: 0.5852351188659668\n",
      "Epoch 16, Batch 707, Loss: 0.41164034605026245\n",
      "Epoch 16, Batch 708, Loss: 0.5977569222450256\n",
      "Epoch 16, Batch 709, Loss: 0.37105655670166016\n",
      "Epoch 16, Batch 710, Loss: 0.5321080088615417\n",
      "Epoch 16, Batch 711, Loss: 0.3911285400390625\n",
      "Epoch 16, Batch 712, Loss: 0.5244227051734924\n",
      "Epoch 16, Batch 713, Loss: 0.6599398255348206\n",
      "Epoch 16, Batch 714, Loss: 0.3920571804046631\n",
      "Epoch 16, Batch 715, Loss: 0.30413079261779785\n",
      "Epoch 16, Batch 716, Loss: 0.4949752390384674\n",
      "Epoch 16, Batch 717, Loss: 0.35741835832595825\n",
      "Epoch 16, Batch 718, Loss: 0.3157186806201935\n",
      "Epoch 16, Batch 719, Loss: 0.4390278458595276\n",
      "Epoch 16, Batch 720, Loss: 0.48652106523513794\n",
      "Epoch 16, Batch 721, Loss: 0.5374554395675659\n",
      "Epoch 16, Batch 722, Loss: 0.5658823251724243\n",
      "Epoch 16, Batch 723, Loss: 0.4380856156349182\n",
      "Epoch 16, Batch 724, Loss: 0.45168960094451904\n",
      "Epoch 16, Batch 725, Loss: 0.333901971578598\n",
      "Epoch 16, Batch 726, Loss: 0.46704113483428955\n",
      "Epoch 16, Batch 727, Loss: 0.6671189069747925\n",
      "Epoch 16, Batch 728, Loss: 0.4809552729129791\n",
      "Epoch 16, Batch 729, Loss: 0.45642995834350586\n",
      "Epoch 16, Batch 730, Loss: 0.3394041061401367\n",
      "Epoch 16, Batch 731, Loss: 0.5146852731704712\n",
      "Epoch 16, Batch 732, Loss: 0.6118572354316711\n",
      "Epoch 16, Batch 733, Loss: 0.49621838331222534\n",
      "Epoch 16, Batch 734, Loss: 0.5144602060317993\n",
      "Epoch 16, Batch 735, Loss: 0.40921375155448914\n",
      "Epoch 16, Batch 736, Loss: 0.43931490182876587\n",
      "Epoch 16, Batch 737, Loss: 0.7127863168716431\n",
      "Epoch 16, Batch 738, Loss: 0.44519880414009094\n",
      "Epoch 16, Batch 739, Loss: 0.4932040870189667\n",
      "Epoch 16, Batch 740, Loss: 0.4131225049495697\n",
      "Epoch 16, Batch 741, Loss: 0.3750823140144348\n",
      "Epoch 16, Batch 742, Loss: 0.6860771775245667\n",
      "Epoch 16, Batch 743, Loss: 0.5876549482345581\n",
      "Epoch 16, Batch 744, Loss: 0.526166558265686\n",
      "Epoch 16, Batch 745, Loss: 0.45151036977767944\n",
      "Epoch 16, Batch 746, Loss: 0.4997934103012085\n",
      "Epoch 16, Batch 747, Loss: 0.3905903100967407\n",
      "Epoch 16, Batch 748, Loss: 0.31040799617767334\n",
      "Epoch 16, Batch 749, Loss: 0.5402314066886902\n",
      "Epoch 16, Batch 750, Loss: 0.44795528054237366\n",
      "Epoch 16, Batch 751, Loss: 0.2649480402469635\n",
      "Epoch 16, Batch 752, Loss: 0.49079281091690063\n",
      "Epoch 16, Batch 753, Loss: 0.46446794271469116\n",
      "Epoch 16, Batch 754, Loss: 0.39409205317497253\n",
      "Epoch 16, Batch 755, Loss: 0.5073054432868958\n",
      "Epoch 16, Batch 756, Loss: 0.3633328378200531\n",
      "Epoch 16, Batch 757, Loss: 0.47058194875717163\n",
      "Epoch 16, Batch 758, Loss: 0.45314332842826843\n",
      "Epoch 16, Batch 759, Loss: 0.3402068316936493\n",
      "Epoch 16, Batch 760, Loss: 0.3808470070362091\n",
      "Epoch 16, Batch 761, Loss: 0.49975332617759705\n",
      "Epoch 16, Batch 762, Loss: 0.3523878753185272\n",
      "Epoch 16, Batch 763, Loss: 0.48532992601394653\n",
      "Epoch 16, Batch 764, Loss: 0.4852777123451233\n",
      "Epoch 16, Batch 765, Loss: 0.37282615900039673\n",
      "Epoch 16, Batch 766, Loss: 0.552304744720459\n",
      "Epoch 16, Batch 767, Loss: 0.4277331531047821\n",
      "Epoch 16, Batch 768, Loss: 0.4664166569709778\n",
      "Epoch 16, Batch 769, Loss: 0.583213746547699\n",
      "Epoch 16, Batch 770, Loss: 0.461296021938324\n",
      "Epoch 16, Batch 771, Loss: 0.4719727039337158\n",
      "Epoch 16, Batch 772, Loss: 0.36453965306282043\n",
      "Epoch 16, Batch 773, Loss: 0.40342485904693604\n",
      "Epoch 16, Batch 774, Loss: 0.40310150384902954\n",
      "Epoch 16, Batch 775, Loss: 0.20914128422737122\n",
      "Epoch 16, Batch 776, Loss: 0.3711513578891754\n",
      "Epoch 16, Batch 777, Loss: 0.37376612424850464\n",
      "Epoch 16, Batch 778, Loss: 0.5658959150314331\n",
      "Epoch 16, Batch 779, Loss: 0.41550931334495544\n",
      "Epoch 16, Batch 780, Loss: 0.3332909643650055\n",
      "Epoch 16, Batch 781, Loss: 0.5066195726394653\n",
      "Epoch 16, Batch 782, Loss: 0.29014909267425537\n",
      "Epoch 16, Batch 783, Loss: 0.49288949370384216\n",
      "Epoch 16, Batch 784, Loss: 0.44430509209632874\n",
      "Epoch 16, Batch 785, Loss: 0.3379811644554138\n",
      "Epoch 16, Batch 786, Loss: 0.5444721579551697\n",
      "Epoch 16, Batch 787, Loss: 0.38387933373451233\n",
      "Epoch 16, Batch 788, Loss: 0.30918940901756287\n",
      "Epoch 16, Batch 789, Loss: 0.582131564617157\n",
      "Epoch 16, Batch 790, Loss: 0.3096276521682739\n",
      "Epoch 16, Batch 791, Loss: 0.29710519313812256\n",
      "Epoch 16, Batch 792, Loss: 0.276422381401062\n",
      "Epoch 16, Batch 793, Loss: 0.38058340549468994\n",
      "Epoch 16, Batch 794, Loss: 0.3788174092769623\n",
      "Epoch 16, Batch 795, Loss: 0.4487000107765198\n",
      "Epoch 16, Batch 796, Loss: 0.320067822933197\n",
      "Epoch 16, Batch 797, Loss: 0.4549620449542999\n",
      "Epoch 16, Batch 798, Loss: 0.29254674911499023\n",
      "Epoch 16, Batch 799, Loss: 0.37855178117752075\n",
      "Epoch 16, Batch 800, Loss: 0.6065751314163208\n",
      "Epoch 16, Batch 801, Loss: 0.3596789836883545\n",
      "Epoch 16, Batch 802, Loss: 0.705066442489624\n",
      "Epoch 16, Batch 803, Loss: 0.5223241448402405\n",
      "Epoch 16, Batch 804, Loss: 0.3887903690338135\n",
      "Epoch 16, Batch 805, Loss: 0.3627329468727112\n",
      "Epoch 16, Batch 806, Loss: 0.4737851619720459\n",
      "Epoch 16, Batch 807, Loss: 0.35457396507263184\n",
      "Epoch 16, Batch 808, Loss: 0.5117434859275818\n",
      "Epoch 16, Batch 809, Loss: 0.397778183221817\n",
      "Epoch 16, Batch 810, Loss: 0.41738495230674744\n",
      "Epoch 16, Batch 811, Loss: 0.36116909980773926\n",
      "Epoch 16, Batch 812, Loss: 0.37514203786849976\n",
      "Epoch 16, Batch 813, Loss: 0.3394355773925781\n",
      "Epoch 16, Batch 814, Loss: 0.5350188612937927\n",
      "Epoch 16, Batch 815, Loss: 0.4383464455604553\n",
      "Epoch 16, Batch 816, Loss: 0.677100658416748\n",
      "Epoch 16, Batch 817, Loss: 0.25737640261650085\n",
      "Epoch 16, Batch 818, Loss: 0.6684803366661072\n",
      "Epoch 16, Batch 819, Loss: 0.478562593460083\n",
      "Epoch 16, Batch 820, Loss: 0.3403971791267395\n",
      "Epoch 16, Batch 821, Loss: 0.30340903997421265\n",
      "Epoch 16, Batch 822, Loss: 0.3552582859992981\n",
      "Epoch 16, Batch 823, Loss: 0.6605039834976196\n",
      "Epoch 16, Batch 824, Loss: 0.5061339735984802\n",
      "Epoch 16, Batch 825, Loss: 0.5331377387046814\n",
      "Epoch 16, Batch 826, Loss: 0.4646267592906952\n",
      "Epoch 16, Batch 827, Loss: 0.6345595121383667\n",
      "Epoch 16, Batch 828, Loss: 0.5373843312263489\n",
      "Epoch 16, Batch 829, Loss: 0.4405866265296936\n",
      "Epoch 16, Batch 830, Loss: 0.3827521502971649\n",
      "Epoch 16, Batch 831, Loss: 0.7682134509086609\n",
      "Epoch 16, Batch 832, Loss: 0.3722907304763794\n",
      "Epoch 16, Batch 833, Loss: 0.42152413725852966\n",
      "Epoch 16, Batch 834, Loss: 0.4303063154220581\n",
      "Epoch 16, Batch 835, Loss: 0.29472362995147705\n",
      "Epoch 16, Batch 836, Loss: 0.29158321022987366\n",
      "Epoch 16, Batch 837, Loss: 0.31831833720207214\n",
      "Epoch 16, Batch 838, Loss: 0.6027475595474243\n",
      "Epoch 16, Batch 839, Loss: 0.4809049069881439\n",
      "Epoch 16, Batch 840, Loss: 0.4936259686946869\n",
      "Epoch 16, Batch 841, Loss: 0.39169904589653015\n",
      "Epoch 16, Batch 842, Loss: 0.5244011282920837\n",
      "Epoch 16, Batch 843, Loss: 0.2814803421497345\n",
      "Epoch 16, Batch 844, Loss: 0.5727088451385498\n",
      "Epoch 16, Batch 845, Loss: 0.670604407787323\n",
      "Epoch 16, Batch 846, Loss: 0.505943775177002\n",
      "Epoch 16, Batch 847, Loss: 0.519909143447876\n",
      "Epoch 16, Batch 848, Loss: 0.3959920406341553\n",
      "Epoch 16, Batch 849, Loss: 0.253465473651886\n",
      "Epoch 16, Batch 850, Loss: 0.49500420689582825\n",
      "Epoch 16, Batch 851, Loss: 0.5138953924179077\n",
      "Epoch 16, Batch 852, Loss: 0.40614616870880127\n",
      "Epoch 16, Batch 853, Loss: 0.5115904211997986\n",
      "Epoch 16, Batch 854, Loss: 0.48228102922439575\n",
      "Epoch 16, Batch 855, Loss: 0.4106142818927765\n",
      "Epoch 16, Batch 856, Loss: 0.6148151755332947\n",
      "Epoch 16, Batch 857, Loss: 0.6520775556564331\n",
      "Epoch 16, Batch 858, Loss: 0.3601665198802948\n",
      "Epoch 16, Batch 859, Loss: 0.5936488509178162\n",
      "Epoch 16, Batch 860, Loss: 0.5710269212722778\n",
      "Epoch 16, Batch 861, Loss: 0.47755366563796997\n",
      "Epoch 16, Batch 862, Loss: 0.37262168526649475\n",
      "Epoch 16, Batch 863, Loss: 0.45119571685791016\n",
      "Epoch 16, Batch 864, Loss: 0.4999505281448364\n",
      "Epoch 16, Batch 865, Loss: 0.4170967638492584\n",
      "Epoch 16, Batch 866, Loss: 0.39761123061180115\n",
      "Epoch 16, Batch 867, Loss: 0.35936182737350464\n",
      "Epoch 16, Batch 868, Loss: 0.47195160388946533\n",
      "Epoch 16, Batch 869, Loss: 0.41647422313690186\n",
      "Epoch 16, Batch 870, Loss: 0.44988930225372314\n",
      "Epoch 16, Batch 871, Loss: 0.48329007625579834\n",
      "Epoch 16, Batch 872, Loss: 0.6327349543571472\n",
      "Epoch 16, Batch 873, Loss: 0.4754943549633026\n",
      "Epoch 16, Batch 874, Loss: 0.36815962195396423\n",
      "Epoch 16, Batch 875, Loss: 0.30715128779411316\n",
      "Epoch 16, Batch 876, Loss: 0.5482076406478882\n",
      "Epoch 16, Batch 877, Loss: 0.48422810435295105\n",
      "Epoch 16, Batch 878, Loss: 0.5445687174797058\n",
      "Epoch 16, Batch 879, Loss: 0.43538570404052734\n",
      "Epoch 16, Batch 880, Loss: 0.41404563188552856\n",
      "Epoch 16, Batch 881, Loss: 0.4978075921535492\n",
      "Epoch 16, Batch 882, Loss: 0.3829440176486969\n",
      "Epoch 16, Batch 883, Loss: 0.4796331822872162\n",
      "Epoch 16, Batch 884, Loss: 0.41796016693115234\n",
      "Epoch 16, Batch 885, Loss: 0.3560504615306854\n",
      "Epoch 16, Batch 886, Loss: 0.3926040828227997\n",
      "Epoch 16, Batch 887, Loss: 0.3530322313308716\n",
      "Epoch 16, Batch 888, Loss: 0.40960806608200073\n",
      "Epoch 16, Batch 889, Loss: 0.6519496440887451\n",
      "Epoch 16, Batch 890, Loss: 0.37251195311546326\n",
      "Epoch 16, Batch 891, Loss: 0.3402623236179352\n",
      "Epoch 16, Batch 892, Loss: 0.5106120705604553\n",
      "Epoch 16, Batch 893, Loss: 0.37845340371131897\n",
      "Epoch 16, Batch 894, Loss: 0.571657657623291\n",
      "Epoch 16, Batch 895, Loss: 0.7643692493438721\n",
      "Epoch 16, Batch 896, Loss: 0.46334198117256165\n",
      "Epoch 16, Batch 897, Loss: 0.558044970035553\n",
      "Epoch 16, Batch 898, Loss: 0.43245410919189453\n",
      "Epoch 16, Batch 899, Loss: 0.5972205400466919\n",
      "Epoch 16, Batch 900, Loss: 0.5521034002304077\n",
      "Epoch 16, Batch 901, Loss: 0.5752774477005005\n",
      "Epoch 16, Batch 902, Loss: 0.5158693790435791\n",
      "Epoch 16, Batch 903, Loss: 0.3693698048591614\n",
      "Epoch 16, Batch 904, Loss: 0.443460077047348\n",
      "Epoch 16, Batch 905, Loss: 0.7373084425926208\n",
      "Epoch 16, Batch 906, Loss: 0.4079587459564209\n",
      "Epoch 16, Batch 907, Loss: 0.5575685501098633\n",
      "Epoch 16, Batch 908, Loss: 0.40896132588386536\n",
      "Epoch 16, Batch 909, Loss: 0.3593932092189789\n",
      "Epoch 16, Batch 910, Loss: 0.44474828243255615\n",
      "Epoch 16, Batch 911, Loss: 0.4425961673259735\n",
      "Epoch 16, Batch 912, Loss: 0.26213812828063965\n",
      "Epoch 16, Batch 913, Loss: 0.3291882574558258\n",
      "Epoch 16, Batch 914, Loss: 0.43498122692108154\n",
      "Epoch 16, Batch 915, Loss: 0.37186574935913086\n",
      "Epoch 16, Batch 916, Loss: 0.5067445039749146\n",
      "Epoch 16, Batch 917, Loss: 0.4884185194969177\n",
      "Epoch 16, Batch 918, Loss: 0.4567103087902069\n",
      "Epoch 16, Batch 919, Loss: 0.6348050832748413\n",
      "Epoch 16, Batch 920, Loss: 0.6532288789749146\n",
      "Epoch 16, Batch 921, Loss: 0.6394984126091003\n",
      "Epoch 16, Batch 922, Loss: 0.4682534337043762\n",
      "Epoch 16, Batch 923, Loss: 0.41476568579673767\n",
      "Epoch 16, Batch 924, Loss: 0.3800581693649292\n",
      "Epoch 16, Batch 925, Loss: 0.3414883017539978\n",
      "Epoch 16, Batch 926, Loss: 0.42953264713287354\n",
      "Epoch 16, Batch 927, Loss: 0.36546590924263\n",
      "Epoch 16, Batch 928, Loss: 0.4048715829849243\n",
      "Epoch 16, Batch 929, Loss: 0.3388931155204773\n",
      "Epoch 16, Batch 930, Loss: 0.6167045831680298\n",
      "Epoch 16, Batch 931, Loss: 0.8609325289726257\n",
      "Epoch 16, Batch 932, Loss: 0.30282482504844666\n",
      "Epoch 16, Batch 933, Loss: 0.6478044390678406\n",
      "Epoch 16, Batch 934, Loss: 0.4071989357471466\n",
      "Epoch 16, Batch 935, Loss: 0.505580484867096\n",
      "Epoch 16, Batch 936, Loss: 0.4030057191848755\n",
      "Epoch 16, Batch 937, Loss: 0.4709184765815735\n",
      "Epoch 16, Batch 938, Loss: 0.3464803397655487\n",
      "Accuracy of train set: 0.8420166666666666\n",
      "Epoch 16, Batch 1, Test Loss: 0.4171038269996643\n",
      "Epoch 16, Batch 2, Test Loss: 0.3118986487388611\n",
      "Epoch 16, Batch 3, Test Loss: 0.4857000410556793\n",
      "Epoch 16, Batch 4, Test Loss: 0.40281182527542114\n",
      "Epoch 16, Batch 5, Test Loss: 0.45266634225845337\n",
      "Epoch 16, Batch 6, Test Loss: 0.3792741000652313\n",
      "Epoch 16, Batch 7, Test Loss: 0.5789365172386169\n",
      "Epoch 16, Batch 8, Test Loss: 0.40844714641571045\n",
      "Epoch 16, Batch 9, Test Loss: 0.6433701515197754\n",
      "Epoch 16, Batch 10, Test Loss: 0.6192623972892761\n",
      "Epoch 16, Batch 11, Test Loss: 0.4314640164375305\n",
      "Epoch 16, Batch 12, Test Loss: 0.3009447753429413\n",
      "Epoch 16, Batch 13, Test Loss: 0.44837120175361633\n",
      "Epoch 16, Batch 14, Test Loss: 0.33124813437461853\n",
      "Epoch 16, Batch 15, Test Loss: 0.4543231725692749\n",
      "Epoch 16, Batch 16, Test Loss: 0.33670586347579956\n",
      "Epoch 16, Batch 17, Test Loss: 0.4392719268798828\n",
      "Epoch 16, Batch 18, Test Loss: 0.43153494596481323\n",
      "Epoch 16, Batch 19, Test Loss: 0.5031141042709351\n",
      "Epoch 16, Batch 20, Test Loss: 0.5336695313453674\n",
      "Epoch 16, Batch 21, Test Loss: 0.4206136465072632\n",
      "Epoch 16, Batch 22, Test Loss: 0.3182186484336853\n",
      "Epoch 16, Batch 23, Test Loss: 0.4861566722393036\n",
      "Epoch 16, Batch 24, Test Loss: 0.5560118556022644\n",
      "Epoch 16, Batch 25, Test Loss: 0.49854394793510437\n",
      "Epoch 16, Batch 26, Test Loss: 0.4345026910305023\n",
      "Epoch 16, Batch 27, Test Loss: 0.3280898630619049\n",
      "Epoch 16, Batch 28, Test Loss: 0.40173107385635376\n",
      "Epoch 16, Batch 29, Test Loss: 0.442874014377594\n",
      "Epoch 16, Batch 30, Test Loss: 0.589239776134491\n",
      "Epoch 16, Batch 31, Test Loss: 0.3640674650669098\n",
      "Epoch 16, Batch 32, Test Loss: 0.5471204519271851\n",
      "Epoch 16, Batch 33, Test Loss: 0.4436700642108917\n",
      "Epoch 16, Batch 34, Test Loss: 0.30259329080581665\n",
      "Epoch 16, Batch 35, Test Loss: 0.4858526587486267\n",
      "Epoch 16, Batch 36, Test Loss: 0.4893055558204651\n",
      "Epoch 16, Batch 37, Test Loss: 0.5248037576675415\n",
      "Epoch 16, Batch 38, Test Loss: 0.48626819252967834\n",
      "Epoch 16, Batch 39, Test Loss: 0.46798378229141235\n",
      "Epoch 16, Batch 40, Test Loss: 0.5429973602294922\n",
      "Epoch 16, Batch 41, Test Loss: 0.317480206489563\n",
      "Epoch 16, Batch 42, Test Loss: 0.5669733881950378\n",
      "Epoch 16, Batch 43, Test Loss: 0.6610758304595947\n",
      "Epoch 16, Batch 44, Test Loss: 0.43805181980133057\n",
      "Epoch 16, Batch 45, Test Loss: 0.34812188148498535\n",
      "Epoch 16, Batch 46, Test Loss: 0.5404120087623596\n",
      "Epoch 16, Batch 47, Test Loss: 0.474851131439209\n",
      "Epoch 16, Batch 48, Test Loss: 0.4826454818248749\n",
      "Epoch 16, Batch 49, Test Loss: 0.4071241021156311\n",
      "Epoch 16, Batch 50, Test Loss: 0.49147090315818787\n",
      "Epoch 16, Batch 51, Test Loss: 0.5852497220039368\n",
      "Epoch 16, Batch 52, Test Loss: 0.4839338958263397\n",
      "Epoch 16, Batch 53, Test Loss: 0.27773213386535645\n",
      "Epoch 16, Batch 54, Test Loss: 0.31673499941825867\n",
      "Epoch 16, Batch 55, Test Loss: 0.54561448097229\n",
      "Epoch 16, Batch 56, Test Loss: 0.6640805006027222\n",
      "Epoch 16, Batch 57, Test Loss: 0.3328489065170288\n",
      "Epoch 16, Batch 58, Test Loss: 0.5178804993629456\n",
      "Epoch 16, Batch 59, Test Loss: 0.4501567482948303\n",
      "Epoch 16, Batch 60, Test Loss: 0.5642699003219604\n",
      "Epoch 16, Batch 61, Test Loss: 0.4947555959224701\n",
      "Epoch 16, Batch 62, Test Loss: 0.43632856011390686\n",
      "Epoch 16, Batch 63, Test Loss: 0.4540143311023712\n",
      "Epoch 16, Batch 64, Test Loss: 0.40005582571029663\n",
      "Epoch 16, Batch 65, Test Loss: 0.40587303042411804\n",
      "Epoch 16, Batch 66, Test Loss: 0.5731220841407776\n",
      "Epoch 16, Batch 67, Test Loss: 0.5157071352005005\n",
      "Epoch 16, Batch 68, Test Loss: 0.3734545111656189\n",
      "Epoch 16, Batch 69, Test Loss: 0.378737211227417\n",
      "Epoch 16, Batch 70, Test Loss: 0.4987807869911194\n",
      "Epoch 16, Batch 71, Test Loss: 0.44252610206604004\n",
      "Epoch 16, Batch 72, Test Loss: 0.5264523029327393\n",
      "Epoch 16, Batch 73, Test Loss: 0.46192413568496704\n",
      "Epoch 16, Batch 74, Test Loss: 0.5227289795875549\n",
      "Epoch 16, Batch 75, Test Loss: 0.45237162709236145\n",
      "Epoch 16, Batch 76, Test Loss: 0.4417882561683655\n",
      "Epoch 16, Batch 77, Test Loss: 0.36669453978538513\n",
      "Epoch 16, Batch 78, Test Loss: 0.47691306471824646\n",
      "Epoch 16, Batch 79, Test Loss: 0.3815067708492279\n",
      "Epoch 16, Batch 80, Test Loss: 0.4959093928337097\n",
      "Epoch 16, Batch 81, Test Loss: 0.40408557653427124\n",
      "Epoch 16, Batch 82, Test Loss: 0.40180376172065735\n",
      "Epoch 16, Batch 83, Test Loss: 0.542799174785614\n",
      "Epoch 16, Batch 84, Test Loss: 0.5342968106269836\n",
      "Epoch 16, Batch 85, Test Loss: 0.4245360791683197\n",
      "Epoch 16, Batch 86, Test Loss: 0.456337571144104\n",
      "Epoch 16, Batch 87, Test Loss: 0.4686545729637146\n",
      "Epoch 16, Batch 88, Test Loss: 0.47847139835357666\n",
      "Epoch 16, Batch 89, Test Loss: 0.39081451296806335\n",
      "Epoch 16, Batch 90, Test Loss: 0.2181994467973709\n",
      "Epoch 16, Batch 91, Test Loss: 0.34630492329597473\n",
      "Epoch 16, Batch 92, Test Loss: 0.6792467832565308\n",
      "Epoch 16, Batch 93, Test Loss: 0.44029009342193604\n",
      "Epoch 16, Batch 94, Test Loss: 0.48153653740882874\n",
      "Epoch 16, Batch 95, Test Loss: 0.5389875173568726\n",
      "Epoch 16, Batch 96, Test Loss: 0.5392006635665894\n",
      "Epoch 16, Batch 97, Test Loss: 0.4288505017757416\n",
      "Epoch 16, Batch 98, Test Loss: 0.3769056797027588\n",
      "Epoch 16, Batch 99, Test Loss: 0.5141038298606873\n",
      "Epoch 16, Batch 100, Test Loss: 0.5229653716087341\n",
      "Epoch 16, Batch 101, Test Loss: 0.3578281104564667\n",
      "Epoch 16, Batch 102, Test Loss: 0.28805112838745117\n",
      "Epoch 16, Batch 103, Test Loss: 0.3722987174987793\n",
      "Epoch 16, Batch 104, Test Loss: 0.29989922046661377\n",
      "Epoch 16, Batch 105, Test Loss: 0.46614012122154236\n",
      "Epoch 16, Batch 106, Test Loss: 0.4853438138961792\n",
      "Epoch 16, Batch 107, Test Loss: 0.4525187313556671\n",
      "Epoch 16, Batch 108, Test Loss: 0.36227673292160034\n",
      "Epoch 16, Batch 109, Test Loss: 0.33470678329467773\n",
      "Epoch 16, Batch 110, Test Loss: 0.38994622230529785\n",
      "Epoch 16, Batch 111, Test Loss: 0.4197707176208496\n",
      "Epoch 16, Batch 112, Test Loss: 0.36191317439079285\n",
      "Epoch 16, Batch 113, Test Loss: 0.4613272547721863\n",
      "Epoch 16, Batch 114, Test Loss: 0.2631133198738098\n",
      "Epoch 16, Batch 115, Test Loss: 0.4431420862674713\n",
      "Epoch 16, Batch 116, Test Loss: 0.4978267252445221\n",
      "Epoch 16, Batch 117, Test Loss: 0.42763200402259827\n",
      "Epoch 16, Batch 118, Test Loss: 0.3785826563835144\n",
      "Epoch 16, Batch 119, Test Loss: 0.2317132204771042\n",
      "Epoch 16, Batch 120, Test Loss: 0.4448431432247162\n",
      "Epoch 16, Batch 121, Test Loss: 0.5388127565383911\n",
      "Epoch 16, Batch 122, Test Loss: 0.3971922993659973\n",
      "Epoch 16, Batch 123, Test Loss: 0.5505321621894836\n",
      "Epoch 16, Batch 124, Test Loss: 0.45298293232917786\n",
      "Epoch 16, Batch 125, Test Loss: 0.5595117211341858\n",
      "Epoch 16, Batch 126, Test Loss: 0.5647494792938232\n",
      "Epoch 16, Batch 127, Test Loss: 0.48106738924980164\n",
      "Epoch 16, Batch 128, Test Loss: 0.45507553219795227\n",
      "Epoch 16, Batch 129, Test Loss: 0.37025749683380127\n",
      "Epoch 16, Batch 130, Test Loss: 0.6473298668861389\n",
      "Epoch 16, Batch 131, Test Loss: 0.41118577122688293\n",
      "Epoch 16, Batch 132, Test Loss: 0.5160972476005554\n",
      "Epoch 16, Batch 133, Test Loss: 0.3833168148994446\n",
      "Epoch 16, Batch 134, Test Loss: 0.29335713386535645\n",
      "Epoch 16, Batch 135, Test Loss: 0.7935389280319214\n",
      "Epoch 16, Batch 136, Test Loss: 0.3633122146129608\n",
      "Epoch 16, Batch 137, Test Loss: 0.4471374750137329\n",
      "Epoch 16, Batch 138, Test Loss: 0.27780941128730774\n",
      "Epoch 16, Batch 139, Test Loss: 0.5134554505348206\n",
      "Epoch 16, Batch 140, Test Loss: 0.48190128803253174\n",
      "Epoch 16, Batch 141, Test Loss: 0.5506595969200134\n",
      "Epoch 16, Batch 142, Test Loss: 0.5381776094436646\n",
      "Epoch 16, Batch 143, Test Loss: 0.363089919090271\n",
      "Epoch 16, Batch 144, Test Loss: 0.5183915495872498\n",
      "Epoch 16, Batch 145, Test Loss: 0.28919708728790283\n",
      "Epoch 16, Batch 146, Test Loss: 0.3538980484008789\n",
      "Epoch 16, Batch 147, Test Loss: 0.430573433637619\n",
      "Epoch 16, Batch 148, Test Loss: 0.6413222551345825\n",
      "Epoch 16, Batch 149, Test Loss: 0.49187615513801575\n",
      "Epoch 16, Batch 150, Test Loss: 0.4595217704772949\n",
      "Epoch 16, Batch 151, Test Loss: 0.3941551148891449\n",
      "Epoch 16, Batch 152, Test Loss: 0.45004528760910034\n",
      "Epoch 16, Batch 153, Test Loss: 0.4136962294578552\n",
      "Epoch 16, Batch 154, Test Loss: 0.4812825918197632\n",
      "Epoch 16, Batch 155, Test Loss: 0.48527562618255615\n",
      "Epoch 16, Batch 156, Test Loss: 0.4263751208782196\n",
      "Epoch 16, Batch 157, Test Loss: 0.23921167850494385\n",
      "Epoch 16, Batch 158, Test Loss: 0.344468355178833\n",
      "Epoch 16, Batch 159, Test Loss: 0.3251563012599945\n",
      "Epoch 16, Batch 160, Test Loss: 0.3985055088996887\n",
      "Epoch 16, Batch 161, Test Loss: 0.6188180446624756\n",
      "Epoch 16, Batch 162, Test Loss: 0.4436037540435791\n",
      "Epoch 16, Batch 163, Test Loss: 0.5321987271308899\n",
      "Epoch 16, Batch 164, Test Loss: 0.4975334107875824\n",
      "Epoch 16, Batch 165, Test Loss: 0.3872454762458801\n",
      "Epoch 16, Batch 166, Test Loss: 0.38878968358039856\n",
      "Epoch 16, Batch 167, Test Loss: 0.6052533388137817\n",
      "Epoch 16, Batch 168, Test Loss: 0.6626081466674805\n",
      "Epoch 16, Batch 169, Test Loss: 0.6290820837020874\n",
      "Epoch 16, Batch 170, Test Loss: 0.31836360692977905\n",
      "Epoch 16, Batch 171, Test Loss: 0.35434165596961975\n",
      "Epoch 16, Batch 172, Test Loss: 0.6192365884780884\n",
      "Epoch 16, Batch 173, Test Loss: 0.4206300377845764\n",
      "Epoch 16, Batch 174, Test Loss: 0.4058428704738617\n",
      "Epoch 16, Batch 175, Test Loss: 0.5652468800544739\n",
      "Epoch 16, Batch 176, Test Loss: 0.5151159167289734\n",
      "Epoch 16, Batch 177, Test Loss: 0.4588931202888489\n",
      "Epoch 16, Batch 178, Test Loss: 0.30121880769729614\n",
      "Epoch 16, Batch 179, Test Loss: 0.6269993185997009\n",
      "Epoch 16, Batch 180, Test Loss: 0.39393070340156555\n",
      "Epoch 16, Batch 181, Test Loss: 0.3693191111087799\n",
      "Epoch 16, Batch 182, Test Loss: 0.3860856294631958\n",
      "Epoch 16, Batch 183, Test Loss: 0.3381972908973694\n",
      "Epoch 16, Batch 184, Test Loss: 0.357780784368515\n",
      "Epoch 16, Batch 185, Test Loss: 0.3423691689968109\n",
      "Epoch 16, Batch 186, Test Loss: 0.48489266633987427\n",
      "Epoch 16, Batch 187, Test Loss: 0.3076704144477844\n",
      "Epoch 16, Batch 188, Test Loss: 0.27643439173698425\n",
      "Epoch 16, Batch 189, Test Loss: 0.6308717727661133\n",
      "Epoch 16, Batch 190, Test Loss: 0.340248167514801\n",
      "Epoch 16, Batch 191, Test Loss: 0.30450278520584106\n",
      "Epoch 16, Batch 192, Test Loss: 0.24294045567512512\n",
      "Epoch 16, Batch 193, Test Loss: 0.6808614730834961\n",
      "Epoch 16, Batch 194, Test Loss: 0.5240446329116821\n",
      "Epoch 16, Batch 195, Test Loss: 0.38438189029693604\n",
      "Epoch 16, Batch 196, Test Loss: 0.5099883079528809\n",
      "Epoch 16, Batch 197, Test Loss: 0.33990198373794556\n",
      "Epoch 16, Batch 198, Test Loss: 0.5147853493690491\n",
      "Epoch 16, Batch 199, Test Loss: 0.42355233430862427\n",
      "Epoch 16, Batch 200, Test Loss: 0.4614121913909912\n",
      "Epoch 16, Batch 201, Test Loss: 0.39535003900527954\n",
      "Epoch 16, Batch 202, Test Loss: 0.5175383687019348\n",
      "Epoch 16, Batch 203, Test Loss: 0.3517667055130005\n",
      "Epoch 16, Batch 204, Test Loss: 0.3231114149093628\n",
      "Epoch 16, Batch 205, Test Loss: 0.32793712615966797\n",
      "Epoch 16, Batch 206, Test Loss: 0.4010622501373291\n",
      "Epoch 16, Batch 207, Test Loss: 0.3050415515899658\n",
      "Epoch 16, Batch 208, Test Loss: 0.544569730758667\n",
      "Epoch 16, Batch 209, Test Loss: 0.49927517771720886\n",
      "Epoch 16, Batch 210, Test Loss: 0.2490548938512802\n",
      "Epoch 16, Batch 211, Test Loss: 0.5282465815544128\n",
      "Epoch 16, Batch 212, Test Loss: 0.42802000045776367\n",
      "Epoch 16, Batch 213, Test Loss: 0.47668886184692383\n",
      "Epoch 16, Batch 214, Test Loss: 0.324291855096817\n",
      "Epoch 16, Batch 215, Test Loss: 0.4834839701652527\n",
      "Epoch 16, Batch 216, Test Loss: 0.6390730142593384\n",
      "Epoch 16, Batch 217, Test Loss: 0.3346274495124817\n",
      "Epoch 16, Batch 218, Test Loss: 0.4055761396884918\n",
      "Epoch 16, Batch 219, Test Loss: 0.5311065912246704\n",
      "Epoch 16, Batch 220, Test Loss: 0.5456596612930298\n",
      "Epoch 16, Batch 221, Test Loss: 0.5527365803718567\n",
      "Epoch 16, Batch 222, Test Loss: 0.26128482818603516\n",
      "Epoch 16, Batch 223, Test Loss: 0.3787582516670227\n",
      "Epoch 16, Batch 224, Test Loss: 0.700491726398468\n",
      "Epoch 16, Batch 225, Test Loss: 0.41827625036239624\n",
      "Epoch 16, Batch 226, Test Loss: 0.49624717235565186\n",
      "Epoch 16, Batch 227, Test Loss: 0.5486729145050049\n",
      "Epoch 16, Batch 228, Test Loss: 0.4409266710281372\n",
      "Epoch 16, Batch 229, Test Loss: 0.37590885162353516\n",
      "Epoch 16, Batch 230, Test Loss: 0.545417308807373\n",
      "Epoch 16, Batch 231, Test Loss: 0.417259156703949\n",
      "Epoch 16, Batch 232, Test Loss: 0.30083030462265015\n",
      "Epoch 16, Batch 233, Test Loss: 0.3132697641849518\n",
      "Epoch 16, Batch 234, Test Loss: 0.5057615637779236\n",
      "Epoch 16, Batch 235, Test Loss: 0.4264282286167145\n",
      "Epoch 16, Batch 236, Test Loss: 0.34974944591522217\n",
      "Epoch 16, Batch 237, Test Loss: 0.5760819315910339\n",
      "Epoch 16, Batch 238, Test Loss: 0.5893372297286987\n",
      "Epoch 16, Batch 239, Test Loss: 0.4897208511829376\n",
      "Epoch 16, Batch 240, Test Loss: 0.6087660789489746\n",
      "Epoch 16, Batch 241, Test Loss: 0.3681453764438629\n",
      "Epoch 16, Batch 242, Test Loss: 0.5637856125831604\n",
      "Epoch 16, Batch 243, Test Loss: 0.2711862325668335\n",
      "Epoch 16, Batch 244, Test Loss: 0.31174328923225403\n",
      "Epoch 16, Batch 245, Test Loss: 0.514163613319397\n",
      "Epoch 16, Batch 246, Test Loss: 0.32716643810272217\n",
      "Epoch 16, Batch 247, Test Loss: 0.33082377910614014\n",
      "Epoch 16, Batch 248, Test Loss: 0.4661894142627716\n",
      "Epoch 16, Batch 249, Test Loss: 0.34590405225753784\n",
      "Epoch 16, Batch 250, Test Loss: 0.3497588038444519\n",
      "Epoch 16, Batch 251, Test Loss: 0.28606677055358887\n",
      "Epoch 16, Batch 252, Test Loss: 0.5529376268386841\n",
      "Epoch 16, Batch 253, Test Loss: 0.3995882272720337\n",
      "Epoch 16, Batch 254, Test Loss: 0.3930419683456421\n",
      "Epoch 16, Batch 255, Test Loss: 0.40814080834388733\n",
      "Epoch 16, Batch 256, Test Loss: 0.49596214294433594\n",
      "Epoch 16, Batch 257, Test Loss: 0.34429627656936646\n",
      "Epoch 16, Batch 258, Test Loss: 0.3919045925140381\n",
      "Epoch 16, Batch 259, Test Loss: 0.34135639667510986\n",
      "Epoch 16, Batch 260, Test Loss: 0.7003422975540161\n",
      "Epoch 16, Batch 261, Test Loss: 0.6447969675064087\n",
      "Epoch 16, Batch 262, Test Loss: 0.6125426888465881\n",
      "Epoch 16, Batch 263, Test Loss: 0.6440444588661194\n",
      "Epoch 16, Batch 264, Test Loss: 0.45414453744888306\n",
      "Epoch 16, Batch 265, Test Loss: 0.5184619426727295\n",
      "Epoch 16, Batch 266, Test Loss: 0.2539221942424774\n",
      "Epoch 16, Batch 267, Test Loss: 0.4107206463813782\n",
      "Epoch 16, Batch 268, Test Loss: 0.29243141412734985\n",
      "Epoch 16, Batch 269, Test Loss: 0.3600066006183624\n",
      "Epoch 16, Batch 270, Test Loss: 0.6222916841506958\n",
      "Epoch 16, Batch 271, Test Loss: 0.26308658719062805\n",
      "Epoch 16, Batch 272, Test Loss: 0.3078117370605469\n",
      "Epoch 16, Batch 273, Test Loss: 0.3978021740913391\n",
      "Epoch 16, Batch 274, Test Loss: 0.3809838891029358\n",
      "Epoch 16, Batch 275, Test Loss: 0.5348645448684692\n",
      "Epoch 16, Batch 276, Test Loss: 0.45989158749580383\n",
      "Epoch 16, Batch 277, Test Loss: 0.684029221534729\n",
      "Epoch 16, Batch 278, Test Loss: 0.35955893993377686\n",
      "Epoch 16, Batch 279, Test Loss: 0.5365729331970215\n",
      "Epoch 16, Batch 280, Test Loss: 0.3044948875904083\n",
      "Epoch 16, Batch 281, Test Loss: 0.5213620662689209\n",
      "Epoch 16, Batch 282, Test Loss: 0.42643994092941284\n",
      "Epoch 16, Batch 283, Test Loss: 0.47582685947418213\n",
      "Epoch 16, Batch 284, Test Loss: 0.5359067916870117\n",
      "Epoch 16, Batch 285, Test Loss: 0.5890257954597473\n",
      "Epoch 16, Batch 286, Test Loss: 0.5397404432296753\n",
      "Epoch 16, Batch 287, Test Loss: 0.31498420238494873\n",
      "Epoch 16, Batch 288, Test Loss: 0.5147557854652405\n",
      "Epoch 16, Batch 289, Test Loss: 0.49382710456848145\n",
      "Epoch 16, Batch 290, Test Loss: 0.5504025220870972\n",
      "Epoch 16, Batch 291, Test Loss: 0.5428969264030457\n",
      "Epoch 16, Batch 292, Test Loss: 0.44862088561058044\n",
      "Epoch 16, Batch 293, Test Loss: 0.3797408640384674\n",
      "Epoch 16, Batch 294, Test Loss: 0.7040539979934692\n",
      "Epoch 16, Batch 295, Test Loss: 0.46648457646369934\n",
      "Epoch 16, Batch 296, Test Loss: 0.5332258343696594\n",
      "Epoch 16, Batch 297, Test Loss: 0.487246036529541\n",
      "Epoch 16, Batch 298, Test Loss: 0.38876909017562866\n",
      "Epoch 16, Batch 299, Test Loss: 0.2469983994960785\n",
      "Epoch 16, Batch 300, Test Loss: 0.4275251626968384\n",
      "Epoch 16, Batch 301, Test Loss: 0.3509805202484131\n",
      "Epoch 16, Batch 302, Test Loss: 0.2484341561794281\n",
      "Epoch 16, Batch 303, Test Loss: 0.3526954650878906\n",
      "Epoch 16, Batch 304, Test Loss: 0.36012253165245056\n",
      "Epoch 16, Batch 305, Test Loss: 0.30351459980010986\n",
      "Epoch 16, Batch 306, Test Loss: 0.4342154860496521\n",
      "Epoch 16, Batch 307, Test Loss: 0.3428235650062561\n",
      "Epoch 16, Batch 308, Test Loss: 0.38439857959747314\n",
      "Epoch 16, Batch 309, Test Loss: 0.5902166962623596\n",
      "Epoch 16, Batch 310, Test Loss: 0.286428302526474\n",
      "Epoch 16, Batch 311, Test Loss: 0.5322601199150085\n",
      "Epoch 16, Batch 312, Test Loss: 0.25478628277778625\n",
      "Epoch 16, Batch 313, Test Loss: 0.445215106010437\n",
      "Epoch 16, Batch 314, Test Loss: 0.5693221092224121\n",
      "Epoch 16, Batch 315, Test Loss: 0.4107774496078491\n",
      "Epoch 16, Batch 316, Test Loss: 0.4185872972011566\n",
      "Epoch 16, Batch 317, Test Loss: 0.42156127095222473\n",
      "Epoch 16, Batch 318, Test Loss: 0.477696031332016\n",
      "Epoch 16, Batch 319, Test Loss: 0.5125642418861389\n",
      "Epoch 16, Batch 320, Test Loss: 0.3226745128631592\n",
      "Epoch 16, Batch 321, Test Loss: 0.44974350929260254\n",
      "Epoch 16, Batch 322, Test Loss: 0.2866770327091217\n",
      "Epoch 16, Batch 323, Test Loss: 0.5257992744445801\n",
      "Epoch 16, Batch 324, Test Loss: 0.4238840341567993\n",
      "Epoch 16, Batch 325, Test Loss: 0.4197777509689331\n",
      "Epoch 16, Batch 326, Test Loss: 0.6632881164550781\n",
      "Epoch 16, Batch 327, Test Loss: 0.45274418592453003\n",
      "Epoch 16, Batch 328, Test Loss: 0.2939213812351227\n",
      "Epoch 16, Batch 329, Test Loss: 0.35256513953208923\n",
      "Epoch 16, Batch 330, Test Loss: 0.3836233913898468\n",
      "Epoch 16, Batch 331, Test Loss: 0.5458966493606567\n",
      "Epoch 16, Batch 332, Test Loss: 0.6648335456848145\n",
      "Epoch 16, Batch 333, Test Loss: 0.4370688199996948\n",
      "Epoch 16, Batch 334, Test Loss: 0.29405954480171204\n",
      "Epoch 16, Batch 335, Test Loss: 0.5646979808807373\n",
      "Epoch 16, Batch 336, Test Loss: 0.8578606843948364\n",
      "Epoch 16, Batch 337, Test Loss: 0.440905898809433\n",
      "Epoch 16, Batch 338, Test Loss: 0.42437052726745605\n",
      "Epoch 16, Batch 339, Test Loss: 0.504807710647583\n",
      "Epoch 16, Batch 340, Test Loss: 0.4657580256462097\n",
      "Epoch 16, Batch 341, Test Loss: 0.31563177704811096\n",
      "Epoch 16, Batch 342, Test Loss: 0.4091348648071289\n",
      "Epoch 16, Batch 343, Test Loss: 0.6134482026100159\n",
      "Epoch 16, Batch 344, Test Loss: 0.37368494272232056\n",
      "Epoch 16, Batch 345, Test Loss: 0.2958325445652008\n",
      "Epoch 16, Batch 346, Test Loss: 0.4632076025009155\n",
      "Epoch 16, Batch 347, Test Loss: 0.44419920444488525\n",
      "Epoch 16, Batch 348, Test Loss: 0.4175090193748474\n",
      "Epoch 16, Batch 349, Test Loss: 0.30658191442489624\n",
      "Epoch 16, Batch 350, Test Loss: 0.4638109803199768\n",
      "Epoch 16, Batch 351, Test Loss: 0.39522773027420044\n",
      "Epoch 16, Batch 352, Test Loss: 0.4200429916381836\n",
      "Epoch 16, Batch 353, Test Loss: 0.40507999062538147\n",
      "Epoch 16, Batch 354, Test Loss: 0.3989485204219818\n",
      "Epoch 16, Batch 355, Test Loss: 0.48542600870132446\n",
      "Epoch 16, Batch 356, Test Loss: 0.50099778175354\n",
      "Epoch 16, Batch 357, Test Loss: 0.3178943395614624\n",
      "Epoch 16, Batch 358, Test Loss: 0.29418590664863586\n",
      "Epoch 16, Batch 359, Test Loss: 0.6201984286308289\n",
      "Epoch 16, Batch 360, Test Loss: 0.6259968280792236\n",
      "Epoch 16, Batch 361, Test Loss: 0.5468811988830566\n",
      "Epoch 16, Batch 362, Test Loss: 0.4938543140888214\n",
      "Epoch 16, Batch 363, Test Loss: 0.22155380249023438\n",
      "Epoch 16, Batch 364, Test Loss: 0.7039808034896851\n",
      "Epoch 16, Batch 365, Test Loss: 0.5296151638031006\n",
      "Epoch 16, Batch 366, Test Loss: 0.37675249576568604\n",
      "Epoch 16, Batch 367, Test Loss: 0.2836240530014038\n",
      "Epoch 16, Batch 368, Test Loss: 0.4602174460887909\n",
      "Epoch 16, Batch 369, Test Loss: 0.775264322757721\n",
      "Epoch 16, Batch 370, Test Loss: 0.45276594161987305\n",
      "Epoch 16, Batch 371, Test Loss: 0.3896237015724182\n",
      "Epoch 16, Batch 372, Test Loss: 0.4846761226654053\n",
      "Epoch 16, Batch 373, Test Loss: 0.4646604657173157\n",
      "Epoch 16, Batch 374, Test Loss: 0.485779345035553\n",
      "Epoch 16, Batch 375, Test Loss: 0.38541415333747864\n",
      "Epoch 16, Batch 376, Test Loss: 0.5687117576599121\n",
      "Epoch 16, Batch 377, Test Loss: 0.47673729062080383\n",
      "Epoch 16, Batch 378, Test Loss: 0.35275474190711975\n",
      "Epoch 16, Batch 379, Test Loss: 0.431653767824173\n",
      "Epoch 16, Batch 380, Test Loss: 0.30345839262008667\n",
      "Epoch 16, Batch 381, Test Loss: 0.6309988498687744\n",
      "Epoch 16, Batch 382, Test Loss: 0.3871968984603882\n",
      "Epoch 16, Batch 383, Test Loss: 0.5916258096694946\n",
      "Epoch 16, Batch 384, Test Loss: 0.33583810925483704\n",
      "Epoch 16, Batch 385, Test Loss: 0.5204244256019592\n",
      "Epoch 16, Batch 386, Test Loss: 0.5178943276405334\n",
      "Epoch 16, Batch 387, Test Loss: 0.6298262476921082\n",
      "Epoch 16, Batch 388, Test Loss: 0.37588682770729065\n",
      "Epoch 16, Batch 389, Test Loss: 0.3313139081001282\n",
      "Epoch 16, Batch 390, Test Loss: 0.4819965362548828\n",
      "Epoch 16, Batch 391, Test Loss: 0.6911942362785339\n",
      "Epoch 16, Batch 392, Test Loss: 0.6412028074264526\n",
      "Epoch 16, Batch 393, Test Loss: 0.5167072415351868\n",
      "Epoch 16, Batch 394, Test Loss: 0.5501821637153625\n",
      "Epoch 16, Batch 395, Test Loss: 0.3712807595729828\n",
      "Epoch 16, Batch 396, Test Loss: 0.4544403553009033\n",
      "Epoch 16, Batch 397, Test Loss: 0.44427719712257385\n",
      "Epoch 16, Batch 398, Test Loss: 0.31961095333099365\n",
      "Epoch 16, Batch 399, Test Loss: 0.4000470042228699\n",
      "Epoch 16, Batch 400, Test Loss: 0.5683280229568481\n",
      "Epoch 16, Batch 401, Test Loss: 0.4392295181751251\n",
      "Epoch 16, Batch 402, Test Loss: 0.29975220561027527\n",
      "Epoch 16, Batch 403, Test Loss: 0.30937522649765015\n",
      "Epoch 16, Batch 404, Test Loss: 0.31791141629219055\n",
      "Epoch 16, Batch 405, Test Loss: 0.3092571794986725\n",
      "Epoch 16, Batch 406, Test Loss: 0.516521692276001\n",
      "Epoch 16, Batch 407, Test Loss: 0.3339962959289551\n",
      "Epoch 16, Batch 408, Test Loss: 0.4677472412586212\n",
      "Epoch 16, Batch 409, Test Loss: 0.5461693406105042\n",
      "Epoch 16, Batch 410, Test Loss: 0.4612842798233032\n",
      "Epoch 16, Batch 411, Test Loss: 0.34022757411003113\n",
      "Epoch 16, Batch 412, Test Loss: 0.4070414900779724\n",
      "Epoch 16, Batch 413, Test Loss: 0.5031706094741821\n",
      "Epoch 16, Batch 414, Test Loss: 0.5503078699111938\n",
      "Epoch 16, Batch 415, Test Loss: 0.354442834854126\n",
      "Epoch 16, Batch 416, Test Loss: 0.38467973470687866\n",
      "Epoch 16, Batch 417, Test Loss: 0.48250654339790344\n",
      "Epoch 16, Batch 418, Test Loss: 0.5577418804168701\n",
      "Epoch 16, Batch 419, Test Loss: 0.5376020669937134\n",
      "Epoch 16, Batch 420, Test Loss: 0.47862708568573\n",
      "Epoch 16, Batch 421, Test Loss: 0.39076757431030273\n",
      "Epoch 16, Batch 422, Test Loss: 0.41298171877861023\n",
      "Epoch 16, Batch 423, Test Loss: 0.5751624703407288\n",
      "Epoch 16, Batch 424, Test Loss: 0.5948948860168457\n",
      "Epoch 16, Batch 425, Test Loss: 0.6219237446784973\n",
      "Epoch 16, Batch 426, Test Loss: 0.4709201753139496\n",
      "Epoch 16, Batch 427, Test Loss: 0.5624528527259827\n",
      "Epoch 16, Batch 428, Test Loss: 0.44579383730888367\n",
      "Epoch 16, Batch 429, Test Loss: 0.38809797167778015\n",
      "Epoch 16, Batch 430, Test Loss: 0.5400217771530151\n",
      "Epoch 16, Batch 431, Test Loss: 0.4288000464439392\n",
      "Epoch 16, Batch 432, Test Loss: 0.349883109331131\n",
      "Epoch 16, Batch 433, Test Loss: 0.5233624577522278\n",
      "Epoch 16, Batch 434, Test Loss: 0.31064456701278687\n",
      "Epoch 16, Batch 435, Test Loss: 0.45225459337234497\n",
      "Epoch 16, Batch 436, Test Loss: 0.4059363305568695\n",
      "Epoch 16, Batch 437, Test Loss: 0.3840779662132263\n",
      "Epoch 16, Batch 438, Test Loss: 0.4210897982120514\n",
      "Epoch 16, Batch 439, Test Loss: 0.45248472690582275\n",
      "Epoch 16, Batch 440, Test Loss: 0.5102005004882812\n",
      "Epoch 16, Batch 441, Test Loss: 0.2794124484062195\n",
      "Epoch 16, Batch 442, Test Loss: 0.48767155408859253\n",
      "Epoch 16, Batch 443, Test Loss: 0.44963982701301575\n",
      "Epoch 16, Batch 444, Test Loss: 0.6232506632804871\n",
      "Epoch 16, Batch 445, Test Loss: 0.41663461923599243\n",
      "Epoch 16, Batch 446, Test Loss: 0.4015093445777893\n",
      "Epoch 16, Batch 447, Test Loss: 0.42418143153190613\n",
      "Epoch 16, Batch 448, Test Loss: 0.47490912675857544\n",
      "Epoch 16, Batch 449, Test Loss: 0.4541013538837433\n",
      "Epoch 16, Batch 450, Test Loss: 0.4487638771533966\n",
      "Epoch 16, Batch 451, Test Loss: 0.3250260055065155\n",
      "Epoch 16, Batch 452, Test Loss: 0.5880077481269836\n",
      "Epoch 16, Batch 453, Test Loss: 0.45760416984558105\n",
      "Epoch 16, Batch 454, Test Loss: 0.6557178497314453\n",
      "Epoch 16, Batch 455, Test Loss: 0.37498921155929565\n",
      "Epoch 16, Batch 456, Test Loss: 0.47635504603385925\n",
      "Epoch 16, Batch 457, Test Loss: 0.5200513005256653\n",
      "Epoch 16, Batch 458, Test Loss: 0.6488730907440186\n",
      "Epoch 16, Batch 459, Test Loss: 0.37150970101356506\n",
      "Epoch 16, Batch 460, Test Loss: 0.5624957084655762\n",
      "Epoch 16, Batch 461, Test Loss: 0.627090573310852\n",
      "Epoch 16, Batch 462, Test Loss: 0.4798972010612488\n",
      "Epoch 16, Batch 463, Test Loss: 0.5975610017776489\n",
      "Epoch 16, Batch 464, Test Loss: 0.24333122372627258\n",
      "Epoch 16, Batch 465, Test Loss: 0.3031122386455536\n",
      "Epoch 16, Batch 466, Test Loss: 0.5055550336837769\n",
      "Epoch 16, Batch 467, Test Loss: 0.570900559425354\n",
      "Epoch 16, Batch 468, Test Loss: 0.4477520287036896\n",
      "Epoch 16, Batch 469, Test Loss: 0.4091002345085144\n",
      "Epoch 16, Batch 470, Test Loss: 0.46475765109062195\n",
      "Epoch 16, Batch 471, Test Loss: 0.3709260821342468\n",
      "Epoch 16, Batch 472, Test Loss: 0.3912816643714905\n",
      "Epoch 16, Batch 473, Test Loss: 0.6938559412956238\n",
      "Epoch 16, Batch 474, Test Loss: 0.6089405417442322\n",
      "Epoch 16, Batch 475, Test Loss: 0.3395809531211853\n",
      "Epoch 16, Batch 476, Test Loss: 0.40975961089134216\n",
      "Epoch 16, Batch 477, Test Loss: 0.2933071553707123\n",
      "Epoch 16, Batch 478, Test Loss: 0.3301767408847809\n",
      "Epoch 16, Batch 479, Test Loss: 0.7091919779777527\n",
      "Epoch 16, Batch 480, Test Loss: 0.41425979137420654\n",
      "Epoch 16, Batch 481, Test Loss: 0.47701507806777954\n",
      "Epoch 16, Batch 482, Test Loss: 0.48478060960769653\n",
      "Epoch 16, Batch 483, Test Loss: 0.6353251934051514\n",
      "Epoch 16, Batch 484, Test Loss: 0.33262449502944946\n",
      "Epoch 16, Batch 485, Test Loss: 0.358207106590271\n",
      "Epoch 16, Batch 486, Test Loss: 0.5805689096450806\n",
      "Epoch 16, Batch 487, Test Loss: 0.27961382269859314\n",
      "Epoch 16, Batch 488, Test Loss: 0.5221892595291138\n",
      "Epoch 16, Batch 489, Test Loss: 0.3612331449985504\n",
      "Epoch 16, Batch 490, Test Loss: 0.46634992957115173\n",
      "Epoch 16, Batch 491, Test Loss: 0.3279772698879242\n",
      "Epoch 16, Batch 492, Test Loss: 0.41949111223220825\n",
      "Epoch 16, Batch 493, Test Loss: 0.32618245482444763\n",
      "Epoch 16, Batch 494, Test Loss: 0.3188556432723999\n",
      "Epoch 16, Batch 495, Test Loss: 0.44818246364593506\n",
      "Epoch 16, Batch 496, Test Loss: 0.50459223985672\n",
      "Epoch 16, Batch 497, Test Loss: 0.4274064898490906\n",
      "Epoch 16, Batch 498, Test Loss: 0.2957245707511902\n",
      "Epoch 16, Batch 499, Test Loss: 0.3974163830280304\n",
      "Epoch 16, Batch 500, Test Loss: 0.2367120236158371\n",
      "Epoch 16, Batch 501, Test Loss: 0.38855528831481934\n",
      "Epoch 16, Batch 502, Test Loss: 0.4875975251197815\n",
      "Epoch 16, Batch 503, Test Loss: 0.563742995262146\n",
      "Epoch 16, Batch 504, Test Loss: 0.36723753809928894\n",
      "Epoch 16, Batch 505, Test Loss: 0.5351764559745789\n",
      "Epoch 16, Batch 506, Test Loss: 0.25367477536201477\n",
      "Epoch 16, Batch 507, Test Loss: 0.3772585988044739\n",
      "Epoch 16, Batch 508, Test Loss: 0.3090810179710388\n",
      "Epoch 16, Batch 509, Test Loss: 0.3754565417766571\n",
      "Epoch 16, Batch 510, Test Loss: 0.5394323468208313\n",
      "Epoch 16, Batch 511, Test Loss: 0.36626285314559937\n",
      "Epoch 16, Batch 512, Test Loss: 0.7086900472640991\n",
      "Epoch 16, Batch 513, Test Loss: 0.5485515594482422\n",
      "Epoch 16, Batch 514, Test Loss: 0.39688196778297424\n",
      "Epoch 16, Batch 515, Test Loss: 0.5036812424659729\n",
      "Epoch 16, Batch 516, Test Loss: 0.3935703635215759\n",
      "Epoch 16, Batch 517, Test Loss: 0.4035528302192688\n",
      "Epoch 16, Batch 518, Test Loss: 0.38067615032196045\n",
      "Epoch 16, Batch 519, Test Loss: 0.47325921058654785\n",
      "Epoch 16, Batch 520, Test Loss: 0.2569110691547394\n",
      "Epoch 16, Batch 521, Test Loss: 0.45653849840164185\n",
      "Epoch 16, Batch 522, Test Loss: 0.5995302796363831\n",
      "Epoch 16, Batch 523, Test Loss: 0.4496130049228668\n",
      "Epoch 16, Batch 524, Test Loss: 0.5658029317855835\n",
      "Epoch 16, Batch 525, Test Loss: 0.5514986515045166\n",
      "Epoch 16, Batch 526, Test Loss: 0.4014413058757782\n",
      "Epoch 16, Batch 527, Test Loss: 0.4629439413547516\n",
      "Epoch 16, Batch 528, Test Loss: 0.47132980823516846\n",
      "Epoch 16, Batch 529, Test Loss: 0.5948939323425293\n",
      "Epoch 16, Batch 530, Test Loss: 0.4208764433860779\n",
      "Epoch 16, Batch 531, Test Loss: 0.3788933753967285\n",
      "Epoch 16, Batch 532, Test Loss: 0.4334142506122589\n",
      "Epoch 16, Batch 533, Test Loss: 0.4468569755554199\n",
      "Epoch 16, Batch 534, Test Loss: 0.42721471190452576\n",
      "Epoch 16, Batch 535, Test Loss: 0.5483885407447815\n",
      "Epoch 16, Batch 536, Test Loss: 0.3842473328113556\n",
      "Epoch 16, Batch 537, Test Loss: 0.5064816474914551\n",
      "Epoch 16, Batch 538, Test Loss: 0.41990765929222107\n",
      "Epoch 16, Batch 539, Test Loss: 0.4370606541633606\n",
      "Epoch 16, Batch 540, Test Loss: 0.5496219396591187\n",
      "Epoch 16, Batch 541, Test Loss: 0.5287962555885315\n",
      "Epoch 16, Batch 542, Test Loss: 0.4719071388244629\n",
      "Epoch 16, Batch 543, Test Loss: 0.3529704213142395\n",
      "Epoch 16, Batch 544, Test Loss: 0.5824496746063232\n",
      "Epoch 16, Batch 545, Test Loss: 0.42659977078437805\n",
      "Epoch 16, Batch 546, Test Loss: 0.37823209166526794\n",
      "Epoch 16, Batch 547, Test Loss: 0.35517439246177673\n",
      "Epoch 16, Batch 548, Test Loss: 0.38471201062202454\n",
      "Epoch 16, Batch 549, Test Loss: 0.4352506697177887\n",
      "Epoch 16, Batch 550, Test Loss: 0.46758508682250977\n",
      "Epoch 16, Batch 551, Test Loss: 0.2728867530822754\n",
      "Epoch 16, Batch 552, Test Loss: 0.4826814532279968\n",
      "Epoch 16, Batch 553, Test Loss: 0.5367733240127563\n",
      "Epoch 16, Batch 554, Test Loss: 0.27518829703330994\n",
      "Epoch 16, Batch 555, Test Loss: 0.38199055194854736\n",
      "Epoch 16, Batch 556, Test Loss: 0.3100191354751587\n",
      "Epoch 16, Batch 557, Test Loss: 0.62456876039505\n",
      "Epoch 16, Batch 558, Test Loss: 0.4855652451515198\n",
      "Epoch 16, Batch 559, Test Loss: 0.4708869755268097\n",
      "Epoch 16, Batch 560, Test Loss: 0.43757614493370056\n",
      "Epoch 16, Batch 561, Test Loss: 0.4975831210613251\n",
      "Epoch 16, Batch 562, Test Loss: 0.43208932876586914\n",
      "Epoch 16, Batch 563, Test Loss: 0.4650607109069824\n",
      "Epoch 16, Batch 564, Test Loss: 0.5731135010719299\n",
      "Epoch 16, Batch 565, Test Loss: 0.3051777482032776\n",
      "Epoch 16, Batch 566, Test Loss: 0.4080733060836792\n",
      "Epoch 16, Batch 567, Test Loss: 0.21491873264312744\n",
      "Epoch 16, Batch 568, Test Loss: 0.4501902163028717\n",
      "Epoch 16, Batch 569, Test Loss: 0.4517727494239807\n",
      "Epoch 16, Batch 570, Test Loss: 0.37720999121665955\n",
      "Epoch 16, Batch 571, Test Loss: 0.40215831995010376\n",
      "Epoch 16, Batch 572, Test Loss: 0.4293805956840515\n",
      "Epoch 16, Batch 573, Test Loss: 0.4126893877983093\n",
      "Epoch 16, Batch 574, Test Loss: 0.37617841362953186\n",
      "Epoch 16, Batch 575, Test Loss: 0.5853173732757568\n",
      "Epoch 16, Batch 576, Test Loss: 0.6904541254043579\n",
      "Epoch 16, Batch 577, Test Loss: 0.3477897346019745\n",
      "Epoch 16, Batch 578, Test Loss: 0.289171427488327\n",
      "Epoch 16, Batch 579, Test Loss: 0.3062504231929779\n",
      "Epoch 16, Batch 580, Test Loss: 0.6189613342285156\n",
      "Epoch 16, Batch 581, Test Loss: 0.49180737137794495\n",
      "Epoch 16, Batch 582, Test Loss: 0.3702225089073181\n",
      "Epoch 16, Batch 583, Test Loss: 0.3842713236808777\n",
      "Epoch 16, Batch 584, Test Loss: 0.4107246398925781\n",
      "Epoch 16, Batch 585, Test Loss: 0.371096134185791\n",
      "Epoch 16, Batch 586, Test Loss: 0.4083712100982666\n",
      "Epoch 16, Batch 587, Test Loss: 0.7817618250846863\n",
      "Epoch 16, Batch 588, Test Loss: 0.4948185682296753\n",
      "Epoch 16, Batch 589, Test Loss: 0.5984976887702942\n",
      "Epoch 16, Batch 590, Test Loss: 0.5008034706115723\n",
      "Epoch 16, Batch 591, Test Loss: 0.4151298403739929\n",
      "Epoch 16, Batch 592, Test Loss: 0.3784448206424713\n",
      "Epoch 16, Batch 593, Test Loss: 0.6759005784988403\n",
      "Epoch 16, Batch 594, Test Loss: 0.7563581466674805\n",
      "Epoch 16, Batch 595, Test Loss: 0.2600167691707611\n",
      "Epoch 16, Batch 596, Test Loss: 0.45118600130081177\n",
      "Epoch 16, Batch 597, Test Loss: 0.4243524670600891\n",
      "Epoch 16, Batch 598, Test Loss: 0.45045527815818787\n",
      "Epoch 16, Batch 599, Test Loss: 0.38872161507606506\n",
      "Epoch 16, Batch 600, Test Loss: 0.5625601410865784\n",
      "Epoch 16, Batch 601, Test Loss: 0.5018638372421265\n",
      "Epoch 16, Batch 602, Test Loss: 0.36821818351745605\n",
      "Epoch 16, Batch 603, Test Loss: 0.5166890621185303\n",
      "Epoch 16, Batch 604, Test Loss: 0.31320637464523315\n",
      "Epoch 16, Batch 605, Test Loss: 0.4762696623802185\n",
      "Epoch 16, Batch 606, Test Loss: 0.43642890453338623\n",
      "Epoch 16, Batch 607, Test Loss: 0.3631170094013214\n",
      "Epoch 16, Batch 608, Test Loss: 0.361997127532959\n",
      "Epoch 16, Batch 609, Test Loss: 0.4387570917606354\n",
      "Epoch 16, Batch 610, Test Loss: 0.44268250465393066\n",
      "Epoch 16, Batch 611, Test Loss: 0.7537161111831665\n",
      "Epoch 16, Batch 612, Test Loss: 0.6939643621444702\n",
      "Epoch 16, Batch 613, Test Loss: 0.4457005262374878\n",
      "Epoch 16, Batch 614, Test Loss: 0.5048933029174805\n",
      "Epoch 16, Batch 615, Test Loss: 0.4736587405204773\n",
      "Epoch 16, Batch 616, Test Loss: 0.43869924545288086\n",
      "Epoch 16, Batch 617, Test Loss: 0.38267219066619873\n",
      "Epoch 16, Batch 618, Test Loss: 0.3950228691101074\n",
      "Epoch 16, Batch 619, Test Loss: 0.4632207155227661\n",
      "Epoch 16, Batch 620, Test Loss: 0.3856677711009979\n",
      "Epoch 16, Batch 621, Test Loss: 0.495197057723999\n",
      "Epoch 16, Batch 622, Test Loss: 0.46092185378074646\n",
      "Epoch 16, Batch 623, Test Loss: 0.36204642057418823\n",
      "Epoch 16, Batch 624, Test Loss: 0.7046428322792053\n",
      "Epoch 16, Batch 625, Test Loss: 0.5496799349784851\n",
      "Epoch 16, Batch 626, Test Loss: 0.42207303643226624\n",
      "Epoch 16, Batch 627, Test Loss: 0.44450023770332336\n",
      "Epoch 16, Batch 628, Test Loss: 0.4792243540287018\n",
      "Epoch 16, Batch 629, Test Loss: 0.46150481700897217\n",
      "Epoch 16, Batch 630, Test Loss: 0.44131240248680115\n",
      "Epoch 16, Batch 631, Test Loss: 0.3739910125732422\n",
      "Epoch 16, Batch 632, Test Loss: 0.4293496906757355\n",
      "Epoch 16, Batch 633, Test Loss: 0.3793654143810272\n",
      "Epoch 16, Batch 634, Test Loss: 0.49401700496673584\n",
      "Epoch 16, Batch 635, Test Loss: 0.5048002600669861\n",
      "Epoch 16, Batch 636, Test Loss: 0.513638973236084\n",
      "Epoch 16, Batch 637, Test Loss: 0.5043115019798279\n",
      "Epoch 16, Batch 638, Test Loss: 0.39973771572113037\n",
      "Epoch 16, Batch 639, Test Loss: 0.5742301940917969\n",
      "Epoch 16, Batch 640, Test Loss: 0.4092080295085907\n",
      "Epoch 16, Batch 641, Test Loss: 0.48366227746009827\n",
      "Epoch 16, Batch 642, Test Loss: 0.45325344800949097\n",
      "Epoch 16, Batch 643, Test Loss: 0.47109755873680115\n",
      "Epoch 16, Batch 644, Test Loss: 0.4197619557380676\n",
      "Epoch 16, Batch 645, Test Loss: 0.42755645513534546\n",
      "Epoch 16, Batch 646, Test Loss: 0.5031303763389587\n",
      "Epoch 16, Batch 647, Test Loss: 0.5357968211174011\n",
      "Epoch 16, Batch 648, Test Loss: 0.431637167930603\n",
      "Epoch 16, Batch 649, Test Loss: 0.5832632184028625\n",
      "Epoch 16, Batch 650, Test Loss: 0.3327588438987732\n",
      "Epoch 16, Batch 651, Test Loss: 0.5799596905708313\n",
      "Epoch 16, Batch 652, Test Loss: 0.47522029280662537\n",
      "Epoch 16, Batch 653, Test Loss: 0.3387380838394165\n",
      "Epoch 16, Batch 654, Test Loss: 0.5585912466049194\n",
      "Epoch 16, Batch 655, Test Loss: 0.4623114764690399\n",
      "Epoch 16, Batch 656, Test Loss: 0.4344884157180786\n",
      "Epoch 16, Batch 657, Test Loss: 0.6336774826049805\n",
      "Epoch 16, Batch 658, Test Loss: 0.4885614514350891\n",
      "Epoch 16, Batch 659, Test Loss: 0.44080325961112976\n",
      "Epoch 16, Batch 660, Test Loss: 0.5750272274017334\n",
      "Epoch 16, Batch 661, Test Loss: 0.5545768141746521\n",
      "Epoch 16, Batch 662, Test Loss: 0.3586844801902771\n",
      "Epoch 16, Batch 663, Test Loss: 0.5587631464004517\n",
      "Epoch 16, Batch 664, Test Loss: 0.4270349144935608\n",
      "Epoch 16, Batch 665, Test Loss: 0.5041372776031494\n",
      "Epoch 16, Batch 666, Test Loss: 0.34753701090812683\n",
      "Epoch 16, Batch 667, Test Loss: 0.35156840085983276\n",
      "Epoch 16, Batch 668, Test Loss: 0.4757733643054962\n",
      "Epoch 16, Batch 669, Test Loss: 0.33939141035079956\n",
      "Epoch 16, Batch 670, Test Loss: 0.3717426061630249\n",
      "Epoch 16, Batch 671, Test Loss: 0.35351651906967163\n",
      "Epoch 16, Batch 672, Test Loss: 0.4557880163192749\n",
      "Epoch 16, Batch 673, Test Loss: 0.2971545457839966\n",
      "Epoch 16, Batch 674, Test Loss: 0.4489254355430603\n",
      "Epoch 16, Batch 675, Test Loss: 0.4413554072380066\n",
      "Epoch 16, Batch 676, Test Loss: 0.37845614552497864\n",
      "Epoch 16, Batch 677, Test Loss: 0.49236249923706055\n",
      "Epoch 16, Batch 678, Test Loss: 0.26889094710350037\n",
      "Epoch 16, Batch 679, Test Loss: 0.4300970137119293\n",
      "Epoch 16, Batch 680, Test Loss: 0.6652895212173462\n",
      "Epoch 16, Batch 681, Test Loss: 0.694604754447937\n",
      "Epoch 16, Batch 682, Test Loss: 0.46339040994644165\n",
      "Epoch 16, Batch 683, Test Loss: 0.4045330882072449\n",
      "Epoch 16, Batch 684, Test Loss: 0.375469833612442\n",
      "Epoch 16, Batch 685, Test Loss: 0.4825107157230377\n",
      "Epoch 16, Batch 686, Test Loss: 0.62197345495224\n",
      "Epoch 16, Batch 687, Test Loss: 0.4227183759212494\n",
      "Epoch 16, Batch 688, Test Loss: 0.3684680461883545\n",
      "Epoch 16, Batch 689, Test Loss: 0.3680269122123718\n",
      "Epoch 16, Batch 690, Test Loss: 0.34532856941223145\n",
      "Epoch 16, Batch 691, Test Loss: 0.319011926651001\n",
      "Epoch 16, Batch 692, Test Loss: 0.4332917630672455\n",
      "Epoch 16, Batch 693, Test Loss: 0.4159878194332123\n",
      "Epoch 16, Batch 694, Test Loss: 0.5515576004981995\n",
      "Epoch 16, Batch 695, Test Loss: 0.5607068538665771\n",
      "Epoch 16, Batch 696, Test Loss: 0.5011491775512695\n",
      "Epoch 16, Batch 697, Test Loss: 0.6815215349197388\n",
      "Epoch 16, Batch 698, Test Loss: 0.2692796289920807\n",
      "Epoch 16, Batch 699, Test Loss: 0.6260184049606323\n",
      "Epoch 16, Batch 700, Test Loss: 0.4111317992210388\n",
      "Epoch 16, Batch 701, Test Loss: 0.40484654903411865\n",
      "Epoch 16, Batch 702, Test Loss: 0.3727058470249176\n",
      "Epoch 16, Batch 703, Test Loss: 0.5703600645065308\n",
      "Epoch 16, Batch 704, Test Loss: 0.3800865411758423\n",
      "Epoch 16, Batch 705, Test Loss: 0.5387033224105835\n",
      "Epoch 16, Batch 706, Test Loss: 0.45561468601226807\n",
      "Epoch 16, Batch 707, Test Loss: 0.547966480255127\n",
      "Epoch 16, Batch 708, Test Loss: 0.4425184428691864\n",
      "Epoch 16, Batch 709, Test Loss: 0.5986705422401428\n",
      "Epoch 16, Batch 710, Test Loss: 0.3149056136608124\n",
      "Epoch 16, Batch 711, Test Loss: 0.4666092097759247\n",
      "Epoch 16, Batch 712, Test Loss: 0.564288854598999\n",
      "Epoch 16, Batch 713, Test Loss: 0.44964829087257385\n",
      "Epoch 16, Batch 714, Test Loss: 0.32872891426086426\n",
      "Epoch 16, Batch 715, Test Loss: 0.5259014964103699\n",
      "Epoch 16, Batch 716, Test Loss: 0.35059210658073425\n",
      "Epoch 16, Batch 717, Test Loss: 0.31410300731658936\n",
      "Epoch 16, Batch 718, Test Loss: 0.5900190472602844\n",
      "Epoch 16, Batch 719, Test Loss: 0.54176926612854\n",
      "Epoch 16, Batch 720, Test Loss: 0.4402008652687073\n",
      "Epoch 16, Batch 721, Test Loss: 0.5491671562194824\n",
      "Epoch 16, Batch 722, Test Loss: 0.3956924080848694\n",
      "Epoch 16, Batch 723, Test Loss: 0.5044910907745361\n",
      "Epoch 16, Batch 724, Test Loss: 0.2038540095090866\n",
      "Epoch 16, Batch 725, Test Loss: 0.32830315828323364\n",
      "Epoch 16, Batch 726, Test Loss: 0.24470478296279907\n",
      "Epoch 16, Batch 727, Test Loss: 0.3905612528324127\n",
      "Epoch 16, Batch 728, Test Loss: 0.5321760773658752\n",
      "Epoch 16, Batch 729, Test Loss: 0.3880186080932617\n",
      "Epoch 16, Batch 730, Test Loss: 0.4359222948551178\n",
      "Epoch 16, Batch 731, Test Loss: 0.37626808881759644\n",
      "Epoch 16, Batch 732, Test Loss: 0.7957285046577454\n",
      "Epoch 16, Batch 733, Test Loss: 0.4224916994571686\n",
      "Epoch 16, Batch 734, Test Loss: 0.4296308755874634\n",
      "Epoch 16, Batch 735, Test Loss: 0.3080179989337921\n",
      "Epoch 16, Batch 736, Test Loss: 0.4868425726890564\n",
      "Epoch 16, Batch 737, Test Loss: 0.37574994564056396\n",
      "Epoch 16, Batch 738, Test Loss: 0.35455867648124695\n",
      "Epoch 16, Batch 739, Test Loss: 0.5760375261306763\n",
      "Epoch 16, Batch 740, Test Loss: 0.31893470883369446\n",
      "Epoch 16, Batch 741, Test Loss: 0.3723081052303314\n",
      "Epoch 16, Batch 742, Test Loss: 0.42610061168670654\n",
      "Epoch 16, Batch 743, Test Loss: 0.5802114009857178\n",
      "Epoch 16, Batch 744, Test Loss: 0.43192625045776367\n",
      "Epoch 16, Batch 745, Test Loss: 0.43791407346725464\n",
      "Epoch 16, Batch 746, Test Loss: 0.5003376603126526\n",
      "Epoch 16, Batch 747, Test Loss: 0.3910277485847473\n",
      "Epoch 16, Batch 748, Test Loss: 0.44733288884162903\n",
      "Epoch 16, Batch 749, Test Loss: 0.3852476179599762\n",
      "Epoch 16, Batch 750, Test Loss: 0.48162564635276794\n",
      "Epoch 16, Batch 751, Test Loss: 0.29265403747558594\n",
      "Epoch 16, Batch 752, Test Loss: 0.47125697135925293\n",
      "Epoch 16, Batch 753, Test Loss: 0.3521680235862732\n",
      "Epoch 16, Batch 754, Test Loss: 0.49803611636161804\n",
      "Epoch 16, Batch 755, Test Loss: 0.4577789008617401\n",
      "Epoch 16, Batch 756, Test Loss: 0.47019708156585693\n",
      "Epoch 16, Batch 757, Test Loss: 0.421814888715744\n",
      "Epoch 16, Batch 758, Test Loss: 0.4592489004135132\n",
      "Epoch 16, Batch 759, Test Loss: 0.3902397155761719\n",
      "Epoch 16, Batch 760, Test Loss: 0.5073437094688416\n",
      "Epoch 16, Batch 761, Test Loss: 0.5983178019523621\n",
      "Epoch 16, Batch 762, Test Loss: 0.5385928153991699\n",
      "Epoch 16, Batch 763, Test Loss: 0.49300524592399597\n",
      "Epoch 16, Batch 764, Test Loss: 0.55111163854599\n",
      "Epoch 16, Batch 765, Test Loss: 0.3138706684112549\n",
      "Epoch 16, Batch 766, Test Loss: 0.49999120831489563\n",
      "Epoch 16, Batch 767, Test Loss: 0.3798469603061676\n",
      "Epoch 16, Batch 768, Test Loss: 0.5721933245658875\n",
      "Epoch 16, Batch 769, Test Loss: 0.47521764039993286\n",
      "Epoch 16, Batch 770, Test Loss: 0.6707597970962524\n",
      "Epoch 16, Batch 771, Test Loss: 0.27616026997566223\n",
      "Epoch 16, Batch 772, Test Loss: 0.3761254549026489\n",
      "Epoch 16, Batch 773, Test Loss: 0.5668034553527832\n",
      "Epoch 16, Batch 774, Test Loss: 0.3156254291534424\n",
      "Epoch 16, Batch 775, Test Loss: 0.3805786371231079\n",
      "Epoch 16, Batch 776, Test Loss: 0.2878206968307495\n",
      "Epoch 16, Batch 777, Test Loss: 0.3338809311389923\n",
      "Epoch 16, Batch 778, Test Loss: 0.2212303876876831\n",
      "Epoch 16, Batch 779, Test Loss: 0.35153141617774963\n",
      "Epoch 16, Batch 780, Test Loss: 0.7880812287330627\n",
      "Epoch 16, Batch 781, Test Loss: 0.43257659673690796\n",
      "Epoch 16, Batch 782, Test Loss: 0.3535645604133606\n",
      "Epoch 16, Batch 783, Test Loss: 0.3182932436466217\n",
      "Epoch 16, Batch 784, Test Loss: 0.3969884514808655\n",
      "Epoch 16, Batch 785, Test Loss: 0.3007831573486328\n",
      "Epoch 16, Batch 786, Test Loss: 0.596970796585083\n",
      "Epoch 16, Batch 787, Test Loss: 0.5063644647598267\n",
      "Epoch 16, Batch 788, Test Loss: 0.34194517135620117\n",
      "Epoch 16, Batch 789, Test Loss: 0.37635722756385803\n",
      "Epoch 16, Batch 790, Test Loss: 0.282551646232605\n",
      "Epoch 16, Batch 791, Test Loss: 0.5025825500488281\n",
      "Epoch 16, Batch 792, Test Loss: 0.5106596946716309\n",
      "Epoch 16, Batch 793, Test Loss: 0.582356333732605\n",
      "Epoch 16, Batch 794, Test Loss: 0.5320653915405273\n",
      "Epoch 16, Batch 795, Test Loss: 0.6845657229423523\n",
      "Epoch 16, Batch 796, Test Loss: 0.34264788031578064\n",
      "Epoch 16, Batch 797, Test Loss: 0.6238588690757751\n",
      "Epoch 16, Batch 798, Test Loss: 0.4169314503669739\n",
      "Epoch 16, Batch 799, Test Loss: 0.43781691789627075\n",
      "Epoch 16, Batch 800, Test Loss: 0.5337473154067993\n",
      "Epoch 16, Batch 801, Test Loss: 0.7632710337638855\n",
      "Epoch 16, Batch 802, Test Loss: 0.5334004163742065\n",
      "Epoch 16, Batch 803, Test Loss: 0.4808369576931\n",
      "Epoch 16, Batch 804, Test Loss: 0.5720206499099731\n",
      "Epoch 16, Batch 805, Test Loss: 0.42627400159835815\n",
      "Epoch 16, Batch 806, Test Loss: 0.6267983913421631\n",
      "Epoch 16, Batch 807, Test Loss: 0.3775828182697296\n",
      "Epoch 16, Batch 808, Test Loss: 0.6523668766021729\n",
      "Epoch 16, Batch 809, Test Loss: 0.3226401209831238\n",
      "Epoch 16, Batch 810, Test Loss: 0.360993891954422\n",
      "Epoch 16, Batch 811, Test Loss: 0.5704432725906372\n",
      "Epoch 16, Batch 812, Test Loss: 0.6395658850669861\n",
      "Epoch 16, Batch 813, Test Loss: 0.4786146283149719\n",
      "Epoch 16, Batch 814, Test Loss: 0.4337465763092041\n",
      "Epoch 16, Batch 815, Test Loss: 0.5000509023666382\n",
      "Epoch 16, Batch 816, Test Loss: 0.47489872574806213\n",
      "Epoch 16, Batch 817, Test Loss: 0.5391882658004761\n",
      "Epoch 16, Batch 818, Test Loss: 0.5446125864982605\n",
      "Epoch 16, Batch 819, Test Loss: 0.2983356714248657\n",
      "Epoch 16, Batch 820, Test Loss: 0.44451984763145447\n",
      "Epoch 16, Batch 821, Test Loss: 0.5098907947540283\n",
      "Epoch 16, Batch 822, Test Loss: 0.680141806602478\n",
      "Epoch 16, Batch 823, Test Loss: 0.41427284479141235\n",
      "Epoch 16, Batch 824, Test Loss: 0.2727760970592499\n",
      "Epoch 16, Batch 825, Test Loss: 0.3314584195613861\n",
      "Epoch 16, Batch 826, Test Loss: 0.49377110600471497\n",
      "Epoch 16, Batch 827, Test Loss: 0.3364906907081604\n",
      "Epoch 16, Batch 828, Test Loss: 0.4822298288345337\n",
      "Epoch 16, Batch 829, Test Loss: 0.44571036100387573\n",
      "Epoch 16, Batch 830, Test Loss: 0.5403278470039368\n",
      "Epoch 16, Batch 831, Test Loss: 0.4547278583049774\n",
      "Epoch 16, Batch 832, Test Loss: 0.2909022271633148\n",
      "Epoch 16, Batch 833, Test Loss: 0.3051561713218689\n",
      "Epoch 16, Batch 834, Test Loss: 0.3906133472919464\n",
      "Epoch 16, Batch 835, Test Loss: 0.25945064425468445\n",
      "Epoch 16, Batch 836, Test Loss: 0.516914963722229\n",
      "Epoch 16, Batch 837, Test Loss: 0.6156441569328308\n",
      "Epoch 16, Batch 838, Test Loss: 0.5425456166267395\n",
      "Epoch 16, Batch 839, Test Loss: 0.4963538646697998\n",
      "Epoch 16, Batch 840, Test Loss: 0.34109967947006226\n",
      "Epoch 16, Batch 841, Test Loss: 0.6272101402282715\n",
      "Epoch 16, Batch 842, Test Loss: 0.4950293302536011\n",
      "Epoch 16, Batch 843, Test Loss: 0.44004154205322266\n",
      "Epoch 16, Batch 844, Test Loss: 0.5383738875389099\n",
      "Epoch 16, Batch 845, Test Loss: 0.5225173234939575\n",
      "Epoch 16, Batch 846, Test Loss: 0.6724574565887451\n",
      "Epoch 16, Batch 847, Test Loss: 0.4848182201385498\n",
      "Epoch 16, Batch 848, Test Loss: 0.45832541584968567\n",
      "Epoch 16, Batch 849, Test Loss: 0.3564557433128357\n",
      "Epoch 16, Batch 850, Test Loss: 0.3756052255630493\n",
      "Epoch 16, Batch 851, Test Loss: 0.4689623713493347\n",
      "Epoch 16, Batch 852, Test Loss: 0.44706934690475464\n",
      "Epoch 16, Batch 853, Test Loss: 0.3770248591899872\n",
      "Epoch 16, Batch 854, Test Loss: 0.4959138035774231\n",
      "Epoch 16, Batch 855, Test Loss: 0.42699140310287476\n",
      "Epoch 16, Batch 856, Test Loss: 0.4826638996601105\n",
      "Epoch 16, Batch 857, Test Loss: 0.5741782188415527\n",
      "Epoch 16, Batch 858, Test Loss: 0.31930720806121826\n",
      "Epoch 16, Batch 859, Test Loss: 0.4228704571723938\n",
      "Epoch 16, Batch 860, Test Loss: 0.5229259133338928\n",
      "Epoch 16, Batch 861, Test Loss: 0.4582495093345642\n",
      "Epoch 16, Batch 862, Test Loss: 0.5632818937301636\n",
      "Epoch 16, Batch 863, Test Loss: 0.5993826985359192\n",
      "Epoch 16, Batch 864, Test Loss: 0.3042939603328705\n",
      "Epoch 16, Batch 865, Test Loss: 0.29390427470207214\n",
      "Epoch 16, Batch 866, Test Loss: 0.41416770219802856\n",
      "Epoch 16, Batch 867, Test Loss: 0.4909072518348694\n",
      "Epoch 16, Batch 868, Test Loss: 0.6393631100654602\n",
      "Epoch 16, Batch 869, Test Loss: 0.39759767055511475\n",
      "Epoch 16, Batch 870, Test Loss: 0.5139673352241516\n",
      "Epoch 16, Batch 871, Test Loss: 0.5412912964820862\n",
      "Epoch 16, Batch 872, Test Loss: 0.3430241048336029\n",
      "Epoch 16, Batch 873, Test Loss: 0.4164179265499115\n",
      "Epoch 16, Batch 874, Test Loss: 0.5017765760421753\n",
      "Epoch 16, Batch 875, Test Loss: 0.45236650109291077\n",
      "Epoch 16, Batch 876, Test Loss: 0.6169589757919312\n",
      "Epoch 16, Batch 877, Test Loss: 0.4624747037887573\n",
      "Epoch 16, Batch 878, Test Loss: 0.32277700304985046\n",
      "Epoch 16, Batch 879, Test Loss: 0.5942208170890808\n",
      "Epoch 16, Batch 880, Test Loss: 0.33742451667785645\n",
      "Epoch 16, Batch 881, Test Loss: 0.3940756022930145\n",
      "Epoch 16, Batch 882, Test Loss: 0.5945716500282288\n",
      "Epoch 16, Batch 883, Test Loss: 0.35231339931488037\n",
      "Epoch 16, Batch 884, Test Loss: 0.44819167256355286\n",
      "Epoch 16, Batch 885, Test Loss: 0.26019564270973206\n",
      "Epoch 16, Batch 886, Test Loss: 0.40836071968078613\n",
      "Epoch 16, Batch 887, Test Loss: 0.6498152017593384\n",
      "Epoch 16, Batch 888, Test Loss: 0.5702038407325745\n",
      "Epoch 16, Batch 889, Test Loss: 0.5440480709075928\n",
      "Epoch 16, Batch 890, Test Loss: 0.3940311372280121\n",
      "Epoch 16, Batch 891, Test Loss: 0.4705919325351715\n",
      "Epoch 16, Batch 892, Test Loss: 0.5781445503234863\n",
      "Epoch 16, Batch 893, Test Loss: 0.4116262197494507\n",
      "Epoch 16, Batch 894, Test Loss: 0.3948860168457031\n",
      "Epoch 16, Batch 895, Test Loss: 0.6485227346420288\n",
      "Epoch 16, Batch 896, Test Loss: 0.41010648012161255\n",
      "Epoch 16, Batch 897, Test Loss: 0.44786298274993896\n",
      "Epoch 16, Batch 898, Test Loss: 0.42596524953842163\n",
      "Epoch 16, Batch 899, Test Loss: 0.25630152225494385\n",
      "Epoch 16, Batch 900, Test Loss: 0.33505144715309143\n",
      "Epoch 16, Batch 901, Test Loss: 0.474115252494812\n",
      "Epoch 16, Batch 902, Test Loss: 0.30572131276130676\n",
      "Epoch 16, Batch 903, Test Loss: 0.38333773612976074\n",
      "Epoch 16, Batch 904, Test Loss: 0.24720880389213562\n",
      "Epoch 16, Batch 905, Test Loss: 0.32946041226387024\n",
      "Epoch 16, Batch 906, Test Loss: 0.3179709315299988\n",
      "Epoch 16, Batch 907, Test Loss: 0.448837012052536\n",
      "Epoch 16, Batch 908, Test Loss: 0.47551587224006653\n",
      "Epoch 16, Batch 909, Test Loss: 0.5051687359809875\n",
      "Epoch 16, Batch 910, Test Loss: 0.5962433218955994\n",
      "Epoch 16, Batch 911, Test Loss: 0.6059637069702148\n",
      "Epoch 16, Batch 912, Test Loss: 0.3270033895969391\n",
      "Epoch 16, Batch 913, Test Loss: 0.3341049551963806\n",
      "Epoch 16, Batch 914, Test Loss: 0.45666128396987915\n",
      "Epoch 16, Batch 915, Test Loss: 0.3560083508491516\n",
      "Epoch 16, Batch 916, Test Loss: 0.42397743463516235\n",
      "Epoch 16, Batch 917, Test Loss: 0.4590701162815094\n",
      "Epoch 16, Batch 918, Test Loss: 0.42160749435424805\n",
      "Epoch 16, Batch 919, Test Loss: 0.4785670042037964\n",
      "Epoch 16, Batch 920, Test Loss: 0.30784595012664795\n",
      "Epoch 16, Batch 921, Test Loss: 0.5586143732070923\n",
      "Epoch 16, Batch 922, Test Loss: 0.4007750451564789\n",
      "Epoch 16, Batch 923, Test Loss: 0.4160486161708832\n",
      "Epoch 16, Batch 924, Test Loss: 0.3594527840614319\n",
      "Epoch 16, Batch 925, Test Loss: 0.35508963465690613\n",
      "Epoch 16, Batch 926, Test Loss: 0.3221760392189026\n",
      "Epoch 16, Batch 927, Test Loss: 0.35504409670829773\n",
      "Epoch 16, Batch 928, Test Loss: 0.4241982698440552\n",
      "Epoch 16, Batch 929, Test Loss: 0.4173136055469513\n",
      "Epoch 16, Batch 930, Test Loss: 0.2823452651500702\n",
      "Epoch 16, Batch 931, Test Loss: 0.3817615509033203\n",
      "Epoch 16, Batch 932, Test Loss: 0.36365270614624023\n",
      "Epoch 16, Batch 933, Test Loss: 0.5270283222198486\n",
      "Epoch 16, Batch 934, Test Loss: 0.4313433766365051\n",
      "Epoch 16, Batch 935, Test Loss: 0.43765687942504883\n",
      "Epoch 16, Batch 936, Test Loss: 0.2694298028945923\n",
      "Epoch 16, Batch 937, Test Loss: 0.3743478059768677\n",
      "Epoch 16, Batch 938, Test Loss: 0.5728999972343445\n",
      "Accuracy of Test set: 0.8432666666666667\n",
      "Epoch 17, Batch 1, Loss: 0.5524543523788452\n",
      "Epoch 17, Batch 2, Loss: 0.5336386561393738\n",
      "Epoch 17, Batch 3, Loss: 0.32818490266799927\n",
      "Epoch 17, Batch 4, Loss: 0.4917260706424713\n",
      "Epoch 17, Batch 5, Loss: 0.4387834072113037\n",
      "Epoch 17, Batch 6, Loss: 0.5931587815284729\n",
      "Epoch 17, Batch 7, Loss: 0.5115915536880493\n",
      "Epoch 17, Batch 8, Loss: 0.3442395031452179\n",
      "Epoch 17, Batch 9, Loss: 0.3883669078350067\n",
      "Epoch 17, Batch 10, Loss: 0.463810533285141\n",
      "Epoch 17, Batch 11, Loss: 0.41785144805908203\n",
      "Epoch 17, Batch 12, Loss: 0.3998485803604126\n",
      "Epoch 17, Batch 13, Loss: 0.5454624891281128\n",
      "Epoch 17, Batch 14, Loss: 0.4609145224094391\n",
      "Epoch 17, Batch 15, Loss: 0.39285343885421753\n",
      "Epoch 17, Batch 16, Loss: 0.6186884045600891\n",
      "Epoch 17, Batch 17, Loss: 0.4991363286972046\n",
      "Epoch 17, Batch 18, Loss: 0.4535752534866333\n",
      "Epoch 17, Batch 19, Loss: 0.4708154499530792\n",
      "Epoch 17, Batch 20, Loss: 0.29776665568351746\n",
      "Epoch 17, Batch 21, Loss: 0.4627581834793091\n",
      "Epoch 17, Batch 22, Loss: 0.3371461033821106\n",
      "Epoch 17, Batch 23, Loss: 0.3070554733276367\n",
      "Epoch 17, Batch 24, Loss: 0.3121269941329956\n",
      "Epoch 17, Batch 25, Loss: 0.323067307472229\n",
      "Epoch 17, Batch 26, Loss: 0.5070235133171082\n",
      "Epoch 17, Batch 27, Loss: 0.7039470672607422\n",
      "Epoch 17, Batch 28, Loss: 0.34777921438217163\n",
      "Epoch 17, Batch 29, Loss: 0.4990535080432892\n",
      "Epoch 17, Batch 30, Loss: 0.441760390996933\n",
      "Epoch 17, Batch 31, Loss: 0.5302120447158813\n",
      "Epoch 17, Batch 32, Loss: 0.39663267135620117\n",
      "Epoch 17, Batch 33, Loss: 0.2807147204875946\n",
      "Epoch 17, Batch 34, Loss: 0.32993265986442566\n",
      "Epoch 17, Batch 35, Loss: 0.3951572775840759\n",
      "Epoch 17, Batch 36, Loss: 0.4035792052745819\n",
      "Epoch 17, Batch 37, Loss: 0.46417325735092163\n",
      "Epoch 17, Batch 38, Loss: 0.5661855936050415\n",
      "Epoch 17, Batch 39, Loss: 0.3695773780345917\n",
      "Epoch 17, Batch 40, Loss: 0.39909154176712036\n",
      "Epoch 17, Batch 41, Loss: 0.3752560019493103\n",
      "Epoch 17, Batch 42, Loss: 0.5882850885391235\n",
      "Epoch 17, Batch 43, Loss: 0.5889967679977417\n",
      "Epoch 17, Batch 44, Loss: 0.31767770648002625\n",
      "Epoch 17, Batch 45, Loss: 0.5706021785736084\n",
      "Epoch 17, Batch 46, Loss: 0.46770042181015015\n",
      "Epoch 17, Batch 47, Loss: 0.4816978871822357\n",
      "Epoch 17, Batch 48, Loss: 0.44921982288360596\n",
      "Epoch 17, Batch 49, Loss: 0.4198411703109741\n",
      "Epoch 17, Batch 50, Loss: 0.5427049994468689\n",
      "Epoch 17, Batch 51, Loss: 0.38872525095939636\n",
      "Epoch 17, Batch 52, Loss: 0.34739741683006287\n",
      "Epoch 17, Batch 53, Loss: 0.6046925783157349\n",
      "Epoch 17, Batch 54, Loss: 0.3057858943939209\n",
      "Epoch 17, Batch 55, Loss: 0.33639490604400635\n",
      "Epoch 17, Batch 56, Loss: 0.4451045095920563\n",
      "Epoch 17, Batch 57, Loss: 0.2787801921367645\n",
      "Epoch 17, Batch 58, Loss: 0.37854132056236267\n",
      "Epoch 17, Batch 59, Loss: 0.5611574649810791\n",
      "Epoch 17, Batch 60, Loss: 0.4008275866508484\n",
      "Epoch 17, Batch 61, Loss: 0.4266936182975769\n",
      "Epoch 17, Batch 62, Loss: 0.3721711039543152\n",
      "Epoch 17, Batch 63, Loss: 0.47386860847473145\n",
      "Epoch 17, Batch 64, Loss: 0.3700098991394043\n",
      "Epoch 17, Batch 65, Loss: 0.4691123366355896\n",
      "Epoch 17, Batch 66, Loss: 0.5035672187805176\n",
      "Epoch 17, Batch 67, Loss: 0.46118324995040894\n",
      "Epoch 17, Batch 68, Loss: 0.4218752682209015\n",
      "Epoch 17, Batch 69, Loss: 0.27796027064323425\n",
      "Epoch 17, Batch 70, Loss: 0.6349935531616211\n",
      "Epoch 17, Batch 71, Loss: 0.5548316836357117\n",
      "Epoch 17, Batch 72, Loss: 0.5696930289268494\n",
      "Epoch 17, Batch 73, Loss: 0.4340854287147522\n",
      "Epoch 17, Batch 74, Loss: 0.5607916116714478\n",
      "Epoch 17, Batch 75, Loss: 0.3646122217178345\n",
      "Epoch 17, Batch 76, Loss: 0.468833863735199\n",
      "Epoch 17, Batch 77, Loss: 0.35788869857788086\n",
      "Epoch 17, Batch 78, Loss: 0.3443405330181122\n",
      "Epoch 17, Batch 79, Loss: 0.5577159523963928\n",
      "Epoch 17, Batch 80, Loss: 0.37356266379356384\n",
      "Epoch 17, Batch 81, Loss: 0.5922543406486511\n",
      "Epoch 17, Batch 82, Loss: 0.5262572169303894\n",
      "Epoch 17, Batch 83, Loss: 0.5459089875221252\n",
      "Epoch 17, Batch 84, Loss: 0.44475919008255005\n",
      "Epoch 17, Batch 85, Loss: 0.41049519181251526\n",
      "Epoch 17, Batch 86, Loss: 0.266793429851532\n",
      "Epoch 17, Batch 87, Loss: 0.5231772065162659\n",
      "Epoch 17, Batch 88, Loss: 0.6567996144294739\n",
      "Epoch 17, Batch 89, Loss: 0.34807687997817993\n",
      "Epoch 17, Batch 90, Loss: 0.4536227583885193\n",
      "Epoch 17, Batch 91, Loss: 0.4449511766433716\n",
      "Epoch 17, Batch 92, Loss: 0.7896795868873596\n",
      "Epoch 17, Batch 93, Loss: 0.34706583619117737\n",
      "Epoch 17, Batch 94, Loss: 0.456669420003891\n",
      "Epoch 17, Batch 95, Loss: 0.27305716276168823\n",
      "Epoch 17, Batch 96, Loss: 0.5819331407546997\n",
      "Epoch 17, Batch 97, Loss: 0.44287630915641785\n",
      "Epoch 17, Batch 98, Loss: 0.30616357922554016\n",
      "Epoch 17, Batch 99, Loss: 0.32671093940734863\n",
      "Epoch 17, Batch 100, Loss: 0.2913006544113159\n",
      "Epoch 17, Batch 101, Loss: 0.4320473372936249\n",
      "Epoch 17, Batch 102, Loss: 0.5740997195243835\n",
      "Epoch 17, Batch 103, Loss: 0.5405341982841492\n",
      "Epoch 17, Batch 104, Loss: 0.46662279963493347\n",
      "Epoch 17, Batch 105, Loss: 0.32128843665122986\n",
      "Epoch 17, Batch 106, Loss: 0.308541476726532\n",
      "Epoch 17, Batch 107, Loss: 0.41258057951927185\n",
      "Epoch 17, Batch 108, Loss: 0.41956090927124023\n",
      "Epoch 17, Batch 109, Loss: 0.26405924558639526\n",
      "Epoch 17, Batch 110, Loss: 0.4474603533744812\n",
      "Epoch 17, Batch 111, Loss: 0.30736902356147766\n",
      "Epoch 17, Batch 112, Loss: 0.47185105085372925\n",
      "Epoch 17, Batch 113, Loss: 0.24780753254890442\n",
      "Epoch 17, Batch 114, Loss: 0.5211414098739624\n",
      "Epoch 17, Batch 115, Loss: 0.35484346747398376\n",
      "Epoch 17, Batch 116, Loss: 0.5793372988700867\n",
      "Epoch 17, Batch 117, Loss: 0.5166479349136353\n",
      "Epoch 17, Batch 118, Loss: 0.2786145806312561\n",
      "Epoch 17, Batch 119, Loss: 0.3900372087955475\n",
      "Epoch 17, Batch 120, Loss: 0.41321903467178345\n",
      "Epoch 17, Batch 121, Loss: 0.4300222396850586\n",
      "Epoch 17, Batch 122, Loss: 0.39796531200408936\n",
      "Epoch 17, Batch 123, Loss: 0.4134393334388733\n",
      "Epoch 17, Batch 124, Loss: 0.3796501159667969\n",
      "Epoch 17, Batch 125, Loss: 0.5270757675170898\n",
      "Epoch 17, Batch 126, Loss: 0.3539670407772064\n",
      "Epoch 17, Batch 127, Loss: 0.5490186810493469\n",
      "Epoch 17, Batch 128, Loss: 0.46430230140686035\n",
      "Epoch 17, Batch 129, Loss: 0.31581810116767883\n",
      "Epoch 17, Batch 130, Loss: 0.39803019165992737\n",
      "Epoch 17, Batch 131, Loss: 0.2840699255466461\n",
      "Epoch 17, Batch 132, Loss: 0.5159444808959961\n",
      "Epoch 17, Batch 133, Loss: 0.4930541515350342\n",
      "Epoch 17, Batch 134, Loss: 0.3678438663482666\n",
      "Epoch 17, Batch 135, Loss: 0.4116033911705017\n",
      "Epoch 17, Batch 136, Loss: 0.3508492708206177\n",
      "Epoch 17, Batch 137, Loss: 0.7185256481170654\n",
      "Epoch 17, Batch 138, Loss: 0.4579114317893982\n",
      "Epoch 17, Batch 139, Loss: 0.6751922965049744\n",
      "Epoch 17, Batch 140, Loss: 0.5437771081924438\n",
      "Epoch 17, Batch 141, Loss: 0.46988049149513245\n",
      "Epoch 17, Batch 142, Loss: 0.5283002257347107\n",
      "Epoch 17, Batch 143, Loss: 0.3516708016395569\n",
      "Epoch 17, Batch 144, Loss: 0.5905451774597168\n",
      "Epoch 17, Batch 145, Loss: 0.2642856240272522\n",
      "Epoch 17, Batch 146, Loss: 0.3335065245628357\n",
      "Epoch 17, Batch 147, Loss: 0.48958921432495117\n",
      "Epoch 17, Batch 148, Loss: 0.4739469885826111\n",
      "Epoch 17, Batch 149, Loss: 0.3369120955467224\n",
      "Epoch 17, Batch 150, Loss: 0.5047653913497925\n",
      "Epoch 17, Batch 151, Loss: 0.25205162167549133\n",
      "Epoch 17, Batch 152, Loss: 0.36054089665412903\n",
      "Epoch 17, Batch 153, Loss: 0.3967798352241516\n",
      "Epoch 17, Batch 154, Loss: 0.41464027762413025\n",
      "Epoch 17, Batch 155, Loss: 0.325215220451355\n",
      "Epoch 17, Batch 156, Loss: 0.730659008026123\n",
      "Epoch 17, Batch 157, Loss: 0.4972870945930481\n",
      "Epoch 17, Batch 158, Loss: 0.48597395420074463\n",
      "Epoch 17, Batch 159, Loss: 0.4102049469947815\n",
      "Epoch 17, Batch 160, Loss: 0.5170735120773315\n",
      "Epoch 17, Batch 161, Loss: 0.5121611952781677\n",
      "Epoch 17, Batch 162, Loss: 0.5176148414611816\n",
      "Epoch 17, Batch 163, Loss: 0.456047385931015\n",
      "Epoch 17, Batch 164, Loss: 0.47383010387420654\n",
      "Epoch 17, Batch 165, Loss: 0.3782278299331665\n",
      "Epoch 17, Batch 166, Loss: 0.526336669921875\n",
      "Epoch 17, Batch 167, Loss: 0.27268972992897034\n",
      "Epoch 17, Batch 168, Loss: 0.4720778465270996\n",
      "Epoch 17, Batch 169, Loss: 0.43024635314941406\n",
      "Epoch 17, Batch 170, Loss: 0.3903564512729645\n",
      "Epoch 17, Batch 171, Loss: 0.43197908997535706\n",
      "Epoch 17, Batch 172, Loss: 0.3916997015476227\n",
      "Epoch 17, Batch 173, Loss: 0.6735715866088867\n",
      "Epoch 17, Batch 174, Loss: 0.5061874985694885\n",
      "Epoch 17, Batch 175, Loss: 0.5058223009109497\n",
      "Epoch 17, Batch 176, Loss: 0.3479481339454651\n",
      "Epoch 17, Batch 177, Loss: 0.5076584815979004\n",
      "Epoch 17, Batch 178, Loss: 0.5802596807479858\n",
      "Epoch 17, Batch 179, Loss: 0.6604475378990173\n",
      "Epoch 17, Batch 180, Loss: 0.42478805780410767\n",
      "Epoch 17, Batch 181, Loss: 0.43756213784217834\n",
      "Epoch 17, Batch 182, Loss: 0.5088968276977539\n",
      "Epoch 17, Batch 183, Loss: 0.32168304920196533\n",
      "Epoch 17, Batch 184, Loss: 0.43906381726264954\n",
      "Epoch 17, Batch 185, Loss: 0.6359043717384338\n",
      "Epoch 17, Batch 186, Loss: 0.45735037326812744\n",
      "Epoch 17, Batch 187, Loss: 0.9046254754066467\n",
      "Epoch 17, Batch 188, Loss: 0.5422324538230896\n",
      "Epoch 17, Batch 189, Loss: 0.5085780620574951\n",
      "Epoch 17, Batch 190, Loss: 0.45271366834640503\n",
      "Epoch 17, Batch 191, Loss: 0.6975235342979431\n",
      "Epoch 17, Batch 192, Loss: 0.6207254528999329\n",
      "Epoch 17, Batch 193, Loss: 0.28225553035736084\n",
      "Epoch 17, Batch 194, Loss: 0.374001681804657\n",
      "Epoch 17, Batch 195, Loss: 0.3384611904621124\n",
      "Epoch 17, Batch 196, Loss: 0.4578745365142822\n",
      "Epoch 17, Batch 197, Loss: 0.49410781264305115\n",
      "Epoch 17, Batch 198, Loss: 0.4718215763568878\n",
      "Epoch 17, Batch 199, Loss: 0.6781215071678162\n",
      "Epoch 17, Batch 200, Loss: 0.36829516291618347\n",
      "Epoch 17, Batch 201, Loss: 0.4360119700431824\n",
      "Epoch 17, Batch 202, Loss: 0.5068529844284058\n",
      "Epoch 17, Batch 203, Loss: 0.4414028525352478\n",
      "Epoch 17, Batch 204, Loss: 0.2500985562801361\n",
      "Epoch 17, Batch 205, Loss: 0.665557324886322\n",
      "Epoch 17, Batch 206, Loss: 0.32171574234962463\n",
      "Epoch 17, Batch 207, Loss: 0.41524383425712585\n",
      "Epoch 17, Batch 208, Loss: 0.452714204788208\n",
      "Epoch 17, Batch 209, Loss: 0.40199142694473267\n",
      "Epoch 17, Batch 210, Loss: 0.5426611304283142\n",
      "Epoch 17, Batch 211, Loss: 0.26571112871170044\n",
      "Epoch 17, Batch 212, Loss: 0.49032115936279297\n",
      "Epoch 17, Batch 213, Loss: 0.34163427352905273\n",
      "Epoch 17, Batch 214, Loss: 0.35272011160850525\n",
      "Epoch 17, Batch 215, Loss: 0.2892480194568634\n",
      "Epoch 17, Batch 216, Loss: 0.6768501996994019\n",
      "Epoch 17, Batch 217, Loss: 0.5504822134971619\n",
      "Epoch 17, Batch 218, Loss: 0.48615705966949463\n",
      "Epoch 17, Batch 219, Loss: 0.538425862789154\n",
      "Epoch 17, Batch 220, Loss: 0.4767110049724579\n",
      "Epoch 17, Batch 221, Loss: 0.33996105194091797\n",
      "Epoch 17, Batch 222, Loss: 0.3333582580089569\n",
      "Epoch 17, Batch 223, Loss: 0.539446234703064\n",
      "Epoch 17, Batch 224, Loss: 0.44323498010635376\n",
      "Epoch 17, Batch 225, Loss: 0.48024213314056396\n",
      "Epoch 17, Batch 226, Loss: 0.35536104440689087\n",
      "Epoch 17, Batch 227, Loss: 0.36379122734069824\n",
      "Epoch 17, Batch 228, Loss: 0.3719298243522644\n",
      "Epoch 17, Batch 229, Loss: 0.42581480741500854\n",
      "Epoch 17, Batch 230, Loss: 0.619573175907135\n",
      "Epoch 17, Batch 231, Loss: 0.4087275564670563\n",
      "Epoch 17, Batch 232, Loss: 0.607001781463623\n",
      "Epoch 17, Batch 233, Loss: 0.37289562821388245\n",
      "Epoch 17, Batch 234, Loss: 0.2988976240158081\n",
      "Epoch 17, Batch 235, Loss: 0.3767935037612915\n",
      "Epoch 17, Batch 236, Loss: 0.6735739707946777\n",
      "Epoch 17, Batch 237, Loss: 0.3691425025463104\n",
      "Epoch 17, Batch 238, Loss: 0.3944936692714691\n",
      "Epoch 17, Batch 239, Loss: 0.4306466281414032\n",
      "Epoch 17, Batch 240, Loss: 0.2976118326187134\n",
      "Epoch 17, Batch 241, Loss: 0.24246177077293396\n",
      "Epoch 17, Batch 242, Loss: 0.294029176235199\n",
      "Epoch 17, Batch 243, Loss: 0.33004438877105713\n",
      "Epoch 17, Batch 244, Loss: 0.5061245560646057\n",
      "Epoch 17, Batch 245, Loss: 0.3880824148654938\n",
      "Epoch 17, Batch 246, Loss: 0.2691270112991333\n",
      "Epoch 17, Batch 247, Loss: 0.4459904432296753\n",
      "Epoch 17, Batch 248, Loss: 0.5065957307815552\n",
      "Epoch 17, Batch 249, Loss: 0.41687941551208496\n",
      "Epoch 17, Batch 250, Loss: 0.3793019950389862\n",
      "Epoch 17, Batch 251, Loss: 0.5558235049247742\n",
      "Epoch 17, Batch 252, Loss: 0.44244635105133057\n",
      "Epoch 17, Batch 253, Loss: 0.3047904372215271\n",
      "Epoch 17, Batch 254, Loss: 0.4646339416503906\n",
      "Epoch 17, Batch 255, Loss: 0.32114481925964355\n",
      "Epoch 17, Batch 256, Loss: 0.6422562003135681\n",
      "Epoch 17, Batch 257, Loss: 0.3641626536846161\n",
      "Epoch 17, Batch 258, Loss: 0.6264911890029907\n",
      "Epoch 17, Batch 259, Loss: 0.4819362759590149\n",
      "Epoch 17, Batch 260, Loss: 0.3819991946220398\n",
      "Epoch 17, Batch 261, Loss: 0.49860453605651855\n",
      "Epoch 17, Batch 262, Loss: 0.23785944283008575\n",
      "Epoch 17, Batch 263, Loss: 0.5092836618423462\n",
      "Epoch 17, Batch 264, Loss: 0.40161725878715515\n",
      "Epoch 17, Batch 265, Loss: 0.4401901066303253\n",
      "Epoch 17, Batch 266, Loss: 0.5663254857063293\n",
      "Epoch 17, Batch 267, Loss: 0.3468143939971924\n",
      "Epoch 17, Batch 268, Loss: 0.5663267374038696\n",
      "Epoch 17, Batch 269, Loss: 0.40890857577323914\n",
      "Epoch 17, Batch 270, Loss: 0.39574605226516724\n",
      "Epoch 17, Batch 271, Loss: 0.48656636476516724\n",
      "Epoch 17, Batch 272, Loss: 0.3330577313899994\n",
      "Epoch 17, Batch 273, Loss: 0.3689383864402771\n",
      "Epoch 17, Batch 274, Loss: 0.3502633273601532\n",
      "Epoch 17, Batch 275, Loss: 0.522494912147522\n",
      "Epoch 17, Batch 276, Loss: 0.43048474192619324\n",
      "Epoch 17, Batch 277, Loss: 0.7772045135498047\n",
      "Epoch 17, Batch 278, Loss: 0.43483781814575195\n",
      "Epoch 17, Batch 279, Loss: 0.3916069269180298\n",
      "Epoch 17, Batch 280, Loss: 0.4121752977371216\n",
      "Epoch 17, Batch 281, Loss: 0.42815151810646057\n",
      "Epoch 17, Batch 282, Loss: 0.30667805671691895\n",
      "Epoch 17, Batch 283, Loss: 0.45196282863616943\n",
      "Epoch 17, Batch 284, Loss: 0.3625946044921875\n",
      "Epoch 17, Batch 285, Loss: 0.6001577377319336\n",
      "Epoch 17, Batch 286, Loss: 0.3765817880630493\n",
      "Epoch 17, Batch 287, Loss: 0.39198845624923706\n",
      "Epoch 17, Batch 288, Loss: 0.2828786373138428\n",
      "Epoch 17, Batch 289, Loss: 0.4637661874294281\n",
      "Epoch 17, Batch 290, Loss: 0.5955997109413147\n",
      "Epoch 17, Batch 291, Loss: 0.5215936303138733\n",
      "Epoch 17, Batch 292, Loss: 0.5292932987213135\n",
      "Epoch 17, Batch 293, Loss: 0.5748814940452576\n",
      "Epoch 17, Batch 294, Loss: 0.3858000636100769\n",
      "Epoch 17, Batch 295, Loss: 0.46247607469558716\n",
      "Epoch 17, Batch 296, Loss: 0.35905492305755615\n",
      "Epoch 17, Batch 297, Loss: 0.593117356300354\n",
      "Epoch 17, Batch 298, Loss: 0.5650593042373657\n",
      "Epoch 17, Batch 299, Loss: 0.509318470954895\n",
      "Epoch 17, Batch 300, Loss: 0.4781823456287384\n",
      "Epoch 17, Batch 301, Loss: 0.44257262349128723\n",
      "Epoch 17, Batch 302, Loss: 0.40073245763778687\n",
      "Epoch 17, Batch 303, Loss: 0.40259236097335815\n",
      "Epoch 17, Batch 304, Loss: 0.3546617031097412\n",
      "Epoch 17, Batch 305, Loss: 0.5004739761352539\n",
      "Epoch 17, Batch 306, Loss: 0.4576869010925293\n",
      "Epoch 17, Batch 307, Loss: 0.5009770393371582\n",
      "Epoch 17, Batch 308, Loss: 0.30437079071998596\n",
      "Epoch 17, Batch 309, Loss: 0.48595112562179565\n",
      "Epoch 17, Batch 310, Loss: 0.4177674651145935\n",
      "Epoch 17, Batch 311, Loss: 0.30385497212409973\n",
      "Epoch 17, Batch 312, Loss: 0.5116734504699707\n",
      "Epoch 17, Batch 313, Loss: 0.5336381196975708\n",
      "Epoch 17, Batch 314, Loss: 0.5458568334579468\n",
      "Epoch 17, Batch 315, Loss: 0.7946995496749878\n",
      "Epoch 17, Batch 316, Loss: 0.32508620619773865\n",
      "Epoch 17, Batch 317, Loss: 0.5734531283378601\n",
      "Epoch 17, Batch 318, Loss: 0.569028377532959\n",
      "Epoch 17, Batch 319, Loss: 0.4118022918701172\n",
      "Epoch 17, Batch 320, Loss: 0.42719316482543945\n",
      "Epoch 17, Batch 321, Loss: 0.4135517477989197\n",
      "Epoch 17, Batch 322, Loss: 0.4069557785987854\n",
      "Epoch 17, Batch 323, Loss: 0.4385069012641907\n",
      "Epoch 17, Batch 324, Loss: 0.4659477770328522\n",
      "Epoch 17, Batch 325, Loss: 0.558054506778717\n",
      "Epoch 17, Batch 326, Loss: 0.28985652327537537\n",
      "Epoch 17, Batch 327, Loss: 0.4238061010837555\n",
      "Epoch 17, Batch 328, Loss: 0.43436482548713684\n",
      "Epoch 17, Batch 329, Loss: 0.42607730627059937\n",
      "Epoch 17, Batch 330, Loss: 0.44966477155685425\n",
      "Epoch 17, Batch 331, Loss: 0.26665714383125305\n",
      "Epoch 17, Batch 332, Loss: 0.3980877995491028\n",
      "Epoch 17, Batch 333, Loss: 0.37277257442474365\n",
      "Epoch 17, Batch 334, Loss: 0.34269291162490845\n",
      "Epoch 17, Batch 335, Loss: 0.41669225692749023\n",
      "Epoch 17, Batch 336, Loss: 0.33868885040283203\n",
      "Epoch 17, Batch 337, Loss: 0.409943550825119\n",
      "Epoch 17, Batch 338, Loss: 0.4284657835960388\n",
      "Epoch 17, Batch 339, Loss: 0.384907603263855\n",
      "Epoch 17, Batch 340, Loss: 0.24076618254184723\n",
      "Epoch 17, Batch 341, Loss: 0.3765101730823517\n",
      "Epoch 17, Batch 342, Loss: 0.43688249588012695\n",
      "Epoch 17, Batch 343, Loss: 0.6693200469017029\n",
      "Epoch 17, Batch 344, Loss: 0.4412594437599182\n",
      "Epoch 17, Batch 345, Loss: 0.3742319345474243\n",
      "Epoch 17, Batch 346, Loss: 0.4470197856426239\n",
      "Epoch 17, Batch 347, Loss: 0.36035552620887756\n",
      "Epoch 17, Batch 348, Loss: 0.46972328424453735\n",
      "Epoch 17, Batch 349, Loss: 0.28342321515083313\n",
      "Epoch 17, Batch 350, Loss: 0.3187062442302704\n",
      "Epoch 17, Batch 351, Loss: 0.335162878036499\n",
      "Epoch 17, Batch 352, Loss: 0.4161462187767029\n",
      "Epoch 17, Batch 353, Loss: 0.4663231670856476\n",
      "Epoch 17, Batch 354, Loss: 0.4388851225376129\n",
      "Epoch 17, Batch 355, Loss: 0.40730521082878113\n",
      "Epoch 17, Batch 356, Loss: 0.49583762884140015\n",
      "Epoch 17, Batch 357, Loss: 0.38069963455200195\n",
      "Epoch 17, Batch 358, Loss: 0.6427866220474243\n",
      "Epoch 17, Batch 359, Loss: 0.48508214950561523\n",
      "Epoch 17, Batch 360, Loss: 0.6042250990867615\n",
      "Epoch 17, Batch 361, Loss: 0.3763430714607239\n",
      "Epoch 17, Batch 362, Loss: 0.4035246968269348\n",
      "Epoch 17, Batch 363, Loss: 0.3600829243659973\n",
      "Epoch 17, Batch 364, Loss: 0.6287606358528137\n",
      "Epoch 17, Batch 365, Loss: 0.4330366849899292\n",
      "Epoch 17, Batch 366, Loss: 0.37080398201942444\n",
      "Epoch 17, Batch 367, Loss: 0.41690507531166077\n",
      "Epoch 17, Batch 368, Loss: 0.44769594073295593\n",
      "Epoch 17, Batch 369, Loss: 0.43662187457084656\n",
      "Epoch 17, Batch 370, Loss: 0.404540479183197\n",
      "Epoch 17, Batch 371, Loss: 0.4014836847782135\n",
      "Epoch 17, Batch 372, Loss: 0.3960376977920532\n",
      "Epoch 17, Batch 373, Loss: 0.7511321306228638\n",
      "Epoch 17, Batch 374, Loss: 0.452935129404068\n",
      "Epoch 17, Batch 375, Loss: 0.4701688885688782\n",
      "Epoch 17, Batch 376, Loss: 0.4843291938304901\n",
      "Epoch 17, Batch 377, Loss: 0.4540810286998749\n",
      "Epoch 17, Batch 378, Loss: 0.4754851460456848\n",
      "Epoch 17, Batch 379, Loss: 0.3139950633049011\n",
      "Epoch 17, Batch 380, Loss: 0.45068448781967163\n",
      "Epoch 17, Batch 381, Loss: 0.548034131526947\n",
      "Epoch 17, Batch 382, Loss: 0.3881380558013916\n",
      "Epoch 17, Batch 383, Loss: 0.3958803713321686\n",
      "Epoch 17, Batch 384, Loss: 0.39976608753204346\n",
      "Epoch 17, Batch 385, Loss: 0.5000489950180054\n",
      "Epoch 17, Batch 386, Loss: 0.44807499647140503\n",
      "Epoch 17, Batch 387, Loss: 0.4638662040233612\n",
      "Epoch 17, Batch 388, Loss: 0.775603711605072\n",
      "Epoch 17, Batch 389, Loss: 0.5668607950210571\n",
      "Epoch 17, Batch 390, Loss: 0.47600018978118896\n",
      "Epoch 17, Batch 391, Loss: 0.3994033932685852\n",
      "Epoch 17, Batch 392, Loss: 0.320021390914917\n",
      "Epoch 17, Batch 393, Loss: 0.4171137511730194\n",
      "Epoch 17, Batch 394, Loss: 0.45107558369636536\n",
      "Epoch 17, Batch 395, Loss: 0.360213041305542\n",
      "Epoch 17, Batch 396, Loss: 0.5297540426254272\n",
      "Epoch 17, Batch 397, Loss: 0.4095090627670288\n",
      "Epoch 17, Batch 398, Loss: 0.40386658906936646\n",
      "Epoch 17, Batch 399, Loss: 0.4187535047531128\n",
      "Epoch 17, Batch 400, Loss: 0.425838440656662\n",
      "Epoch 17, Batch 401, Loss: 0.42892906069755554\n",
      "Epoch 17, Batch 402, Loss: 0.6271732449531555\n",
      "Epoch 17, Batch 403, Loss: 0.5060665607452393\n",
      "Epoch 17, Batch 404, Loss: 0.37609249353408813\n",
      "Epoch 17, Batch 405, Loss: 0.48632293939590454\n",
      "Epoch 17, Batch 406, Loss: 0.32517844438552856\n",
      "Epoch 17, Batch 407, Loss: 0.44266781210899353\n",
      "Epoch 17, Batch 408, Loss: 0.42989468574523926\n",
      "Epoch 17, Batch 409, Loss: 0.4442824125289917\n",
      "Epoch 17, Batch 410, Loss: 0.5029682517051697\n",
      "Epoch 17, Batch 411, Loss: 0.3982987105846405\n",
      "Epoch 17, Batch 412, Loss: 0.48849818110466003\n",
      "Epoch 17, Batch 413, Loss: 0.31228137016296387\n",
      "Epoch 17, Batch 414, Loss: 0.4299165606498718\n",
      "Epoch 17, Batch 415, Loss: 0.5482393503189087\n",
      "Epoch 17, Batch 416, Loss: 0.35252442955970764\n",
      "Epoch 17, Batch 417, Loss: 0.4711204767227173\n",
      "Epoch 17, Batch 418, Loss: 0.2617209851741791\n",
      "Epoch 17, Batch 419, Loss: 0.42883166670799255\n",
      "Epoch 17, Batch 420, Loss: 0.5365594625473022\n",
      "Epoch 17, Batch 421, Loss: 0.46252068877220154\n",
      "Epoch 17, Batch 422, Loss: 0.4283764362335205\n",
      "Epoch 17, Batch 423, Loss: 0.30530670285224915\n",
      "Epoch 17, Batch 424, Loss: 0.3065306842327118\n",
      "Epoch 17, Batch 425, Loss: 0.2952837347984314\n",
      "Epoch 17, Batch 426, Loss: 0.6766989827156067\n",
      "Epoch 17, Batch 427, Loss: 0.3880498707294464\n",
      "Epoch 17, Batch 428, Loss: 0.5872306227684021\n",
      "Epoch 17, Batch 429, Loss: 0.4661652147769928\n",
      "Epoch 17, Batch 430, Loss: 0.6514416933059692\n",
      "Epoch 17, Batch 431, Loss: 0.39371293783187866\n",
      "Epoch 17, Batch 432, Loss: 0.30730411410331726\n",
      "Epoch 17, Batch 433, Loss: 0.40693238377571106\n",
      "Epoch 17, Batch 434, Loss: 0.5556626915931702\n",
      "Epoch 17, Batch 435, Loss: 0.4147963225841522\n",
      "Epoch 17, Batch 436, Loss: 0.47798633575439453\n",
      "Epoch 17, Batch 437, Loss: 0.4376300573348999\n",
      "Epoch 17, Batch 438, Loss: 0.45976707339286804\n",
      "Epoch 17, Batch 439, Loss: 0.5555917024612427\n",
      "Epoch 17, Batch 440, Loss: 0.5253148078918457\n",
      "Epoch 17, Batch 441, Loss: 0.4989723563194275\n",
      "Epoch 17, Batch 442, Loss: 0.33022475242614746\n",
      "Epoch 17, Batch 443, Loss: 0.36232054233551025\n",
      "Epoch 17, Batch 444, Loss: 0.37975698709487915\n",
      "Epoch 17, Batch 445, Loss: 0.6694451570510864\n",
      "Epoch 17, Batch 446, Loss: 0.3625773787498474\n",
      "Epoch 17, Batch 447, Loss: 0.40066948533058167\n",
      "Epoch 17, Batch 448, Loss: 0.4961393475532532\n",
      "Epoch 17, Batch 449, Loss: 0.5258057117462158\n",
      "Epoch 17, Batch 450, Loss: 0.4523719847202301\n",
      "Epoch 17, Batch 451, Loss: 0.3045235276222229\n",
      "Epoch 17, Batch 452, Loss: 0.46043097972869873\n",
      "Epoch 17, Batch 453, Loss: 0.35056936740875244\n",
      "Epoch 17, Batch 454, Loss: 0.4330780506134033\n",
      "Epoch 17, Batch 455, Loss: 0.614311695098877\n",
      "Epoch 17, Batch 456, Loss: 0.3362849950790405\n",
      "Epoch 17, Batch 457, Loss: 0.3788355588912964\n",
      "Epoch 17, Batch 458, Loss: 0.2850615680217743\n",
      "Epoch 17, Batch 459, Loss: 0.4285544753074646\n",
      "Epoch 17, Batch 460, Loss: 0.3972398638725281\n",
      "Epoch 17, Batch 461, Loss: 0.4666249454021454\n",
      "Epoch 17, Batch 462, Loss: 0.6808902621269226\n",
      "Epoch 17, Batch 463, Loss: 0.39918002486228943\n",
      "Epoch 17, Batch 464, Loss: 0.47669780254364014\n",
      "Epoch 17, Batch 465, Loss: 0.48186326026916504\n",
      "Epoch 17, Batch 466, Loss: 0.4114813208580017\n",
      "Epoch 17, Batch 467, Loss: 0.38759204745292664\n",
      "Epoch 17, Batch 468, Loss: 0.4966698884963989\n",
      "Epoch 17, Batch 469, Loss: 0.465863972902298\n",
      "Epoch 17, Batch 470, Loss: 0.30120375752449036\n",
      "Epoch 17, Batch 471, Loss: 0.4522896707057953\n",
      "Epoch 17, Batch 472, Loss: 0.6131088733673096\n",
      "Epoch 17, Batch 473, Loss: 0.6793590784072876\n",
      "Epoch 17, Batch 474, Loss: 0.322420209646225\n",
      "Epoch 17, Batch 475, Loss: 0.48391079902648926\n",
      "Epoch 17, Batch 476, Loss: 0.271327406167984\n",
      "Epoch 17, Batch 477, Loss: 0.46093952655792236\n",
      "Epoch 17, Batch 478, Loss: 0.5387370586395264\n",
      "Epoch 17, Batch 479, Loss: 0.4166931211948395\n",
      "Epoch 17, Batch 480, Loss: 0.42132753133773804\n",
      "Epoch 17, Batch 481, Loss: 0.4104137122631073\n",
      "Epoch 17, Batch 482, Loss: 0.4992910921573639\n",
      "Epoch 17, Batch 483, Loss: 0.25951212644577026\n",
      "Epoch 17, Batch 484, Loss: 0.3910234570503235\n",
      "Epoch 17, Batch 485, Loss: 0.5217575430870056\n",
      "Epoch 17, Batch 486, Loss: 0.6065625548362732\n",
      "Epoch 17, Batch 487, Loss: 0.31255853176116943\n",
      "Epoch 17, Batch 488, Loss: 0.2843323349952698\n",
      "Epoch 17, Batch 489, Loss: 0.3505449891090393\n",
      "Epoch 17, Batch 490, Loss: 0.5636326670646667\n",
      "Epoch 17, Batch 491, Loss: 0.40529051423072815\n",
      "Epoch 17, Batch 492, Loss: 0.45733070373535156\n",
      "Epoch 17, Batch 493, Loss: 0.3492242097854614\n",
      "Epoch 17, Batch 494, Loss: 0.664395272731781\n",
      "Epoch 17, Batch 495, Loss: 0.3889773190021515\n",
      "Epoch 17, Batch 496, Loss: 0.5076345205307007\n",
      "Epoch 17, Batch 497, Loss: 0.4672732949256897\n",
      "Epoch 17, Batch 498, Loss: 0.4496607780456543\n",
      "Epoch 17, Batch 499, Loss: 0.2416771501302719\n",
      "Epoch 17, Batch 500, Loss: 0.4882197380065918\n",
      "Epoch 17, Batch 501, Loss: 0.8396382331848145\n",
      "Epoch 17, Batch 502, Loss: 0.34758511185646057\n",
      "Epoch 17, Batch 503, Loss: 0.3363497257232666\n",
      "Epoch 17, Batch 504, Loss: 0.5740228295326233\n",
      "Epoch 17, Batch 505, Loss: 0.5596973896026611\n",
      "Epoch 17, Batch 506, Loss: 0.47861263155937195\n",
      "Epoch 17, Batch 507, Loss: 0.6662886142730713\n",
      "Epoch 17, Batch 508, Loss: 0.4123438000679016\n",
      "Epoch 17, Batch 509, Loss: 0.44175779819488525\n",
      "Epoch 17, Batch 510, Loss: 0.6746718883514404\n",
      "Epoch 17, Batch 511, Loss: 0.7719243168830872\n",
      "Epoch 17, Batch 512, Loss: 0.4668032228946686\n",
      "Epoch 17, Batch 513, Loss: 0.38294631242752075\n",
      "Epoch 17, Batch 514, Loss: 0.446786493062973\n",
      "Epoch 17, Batch 515, Loss: 0.5584889650344849\n",
      "Epoch 17, Batch 516, Loss: 0.3975965082645416\n",
      "Epoch 17, Batch 517, Loss: 0.5058988332748413\n",
      "Epoch 17, Batch 518, Loss: 0.5277830362319946\n",
      "Epoch 17, Batch 519, Loss: 0.5693107843399048\n",
      "Epoch 17, Batch 520, Loss: 0.3240230083465576\n",
      "Epoch 17, Batch 521, Loss: 0.37311479449272156\n",
      "Epoch 17, Batch 522, Loss: 0.4476625323295593\n",
      "Epoch 17, Batch 523, Loss: 0.38369885087013245\n",
      "Epoch 17, Batch 524, Loss: 0.2795065641403198\n",
      "Epoch 17, Batch 525, Loss: 0.34963932633399963\n",
      "Epoch 17, Batch 526, Loss: 0.4245893955230713\n",
      "Epoch 17, Batch 527, Loss: 0.6357734799385071\n",
      "Epoch 17, Batch 528, Loss: 0.3490607440471649\n",
      "Epoch 17, Batch 529, Loss: 0.5866091251373291\n",
      "Epoch 17, Batch 530, Loss: 0.34591931104660034\n",
      "Epoch 17, Batch 531, Loss: 0.35529202222824097\n",
      "Epoch 17, Batch 532, Loss: 0.32608869671821594\n",
      "Epoch 17, Batch 533, Loss: 0.35110053420066833\n",
      "Epoch 17, Batch 534, Loss: 0.46108123660087585\n",
      "Epoch 17, Batch 535, Loss: 0.5226028561592102\n",
      "Epoch 17, Batch 536, Loss: 0.41671204566955566\n",
      "Epoch 17, Batch 537, Loss: 0.3623093366622925\n",
      "Epoch 17, Batch 538, Loss: 0.35219231247901917\n",
      "Epoch 17, Batch 539, Loss: 0.5477023720741272\n",
      "Epoch 17, Batch 540, Loss: 0.4769095778465271\n",
      "Epoch 17, Batch 541, Loss: 0.5000064373016357\n",
      "Epoch 17, Batch 542, Loss: 0.2671394348144531\n",
      "Epoch 17, Batch 543, Loss: 0.3675697445869446\n",
      "Epoch 17, Batch 544, Loss: 0.27047502994537354\n",
      "Epoch 17, Batch 545, Loss: 0.5875536799430847\n",
      "Epoch 17, Batch 546, Loss: 0.4833393692970276\n",
      "Epoch 17, Batch 547, Loss: 0.3648981750011444\n",
      "Epoch 17, Batch 548, Loss: 0.5747063755989075\n",
      "Epoch 17, Batch 549, Loss: 0.4236127436161041\n",
      "Epoch 17, Batch 550, Loss: 0.482349157333374\n",
      "Epoch 17, Batch 551, Loss: 0.44299834966659546\n",
      "Epoch 17, Batch 552, Loss: 0.6839819550514221\n",
      "Epoch 17, Batch 553, Loss: 0.3186854422092438\n",
      "Epoch 17, Batch 554, Loss: 0.5677818059921265\n",
      "Epoch 17, Batch 555, Loss: 0.5478098392486572\n",
      "Epoch 17, Batch 556, Loss: 0.40352869033813477\n",
      "Epoch 17, Batch 557, Loss: 0.27878129482269287\n",
      "Epoch 17, Batch 558, Loss: 0.39624959230422974\n",
      "Epoch 17, Batch 559, Loss: 0.4983213543891907\n",
      "Epoch 17, Batch 560, Loss: 0.4221172332763672\n",
      "Epoch 17, Batch 561, Loss: 0.6480575799942017\n",
      "Epoch 17, Batch 562, Loss: 0.43144792318344116\n",
      "Epoch 17, Batch 563, Loss: 0.2780577540397644\n",
      "Epoch 17, Batch 564, Loss: 0.3862934708595276\n",
      "Epoch 17, Batch 565, Loss: 0.34616512060165405\n",
      "Epoch 17, Batch 566, Loss: 0.35122931003570557\n",
      "Epoch 17, Batch 567, Loss: 0.5678409337997437\n",
      "Epoch 17, Batch 568, Loss: 0.2789711058139801\n",
      "Epoch 17, Batch 569, Loss: 0.5347091555595398\n",
      "Epoch 17, Batch 570, Loss: 0.40164172649383545\n",
      "Epoch 17, Batch 571, Loss: 0.5249680876731873\n",
      "Epoch 17, Batch 572, Loss: 0.3945389986038208\n",
      "Epoch 17, Batch 573, Loss: 0.3682490885257721\n",
      "Epoch 17, Batch 574, Loss: 0.4216530919075012\n",
      "Epoch 17, Batch 575, Loss: 0.2739804983139038\n",
      "Epoch 17, Batch 576, Loss: 0.4144674241542816\n",
      "Epoch 17, Batch 577, Loss: 0.5454511642456055\n",
      "Epoch 17, Batch 578, Loss: 0.4762140214443207\n",
      "Epoch 17, Batch 579, Loss: 0.30910205841064453\n",
      "Epoch 17, Batch 580, Loss: 0.46774405241012573\n",
      "Epoch 17, Batch 581, Loss: 0.4623207151889801\n",
      "Epoch 17, Batch 582, Loss: 0.35790854692459106\n",
      "Epoch 17, Batch 583, Loss: 0.39776426553726196\n",
      "Epoch 17, Batch 584, Loss: 0.4307445287704468\n",
      "Epoch 17, Batch 585, Loss: 0.5664967894554138\n",
      "Epoch 17, Batch 586, Loss: 0.4889530539512634\n",
      "Epoch 17, Batch 587, Loss: 0.46241652965545654\n",
      "Epoch 17, Batch 588, Loss: 0.4731723964214325\n",
      "Epoch 17, Batch 589, Loss: 0.44993847608566284\n",
      "Epoch 17, Batch 590, Loss: 0.41423508524894714\n",
      "Epoch 17, Batch 591, Loss: 0.40700608491897583\n",
      "Epoch 17, Batch 592, Loss: 0.512067973613739\n",
      "Epoch 17, Batch 593, Loss: 0.4497089385986328\n",
      "Epoch 17, Batch 594, Loss: 0.719700276851654\n",
      "Epoch 17, Batch 595, Loss: 0.4396108090877533\n",
      "Epoch 17, Batch 596, Loss: 0.4146069586277008\n",
      "Epoch 17, Batch 597, Loss: 0.38377657532691956\n",
      "Epoch 17, Batch 598, Loss: 0.46148037910461426\n",
      "Epoch 17, Batch 599, Loss: 0.5067011117935181\n",
      "Epoch 17, Batch 600, Loss: 0.479404091835022\n",
      "Epoch 17, Batch 601, Loss: 0.38818636536598206\n",
      "Epoch 17, Batch 602, Loss: 0.5213838815689087\n",
      "Epoch 17, Batch 603, Loss: 0.38156747817993164\n",
      "Epoch 17, Batch 604, Loss: 0.42057257890701294\n",
      "Epoch 17, Batch 605, Loss: 0.5480473637580872\n",
      "Epoch 17, Batch 606, Loss: 0.369645893573761\n",
      "Epoch 17, Batch 607, Loss: 0.3430938720703125\n",
      "Epoch 17, Batch 608, Loss: 0.4794854521751404\n",
      "Epoch 17, Batch 609, Loss: 0.2143482118844986\n",
      "Epoch 17, Batch 610, Loss: 0.4486055076122284\n",
      "Epoch 17, Batch 611, Loss: 0.3021884858608246\n",
      "Epoch 17, Batch 612, Loss: 0.5104387998580933\n",
      "Epoch 17, Batch 613, Loss: 0.4416753947734833\n",
      "Epoch 17, Batch 614, Loss: 0.4160279333591461\n",
      "Epoch 17, Batch 615, Loss: 0.25821906328201294\n",
      "Epoch 17, Batch 616, Loss: 0.5116704702377319\n",
      "Epoch 17, Batch 617, Loss: 0.358428955078125\n",
      "Epoch 17, Batch 618, Loss: 0.44799208641052246\n",
      "Epoch 17, Batch 619, Loss: 0.4175146818161011\n",
      "Epoch 17, Batch 620, Loss: 0.5229539275169373\n",
      "Epoch 17, Batch 621, Loss: 0.5743858218193054\n",
      "Epoch 17, Batch 622, Loss: 0.6363216042518616\n",
      "Epoch 17, Batch 623, Loss: 0.4713434875011444\n",
      "Epoch 17, Batch 624, Loss: 0.4505462348461151\n",
      "Epoch 17, Batch 625, Loss: 0.339337557554245\n",
      "Epoch 17, Batch 626, Loss: 0.4302562475204468\n",
      "Epoch 17, Batch 627, Loss: 0.2275867760181427\n",
      "Epoch 17, Batch 628, Loss: 0.5176590085029602\n",
      "Epoch 17, Batch 629, Loss: 0.4039571285247803\n",
      "Epoch 17, Batch 630, Loss: 0.5453392267227173\n",
      "Epoch 17, Batch 631, Loss: 0.35798323154449463\n",
      "Epoch 17, Batch 632, Loss: 0.5110828280448914\n",
      "Epoch 17, Batch 633, Loss: 0.5684213638305664\n",
      "Epoch 17, Batch 634, Loss: 0.4176974892616272\n",
      "Epoch 17, Batch 635, Loss: 0.4880625009536743\n",
      "Epoch 17, Batch 636, Loss: 0.43474188446998596\n",
      "Epoch 17, Batch 637, Loss: 0.5186037421226501\n",
      "Epoch 17, Batch 638, Loss: 0.7667264938354492\n",
      "Epoch 17, Batch 639, Loss: 0.5260041356086731\n",
      "Epoch 17, Batch 640, Loss: 0.48829346895217896\n",
      "Epoch 17, Batch 641, Loss: 0.5737920999526978\n",
      "Epoch 17, Batch 642, Loss: 0.45614004135131836\n",
      "Epoch 17, Batch 643, Loss: 0.42160409688949585\n",
      "Epoch 17, Batch 644, Loss: 0.5366961359977722\n",
      "Epoch 17, Batch 645, Loss: 0.40566256642341614\n",
      "Epoch 17, Batch 646, Loss: 0.5867446064949036\n",
      "Epoch 17, Batch 647, Loss: 0.29792362451553345\n",
      "Epoch 17, Batch 648, Loss: 0.5068411827087402\n",
      "Epoch 17, Batch 649, Loss: 0.5196927189826965\n",
      "Epoch 17, Batch 650, Loss: 0.369409441947937\n",
      "Epoch 17, Batch 651, Loss: 0.5363097786903381\n",
      "Epoch 17, Batch 652, Loss: 0.36497706174850464\n",
      "Epoch 17, Batch 653, Loss: 0.40725576877593994\n",
      "Epoch 17, Batch 654, Loss: 0.5371947884559631\n",
      "Epoch 17, Batch 655, Loss: 0.6295505166053772\n",
      "Epoch 17, Batch 656, Loss: 0.5761492252349854\n",
      "Epoch 17, Batch 657, Loss: 0.3760284185409546\n",
      "Epoch 17, Batch 658, Loss: 0.6825647950172424\n",
      "Epoch 17, Batch 659, Loss: 0.4379189610481262\n",
      "Epoch 17, Batch 660, Loss: 0.4306325912475586\n",
      "Epoch 17, Batch 661, Loss: 0.3531312048435211\n",
      "Epoch 17, Batch 662, Loss: 0.3403073847293854\n",
      "Epoch 17, Batch 663, Loss: 0.5066652894020081\n",
      "Epoch 17, Batch 664, Loss: 0.43945619463920593\n",
      "Epoch 17, Batch 665, Loss: 0.5123082399368286\n",
      "Epoch 17, Batch 666, Loss: 0.24802149832248688\n",
      "Epoch 17, Batch 667, Loss: 0.7329612374305725\n",
      "Epoch 17, Batch 668, Loss: 0.40775415301322937\n",
      "Epoch 17, Batch 669, Loss: 0.4615580439567566\n",
      "Epoch 17, Batch 670, Loss: 0.573750913143158\n",
      "Epoch 17, Batch 671, Loss: 0.5386229753494263\n",
      "Epoch 17, Batch 672, Loss: 0.4755788743495941\n",
      "Epoch 17, Batch 673, Loss: 0.28909820318222046\n",
      "Epoch 17, Batch 674, Loss: 0.31152766942977905\n",
      "Epoch 17, Batch 675, Loss: 0.5007770657539368\n",
      "Epoch 17, Batch 676, Loss: 0.4102213978767395\n",
      "Epoch 17, Batch 677, Loss: 0.6806644797325134\n",
      "Epoch 17, Batch 678, Loss: 0.43617314100265503\n",
      "Epoch 17, Batch 679, Loss: 0.42863306403160095\n",
      "Epoch 17, Batch 680, Loss: 0.354400634765625\n",
      "Epoch 17, Batch 681, Loss: 0.5400074124336243\n",
      "Epoch 17, Batch 682, Loss: 0.6873530149459839\n",
      "Epoch 17, Batch 683, Loss: 0.5452986359596252\n",
      "Epoch 17, Batch 684, Loss: 0.5426015257835388\n",
      "Epoch 17, Batch 685, Loss: 0.4969881474971771\n",
      "Epoch 17, Batch 686, Loss: 0.4890766739845276\n",
      "Epoch 17, Batch 687, Loss: 0.37564167380332947\n",
      "Epoch 17, Batch 688, Loss: 0.6539899110794067\n",
      "Epoch 17, Batch 689, Loss: 0.42312800884246826\n",
      "Epoch 17, Batch 690, Loss: 0.43016475439071655\n",
      "Epoch 17, Batch 691, Loss: 0.41674304008483887\n",
      "Epoch 17, Batch 692, Loss: 0.42340049147605896\n",
      "Epoch 17, Batch 693, Loss: 0.5508230924606323\n",
      "Epoch 17, Batch 694, Loss: 0.2616060674190521\n",
      "Epoch 17, Batch 695, Loss: 0.3266938328742981\n",
      "Epoch 17, Batch 696, Loss: 0.5102561712265015\n",
      "Epoch 17, Batch 697, Loss: 0.44256192445755005\n",
      "Epoch 17, Batch 698, Loss: 0.4375603199005127\n",
      "Epoch 17, Batch 699, Loss: 0.3031872808933258\n",
      "Epoch 17, Batch 700, Loss: 0.3761190176010132\n",
      "Epoch 17, Batch 701, Loss: 0.3540443480014801\n",
      "Epoch 17, Batch 702, Loss: 0.376190721988678\n",
      "Epoch 17, Batch 703, Loss: 0.4460510313510895\n",
      "Epoch 17, Batch 704, Loss: 0.29041796922683716\n",
      "Epoch 17, Batch 705, Loss: 0.6979672908782959\n",
      "Epoch 17, Batch 706, Loss: 0.461439847946167\n",
      "Epoch 17, Batch 707, Loss: 0.3523377776145935\n",
      "Epoch 17, Batch 708, Loss: 0.3686579763889313\n",
      "Epoch 17, Batch 709, Loss: 0.5791609287261963\n",
      "Epoch 17, Batch 710, Loss: 0.25511014461517334\n",
      "Epoch 17, Batch 711, Loss: 0.5713943243026733\n",
      "Epoch 17, Batch 712, Loss: 0.5515049695968628\n",
      "Epoch 17, Batch 713, Loss: 0.47260013222694397\n",
      "Epoch 17, Batch 714, Loss: 0.34145843982696533\n",
      "Epoch 17, Batch 715, Loss: 0.2899053394794464\n",
      "Epoch 17, Batch 716, Loss: 0.4338328540325165\n",
      "Epoch 17, Batch 717, Loss: 0.5824121236801147\n",
      "Epoch 17, Batch 718, Loss: 0.3086078464984894\n",
      "Epoch 17, Batch 719, Loss: 0.32998937368392944\n",
      "Epoch 17, Batch 720, Loss: 0.34213706851005554\n",
      "Epoch 17, Batch 721, Loss: 0.3829364478588104\n",
      "Epoch 17, Batch 722, Loss: 0.32240158319473267\n",
      "Epoch 17, Batch 723, Loss: 0.24835215508937836\n",
      "Epoch 17, Batch 724, Loss: 0.4437524080276489\n",
      "Epoch 17, Batch 725, Loss: 0.44146984815597534\n",
      "Epoch 17, Batch 726, Loss: 0.6036030650138855\n",
      "Epoch 17, Batch 727, Loss: 0.36285319924354553\n",
      "Epoch 17, Batch 728, Loss: 0.4089567959308624\n",
      "Epoch 17, Batch 729, Loss: 0.23706865310668945\n",
      "Epoch 17, Batch 730, Loss: 0.5031895637512207\n",
      "Epoch 17, Batch 731, Loss: 0.3161994218826294\n",
      "Epoch 17, Batch 732, Loss: 0.5590468049049377\n",
      "Epoch 17, Batch 733, Loss: 0.40984979271888733\n",
      "Epoch 17, Batch 734, Loss: 0.5497103929519653\n",
      "Epoch 17, Batch 735, Loss: 0.47593432664871216\n",
      "Epoch 17, Batch 736, Loss: 0.3171056807041168\n",
      "Epoch 17, Batch 737, Loss: 0.33785122632980347\n",
      "Epoch 17, Batch 738, Loss: 0.28150323033332825\n",
      "Epoch 17, Batch 739, Loss: 0.35234686732292175\n",
      "Epoch 17, Batch 740, Loss: 0.3054666817188263\n",
      "Epoch 17, Batch 741, Loss: 0.47218114137649536\n",
      "Epoch 17, Batch 742, Loss: 0.5945016145706177\n",
      "Epoch 17, Batch 743, Loss: 0.7359766364097595\n",
      "Epoch 17, Batch 744, Loss: 0.2197723537683487\n",
      "Epoch 17, Batch 745, Loss: 0.44973209500312805\n",
      "Epoch 17, Batch 746, Loss: 0.266997367143631\n",
      "Epoch 17, Batch 747, Loss: 0.3459722697734833\n",
      "Epoch 17, Batch 748, Loss: 0.3548327386379242\n",
      "Epoch 17, Batch 749, Loss: 0.5307667851448059\n",
      "Epoch 17, Batch 750, Loss: 0.3688049912452698\n",
      "Epoch 17, Batch 751, Loss: 0.4854790270328522\n",
      "Epoch 17, Batch 752, Loss: 0.3537907600402832\n",
      "Epoch 17, Batch 753, Loss: 0.40176698565483093\n",
      "Epoch 17, Batch 754, Loss: 0.3992619514465332\n",
      "Epoch 17, Batch 755, Loss: 0.3070320188999176\n",
      "Epoch 17, Batch 756, Loss: 0.19455212354660034\n",
      "Epoch 17, Batch 757, Loss: 0.5665860176086426\n",
      "Epoch 17, Batch 758, Loss: 0.2972218990325928\n",
      "Epoch 17, Batch 759, Loss: 0.32024213671684265\n",
      "Epoch 17, Batch 760, Loss: 0.3168060779571533\n",
      "Epoch 17, Batch 761, Loss: 0.31391021609306335\n",
      "Epoch 17, Batch 762, Loss: 0.6245793700218201\n",
      "Epoch 17, Batch 763, Loss: 0.4237351417541504\n",
      "Epoch 17, Batch 764, Loss: 0.31563711166381836\n",
      "Epoch 17, Batch 765, Loss: 0.4583853781223297\n",
      "Epoch 17, Batch 766, Loss: 0.42947277426719666\n",
      "Epoch 17, Batch 767, Loss: 0.29798296093940735\n",
      "Epoch 17, Batch 768, Loss: 0.29990655183792114\n",
      "Epoch 17, Batch 769, Loss: 0.4147907495498657\n",
      "Epoch 17, Batch 770, Loss: 0.48442837595939636\n",
      "Epoch 17, Batch 771, Loss: 0.3166400194168091\n",
      "Epoch 17, Batch 772, Loss: 0.34182295203208923\n",
      "Epoch 17, Batch 773, Loss: 0.3386800289154053\n",
      "Epoch 17, Batch 774, Loss: 0.34772610664367676\n",
      "Epoch 17, Batch 775, Loss: 0.2741986811161041\n",
      "Epoch 17, Batch 776, Loss: 0.27188608050346375\n",
      "Epoch 17, Batch 777, Loss: 0.34509485960006714\n",
      "Epoch 17, Batch 778, Loss: 0.5044150352478027\n",
      "Epoch 17, Batch 779, Loss: 0.422518789768219\n",
      "Epoch 17, Batch 780, Loss: 0.3469131886959076\n",
      "Epoch 17, Batch 781, Loss: 0.3687841296195984\n",
      "Epoch 17, Batch 782, Loss: 0.4728621244430542\n",
      "Epoch 17, Batch 783, Loss: 0.4439447820186615\n",
      "Epoch 17, Batch 784, Loss: 0.5517320036888123\n",
      "Epoch 17, Batch 785, Loss: 0.4075815677642822\n",
      "Epoch 17, Batch 786, Loss: 0.4604153037071228\n",
      "Epoch 17, Batch 787, Loss: 0.6272220611572266\n",
      "Epoch 17, Batch 788, Loss: 0.4000454843044281\n",
      "Epoch 17, Batch 789, Loss: 0.5243721008300781\n",
      "Epoch 17, Batch 790, Loss: 0.4680183231830597\n",
      "Epoch 17, Batch 791, Loss: 0.5063492655754089\n",
      "Epoch 17, Batch 792, Loss: 0.36031681299209595\n",
      "Epoch 17, Batch 793, Loss: 0.44581666588783264\n",
      "Epoch 17, Batch 794, Loss: 0.5738060474395752\n",
      "Epoch 17, Batch 795, Loss: 0.33077341318130493\n",
      "Epoch 17, Batch 796, Loss: 0.43389201164245605\n",
      "Epoch 17, Batch 797, Loss: 0.575842022895813\n",
      "Epoch 17, Batch 798, Loss: 0.511053740978241\n",
      "Epoch 17, Batch 799, Loss: 0.28097593784332275\n",
      "Epoch 17, Batch 800, Loss: 0.5165534019470215\n",
      "Epoch 17, Batch 801, Loss: 0.4818142354488373\n",
      "Epoch 17, Batch 802, Loss: 0.3701285123825073\n",
      "Epoch 17, Batch 803, Loss: 0.4112934172153473\n",
      "Epoch 17, Batch 804, Loss: 0.39567241072654724\n",
      "Epoch 17, Batch 805, Loss: 0.4595140516757965\n",
      "Epoch 17, Batch 806, Loss: 0.30370640754699707\n",
      "Epoch 17, Batch 807, Loss: 0.42159008979797363\n",
      "Epoch 17, Batch 808, Loss: 0.4009585976600647\n",
      "Epoch 17, Batch 809, Loss: 0.5362208485603333\n",
      "Epoch 17, Batch 810, Loss: 0.58558189868927\n",
      "Epoch 17, Batch 811, Loss: 0.4211587905883789\n",
      "Epoch 17, Batch 812, Loss: 0.5368527173995972\n",
      "Epoch 17, Batch 813, Loss: 0.3724897801876068\n",
      "Epoch 17, Batch 814, Loss: 0.3495718538761139\n",
      "Epoch 17, Batch 815, Loss: 0.5354085564613342\n",
      "Epoch 17, Batch 816, Loss: 0.3270898163318634\n",
      "Epoch 17, Batch 817, Loss: 0.4815482497215271\n",
      "Epoch 17, Batch 818, Loss: 0.31666895747184753\n",
      "Epoch 17, Batch 819, Loss: 0.3541398048400879\n",
      "Epoch 17, Batch 820, Loss: 0.6412206888198853\n",
      "Epoch 17, Batch 821, Loss: 0.49171602725982666\n",
      "Epoch 17, Batch 822, Loss: 0.38538119196891785\n",
      "Epoch 17, Batch 823, Loss: 0.3370154798030853\n",
      "Epoch 17, Batch 824, Loss: 0.37619832158088684\n",
      "Epoch 17, Batch 825, Loss: 0.3876579701900482\n",
      "Epoch 17, Batch 826, Loss: 0.29940682649612427\n",
      "Epoch 17, Batch 827, Loss: 0.4950072169303894\n",
      "Epoch 17, Batch 828, Loss: 0.2547292411327362\n",
      "Epoch 17, Batch 829, Loss: 0.5399088263511658\n",
      "Epoch 17, Batch 830, Loss: 0.5148938894271851\n",
      "Epoch 17, Batch 831, Loss: 0.41364815831184387\n",
      "Epoch 17, Batch 832, Loss: 0.3089630603790283\n",
      "Epoch 17, Batch 833, Loss: 0.412052184343338\n",
      "Epoch 17, Batch 834, Loss: 0.33736515045166016\n",
      "Epoch 17, Batch 835, Loss: 0.33136215806007385\n",
      "Epoch 17, Batch 836, Loss: 0.31735649704933167\n",
      "Epoch 17, Batch 837, Loss: 0.4812045097351074\n",
      "Epoch 17, Batch 838, Loss: 0.5923488736152649\n",
      "Epoch 17, Batch 839, Loss: 0.25752705335617065\n",
      "Epoch 17, Batch 840, Loss: 0.39237022399902344\n",
      "Epoch 17, Batch 841, Loss: 0.3656750023365021\n",
      "Epoch 17, Batch 842, Loss: 0.42832040786743164\n",
      "Epoch 17, Batch 843, Loss: 0.47036534547805786\n",
      "Epoch 17, Batch 844, Loss: 0.5490332841873169\n",
      "Epoch 17, Batch 845, Loss: 0.4815281629562378\n",
      "Epoch 17, Batch 846, Loss: 0.31408244371414185\n",
      "Epoch 17, Batch 847, Loss: 0.4484671354293823\n",
      "Epoch 17, Batch 848, Loss: 0.38544127345085144\n",
      "Epoch 17, Batch 849, Loss: 0.32690152525901794\n",
      "Epoch 17, Batch 850, Loss: 0.29834115505218506\n",
      "Epoch 17, Batch 851, Loss: 0.3634684085845947\n",
      "Epoch 17, Batch 852, Loss: 0.557697057723999\n",
      "Epoch 17, Batch 853, Loss: 0.665187418460846\n",
      "Epoch 17, Batch 854, Loss: 0.3723234534263611\n",
      "Epoch 17, Batch 855, Loss: 0.3577149212360382\n",
      "Epoch 17, Batch 856, Loss: 0.38957682251930237\n",
      "Epoch 17, Batch 857, Loss: 0.47032642364501953\n",
      "Epoch 17, Batch 858, Loss: 0.34969615936279297\n",
      "Epoch 17, Batch 859, Loss: 0.44672706723213196\n",
      "Epoch 17, Batch 860, Loss: 0.4247443675994873\n",
      "Epoch 17, Batch 861, Loss: 0.4845350980758667\n",
      "Epoch 17, Batch 862, Loss: 0.3772938847541809\n",
      "Epoch 17, Batch 863, Loss: 0.36847490072250366\n",
      "Epoch 17, Batch 864, Loss: 0.473389208316803\n",
      "Epoch 17, Batch 865, Loss: 0.4795030355453491\n",
      "Epoch 17, Batch 866, Loss: 0.37344270944595337\n",
      "Epoch 17, Batch 867, Loss: 0.4282519519329071\n",
      "Epoch 17, Batch 868, Loss: 0.3890331983566284\n",
      "Epoch 17, Batch 869, Loss: 0.5447344779968262\n",
      "Epoch 17, Batch 870, Loss: 0.3369731307029724\n",
      "Epoch 17, Batch 871, Loss: 0.25095507502555847\n",
      "Epoch 17, Batch 872, Loss: 0.4161895513534546\n",
      "Epoch 17, Batch 873, Loss: 0.3392157554626465\n",
      "Epoch 17, Batch 874, Loss: 0.2991645038127899\n",
      "Epoch 17, Batch 875, Loss: 0.4748172163963318\n",
      "Epoch 17, Batch 876, Loss: 0.5880765914916992\n",
      "Epoch 17, Batch 877, Loss: 0.41499263048171997\n",
      "Epoch 17, Batch 878, Loss: 0.2824782133102417\n",
      "Epoch 17, Batch 879, Loss: 0.33034563064575195\n",
      "Epoch 17, Batch 880, Loss: 0.4058608412742615\n",
      "Epoch 17, Batch 881, Loss: 0.5151484608650208\n",
      "Epoch 17, Batch 882, Loss: 0.381306916475296\n",
      "Epoch 17, Batch 883, Loss: 0.4214368462562561\n",
      "Epoch 17, Batch 884, Loss: 0.5346335768699646\n",
      "Epoch 17, Batch 885, Loss: 0.7279964685440063\n",
      "Epoch 17, Batch 886, Loss: 0.5370661616325378\n",
      "Epoch 17, Batch 887, Loss: 0.39134645462036133\n",
      "Epoch 17, Batch 888, Loss: 0.539440393447876\n",
      "Epoch 17, Batch 889, Loss: 0.25873133540153503\n",
      "Epoch 17, Batch 890, Loss: 0.4441033899784088\n",
      "Epoch 17, Batch 891, Loss: 0.377441942691803\n",
      "Epoch 17, Batch 892, Loss: 0.3204895257949829\n",
      "Epoch 17, Batch 893, Loss: 0.41196003556251526\n",
      "Epoch 17, Batch 894, Loss: 0.6530343294143677\n",
      "Epoch 17, Batch 895, Loss: 0.4516737759113312\n",
      "Epoch 17, Batch 896, Loss: 0.3488538861274719\n",
      "Epoch 17, Batch 897, Loss: 0.3498198390007019\n",
      "Epoch 17, Batch 898, Loss: 0.4319699704647064\n",
      "Epoch 17, Batch 899, Loss: 0.5388569831848145\n",
      "Epoch 17, Batch 900, Loss: 0.4650682806968689\n",
      "Epoch 17, Batch 901, Loss: 0.5125817060470581\n",
      "Epoch 17, Batch 902, Loss: 0.3768974244594574\n",
      "Epoch 17, Batch 903, Loss: 0.3118445575237274\n",
      "Epoch 17, Batch 904, Loss: 0.36800292134284973\n",
      "Epoch 17, Batch 905, Loss: 0.500029444694519\n",
      "Epoch 17, Batch 906, Loss: 0.32579106092453003\n",
      "Epoch 17, Batch 907, Loss: 0.31283578276634216\n",
      "Epoch 17, Batch 908, Loss: 0.5031290650367737\n",
      "Epoch 17, Batch 909, Loss: 0.461409330368042\n",
      "Epoch 17, Batch 910, Loss: 0.29353952407836914\n",
      "Epoch 17, Batch 911, Loss: 0.4213128983974457\n",
      "Epoch 17, Batch 912, Loss: 0.4701080918312073\n",
      "Epoch 17, Batch 913, Loss: 0.3933776915073395\n",
      "Epoch 17, Batch 914, Loss: 0.2949674129486084\n",
      "Epoch 17, Batch 915, Loss: 0.551312267780304\n",
      "Epoch 17, Batch 916, Loss: 0.38003915548324585\n",
      "Epoch 17, Batch 917, Loss: 0.518843412399292\n",
      "Epoch 17, Batch 918, Loss: 0.39113470911979675\n",
      "Epoch 17, Batch 919, Loss: 0.40065717697143555\n",
      "Epoch 17, Batch 920, Loss: 0.4044484794139862\n",
      "Epoch 17, Batch 921, Loss: 0.47652667760849\n",
      "Epoch 17, Batch 922, Loss: 0.5248249173164368\n",
      "Epoch 17, Batch 923, Loss: 0.4633817970752716\n",
      "Epoch 17, Batch 924, Loss: 0.5764042139053345\n",
      "Epoch 17, Batch 925, Loss: 0.402707576751709\n",
      "Epoch 17, Batch 926, Loss: 0.32006993889808655\n",
      "Epoch 17, Batch 927, Loss: 0.5321904420852661\n",
      "Epoch 17, Batch 928, Loss: 0.3496834337711334\n",
      "Epoch 17, Batch 929, Loss: 0.536979079246521\n",
      "Epoch 17, Batch 930, Loss: 0.6607396602630615\n",
      "Epoch 17, Batch 931, Loss: 0.5158621072769165\n",
      "Epoch 17, Batch 932, Loss: 0.5204654932022095\n",
      "Epoch 17, Batch 933, Loss: 0.27928924560546875\n",
      "Epoch 17, Batch 934, Loss: 0.5566715598106384\n",
      "Epoch 17, Batch 935, Loss: 0.32919958233833313\n",
      "Epoch 17, Batch 936, Loss: 0.27914100885391235\n",
      "Epoch 17, Batch 937, Loss: 0.5686253905296326\n",
      "Epoch 17, Batch 938, Loss: 0.443028062582016\n",
      "Accuracy of train set: 0.8465\n",
      "Epoch 17, Batch 1, Test Loss: 0.30328482389450073\n",
      "Epoch 17, Batch 2, Test Loss: 0.41332271695137024\n",
      "Epoch 17, Batch 3, Test Loss: 0.42713382840156555\n",
      "Epoch 17, Batch 4, Test Loss: 0.39211273193359375\n",
      "Epoch 17, Batch 5, Test Loss: 0.3109312057495117\n",
      "Epoch 17, Batch 6, Test Loss: 0.5185416340827942\n",
      "Epoch 17, Batch 7, Test Loss: 0.521932065486908\n",
      "Epoch 17, Batch 8, Test Loss: 0.38432571291923523\n",
      "Epoch 17, Batch 9, Test Loss: 0.37793758511543274\n",
      "Epoch 17, Batch 10, Test Loss: 0.4514615535736084\n",
      "Epoch 17, Batch 11, Test Loss: 0.265534907579422\n",
      "Epoch 17, Batch 12, Test Loss: 0.6952118873596191\n",
      "Epoch 17, Batch 13, Test Loss: 0.21379795670509338\n",
      "Epoch 17, Batch 14, Test Loss: 0.619520902633667\n",
      "Epoch 17, Batch 15, Test Loss: 0.35633543133735657\n",
      "Epoch 17, Batch 16, Test Loss: 0.4987223744392395\n",
      "Epoch 17, Batch 17, Test Loss: 0.3939243257045746\n",
      "Epoch 17, Batch 18, Test Loss: 0.5222125053405762\n",
      "Epoch 17, Batch 19, Test Loss: 0.4084671139717102\n",
      "Epoch 17, Batch 20, Test Loss: 0.5619312524795532\n",
      "Epoch 17, Batch 21, Test Loss: 0.48740991950035095\n",
      "Epoch 17, Batch 22, Test Loss: 0.45990750193595886\n",
      "Epoch 17, Batch 23, Test Loss: 0.6643282175064087\n",
      "Epoch 17, Batch 24, Test Loss: 0.3375743329524994\n",
      "Epoch 17, Batch 25, Test Loss: 0.5898309350013733\n",
      "Epoch 17, Batch 26, Test Loss: 0.47969281673431396\n",
      "Epoch 17, Batch 27, Test Loss: 0.5599340796470642\n",
      "Epoch 17, Batch 28, Test Loss: 0.4031727612018585\n",
      "Epoch 17, Batch 29, Test Loss: 0.4541528820991516\n",
      "Epoch 17, Batch 30, Test Loss: 0.3816602826118469\n",
      "Epoch 17, Batch 31, Test Loss: 0.41748109459877014\n",
      "Epoch 17, Batch 32, Test Loss: 0.32478058338165283\n",
      "Epoch 17, Batch 33, Test Loss: 0.6336824297904968\n",
      "Epoch 17, Batch 34, Test Loss: 0.5356954336166382\n",
      "Epoch 17, Batch 35, Test Loss: 0.25546029210090637\n",
      "Epoch 17, Batch 36, Test Loss: 0.4085468053817749\n",
      "Epoch 17, Batch 37, Test Loss: 0.448153018951416\n",
      "Epoch 17, Batch 38, Test Loss: 0.47387027740478516\n",
      "Epoch 17, Batch 39, Test Loss: 0.48726966977119446\n",
      "Epoch 17, Batch 40, Test Loss: 0.2904420495033264\n",
      "Epoch 17, Batch 41, Test Loss: 0.5597919225692749\n",
      "Epoch 17, Batch 42, Test Loss: 0.40700316429138184\n",
      "Epoch 17, Batch 43, Test Loss: 0.2654392421245575\n",
      "Epoch 17, Batch 44, Test Loss: 0.5749471187591553\n",
      "Epoch 17, Batch 45, Test Loss: 0.27425000071525574\n",
      "Epoch 17, Batch 46, Test Loss: 0.3187410235404968\n",
      "Epoch 17, Batch 47, Test Loss: 0.3445168435573578\n",
      "Epoch 17, Batch 48, Test Loss: 0.5752337574958801\n",
      "Epoch 17, Batch 49, Test Loss: 0.5403987169265747\n",
      "Epoch 17, Batch 50, Test Loss: 0.4256931245326996\n",
      "Epoch 17, Batch 51, Test Loss: 0.5157931447029114\n",
      "Epoch 17, Batch 52, Test Loss: 0.4423125386238098\n",
      "Epoch 17, Batch 53, Test Loss: 0.47904640436172485\n",
      "Epoch 17, Batch 54, Test Loss: 0.46684014797210693\n",
      "Epoch 17, Batch 55, Test Loss: 0.42606422305107117\n",
      "Epoch 17, Batch 56, Test Loss: 0.38332808017730713\n",
      "Epoch 17, Batch 57, Test Loss: 0.36427709460258484\n",
      "Epoch 17, Batch 58, Test Loss: 0.5788030028343201\n",
      "Epoch 17, Batch 59, Test Loss: 0.5875435471534729\n",
      "Epoch 17, Batch 60, Test Loss: 0.4634913504123688\n",
      "Epoch 17, Batch 61, Test Loss: 0.36872947216033936\n",
      "Epoch 17, Batch 62, Test Loss: 0.5405094027519226\n",
      "Epoch 17, Batch 63, Test Loss: 0.3418895900249481\n",
      "Epoch 17, Batch 64, Test Loss: 0.2974051833152771\n",
      "Epoch 17, Batch 65, Test Loss: 0.33759796619415283\n",
      "Epoch 17, Batch 66, Test Loss: 0.616233766078949\n",
      "Epoch 17, Batch 67, Test Loss: 0.616001546382904\n",
      "Epoch 17, Batch 68, Test Loss: 0.7166954874992371\n",
      "Epoch 17, Batch 69, Test Loss: 0.3755449056625366\n",
      "Epoch 17, Batch 70, Test Loss: 0.36111292243003845\n",
      "Epoch 17, Batch 71, Test Loss: 0.5356712341308594\n",
      "Epoch 17, Batch 72, Test Loss: 0.29414522647857666\n",
      "Epoch 17, Batch 73, Test Loss: 0.38227379322052\n",
      "Epoch 17, Batch 74, Test Loss: 0.5929218530654907\n",
      "Epoch 17, Batch 75, Test Loss: 0.41521355509757996\n",
      "Epoch 17, Batch 76, Test Loss: 0.39455386996269226\n",
      "Epoch 17, Batch 77, Test Loss: 0.40610337257385254\n",
      "Epoch 17, Batch 78, Test Loss: 0.3505108058452606\n",
      "Epoch 17, Batch 79, Test Loss: 0.37932050228118896\n",
      "Epoch 17, Batch 80, Test Loss: 0.445942759513855\n",
      "Epoch 17, Batch 81, Test Loss: 0.49403682351112366\n",
      "Epoch 17, Batch 82, Test Loss: 0.5298010110855103\n",
      "Epoch 17, Batch 83, Test Loss: 0.4399487376213074\n",
      "Epoch 17, Batch 84, Test Loss: 0.58588707447052\n",
      "Epoch 17, Batch 85, Test Loss: 0.4779205918312073\n",
      "Epoch 17, Batch 86, Test Loss: 0.3921085298061371\n",
      "Epoch 17, Batch 87, Test Loss: 0.3946475386619568\n",
      "Epoch 17, Batch 88, Test Loss: 0.3426324427127838\n",
      "Epoch 17, Batch 89, Test Loss: 0.32647067308425903\n",
      "Epoch 17, Batch 90, Test Loss: 0.23570974171161652\n",
      "Epoch 17, Batch 91, Test Loss: 0.3860131502151489\n",
      "Epoch 17, Batch 92, Test Loss: 0.38407981395721436\n",
      "Epoch 17, Batch 93, Test Loss: 0.5107007622718811\n",
      "Epoch 17, Batch 94, Test Loss: 0.26644760370254517\n",
      "Epoch 17, Batch 95, Test Loss: 0.3284491002559662\n",
      "Epoch 17, Batch 96, Test Loss: 0.43960946798324585\n",
      "Epoch 17, Batch 97, Test Loss: 0.31528374552726746\n",
      "Epoch 17, Batch 98, Test Loss: 0.5350185632705688\n",
      "Epoch 17, Batch 99, Test Loss: 0.5501669049263\n",
      "Epoch 17, Batch 100, Test Loss: 0.30014920234680176\n",
      "Epoch 17, Batch 101, Test Loss: 0.5108615756034851\n",
      "Epoch 17, Batch 102, Test Loss: 0.5720379948616028\n",
      "Epoch 17, Batch 103, Test Loss: 0.38353466987609863\n",
      "Epoch 17, Batch 104, Test Loss: 0.5142068266868591\n",
      "Epoch 17, Batch 105, Test Loss: 0.4176776707172394\n",
      "Epoch 17, Batch 106, Test Loss: 0.6153772473335266\n",
      "Epoch 17, Batch 107, Test Loss: 0.39460939168930054\n",
      "Epoch 17, Batch 108, Test Loss: 0.4707919955253601\n",
      "Epoch 17, Batch 109, Test Loss: 0.3688071072101593\n",
      "Epoch 17, Batch 110, Test Loss: 0.357499361038208\n",
      "Epoch 17, Batch 111, Test Loss: 0.47235995531082153\n",
      "Epoch 17, Batch 112, Test Loss: 0.3388993442058563\n",
      "Epoch 17, Batch 113, Test Loss: 0.6738611459732056\n",
      "Epoch 17, Batch 114, Test Loss: 0.40421563386917114\n",
      "Epoch 17, Batch 115, Test Loss: 0.2843768000602722\n",
      "Epoch 17, Batch 116, Test Loss: 0.40146490931510925\n",
      "Epoch 17, Batch 117, Test Loss: 0.42126527428627014\n",
      "Epoch 17, Batch 118, Test Loss: 0.23073159158229828\n",
      "Epoch 17, Batch 119, Test Loss: 0.42516815662384033\n",
      "Epoch 17, Batch 120, Test Loss: 0.3073834478855133\n",
      "Epoch 17, Batch 121, Test Loss: 0.5048118233680725\n",
      "Epoch 17, Batch 122, Test Loss: 0.5382333397865295\n",
      "Epoch 17, Batch 123, Test Loss: 0.3813796937465668\n",
      "Epoch 17, Batch 124, Test Loss: 0.4594242572784424\n",
      "Epoch 17, Batch 125, Test Loss: 0.45601189136505127\n",
      "Epoch 17, Batch 126, Test Loss: 0.4451689124107361\n",
      "Epoch 17, Batch 127, Test Loss: 0.35031795501708984\n",
      "Epoch 17, Batch 128, Test Loss: 0.41036972403526306\n",
      "Epoch 17, Batch 129, Test Loss: 0.28257113695144653\n",
      "Epoch 17, Batch 130, Test Loss: 0.39199790358543396\n",
      "Epoch 17, Batch 131, Test Loss: 0.43414172530174255\n",
      "Epoch 17, Batch 132, Test Loss: 0.576115608215332\n",
      "Epoch 17, Batch 133, Test Loss: 0.3651103377342224\n",
      "Epoch 17, Batch 134, Test Loss: 0.5218763947486877\n",
      "Epoch 17, Batch 135, Test Loss: 0.4107878506183624\n",
      "Epoch 17, Batch 136, Test Loss: 0.3572588562965393\n",
      "Epoch 17, Batch 137, Test Loss: 0.37064310908317566\n",
      "Epoch 17, Batch 138, Test Loss: 0.4586808979511261\n",
      "Epoch 17, Batch 139, Test Loss: 0.584092915058136\n",
      "Epoch 17, Batch 140, Test Loss: 0.42870548367500305\n",
      "Epoch 17, Batch 141, Test Loss: 0.5282807946205139\n",
      "Epoch 17, Batch 142, Test Loss: 0.354818195104599\n",
      "Epoch 17, Batch 143, Test Loss: 0.36201584339141846\n",
      "Epoch 17, Batch 144, Test Loss: 0.3624023199081421\n",
      "Epoch 17, Batch 145, Test Loss: 0.4334176480770111\n",
      "Epoch 17, Batch 146, Test Loss: 0.41348516941070557\n",
      "Epoch 17, Batch 147, Test Loss: 0.5151787996292114\n",
      "Epoch 17, Batch 148, Test Loss: 0.5292693972587585\n",
      "Epoch 17, Batch 149, Test Loss: 0.4596594572067261\n",
      "Epoch 17, Batch 150, Test Loss: 0.5388107895851135\n",
      "Epoch 17, Batch 151, Test Loss: 0.43233662843704224\n",
      "Epoch 17, Batch 152, Test Loss: 0.42986467480659485\n",
      "Epoch 17, Batch 153, Test Loss: 0.3965734839439392\n",
      "Epoch 17, Batch 154, Test Loss: 0.5564213395118713\n",
      "Epoch 17, Batch 155, Test Loss: 0.5335732698440552\n",
      "Epoch 17, Batch 156, Test Loss: 0.559069037437439\n",
      "Epoch 17, Batch 157, Test Loss: 0.4998301863670349\n",
      "Epoch 17, Batch 158, Test Loss: 0.41731026768684387\n",
      "Epoch 17, Batch 159, Test Loss: 0.344996839761734\n",
      "Epoch 17, Batch 160, Test Loss: 0.2698647379875183\n",
      "Epoch 17, Batch 161, Test Loss: 0.44157347083091736\n",
      "Epoch 17, Batch 162, Test Loss: 0.3989877700805664\n",
      "Epoch 17, Batch 163, Test Loss: 0.5298460125923157\n",
      "Epoch 17, Batch 164, Test Loss: 0.2670271396636963\n",
      "Epoch 17, Batch 165, Test Loss: 0.43317916989326477\n",
      "Epoch 17, Batch 166, Test Loss: 0.5550193786621094\n",
      "Epoch 17, Batch 167, Test Loss: 0.26341068744659424\n",
      "Epoch 17, Batch 168, Test Loss: 0.5890095233917236\n",
      "Epoch 17, Batch 169, Test Loss: 0.5035557150840759\n",
      "Epoch 17, Batch 170, Test Loss: 0.44897061586380005\n",
      "Epoch 17, Batch 171, Test Loss: 0.3897530734539032\n",
      "Epoch 17, Batch 172, Test Loss: 0.4508149027824402\n",
      "Epoch 17, Batch 173, Test Loss: 0.3581829369068146\n",
      "Epoch 17, Batch 174, Test Loss: 0.4424779415130615\n",
      "Epoch 17, Batch 175, Test Loss: 0.41823631525039673\n",
      "Epoch 17, Batch 176, Test Loss: 0.5234756469726562\n",
      "Epoch 17, Batch 177, Test Loss: 0.4022831618785858\n",
      "Epoch 17, Batch 178, Test Loss: 0.379262775182724\n",
      "Epoch 17, Batch 179, Test Loss: 0.5103089213371277\n",
      "Epoch 17, Batch 180, Test Loss: 0.4220699369907379\n",
      "Epoch 17, Batch 181, Test Loss: 0.3923051357269287\n",
      "Epoch 17, Batch 182, Test Loss: 0.5716699361801147\n",
      "Epoch 17, Batch 183, Test Loss: 0.3559403419494629\n",
      "Epoch 17, Batch 184, Test Loss: 0.4057423770427704\n",
      "Epoch 17, Batch 185, Test Loss: 0.4880417585372925\n",
      "Epoch 17, Batch 186, Test Loss: 0.4106293022632599\n",
      "Epoch 17, Batch 187, Test Loss: 0.4626922011375427\n",
      "Epoch 17, Batch 188, Test Loss: 0.4924766421318054\n",
      "Epoch 17, Batch 189, Test Loss: 0.3877875804901123\n",
      "Epoch 17, Batch 190, Test Loss: 0.3057047128677368\n",
      "Epoch 17, Batch 191, Test Loss: 0.3659314811229706\n",
      "Epoch 17, Batch 192, Test Loss: 0.4889329969882965\n",
      "Epoch 17, Batch 193, Test Loss: 0.35008668899536133\n",
      "Epoch 17, Batch 194, Test Loss: 0.4535897672176361\n",
      "Epoch 17, Batch 195, Test Loss: 0.5157606601715088\n",
      "Epoch 17, Batch 196, Test Loss: 0.31250113248825073\n",
      "Epoch 17, Batch 197, Test Loss: 0.5159178972244263\n",
      "Epoch 17, Batch 198, Test Loss: 0.3303745687007904\n",
      "Epoch 17, Batch 199, Test Loss: 0.4334673583507538\n",
      "Epoch 17, Batch 200, Test Loss: 0.3260722756385803\n",
      "Epoch 17, Batch 201, Test Loss: 0.476619690656662\n",
      "Epoch 17, Batch 202, Test Loss: 0.39249682426452637\n",
      "Epoch 17, Batch 203, Test Loss: 0.32834115624427795\n",
      "Epoch 17, Batch 204, Test Loss: 0.3387722373008728\n",
      "Epoch 17, Batch 205, Test Loss: 0.4343372881412506\n",
      "Epoch 17, Batch 206, Test Loss: 0.30189651250839233\n",
      "Epoch 17, Batch 207, Test Loss: 0.33041834831237793\n",
      "Epoch 17, Batch 208, Test Loss: 0.5731021761894226\n",
      "Epoch 17, Batch 209, Test Loss: 0.563992440700531\n",
      "Epoch 17, Batch 210, Test Loss: 0.6429215669631958\n",
      "Epoch 17, Batch 211, Test Loss: 0.4877476692199707\n",
      "Epoch 17, Batch 212, Test Loss: 0.3929409086704254\n",
      "Epoch 17, Batch 213, Test Loss: 0.4428156614303589\n",
      "Epoch 17, Batch 214, Test Loss: 0.32892248034477234\n",
      "Epoch 17, Batch 215, Test Loss: 0.4626518487930298\n",
      "Epoch 17, Batch 216, Test Loss: 0.24651801586151123\n",
      "Epoch 17, Batch 217, Test Loss: 0.47176384925842285\n",
      "Epoch 17, Batch 218, Test Loss: 0.5559569597244263\n",
      "Epoch 17, Batch 219, Test Loss: 0.25164687633514404\n",
      "Epoch 17, Batch 220, Test Loss: 0.5615808963775635\n",
      "Epoch 17, Batch 221, Test Loss: 0.47603484988212585\n",
      "Epoch 17, Batch 222, Test Loss: 0.6109656691551208\n",
      "Epoch 17, Batch 223, Test Loss: 0.259410560131073\n",
      "Epoch 17, Batch 224, Test Loss: 0.36819276213645935\n",
      "Epoch 17, Batch 225, Test Loss: 0.36679139733314514\n",
      "Epoch 17, Batch 226, Test Loss: 0.46015340089797974\n",
      "Epoch 17, Batch 227, Test Loss: 0.40591081976890564\n",
      "Epoch 17, Batch 228, Test Loss: 0.449823260307312\n",
      "Epoch 17, Batch 229, Test Loss: 0.5583385229110718\n",
      "Epoch 17, Batch 230, Test Loss: 0.4554238021373749\n",
      "Epoch 17, Batch 231, Test Loss: 0.5014743804931641\n",
      "Epoch 17, Batch 232, Test Loss: 0.49064967036247253\n",
      "Epoch 17, Batch 233, Test Loss: 0.3582572937011719\n",
      "Epoch 17, Batch 234, Test Loss: 0.6411561965942383\n",
      "Epoch 17, Batch 235, Test Loss: 0.5702397227287292\n",
      "Epoch 17, Batch 236, Test Loss: 0.44637173414230347\n",
      "Epoch 17, Batch 237, Test Loss: 0.4347175359725952\n",
      "Epoch 17, Batch 238, Test Loss: 0.32234901189804077\n",
      "Epoch 17, Batch 239, Test Loss: 0.37724578380584717\n",
      "Epoch 17, Batch 240, Test Loss: 0.47505760192871094\n",
      "Epoch 17, Batch 241, Test Loss: 0.36815345287323\n",
      "Epoch 17, Batch 242, Test Loss: 0.3865095376968384\n",
      "Epoch 17, Batch 243, Test Loss: 0.40169811248779297\n",
      "Epoch 17, Batch 244, Test Loss: 0.40191030502319336\n",
      "Epoch 17, Batch 245, Test Loss: 0.30342936515808105\n",
      "Epoch 17, Batch 246, Test Loss: 0.4765906035900116\n",
      "Epoch 17, Batch 247, Test Loss: 0.3159049153327942\n",
      "Epoch 17, Batch 248, Test Loss: 0.48857834935188293\n",
      "Epoch 17, Batch 249, Test Loss: 0.34968024492263794\n",
      "Epoch 17, Batch 250, Test Loss: 0.5171465277671814\n",
      "Epoch 17, Batch 251, Test Loss: 0.4951243996620178\n",
      "Epoch 17, Batch 252, Test Loss: 0.5064994692802429\n",
      "Epoch 17, Batch 253, Test Loss: 0.2888447344303131\n",
      "Epoch 17, Batch 254, Test Loss: 0.5992448329925537\n",
      "Epoch 17, Batch 255, Test Loss: 0.4474290609359741\n",
      "Epoch 17, Batch 256, Test Loss: 0.4447881579399109\n",
      "Epoch 17, Batch 257, Test Loss: 0.5066981911659241\n",
      "Epoch 17, Batch 258, Test Loss: 0.4822515845298767\n",
      "Epoch 17, Batch 259, Test Loss: 0.39349666237831116\n",
      "Epoch 17, Batch 260, Test Loss: 0.6557126045227051\n",
      "Epoch 17, Batch 261, Test Loss: 0.5326399803161621\n",
      "Epoch 17, Batch 262, Test Loss: 0.5381093621253967\n",
      "Epoch 17, Batch 263, Test Loss: 0.29841622710227966\n",
      "Epoch 17, Batch 264, Test Loss: 0.4198428690433502\n",
      "Epoch 17, Batch 265, Test Loss: 0.5293761491775513\n",
      "Epoch 17, Batch 266, Test Loss: 0.5247265100479126\n",
      "Epoch 17, Batch 267, Test Loss: 0.42494678497314453\n",
      "Epoch 17, Batch 268, Test Loss: 0.3146968185901642\n",
      "Epoch 17, Batch 269, Test Loss: 0.41505151987075806\n",
      "Epoch 17, Batch 270, Test Loss: 0.43084341287612915\n",
      "Epoch 17, Batch 271, Test Loss: 0.40630942583084106\n",
      "Epoch 17, Batch 272, Test Loss: 0.37994757294654846\n",
      "Epoch 17, Batch 273, Test Loss: 0.7878018617630005\n",
      "Epoch 17, Batch 274, Test Loss: 0.5475776791572571\n",
      "Epoch 17, Batch 275, Test Loss: 0.49833089113235474\n",
      "Epoch 17, Batch 276, Test Loss: 0.7085645794868469\n",
      "Epoch 17, Batch 277, Test Loss: 0.31340110301971436\n",
      "Epoch 17, Batch 278, Test Loss: 0.5291194319725037\n",
      "Epoch 17, Batch 279, Test Loss: 0.5989700555801392\n",
      "Epoch 17, Batch 280, Test Loss: 0.3407737910747528\n",
      "Epoch 17, Batch 281, Test Loss: 0.2741042971611023\n",
      "Epoch 17, Batch 282, Test Loss: 0.5152971744537354\n",
      "Epoch 17, Batch 283, Test Loss: 0.3982100486755371\n",
      "Epoch 17, Batch 284, Test Loss: 0.5389690399169922\n",
      "Epoch 17, Batch 285, Test Loss: 0.5293710827827454\n",
      "Epoch 17, Batch 286, Test Loss: 0.5651105046272278\n",
      "Epoch 17, Batch 287, Test Loss: 0.4412190318107605\n",
      "Epoch 17, Batch 288, Test Loss: 0.46422243118286133\n",
      "Epoch 17, Batch 289, Test Loss: 0.3814217150211334\n",
      "Epoch 17, Batch 290, Test Loss: 0.29222214221954346\n",
      "Epoch 17, Batch 291, Test Loss: 0.4976344704627991\n",
      "Epoch 17, Batch 292, Test Loss: 0.6303062438964844\n",
      "Epoch 17, Batch 293, Test Loss: 0.307894766330719\n",
      "Epoch 17, Batch 294, Test Loss: 0.6907618045806885\n",
      "Epoch 17, Batch 295, Test Loss: 0.6217226386070251\n",
      "Epoch 17, Batch 296, Test Loss: 0.3839168846607208\n",
      "Epoch 17, Batch 297, Test Loss: 0.3202894926071167\n",
      "Epoch 17, Batch 298, Test Loss: 0.31155306100845337\n",
      "Epoch 17, Batch 299, Test Loss: 0.44219279289245605\n",
      "Epoch 17, Batch 300, Test Loss: 0.25545811653137207\n",
      "Epoch 17, Batch 301, Test Loss: 0.4027155339717865\n",
      "Epoch 17, Batch 302, Test Loss: 0.5065795183181763\n",
      "Epoch 17, Batch 303, Test Loss: 0.4434013366699219\n",
      "Epoch 17, Batch 304, Test Loss: 0.3531099259853363\n",
      "Epoch 17, Batch 305, Test Loss: 0.4475175738334656\n",
      "Epoch 17, Batch 306, Test Loss: 0.7229881286621094\n",
      "Epoch 17, Batch 307, Test Loss: 0.4017358720302582\n",
      "Epoch 17, Batch 308, Test Loss: 0.4232197701931\n",
      "Epoch 17, Batch 309, Test Loss: 0.6851485967636108\n",
      "Epoch 17, Batch 310, Test Loss: 0.4641936421394348\n",
      "Epoch 17, Batch 311, Test Loss: 0.5568891763687134\n",
      "Epoch 17, Batch 312, Test Loss: 0.33160966634750366\n",
      "Epoch 17, Batch 313, Test Loss: 0.44663435220718384\n",
      "Epoch 17, Batch 314, Test Loss: 0.5264959931373596\n",
      "Epoch 17, Batch 315, Test Loss: 0.4929409623146057\n",
      "Epoch 17, Batch 316, Test Loss: 0.571232795715332\n",
      "Epoch 17, Batch 317, Test Loss: 0.28793951869010925\n",
      "Epoch 17, Batch 318, Test Loss: 0.4064885079860687\n",
      "Epoch 17, Batch 319, Test Loss: 0.46343082189559937\n",
      "Epoch 17, Batch 320, Test Loss: 0.41118624806404114\n",
      "Epoch 17, Batch 321, Test Loss: 0.27625390887260437\n",
      "Epoch 17, Batch 322, Test Loss: 0.5218730568885803\n",
      "Epoch 17, Batch 323, Test Loss: 0.4499955475330353\n",
      "Epoch 17, Batch 324, Test Loss: 0.6052232980728149\n",
      "Epoch 17, Batch 325, Test Loss: 0.395809531211853\n",
      "Epoch 17, Batch 326, Test Loss: 0.5241456627845764\n",
      "Epoch 17, Batch 327, Test Loss: 0.45198917388916016\n",
      "Epoch 17, Batch 328, Test Loss: 0.40843212604522705\n",
      "Epoch 17, Batch 329, Test Loss: 0.31095704436302185\n",
      "Epoch 17, Batch 330, Test Loss: 0.29706281423568726\n",
      "Epoch 17, Batch 331, Test Loss: 0.35918524861335754\n",
      "Epoch 17, Batch 332, Test Loss: 0.29775217175483704\n",
      "Epoch 17, Batch 333, Test Loss: 0.2437199503183365\n",
      "Epoch 17, Batch 334, Test Loss: 0.29047220945358276\n",
      "Epoch 17, Batch 335, Test Loss: 0.47552070021629333\n",
      "Epoch 17, Batch 336, Test Loss: 0.4434070289134979\n",
      "Epoch 17, Batch 337, Test Loss: 0.2721911072731018\n",
      "Epoch 17, Batch 338, Test Loss: 0.4196983873844147\n",
      "Epoch 17, Batch 339, Test Loss: 0.3120921850204468\n",
      "Epoch 17, Batch 340, Test Loss: 0.40459465980529785\n",
      "Epoch 17, Batch 341, Test Loss: 0.5041779279708862\n",
      "Epoch 17, Batch 342, Test Loss: 0.4767531752586365\n",
      "Epoch 17, Batch 343, Test Loss: 0.3719477951526642\n",
      "Epoch 17, Batch 344, Test Loss: 0.3857736587524414\n",
      "Epoch 17, Batch 345, Test Loss: 0.4647810459136963\n",
      "Epoch 17, Batch 346, Test Loss: 0.47426730394363403\n",
      "Epoch 17, Batch 347, Test Loss: 0.48428481817245483\n",
      "Epoch 17, Batch 348, Test Loss: 0.4261740446090698\n",
      "Epoch 17, Batch 349, Test Loss: 0.4066275358200073\n",
      "Epoch 17, Batch 350, Test Loss: 0.3959299623966217\n",
      "Epoch 17, Batch 351, Test Loss: 0.4388793110847473\n",
      "Epoch 17, Batch 352, Test Loss: 0.39857393503189087\n",
      "Epoch 17, Batch 353, Test Loss: 0.3673054277896881\n",
      "Epoch 17, Batch 354, Test Loss: 0.3292194604873657\n",
      "Epoch 17, Batch 355, Test Loss: 0.5999335646629333\n",
      "Epoch 17, Batch 356, Test Loss: 0.46373453736305237\n",
      "Epoch 17, Batch 357, Test Loss: 0.3069900870323181\n",
      "Epoch 17, Batch 358, Test Loss: 0.22650554776191711\n",
      "Epoch 17, Batch 359, Test Loss: 0.5385822653770447\n",
      "Epoch 17, Batch 360, Test Loss: 0.562701940536499\n",
      "Epoch 17, Batch 361, Test Loss: 0.27508893609046936\n",
      "Epoch 17, Batch 362, Test Loss: 0.4443652927875519\n",
      "Epoch 17, Batch 363, Test Loss: 0.4251495599746704\n",
      "Epoch 17, Batch 364, Test Loss: 0.486317902803421\n",
      "Epoch 17, Batch 365, Test Loss: 0.3441077470779419\n",
      "Epoch 17, Batch 366, Test Loss: 0.31410858035087585\n",
      "Epoch 17, Batch 367, Test Loss: 0.5783962607383728\n",
      "Epoch 17, Batch 368, Test Loss: 0.44779202342033386\n",
      "Epoch 17, Batch 369, Test Loss: 0.33306586742401123\n",
      "Epoch 17, Batch 370, Test Loss: 0.37961915135383606\n",
      "Epoch 17, Batch 371, Test Loss: 0.375255286693573\n",
      "Epoch 17, Batch 372, Test Loss: 0.3642426133155823\n",
      "Epoch 17, Batch 373, Test Loss: 0.4326615333557129\n",
      "Epoch 17, Batch 374, Test Loss: 0.4539746940135956\n",
      "Epoch 17, Batch 375, Test Loss: 0.5868281126022339\n",
      "Epoch 17, Batch 376, Test Loss: 0.38345104455947876\n",
      "Epoch 17, Batch 377, Test Loss: 0.49794769287109375\n",
      "Epoch 17, Batch 378, Test Loss: 0.5603514313697815\n",
      "Epoch 17, Batch 379, Test Loss: 0.4573955833911896\n",
      "Epoch 17, Batch 380, Test Loss: 0.32965341210365295\n",
      "Epoch 17, Batch 381, Test Loss: 0.5436702966690063\n",
      "Epoch 17, Batch 382, Test Loss: 0.6430571675300598\n",
      "Epoch 17, Batch 383, Test Loss: 0.5759909749031067\n",
      "Epoch 17, Batch 384, Test Loss: 0.3934750258922577\n",
      "Epoch 17, Batch 385, Test Loss: 0.23846104741096497\n",
      "Epoch 17, Batch 386, Test Loss: 0.4904477298259735\n",
      "Epoch 17, Batch 387, Test Loss: 0.32582274079322815\n",
      "Epoch 17, Batch 388, Test Loss: 0.40758180618286133\n",
      "Epoch 17, Batch 389, Test Loss: 0.28769755363464355\n",
      "Epoch 17, Batch 390, Test Loss: 0.34950876235961914\n",
      "Epoch 17, Batch 391, Test Loss: 0.6410348415374756\n",
      "Epoch 17, Batch 392, Test Loss: 0.4867289960384369\n",
      "Epoch 17, Batch 393, Test Loss: 0.4866390824317932\n",
      "Epoch 17, Batch 394, Test Loss: 0.35697346925735474\n",
      "Epoch 17, Batch 395, Test Loss: 0.4390062391757965\n",
      "Epoch 17, Batch 396, Test Loss: 0.47439253330230713\n",
      "Epoch 17, Batch 397, Test Loss: 0.5102065801620483\n",
      "Epoch 17, Batch 398, Test Loss: 0.302797794342041\n",
      "Epoch 17, Batch 399, Test Loss: 0.4622856676578522\n",
      "Epoch 17, Batch 400, Test Loss: 0.31349846720695496\n",
      "Epoch 17, Batch 401, Test Loss: 0.2622177004814148\n",
      "Epoch 17, Batch 402, Test Loss: 0.5006182789802551\n",
      "Epoch 17, Batch 403, Test Loss: 0.4333776533603668\n",
      "Epoch 17, Batch 404, Test Loss: 0.35286179184913635\n",
      "Epoch 17, Batch 405, Test Loss: 0.4415018558502197\n",
      "Epoch 17, Batch 406, Test Loss: 0.4923059046268463\n",
      "Epoch 17, Batch 407, Test Loss: 0.37592649459838867\n",
      "Epoch 17, Batch 408, Test Loss: 0.4933784604072571\n",
      "Epoch 17, Batch 409, Test Loss: 0.385919451713562\n",
      "Epoch 17, Batch 410, Test Loss: 0.3612547814846039\n",
      "Epoch 17, Batch 411, Test Loss: 0.30057552456855774\n",
      "Epoch 17, Batch 412, Test Loss: 0.3238513171672821\n",
      "Epoch 17, Batch 413, Test Loss: 0.23991534113883972\n",
      "Epoch 17, Batch 414, Test Loss: 0.44269976019859314\n",
      "Epoch 17, Batch 415, Test Loss: 0.6992238759994507\n",
      "Epoch 17, Batch 416, Test Loss: 0.31410491466522217\n",
      "Epoch 17, Batch 417, Test Loss: 0.5894792079925537\n",
      "Epoch 17, Batch 418, Test Loss: 0.4221079349517822\n",
      "Epoch 17, Batch 419, Test Loss: 0.36369219422340393\n",
      "Epoch 17, Batch 420, Test Loss: 0.2460024356842041\n",
      "Epoch 17, Batch 421, Test Loss: 0.29727858304977417\n",
      "Epoch 17, Batch 422, Test Loss: 0.4518294632434845\n",
      "Epoch 17, Batch 423, Test Loss: 0.43935471773147583\n",
      "Epoch 17, Batch 424, Test Loss: 0.36736392974853516\n",
      "Epoch 17, Batch 425, Test Loss: 0.47835683822631836\n",
      "Epoch 17, Batch 426, Test Loss: 0.32806047797203064\n",
      "Epoch 17, Batch 427, Test Loss: 0.6147701740264893\n",
      "Epoch 17, Batch 428, Test Loss: 0.6873133182525635\n",
      "Epoch 17, Batch 429, Test Loss: 0.5089765787124634\n",
      "Epoch 17, Batch 430, Test Loss: 0.3248218595981598\n",
      "Epoch 17, Batch 431, Test Loss: 0.5488640069961548\n",
      "Epoch 17, Batch 432, Test Loss: 0.48270875215530396\n",
      "Epoch 17, Batch 433, Test Loss: 0.4852299690246582\n",
      "Epoch 17, Batch 434, Test Loss: 0.2734260559082031\n",
      "Epoch 17, Batch 435, Test Loss: 0.3752462565898895\n",
      "Epoch 17, Batch 436, Test Loss: 0.47439080476760864\n",
      "Epoch 17, Batch 437, Test Loss: 0.46753785014152527\n",
      "Epoch 17, Batch 438, Test Loss: 0.37658053636550903\n",
      "Epoch 17, Batch 439, Test Loss: 0.47527703642845154\n",
      "Epoch 17, Batch 440, Test Loss: 0.3599007725715637\n",
      "Epoch 17, Batch 441, Test Loss: 0.5227488279342651\n",
      "Epoch 17, Batch 442, Test Loss: 0.32140204310417175\n",
      "Epoch 17, Batch 443, Test Loss: 0.5282952189445496\n",
      "Epoch 17, Batch 444, Test Loss: 0.32777661085128784\n",
      "Epoch 17, Batch 445, Test Loss: 0.3530929386615753\n",
      "Epoch 17, Batch 446, Test Loss: 0.5628185868263245\n",
      "Epoch 17, Batch 447, Test Loss: 0.438970148563385\n",
      "Epoch 17, Batch 448, Test Loss: 0.5603053569793701\n",
      "Epoch 17, Batch 449, Test Loss: 0.4674431383609772\n",
      "Epoch 17, Batch 450, Test Loss: 0.45152872800827026\n",
      "Epoch 17, Batch 451, Test Loss: 0.5550804734230042\n",
      "Epoch 17, Batch 452, Test Loss: 0.2863144874572754\n",
      "Epoch 17, Batch 453, Test Loss: 0.5638473629951477\n",
      "Epoch 17, Batch 454, Test Loss: 0.7007730007171631\n",
      "Epoch 17, Batch 455, Test Loss: 0.49622011184692383\n",
      "Epoch 17, Batch 456, Test Loss: 0.4605288505554199\n",
      "Epoch 17, Batch 457, Test Loss: 0.4418483376502991\n",
      "Epoch 17, Batch 458, Test Loss: 0.46127426624298096\n",
      "Epoch 17, Batch 459, Test Loss: 0.2915569543838501\n",
      "Epoch 17, Batch 460, Test Loss: 0.3744576871395111\n",
      "Epoch 17, Batch 461, Test Loss: 0.4146442115306854\n",
      "Epoch 17, Batch 462, Test Loss: 0.30225059390068054\n",
      "Epoch 17, Batch 463, Test Loss: 0.37441694736480713\n",
      "Epoch 17, Batch 464, Test Loss: 0.3041129410266876\n",
      "Epoch 17, Batch 465, Test Loss: 0.46228548884391785\n",
      "Epoch 17, Batch 466, Test Loss: 0.5297729969024658\n",
      "Epoch 17, Batch 467, Test Loss: 0.26339125633239746\n",
      "Epoch 17, Batch 468, Test Loss: 0.3977513909339905\n",
      "Epoch 17, Batch 469, Test Loss: 0.5567842125892639\n",
      "Epoch 17, Batch 470, Test Loss: 0.5072965025901794\n",
      "Epoch 17, Batch 471, Test Loss: 0.31160035729408264\n",
      "Epoch 17, Batch 472, Test Loss: 0.38727959990501404\n",
      "Epoch 17, Batch 473, Test Loss: 0.38259270787239075\n",
      "Epoch 17, Batch 474, Test Loss: 0.5259826183319092\n",
      "Epoch 17, Batch 475, Test Loss: 0.426090806722641\n",
      "Epoch 17, Batch 476, Test Loss: 0.46173912286758423\n",
      "Epoch 17, Batch 477, Test Loss: 0.49154379963874817\n",
      "Epoch 17, Batch 478, Test Loss: 0.2896399199962616\n",
      "Epoch 17, Batch 479, Test Loss: 0.5193976163864136\n",
      "Epoch 17, Batch 480, Test Loss: 0.3693757653236389\n",
      "Epoch 17, Batch 481, Test Loss: 0.3407074213027954\n",
      "Epoch 17, Batch 482, Test Loss: 0.21040289103984833\n",
      "Epoch 17, Batch 483, Test Loss: 0.46047013998031616\n",
      "Epoch 17, Batch 484, Test Loss: 0.45882222056388855\n",
      "Epoch 17, Batch 485, Test Loss: 0.3609243929386139\n",
      "Epoch 17, Batch 486, Test Loss: 0.5894249677658081\n",
      "Epoch 17, Batch 487, Test Loss: 0.6211830377578735\n",
      "Epoch 17, Batch 488, Test Loss: 0.5090936422348022\n",
      "Epoch 17, Batch 489, Test Loss: 0.3231123983860016\n",
      "Epoch 17, Batch 490, Test Loss: 0.29390716552734375\n",
      "Epoch 17, Batch 491, Test Loss: 0.3385392427444458\n",
      "Epoch 17, Batch 492, Test Loss: 0.4980018734931946\n",
      "Epoch 17, Batch 493, Test Loss: 0.629501461982727\n",
      "Epoch 17, Batch 494, Test Loss: 0.44323334097862244\n",
      "Epoch 17, Batch 495, Test Loss: 0.531408429145813\n",
      "Epoch 17, Batch 496, Test Loss: 0.5062409043312073\n",
      "Epoch 17, Batch 497, Test Loss: 0.4837076961994171\n",
      "Epoch 17, Batch 498, Test Loss: 0.5060874223709106\n",
      "Epoch 17, Batch 499, Test Loss: 0.5835330486297607\n",
      "Epoch 17, Batch 500, Test Loss: 0.49708306789398193\n",
      "Epoch 17, Batch 501, Test Loss: 0.5068671107292175\n",
      "Epoch 17, Batch 502, Test Loss: 0.313397079706192\n",
      "Epoch 17, Batch 503, Test Loss: 0.4255943298339844\n",
      "Epoch 17, Batch 504, Test Loss: 0.5202155113220215\n",
      "Epoch 17, Batch 505, Test Loss: 0.38915586471557617\n",
      "Epoch 17, Batch 506, Test Loss: 0.4282471537590027\n",
      "Epoch 17, Batch 507, Test Loss: 0.4521406590938568\n",
      "Epoch 17, Batch 508, Test Loss: 0.4765298068523407\n",
      "Epoch 17, Batch 509, Test Loss: 0.5244116187095642\n",
      "Epoch 17, Batch 510, Test Loss: 0.23269715905189514\n",
      "Epoch 17, Batch 511, Test Loss: 0.364345908164978\n",
      "Epoch 17, Batch 512, Test Loss: 0.506711483001709\n",
      "Epoch 17, Batch 513, Test Loss: 0.3584177494049072\n",
      "Epoch 17, Batch 514, Test Loss: 0.43159785866737366\n",
      "Epoch 17, Batch 515, Test Loss: 0.427686482667923\n",
      "Epoch 17, Batch 516, Test Loss: 0.5096787810325623\n",
      "Epoch 17, Batch 517, Test Loss: 0.4933440685272217\n",
      "Epoch 17, Batch 518, Test Loss: 0.6549882888793945\n",
      "Epoch 17, Batch 519, Test Loss: 0.5552111864089966\n",
      "Epoch 17, Batch 520, Test Loss: 0.3407119810581207\n",
      "Epoch 17, Batch 521, Test Loss: 0.36910852789878845\n",
      "Epoch 17, Batch 522, Test Loss: 0.6821328401565552\n",
      "Epoch 17, Batch 523, Test Loss: 0.7149196863174438\n",
      "Epoch 17, Batch 524, Test Loss: 0.28645822405815125\n",
      "Epoch 17, Batch 525, Test Loss: 0.37616413831710815\n",
      "Epoch 17, Batch 526, Test Loss: 0.4569748044013977\n",
      "Epoch 17, Batch 527, Test Loss: 0.4376387596130371\n",
      "Epoch 17, Batch 528, Test Loss: 0.3688586354255676\n",
      "Epoch 17, Batch 529, Test Loss: 0.4617162346839905\n",
      "Epoch 17, Batch 530, Test Loss: 0.36284664273262024\n",
      "Epoch 17, Batch 531, Test Loss: 0.5956746935844421\n",
      "Epoch 17, Batch 532, Test Loss: 0.4368862211704254\n",
      "Epoch 17, Batch 533, Test Loss: 0.37189802527427673\n",
      "Epoch 17, Batch 534, Test Loss: 0.466644823551178\n",
      "Epoch 17, Batch 535, Test Loss: 0.6240148544311523\n",
      "Epoch 17, Batch 536, Test Loss: 0.3544751703739166\n",
      "Epoch 17, Batch 537, Test Loss: 0.42288506031036377\n",
      "Epoch 17, Batch 538, Test Loss: 0.411504328250885\n",
      "Epoch 17, Batch 539, Test Loss: 0.4347769320011139\n",
      "Epoch 17, Batch 540, Test Loss: 0.4909733235836029\n",
      "Epoch 17, Batch 541, Test Loss: 0.3705104887485504\n",
      "Epoch 17, Batch 542, Test Loss: 0.665737509727478\n",
      "Epoch 17, Batch 543, Test Loss: 0.37114399671554565\n",
      "Epoch 17, Batch 544, Test Loss: 0.4137316048145294\n",
      "Epoch 17, Batch 545, Test Loss: 0.46625107526779175\n",
      "Epoch 17, Batch 546, Test Loss: 0.33837050199508667\n",
      "Epoch 17, Batch 547, Test Loss: 0.48224937915802\n",
      "Epoch 17, Batch 548, Test Loss: 0.4463701546192169\n",
      "Epoch 17, Batch 549, Test Loss: 0.3673413395881653\n",
      "Epoch 17, Batch 550, Test Loss: 0.2591419517993927\n",
      "Epoch 17, Batch 551, Test Loss: 0.421786367893219\n",
      "Epoch 17, Batch 552, Test Loss: 0.3870256841182709\n",
      "Epoch 17, Batch 553, Test Loss: 0.5127754211425781\n",
      "Epoch 17, Batch 554, Test Loss: 0.6417065262794495\n",
      "Epoch 17, Batch 555, Test Loss: 0.45890945196151733\n",
      "Epoch 17, Batch 556, Test Loss: 0.35184693336486816\n",
      "Epoch 17, Batch 557, Test Loss: 0.4163724184036255\n",
      "Epoch 17, Batch 558, Test Loss: 0.4545164704322815\n",
      "Epoch 17, Batch 559, Test Loss: 0.4330076575279236\n",
      "Epoch 17, Batch 560, Test Loss: 0.38934558629989624\n",
      "Epoch 17, Batch 561, Test Loss: 0.41483479738235474\n",
      "Epoch 17, Batch 562, Test Loss: 0.2963659465312958\n",
      "Epoch 17, Batch 563, Test Loss: 0.5472245812416077\n",
      "Epoch 17, Batch 564, Test Loss: 0.263824462890625\n",
      "Epoch 17, Batch 565, Test Loss: 0.3329043388366699\n",
      "Epoch 17, Batch 566, Test Loss: 0.32307836413383484\n",
      "Epoch 17, Batch 567, Test Loss: 0.3742700219154358\n",
      "Epoch 17, Batch 568, Test Loss: 0.44166260957717896\n",
      "Epoch 17, Batch 569, Test Loss: 0.3935573697090149\n",
      "Epoch 17, Batch 570, Test Loss: 0.4368387758731842\n",
      "Epoch 17, Batch 571, Test Loss: 0.510132908821106\n",
      "Epoch 17, Batch 572, Test Loss: 0.28293123841285706\n",
      "Epoch 17, Batch 573, Test Loss: 0.3612196147441864\n",
      "Epoch 17, Batch 574, Test Loss: 0.33010998368263245\n",
      "Epoch 17, Batch 575, Test Loss: 0.8255297541618347\n",
      "Epoch 17, Batch 576, Test Loss: 0.4537982642650604\n",
      "Epoch 17, Batch 577, Test Loss: 0.3978932797908783\n",
      "Epoch 17, Batch 578, Test Loss: 0.4186834990978241\n",
      "Epoch 17, Batch 579, Test Loss: 0.5031518340110779\n",
      "Epoch 17, Batch 580, Test Loss: 0.5445215106010437\n",
      "Epoch 17, Batch 581, Test Loss: 0.4242083728313446\n",
      "Epoch 17, Batch 582, Test Loss: 0.45977693796157837\n",
      "Epoch 17, Batch 583, Test Loss: 0.279629111289978\n",
      "Epoch 17, Batch 584, Test Loss: 0.3603408932685852\n",
      "Epoch 17, Batch 585, Test Loss: 0.3203769028186798\n",
      "Epoch 17, Batch 586, Test Loss: 0.5878438949584961\n",
      "Epoch 17, Batch 587, Test Loss: 0.39576491713523865\n",
      "Epoch 17, Batch 588, Test Loss: 0.2846079170703888\n",
      "Epoch 17, Batch 589, Test Loss: 0.43871188163757324\n",
      "Epoch 17, Batch 590, Test Loss: 0.3047006130218506\n",
      "Epoch 17, Batch 591, Test Loss: 0.3879619240760803\n",
      "Epoch 17, Batch 592, Test Loss: 0.3885451555252075\n",
      "Epoch 17, Batch 593, Test Loss: 0.36125949025154114\n",
      "Epoch 17, Batch 594, Test Loss: 0.5078911781311035\n",
      "Epoch 17, Batch 595, Test Loss: 0.449255108833313\n",
      "Epoch 17, Batch 596, Test Loss: 0.431024968624115\n",
      "Epoch 17, Batch 597, Test Loss: 0.6741384267807007\n",
      "Epoch 17, Batch 598, Test Loss: 0.36059117317199707\n",
      "Epoch 17, Batch 599, Test Loss: 0.4434368908405304\n",
      "Epoch 17, Batch 600, Test Loss: 0.4803602993488312\n",
      "Epoch 17, Batch 601, Test Loss: 0.28196990489959717\n",
      "Epoch 17, Batch 602, Test Loss: 0.39386922121047974\n",
      "Epoch 17, Batch 603, Test Loss: 0.4616555869579315\n",
      "Epoch 17, Batch 604, Test Loss: 0.469446063041687\n",
      "Epoch 17, Batch 605, Test Loss: 0.2941705584526062\n",
      "Epoch 17, Batch 606, Test Loss: 0.46091604232788086\n",
      "Epoch 17, Batch 607, Test Loss: 0.4723828434944153\n",
      "Epoch 17, Batch 608, Test Loss: 0.3952961564064026\n",
      "Epoch 17, Batch 609, Test Loss: 0.26966869831085205\n",
      "Epoch 17, Batch 610, Test Loss: 0.4067489504814148\n",
      "Epoch 17, Batch 611, Test Loss: 0.40136420726776123\n",
      "Epoch 17, Batch 612, Test Loss: 0.37927332520484924\n",
      "Epoch 17, Batch 613, Test Loss: 0.5157961845397949\n",
      "Epoch 17, Batch 614, Test Loss: 0.45284587144851685\n",
      "Epoch 17, Batch 615, Test Loss: 0.5599814057350159\n",
      "Epoch 17, Batch 616, Test Loss: 0.34513604640960693\n",
      "Epoch 17, Batch 617, Test Loss: 0.4576253294944763\n",
      "Epoch 17, Batch 618, Test Loss: 0.3836042881011963\n",
      "Epoch 17, Batch 619, Test Loss: 0.4051872193813324\n",
      "Epoch 17, Batch 620, Test Loss: 0.37896960973739624\n",
      "Epoch 17, Batch 621, Test Loss: 0.3957050144672394\n",
      "Epoch 17, Batch 622, Test Loss: 0.43505722284317017\n",
      "Epoch 17, Batch 623, Test Loss: 0.5431851744651794\n",
      "Epoch 17, Batch 624, Test Loss: 0.5154610276222229\n",
      "Epoch 17, Batch 625, Test Loss: 0.4628603756427765\n",
      "Epoch 17, Batch 626, Test Loss: 0.40770167112350464\n",
      "Epoch 17, Batch 627, Test Loss: 0.36316975951194763\n",
      "Epoch 17, Batch 628, Test Loss: 0.3286500573158264\n",
      "Epoch 17, Batch 629, Test Loss: 0.5416625142097473\n",
      "Epoch 17, Batch 630, Test Loss: 0.35615310072898865\n",
      "Epoch 17, Batch 631, Test Loss: 0.41614586114883423\n",
      "Epoch 17, Batch 632, Test Loss: 0.34262555837631226\n",
      "Epoch 17, Batch 633, Test Loss: 0.24874666333198547\n",
      "Epoch 17, Batch 634, Test Loss: 0.4955345392227173\n",
      "Epoch 17, Batch 635, Test Loss: 0.43019765615463257\n",
      "Epoch 17, Batch 636, Test Loss: 0.29782408475875854\n",
      "Epoch 17, Batch 637, Test Loss: 0.5345669388771057\n",
      "Epoch 17, Batch 638, Test Loss: 0.4743618965148926\n",
      "Epoch 17, Batch 639, Test Loss: 0.5500873327255249\n",
      "Epoch 17, Batch 640, Test Loss: 0.46304720640182495\n",
      "Epoch 17, Batch 641, Test Loss: 0.4539342522621155\n",
      "Epoch 17, Batch 642, Test Loss: 0.2895375192165375\n",
      "Epoch 17, Batch 643, Test Loss: 0.38279807567596436\n",
      "Epoch 17, Batch 644, Test Loss: 0.424585223197937\n",
      "Epoch 17, Batch 645, Test Loss: 0.374942421913147\n",
      "Epoch 17, Batch 646, Test Loss: 0.4902295172214508\n",
      "Epoch 17, Batch 647, Test Loss: 0.30387720465660095\n",
      "Epoch 17, Batch 648, Test Loss: 0.5009952783584595\n",
      "Epoch 17, Batch 649, Test Loss: 0.4265297055244446\n",
      "Epoch 17, Batch 650, Test Loss: 0.4000895023345947\n",
      "Epoch 17, Batch 651, Test Loss: 0.4557764232158661\n",
      "Epoch 17, Batch 652, Test Loss: 0.34862449765205383\n",
      "Epoch 17, Batch 653, Test Loss: 0.4699421525001526\n",
      "Epoch 17, Batch 654, Test Loss: 0.4797030985355377\n",
      "Epoch 17, Batch 655, Test Loss: 0.41560670733451843\n",
      "Epoch 17, Batch 656, Test Loss: 0.7101766467094421\n",
      "Epoch 17, Batch 657, Test Loss: 0.5526126027107239\n",
      "Epoch 17, Batch 658, Test Loss: 0.6079518795013428\n",
      "Epoch 17, Batch 659, Test Loss: 0.2920963168144226\n",
      "Epoch 17, Batch 660, Test Loss: 0.44038712978363037\n",
      "Epoch 17, Batch 661, Test Loss: 0.4532445967197418\n",
      "Epoch 17, Batch 662, Test Loss: 0.434425413608551\n",
      "Epoch 17, Batch 663, Test Loss: 0.33750197291374207\n",
      "Epoch 17, Batch 664, Test Loss: 0.5009123682975769\n",
      "Epoch 17, Batch 665, Test Loss: 0.4501393139362335\n",
      "Epoch 17, Batch 666, Test Loss: 0.42978763580322266\n",
      "Epoch 17, Batch 667, Test Loss: 0.41166403889656067\n",
      "Epoch 17, Batch 668, Test Loss: 0.5053892731666565\n",
      "Epoch 17, Batch 669, Test Loss: 0.2936174273490906\n",
      "Epoch 17, Batch 670, Test Loss: 0.531446099281311\n",
      "Epoch 17, Batch 671, Test Loss: 0.3767428398132324\n",
      "Epoch 17, Batch 672, Test Loss: 0.22753429412841797\n",
      "Epoch 17, Batch 673, Test Loss: 0.3393797278404236\n",
      "Epoch 17, Batch 674, Test Loss: 0.3289909064769745\n",
      "Epoch 17, Batch 675, Test Loss: 0.3136473596096039\n",
      "Epoch 17, Batch 676, Test Loss: 0.5088599920272827\n",
      "Epoch 17, Batch 677, Test Loss: 0.49500593543052673\n",
      "Epoch 17, Batch 678, Test Loss: 0.5907642841339111\n",
      "Epoch 17, Batch 679, Test Loss: 0.29421889781951904\n",
      "Epoch 17, Batch 680, Test Loss: 0.5492295026779175\n",
      "Epoch 17, Batch 681, Test Loss: 0.4348364472389221\n",
      "Epoch 17, Batch 682, Test Loss: 0.5525960326194763\n",
      "Epoch 17, Batch 683, Test Loss: 0.4036044776439667\n",
      "Epoch 17, Batch 684, Test Loss: 0.4953937828540802\n",
      "Epoch 17, Batch 685, Test Loss: 0.4184359908103943\n",
      "Epoch 17, Batch 686, Test Loss: 0.2953372001647949\n",
      "Epoch 17, Batch 687, Test Loss: 0.49316659569740295\n",
      "Epoch 17, Batch 688, Test Loss: 0.44298216700553894\n",
      "Epoch 17, Batch 689, Test Loss: 0.6803845167160034\n",
      "Epoch 17, Batch 690, Test Loss: 0.4953489303588867\n",
      "Epoch 17, Batch 691, Test Loss: 0.48508507013320923\n",
      "Epoch 17, Batch 692, Test Loss: 0.4197138845920563\n",
      "Epoch 17, Batch 693, Test Loss: 0.42052286863327026\n",
      "Epoch 17, Batch 694, Test Loss: 0.3648492097854614\n",
      "Epoch 17, Batch 695, Test Loss: 0.39661461114883423\n",
      "Epoch 17, Batch 696, Test Loss: 0.33258047699928284\n",
      "Epoch 17, Batch 697, Test Loss: 0.35339438915252686\n",
      "Epoch 17, Batch 698, Test Loss: 0.44982650876045227\n",
      "Epoch 17, Batch 699, Test Loss: 0.3966670036315918\n",
      "Epoch 17, Batch 700, Test Loss: 0.43388575315475464\n",
      "Epoch 17, Batch 701, Test Loss: 0.46717846393585205\n",
      "Epoch 17, Batch 702, Test Loss: 0.35259589552879333\n",
      "Epoch 17, Batch 703, Test Loss: 0.5551449060440063\n",
      "Epoch 17, Batch 704, Test Loss: 0.22376315295696259\n",
      "Epoch 17, Batch 705, Test Loss: 0.37153130769729614\n",
      "Epoch 17, Batch 706, Test Loss: 0.5032374858856201\n",
      "Epoch 17, Batch 707, Test Loss: 0.35911378264427185\n",
      "Epoch 17, Batch 708, Test Loss: 0.4249707758426666\n",
      "Epoch 17, Batch 709, Test Loss: 0.3915984034538269\n",
      "Epoch 17, Batch 710, Test Loss: 0.31198641657829285\n",
      "Epoch 17, Batch 711, Test Loss: 0.2864760756492615\n",
      "Epoch 17, Batch 712, Test Loss: 0.5019410848617554\n",
      "Epoch 17, Batch 713, Test Loss: 0.4644169509410858\n",
      "Epoch 17, Batch 714, Test Loss: 0.24988949298858643\n",
      "Epoch 17, Batch 715, Test Loss: 0.3378840386867523\n",
      "Epoch 17, Batch 716, Test Loss: 0.39553576707839966\n",
      "Epoch 17, Batch 717, Test Loss: 0.28421199321746826\n",
      "Epoch 17, Batch 718, Test Loss: 0.3814370930194855\n",
      "Epoch 17, Batch 719, Test Loss: 0.44534265995025635\n",
      "Epoch 17, Batch 720, Test Loss: 0.4613502025604248\n",
      "Epoch 17, Batch 721, Test Loss: 0.36287635564804077\n",
      "Epoch 17, Batch 722, Test Loss: 0.35557612776756287\n",
      "Epoch 17, Batch 723, Test Loss: 0.39950358867645264\n",
      "Epoch 17, Batch 724, Test Loss: 0.5540832281112671\n",
      "Epoch 17, Batch 725, Test Loss: 0.3548279404640198\n",
      "Epoch 17, Batch 726, Test Loss: 0.43035006523132324\n",
      "Epoch 17, Batch 727, Test Loss: 0.2959447503089905\n",
      "Epoch 17, Batch 728, Test Loss: 0.3971869647502899\n",
      "Epoch 17, Batch 729, Test Loss: 0.3599407374858856\n",
      "Epoch 17, Batch 730, Test Loss: 0.5383749604225159\n",
      "Epoch 17, Batch 731, Test Loss: 0.4051058888435364\n",
      "Epoch 17, Batch 732, Test Loss: 0.5222411751747131\n",
      "Epoch 17, Batch 733, Test Loss: 0.2614045739173889\n",
      "Epoch 17, Batch 734, Test Loss: 0.4817267060279846\n",
      "Epoch 17, Batch 735, Test Loss: 0.3305854797363281\n",
      "Epoch 17, Batch 736, Test Loss: 0.43946701288223267\n",
      "Epoch 17, Batch 737, Test Loss: 0.45188307762145996\n",
      "Epoch 17, Batch 738, Test Loss: 0.5384111404418945\n",
      "Epoch 17, Batch 739, Test Loss: 0.31929197907447815\n",
      "Epoch 17, Batch 740, Test Loss: 0.43285611271858215\n",
      "Epoch 17, Batch 741, Test Loss: 0.24118182063102722\n",
      "Epoch 17, Batch 742, Test Loss: 0.40802645683288574\n",
      "Epoch 17, Batch 743, Test Loss: 0.43546125292778015\n",
      "Epoch 17, Batch 744, Test Loss: 0.43648219108581543\n",
      "Epoch 17, Batch 745, Test Loss: 0.37179163098335266\n",
      "Epoch 17, Batch 746, Test Loss: 0.5417163968086243\n",
      "Epoch 17, Batch 747, Test Loss: 0.3546571433544159\n",
      "Epoch 17, Batch 748, Test Loss: 0.3493833541870117\n",
      "Epoch 17, Batch 749, Test Loss: 0.3294469118118286\n",
      "Epoch 17, Batch 750, Test Loss: 0.40224552154541016\n",
      "Epoch 17, Batch 751, Test Loss: 0.3232198655605316\n",
      "Epoch 17, Batch 752, Test Loss: 0.402303546667099\n",
      "Epoch 17, Batch 753, Test Loss: 0.3125841021537781\n",
      "Epoch 17, Batch 754, Test Loss: 0.3230745196342468\n",
      "Epoch 17, Batch 755, Test Loss: 0.422313928604126\n",
      "Epoch 17, Batch 756, Test Loss: 0.5083553791046143\n",
      "Epoch 17, Batch 757, Test Loss: 0.28229495882987976\n",
      "Epoch 17, Batch 758, Test Loss: 0.39855077862739563\n",
      "Epoch 17, Batch 759, Test Loss: 0.474604994058609\n",
      "Epoch 17, Batch 760, Test Loss: 0.5350216031074524\n",
      "Epoch 17, Batch 761, Test Loss: 0.4990141987800598\n",
      "Epoch 17, Batch 762, Test Loss: 0.3044588565826416\n",
      "Epoch 17, Batch 763, Test Loss: 0.3673701882362366\n",
      "Epoch 17, Batch 764, Test Loss: 0.518309473991394\n",
      "Epoch 17, Batch 765, Test Loss: 0.4035695791244507\n",
      "Epoch 17, Batch 766, Test Loss: 0.3501342236995697\n",
      "Epoch 17, Batch 767, Test Loss: 0.6226364374160767\n",
      "Epoch 17, Batch 768, Test Loss: 0.3386650085449219\n",
      "Epoch 17, Batch 769, Test Loss: 0.43373903632164\n",
      "Epoch 17, Batch 770, Test Loss: 0.3580564856529236\n",
      "Epoch 17, Batch 771, Test Loss: 0.5083602070808411\n",
      "Epoch 17, Batch 772, Test Loss: 0.4411965310573578\n",
      "Epoch 17, Batch 773, Test Loss: 0.4398941099643707\n",
      "Epoch 17, Batch 774, Test Loss: 0.3631054162979126\n",
      "Epoch 17, Batch 775, Test Loss: 0.4246722161769867\n",
      "Epoch 17, Batch 776, Test Loss: 0.4386982321739197\n",
      "Epoch 17, Batch 777, Test Loss: 0.27791130542755127\n",
      "Epoch 17, Batch 778, Test Loss: 0.4709621071815491\n",
      "Epoch 17, Batch 779, Test Loss: 0.36355528235435486\n",
      "Epoch 17, Batch 780, Test Loss: 0.5013017058372498\n",
      "Epoch 17, Batch 781, Test Loss: 0.46791204810142517\n",
      "Epoch 17, Batch 782, Test Loss: 0.4890928268432617\n",
      "Epoch 17, Batch 783, Test Loss: 0.3105933368206024\n",
      "Epoch 17, Batch 784, Test Loss: 0.36418166756629944\n",
      "Epoch 17, Batch 785, Test Loss: 0.37260401248931885\n",
      "Epoch 17, Batch 786, Test Loss: 0.4689277410507202\n",
      "Epoch 17, Batch 787, Test Loss: 0.686211347579956\n",
      "Epoch 17, Batch 788, Test Loss: 0.3581872284412384\n",
      "Epoch 17, Batch 789, Test Loss: 0.42217037081718445\n",
      "Epoch 17, Batch 790, Test Loss: 0.3645486533641815\n",
      "Epoch 17, Batch 791, Test Loss: 0.5562541484832764\n",
      "Epoch 17, Batch 792, Test Loss: 0.8571751117706299\n",
      "Epoch 17, Batch 793, Test Loss: 0.43002134561538696\n",
      "Epoch 17, Batch 794, Test Loss: 0.5040125250816345\n",
      "Epoch 17, Batch 795, Test Loss: 0.33843329548835754\n",
      "Epoch 17, Batch 796, Test Loss: 0.42386114597320557\n",
      "Epoch 17, Batch 797, Test Loss: 0.33984270691871643\n",
      "Epoch 17, Batch 798, Test Loss: 0.24402642250061035\n",
      "Epoch 17, Batch 799, Test Loss: 0.3872735798358917\n",
      "Epoch 17, Batch 800, Test Loss: 0.5524413585662842\n",
      "Epoch 17, Batch 801, Test Loss: 0.4854559302330017\n",
      "Epoch 17, Batch 802, Test Loss: 0.4475950598716736\n",
      "Epoch 17, Batch 803, Test Loss: 0.37569501996040344\n",
      "Epoch 17, Batch 804, Test Loss: 0.38847243785858154\n",
      "Epoch 17, Batch 805, Test Loss: 0.37143731117248535\n",
      "Epoch 17, Batch 806, Test Loss: 0.42225009202957153\n",
      "Epoch 17, Batch 807, Test Loss: 0.3258852958679199\n",
      "Epoch 17, Batch 808, Test Loss: 0.43982070684432983\n",
      "Epoch 17, Batch 809, Test Loss: 0.3697628080844879\n",
      "Epoch 17, Batch 810, Test Loss: 0.46997368335723877\n",
      "Epoch 17, Batch 811, Test Loss: 0.491013765335083\n",
      "Epoch 17, Batch 812, Test Loss: 0.44441577792167664\n",
      "Epoch 17, Batch 813, Test Loss: 0.4910701513290405\n",
      "Epoch 17, Batch 814, Test Loss: 0.31590667366981506\n",
      "Epoch 17, Batch 815, Test Loss: 0.4149572551250458\n",
      "Epoch 17, Batch 816, Test Loss: 0.32125675678253174\n",
      "Epoch 17, Batch 817, Test Loss: 0.4437677264213562\n",
      "Epoch 17, Batch 818, Test Loss: 0.4813891053199768\n",
      "Epoch 17, Batch 819, Test Loss: 0.24895772337913513\n",
      "Epoch 17, Batch 820, Test Loss: 0.28089404106140137\n",
      "Epoch 17, Batch 821, Test Loss: 0.4655705690383911\n",
      "Epoch 17, Batch 822, Test Loss: 0.45021557807922363\n",
      "Epoch 17, Batch 823, Test Loss: 0.41495516896247864\n",
      "Epoch 17, Batch 824, Test Loss: 0.6174530982971191\n",
      "Epoch 17, Batch 825, Test Loss: 0.37522777915000916\n",
      "Epoch 17, Batch 826, Test Loss: 0.33818262815475464\n",
      "Epoch 17, Batch 827, Test Loss: 0.5056940913200378\n",
      "Epoch 17, Batch 828, Test Loss: 0.48999956250190735\n",
      "Epoch 17, Batch 829, Test Loss: 0.35652992129325867\n",
      "Epoch 17, Batch 830, Test Loss: 0.40659773349761963\n",
      "Epoch 17, Batch 831, Test Loss: 0.571083664894104\n",
      "Epoch 17, Batch 832, Test Loss: 0.27250930666923523\n",
      "Epoch 17, Batch 833, Test Loss: 0.3984129726886749\n",
      "Epoch 17, Batch 834, Test Loss: 0.5141664147377014\n",
      "Epoch 17, Batch 835, Test Loss: 0.4929742217063904\n",
      "Epoch 17, Batch 836, Test Loss: 0.40395671129226685\n",
      "Epoch 17, Batch 837, Test Loss: 0.48597684502601624\n",
      "Epoch 17, Batch 838, Test Loss: 0.4878755807876587\n",
      "Epoch 17, Batch 839, Test Loss: 0.41661128401756287\n",
      "Epoch 17, Batch 840, Test Loss: 0.33683517575263977\n",
      "Epoch 17, Batch 841, Test Loss: 0.42162400484085083\n",
      "Epoch 17, Batch 842, Test Loss: 0.2931743264198303\n",
      "Epoch 17, Batch 843, Test Loss: 0.5188507437705994\n",
      "Epoch 17, Batch 844, Test Loss: 0.48458370566368103\n",
      "Epoch 17, Batch 845, Test Loss: 0.26946204900741577\n",
      "Epoch 17, Batch 846, Test Loss: 0.4315396547317505\n",
      "Epoch 17, Batch 847, Test Loss: 0.3693847954273224\n",
      "Epoch 17, Batch 848, Test Loss: 0.3336714506149292\n",
      "Epoch 17, Batch 849, Test Loss: 0.4725118577480316\n",
      "Epoch 17, Batch 850, Test Loss: 0.5722018480300903\n",
      "Epoch 17, Batch 851, Test Loss: 0.42827269434928894\n",
      "Epoch 17, Batch 852, Test Loss: 0.4967832863330841\n",
      "Epoch 17, Batch 853, Test Loss: 0.4937977194786072\n",
      "Epoch 17, Batch 854, Test Loss: 0.5029920339584351\n",
      "Epoch 17, Batch 855, Test Loss: 0.3326735496520996\n",
      "Epoch 17, Batch 856, Test Loss: 0.4216921627521515\n",
      "Epoch 17, Batch 857, Test Loss: 0.3338177502155304\n",
      "Epoch 17, Batch 858, Test Loss: 0.3670099079608917\n",
      "Epoch 17, Batch 859, Test Loss: 0.29370421171188354\n",
      "Epoch 17, Batch 860, Test Loss: 0.49268850684165955\n",
      "Epoch 17, Batch 861, Test Loss: 0.49438533186912537\n",
      "Epoch 17, Batch 862, Test Loss: 0.48597121238708496\n",
      "Epoch 17, Batch 863, Test Loss: 0.4719981253147125\n",
      "Epoch 17, Batch 864, Test Loss: 0.48983970284461975\n",
      "Epoch 17, Batch 865, Test Loss: 0.5139973759651184\n",
      "Epoch 17, Batch 866, Test Loss: 0.38567671179771423\n",
      "Epoch 17, Batch 867, Test Loss: 0.25250181555747986\n",
      "Epoch 17, Batch 868, Test Loss: 0.24277207255363464\n",
      "Epoch 17, Batch 869, Test Loss: 0.5232124328613281\n",
      "Epoch 17, Batch 870, Test Loss: 0.41580307483673096\n",
      "Epoch 17, Batch 871, Test Loss: 0.4549749195575714\n",
      "Epoch 17, Batch 872, Test Loss: 0.40300461649894714\n",
      "Epoch 17, Batch 873, Test Loss: 0.32419851422309875\n",
      "Epoch 17, Batch 874, Test Loss: 0.6099810004234314\n",
      "Epoch 17, Batch 875, Test Loss: 0.5453430414199829\n",
      "Epoch 17, Batch 876, Test Loss: 0.5655009150505066\n",
      "Epoch 17, Batch 877, Test Loss: 0.3598995804786682\n",
      "Epoch 17, Batch 878, Test Loss: 0.2876456677913666\n",
      "Epoch 17, Batch 879, Test Loss: 0.3698848783969879\n",
      "Epoch 17, Batch 880, Test Loss: 0.49931997060775757\n",
      "Epoch 17, Batch 881, Test Loss: 0.37994250655174255\n",
      "Epoch 17, Batch 882, Test Loss: 0.4162006378173828\n",
      "Epoch 17, Batch 883, Test Loss: 0.46402421593666077\n",
      "Epoch 17, Batch 884, Test Loss: 0.4252563416957855\n",
      "Epoch 17, Batch 885, Test Loss: 0.364485502243042\n",
      "Epoch 17, Batch 886, Test Loss: 0.337726354598999\n",
      "Epoch 17, Batch 887, Test Loss: 0.4881017804145813\n",
      "Epoch 17, Batch 888, Test Loss: 0.5361667275428772\n",
      "Epoch 17, Batch 889, Test Loss: 0.33664584159851074\n",
      "Epoch 17, Batch 890, Test Loss: 0.44940072298049927\n",
      "Epoch 17, Batch 891, Test Loss: 0.39275988936424255\n",
      "Epoch 17, Batch 892, Test Loss: 0.29074281454086304\n",
      "Epoch 17, Batch 893, Test Loss: 0.2859499156475067\n",
      "Epoch 17, Batch 894, Test Loss: 0.43026432394981384\n",
      "Epoch 17, Batch 895, Test Loss: 0.5342954397201538\n",
      "Epoch 17, Batch 896, Test Loss: 0.3864813446998596\n",
      "Epoch 17, Batch 897, Test Loss: 0.4308120608329773\n",
      "Epoch 17, Batch 898, Test Loss: 0.4944074749946594\n",
      "Epoch 17, Batch 899, Test Loss: 0.4168117344379425\n",
      "Epoch 17, Batch 900, Test Loss: 0.46797263622283936\n",
      "Epoch 17, Batch 901, Test Loss: 0.4181627929210663\n",
      "Epoch 17, Batch 902, Test Loss: 0.3269808888435364\n",
      "Epoch 17, Batch 903, Test Loss: 0.5379685163497925\n",
      "Epoch 17, Batch 904, Test Loss: 0.474283903837204\n",
      "Epoch 17, Batch 905, Test Loss: 0.504011332988739\n",
      "Epoch 17, Batch 906, Test Loss: 0.278687983751297\n",
      "Epoch 17, Batch 907, Test Loss: 0.44173866510391235\n",
      "Epoch 17, Batch 908, Test Loss: 0.42929717898368835\n",
      "Epoch 17, Batch 909, Test Loss: 0.427675724029541\n",
      "Epoch 17, Batch 910, Test Loss: 0.4326411187648773\n",
      "Epoch 17, Batch 911, Test Loss: 0.3921271860599518\n",
      "Epoch 17, Batch 912, Test Loss: 0.4302401542663574\n",
      "Epoch 17, Batch 913, Test Loss: 0.46815821528434753\n",
      "Epoch 17, Batch 914, Test Loss: 0.3866824805736542\n",
      "Epoch 17, Batch 915, Test Loss: 0.2543300688266754\n",
      "Epoch 17, Batch 916, Test Loss: 0.23121018707752228\n",
      "Epoch 17, Batch 917, Test Loss: 0.34513044357299805\n",
      "Epoch 17, Batch 918, Test Loss: 0.5331342220306396\n",
      "Epoch 17, Batch 919, Test Loss: 0.39729154109954834\n",
      "Epoch 17, Batch 920, Test Loss: 0.38304421305656433\n",
      "Epoch 17, Batch 921, Test Loss: 0.38566556572914124\n",
      "Epoch 17, Batch 922, Test Loss: 0.6395897269248962\n",
      "Epoch 17, Batch 923, Test Loss: 0.3328482508659363\n",
      "Epoch 17, Batch 924, Test Loss: 0.39493727684020996\n",
      "Epoch 17, Batch 925, Test Loss: 0.5291493535041809\n",
      "Epoch 17, Batch 926, Test Loss: 0.4812411069869995\n",
      "Epoch 17, Batch 927, Test Loss: 0.25382980704307556\n",
      "Epoch 17, Batch 928, Test Loss: 0.6098501682281494\n",
      "Epoch 17, Batch 929, Test Loss: 0.4186875820159912\n",
      "Epoch 17, Batch 930, Test Loss: 0.4932604730129242\n",
      "Epoch 17, Batch 931, Test Loss: 0.5975990891456604\n",
      "Epoch 17, Batch 932, Test Loss: 0.8042888045310974\n",
      "Epoch 17, Batch 933, Test Loss: 0.33670854568481445\n",
      "Epoch 17, Batch 934, Test Loss: 0.34946420788764954\n",
      "Epoch 17, Batch 935, Test Loss: 0.5196247100830078\n",
      "Epoch 17, Batch 936, Test Loss: 0.39420434832572937\n",
      "Epoch 17, Batch 937, Test Loss: 0.3396393358707428\n",
      "Epoch 17, Batch 938, Test Loss: 0.37278521060943604\n",
      "Accuracy of Test set: 0.84855\n",
      "Epoch 18, Batch 1, Loss: 0.41333097219467163\n",
      "Epoch 18, Batch 2, Loss: 0.668042004108429\n",
      "Epoch 18, Batch 3, Loss: 0.5247123837471008\n",
      "Epoch 18, Batch 4, Loss: 0.42038393020629883\n",
      "Epoch 18, Batch 5, Loss: 0.3526611924171448\n",
      "Epoch 18, Batch 6, Loss: 0.5744489431381226\n",
      "Epoch 18, Batch 7, Loss: 0.5158307552337646\n",
      "Epoch 18, Batch 8, Loss: 0.38897573947906494\n",
      "Epoch 18, Batch 9, Loss: 0.39292246103286743\n",
      "Epoch 18, Batch 10, Loss: 0.39553651213645935\n",
      "Epoch 18, Batch 11, Loss: 0.5260950326919556\n",
      "Epoch 18, Batch 12, Loss: 0.29079824686050415\n",
      "Epoch 18, Batch 13, Loss: 0.3350440263748169\n",
      "Epoch 18, Batch 14, Loss: 0.6167473793029785\n",
      "Epoch 18, Batch 15, Loss: 0.36978068947792053\n",
      "Epoch 18, Batch 16, Loss: 0.18153086304664612\n",
      "Epoch 18, Batch 17, Loss: 0.3075103759765625\n",
      "Epoch 18, Batch 18, Loss: 0.29251590371131897\n",
      "Epoch 18, Batch 19, Loss: 0.4158824384212494\n",
      "Epoch 18, Batch 20, Loss: 0.37693077325820923\n",
      "Epoch 18, Batch 21, Loss: 0.4295935034751892\n",
      "Epoch 18, Batch 22, Loss: 0.39476069808006287\n",
      "Epoch 18, Batch 23, Loss: 0.6078038215637207\n",
      "Epoch 18, Batch 24, Loss: 0.31853818893432617\n",
      "Epoch 18, Batch 25, Loss: 0.2970208525657654\n",
      "Epoch 18, Batch 26, Loss: 0.6487129926681519\n",
      "Epoch 18, Batch 27, Loss: 0.452395498752594\n",
      "Epoch 18, Batch 28, Loss: 0.3627207577228546\n",
      "Epoch 18, Batch 29, Loss: 0.561516523361206\n",
      "Epoch 18, Batch 30, Loss: 0.41701993346214294\n",
      "Epoch 18, Batch 31, Loss: 0.34450119733810425\n",
      "Epoch 18, Batch 32, Loss: 0.5853848457336426\n",
      "Epoch 18, Batch 33, Loss: 0.4023197889328003\n",
      "Epoch 18, Batch 34, Loss: 0.3683600425720215\n",
      "Epoch 18, Batch 35, Loss: 0.4081367254257202\n",
      "Epoch 18, Batch 36, Loss: 0.29314276576042175\n",
      "Epoch 18, Batch 37, Loss: 0.33687978982925415\n",
      "Epoch 18, Batch 38, Loss: 0.37980517745018005\n",
      "Epoch 18, Batch 39, Loss: 0.5448581576347351\n",
      "Epoch 18, Batch 40, Loss: 0.32724303007125854\n",
      "Epoch 18, Batch 41, Loss: 0.2982146739959717\n",
      "Epoch 18, Batch 42, Loss: 0.3691931664943695\n",
      "Epoch 18, Batch 43, Loss: 0.5246281623840332\n",
      "Epoch 18, Batch 44, Loss: 0.36658334732055664\n",
      "Epoch 18, Batch 45, Loss: 0.3309932351112366\n",
      "Epoch 18, Batch 46, Loss: 0.6120166778564453\n",
      "Epoch 18, Batch 47, Loss: 0.3818146586418152\n",
      "Epoch 18, Batch 48, Loss: 0.7079400420188904\n",
      "Epoch 18, Batch 49, Loss: 0.3526070713996887\n",
      "Epoch 18, Batch 50, Loss: 0.4568978250026703\n",
      "Epoch 18, Batch 51, Loss: 0.45819035172462463\n",
      "Epoch 18, Batch 52, Loss: 0.374143123626709\n",
      "Epoch 18, Batch 53, Loss: 0.41897502541542053\n",
      "Epoch 18, Batch 54, Loss: 0.47558483481407166\n",
      "Epoch 18, Batch 55, Loss: 0.3689202666282654\n",
      "Epoch 18, Batch 56, Loss: 0.5743900537490845\n",
      "Epoch 18, Batch 57, Loss: 0.4317275583744049\n",
      "Epoch 18, Batch 58, Loss: 0.43974557518959045\n",
      "Epoch 18, Batch 59, Loss: 0.6216765642166138\n",
      "Epoch 18, Batch 60, Loss: 0.40473976731300354\n",
      "Epoch 18, Batch 61, Loss: 0.49923816323280334\n",
      "Epoch 18, Batch 62, Loss: 0.39511844515800476\n",
      "Epoch 18, Batch 63, Loss: 0.4172591269016266\n",
      "Epoch 18, Batch 64, Loss: 0.3692304491996765\n",
      "Epoch 18, Batch 65, Loss: 0.3795009255409241\n",
      "Epoch 18, Batch 66, Loss: 0.503316342830658\n",
      "Epoch 18, Batch 67, Loss: 0.5860541462898254\n",
      "Epoch 18, Batch 68, Loss: 0.5575896501541138\n",
      "Epoch 18, Batch 69, Loss: 0.34693777561187744\n",
      "Epoch 18, Batch 70, Loss: 0.2333066612482071\n",
      "Epoch 18, Batch 71, Loss: 0.39137959480285645\n",
      "Epoch 18, Batch 72, Loss: 0.5593345761299133\n",
      "Epoch 18, Batch 73, Loss: 0.3853829503059387\n",
      "Epoch 18, Batch 74, Loss: 0.3976120352745056\n",
      "Epoch 18, Batch 75, Loss: 0.5334223508834839\n",
      "Epoch 18, Batch 76, Loss: 0.5035558938980103\n",
      "Epoch 18, Batch 77, Loss: 0.3949616551399231\n",
      "Epoch 18, Batch 78, Loss: 0.39639413356781006\n",
      "Epoch 18, Batch 79, Loss: 0.4593241512775421\n",
      "Epoch 18, Batch 80, Loss: 0.6792566776275635\n",
      "Epoch 18, Batch 81, Loss: 0.40046021342277527\n",
      "Epoch 18, Batch 82, Loss: 0.37703937292099\n",
      "Epoch 18, Batch 83, Loss: 0.3572055697441101\n",
      "Epoch 18, Batch 84, Loss: 0.26695743203163147\n",
      "Epoch 18, Batch 85, Loss: 0.5159007906913757\n",
      "Epoch 18, Batch 86, Loss: 0.3131524324417114\n",
      "Epoch 18, Batch 87, Loss: 0.46832847595214844\n",
      "Epoch 18, Batch 88, Loss: 0.33569878339767456\n",
      "Epoch 18, Batch 89, Loss: 0.3580849766731262\n",
      "Epoch 18, Batch 90, Loss: 0.6653066277503967\n",
      "Epoch 18, Batch 91, Loss: 0.5239915251731873\n",
      "Epoch 18, Batch 92, Loss: 0.32845431566238403\n",
      "Epoch 18, Batch 93, Loss: 0.333744615316391\n",
      "Epoch 18, Batch 94, Loss: 0.20616832375526428\n",
      "Epoch 18, Batch 95, Loss: 0.31074872612953186\n",
      "Epoch 18, Batch 96, Loss: 0.31926730275154114\n",
      "Epoch 18, Batch 97, Loss: 0.48625221848487854\n",
      "Epoch 18, Batch 98, Loss: 0.42250096797943115\n",
      "Epoch 18, Batch 99, Loss: 0.3719621002674103\n",
      "Epoch 18, Batch 100, Loss: 0.4159238338470459\n",
      "Epoch 18, Batch 101, Loss: 0.4921591579914093\n",
      "Epoch 18, Batch 102, Loss: 0.3486064374446869\n",
      "Epoch 18, Batch 103, Loss: 0.3922156095504761\n",
      "Epoch 18, Batch 104, Loss: 0.5057048201560974\n",
      "Epoch 18, Batch 105, Loss: 0.5178870558738708\n",
      "Epoch 18, Batch 106, Loss: 0.4818209409713745\n",
      "Epoch 18, Batch 107, Loss: 0.44472020864486694\n",
      "Epoch 18, Batch 108, Loss: 0.2821202874183655\n",
      "Epoch 18, Batch 109, Loss: 0.4929988980293274\n",
      "Epoch 18, Batch 110, Loss: 0.33001816272735596\n",
      "Epoch 18, Batch 111, Loss: 0.5209333300590515\n",
      "Epoch 18, Batch 112, Loss: 0.4454405903816223\n",
      "Epoch 18, Batch 113, Loss: 0.3914225101470947\n",
      "Epoch 18, Batch 114, Loss: 0.5189154744148254\n",
      "Epoch 18, Batch 115, Loss: 0.4979478120803833\n",
      "Epoch 18, Batch 116, Loss: 0.3108115792274475\n",
      "Epoch 18, Batch 117, Loss: 0.42186105251312256\n",
      "Epoch 18, Batch 118, Loss: 0.5032325983047485\n",
      "Epoch 18, Batch 119, Loss: 0.38879454135894775\n",
      "Epoch 18, Batch 120, Loss: 0.5908154845237732\n",
      "Epoch 18, Batch 121, Loss: 0.3388170599937439\n",
      "Epoch 18, Batch 122, Loss: 0.4894016981124878\n",
      "Epoch 18, Batch 123, Loss: 0.4307307302951813\n",
      "Epoch 18, Batch 124, Loss: 0.6557439565658569\n",
      "Epoch 18, Batch 125, Loss: 0.36914199590682983\n",
      "Epoch 18, Batch 126, Loss: 0.41989755630493164\n",
      "Epoch 18, Batch 127, Loss: 0.30828574299812317\n",
      "Epoch 18, Batch 128, Loss: 0.3966957926750183\n",
      "Epoch 18, Batch 129, Loss: 0.375519722700119\n",
      "Epoch 18, Batch 130, Loss: 0.3575601875782013\n",
      "Epoch 18, Batch 131, Loss: 0.3939957320690155\n",
      "Epoch 18, Batch 132, Loss: 0.5498936772346497\n",
      "Epoch 18, Batch 133, Loss: 0.42030787467956543\n",
      "Epoch 18, Batch 134, Loss: 0.3430951237678528\n",
      "Epoch 18, Batch 135, Loss: 0.49600425362586975\n",
      "Epoch 18, Batch 136, Loss: 0.5615951418876648\n",
      "Epoch 18, Batch 137, Loss: 0.6190363168716431\n",
      "Epoch 18, Batch 138, Loss: 0.48685964941978455\n",
      "Epoch 18, Batch 139, Loss: 0.5118405818939209\n",
      "Epoch 18, Batch 140, Loss: 0.36462199687957764\n",
      "Epoch 18, Batch 141, Loss: 0.39738160371780396\n",
      "Epoch 18, Batch 142, Loss: 0.3941291272640228\n",
      "Epoch 18, Batch 143, Loss: 0.28591662645339966\n",
      "Epoch 18, Batch 144, Loss: 0.517301619052887\n",
      "Epoch 18, Batch 145, Loss: 0.2935837507247925\n",
      "Epoch 18, Batch 146, Loss: 0.4708455204963684\n",
      "Epoch 18, Batch 147, Loss: 0.592438280582428\n",
      "Epoch 18, Batch 148, Loss: 0.4711645543575287\n",
      "Epoch 18, Batch 149, Loss: 0.5659015774726868\n",
      "Epoch 18, Batch 150, Loss: 0.507604718208313\n",
      "Epoch 18, Batch 151, Loss: 0.4564182460308075\n",
      "Epoch 18, Batch 152, Loss: 0.5633785724639893\n",
      "Epoch 18, Batch 153, Loss: 0.33010685443878174\n",
      "Epoch 18, Batch 154, Loss: 0.591832160949707\n",
      "Epoch 18, Batch 155, Loss: 0.33821579813957214\n",
      "Epoch 18, Batch 156, Loss: 0.48737531900405884\n",
      "Epoch 18, Batch 157, Loss: 0.4173341691493988\n",
      "Epoch 18, Batch 158, Loss: 0.4028562307357788\n",
      "Epoch 18, Batch 159, Loss: 0.42691028118133545\n",
      "Epoch 18, Batch 160, Loss: 0.6480737328529358\n",
      "Epoch 18, Batch 161, Loss: 0.527001678943634\n",
      "Epoch 18, Batch 162, Loss: 0.4317670464515686\n",
      "Epoch 18, Batch 163, Loss: 0.449322372674942\n",
      "Epoch 18, Batch 164, Loss: 0.7950223684310913\n",
      "Epoch 18, Batch 165, Loss: 0.44196927547454834\n",
      "Epoch 18, Batch 166, Loss: 0.2756958603858948\n",
      "Epoch 18, Batch 167, Loss: 0.33181506395339966\n",
      "Epoch 18, Batch 168, Loss: 0.46141234040260315\n",
      "Epoch 18, Batch 169, Loss: 0.41739171743392944\n",
      "Epoch 18, Batch 170, Loss: 0.4439662992954254\n",
      "Epoch 18, Batch 171, Loss: 0.3004497289657593\n",
      "Epoch 18, Batch 172, Loss: 0.34317946434020996\n",
      "Epoch 18, Batch 173, Loss: 0.35977378487586975\n",
      "Epoch 18, Batch 174, Loss: 0.3932941257953644\n",
      "Epoch 18, Batch 175, Loss: 0.8220888376235962\n",
      "Epoch 18, Batch 176, Loss: 0.4863460063934326\n",
      "Epoch 18, Batch 177, Loss: 0.38989290595054626\n",
      "Epoch 18, Batch 178, Loss: 0.6595085263252258\n",
      "Epoch 18, Batch 179, Loss: 0.4020202159881592\n",
      "Epoch 18, Batch 180, Loss: 0.5968256592750549\n",
      "Epoch 18, Batch 181, Loss: 0.5207929015159607\n",
      "Epoch 18, Batch 182, Loss: 0.4216289818286896\n",
      "Epoch 18, Batch 183, Loss: 0.4025690257549286\n",
      "Epoch 18, Batch 184, Loss: 0.4832282066345215\n",
      "Epoch 18, Batch 185, Loss: 0.5481485724449158\n",
      "Epoch 18, Batch 186, Loss: 0.41742613911628723\n",
      "Epoch 18, Batch 187, Loss: 0.33017587661743164\n",
      "Epoch 18, Batch 188, Loss: 0.43403372168540955\n",
      "Epoch 18, Batch 189, Loss: 0.642402172088623\n",
      "Epoch 18, Batch 190, Loss: 0.26078951358795166\n",
      "Epoch 18, Batch 191, Loss: 0.6891793608665466\n",
      "Epoch 18, Batch 192, Loss: 0.3844439685344696\n",
      "Epoch 18, Batch 193, Loss: 0.3740522563457489\n",
      "Epoch 18, Batch 194, Loss: 0.4447559118270874\n",
      "Epoch 18, Batch 195, Loss: 0.4145577549934387\n",
      "Epoch 18, Batch 196, Loss: 0.3173355460166931\n",
      "Epoch 18, Batch 197, Loss: 0.3439440429210663\n",
      "Epoch 18, Batch 198, Loss: 0.33421385288238525\n",
      "Epoch 18, Batch 199, Loss: 0.3318844437599182\n",
      "Epoch 18, Batch 200, Loss: 0.41284358501434326\n",
      "Epoch 18, Batch 201, Loss: 0.3363710641860962\n",
      "Epoch 18, Batch 202, Loss: 0.510561466217041\n",
      "Epoch 18, Batch 203, Loss: 0.43253540992736816\n",
      "Epoch 18, Batch 204, Loss: 0.7213326096534729\n",
      "Epoch 18, Batch 205, Loss: 0.4926716983318329\n",
      "Epoch 18, Batch 206, Loss: 0.4517875015735626\n",
      "Epoch 18, Batch 207, Loss: 0.4386606216430664\n",
      "Epoch 18, Batch 208, Loss: 0.38004451990127563\n",
      "Epoch 18, Batch 209, Loss: 0.3415738642215729\n",
      "Epoch 18, Batch 210, Loss: 0.5542051196098328\n",
      "Epoch 18, Batch 211, Loss: 0.44698095321655273\n",
      "Epoch 18, Batch 212, Loss: 0.3115864396095276\n",
      "Epoch 18, Batch 213, Loss: 0.3318368196487427\n",
      "Epoch 18, Batch 214, Loss: 0.36045578122138977\n",
      "Epoch 18, Batch 215, Loss: 0.35162779688835144\n",
      "Epoch 18, Batch 216, Loss: 0.5428900718688965\n",
      "Epoch 18, Batch 217, Loss: 0.4702187776565552\n",
      "Epoch 18, Batch 218, Loss: 0.39972418546676636\n",
      "Epoch 18, Batch 219, Loss: 0.48491203784942627\n",
      "Epoch 18, Batch 220, Loss: 0.4800317883491516\n",
      "Epoch 18, Batch 221, Loss: 0.48399239778518677\n",
      "Epoch 18, Batch 222, Loss: 0.34916192293167114\n",
      "Epoch 18, Batch 223, Loss: 0.5043057799339294\n",
      "Epoch 18, Batch 224, Loss: 0.6877639889717102\n",
      "Epoch 18, Batch 225, Loss: 0.39519569277763367\n",
      "Epoch 18, Batch 226, Loss: 0.36554330587387085\n",
      "Epoch 18, Batch 227, Loss: 0.3492810130119324\n",
      "Epoch 18, Batch 228, Loss: 0.3810552954673767\n",
      "Epoch 18, Batch 229, Loss: 0.3035220205783844\n",
      "Epoch 18, Batch 230, Loss: 0.7972556352615356\n",
      "Epoch 18, Batch 231, Loss: 0.4804784059524536\n",
      "Epoch 18, Batch 232, Loss: 0.35956794023513794\n",
      "Epoch 18, Batch 233, Loss: 0.39650171995162964\n",
      "Epoch 18, Batch 234, Loss: 0.4234941899776459\n",
      "Epoch 18, Batch 235, Loss: 0.514286994934082\n",
      "Epoch 18, Batch 236, Loss: 0.372060090303421\n",
      "Epoch 18, Batch 237, Loss: 0.31348463892936707\n",
      "Epoch 18, Batch 238, Loss: 0.6598639488220215\n",
      "Epoch 18, Batch 239, Loss: 0.3663548231124878\n",
      "Epoch 18, Batch 240, Loss: 0.36515259742736816\n",
      "Epoch 18, Batch 241, Loss: 0.4031729996204376\n",
      "Epoch 18, Batch 242, Loss: 0.436061829328537\n",
      "Epoch 18, Batch 243, Loss: 0.3523295819759369\n",
      "Epoch 18, Batch 244, Loss: 0.4142594039440155\n",
      "Epoch 18, Batch 245, Loss: 0.4865766763687134\n",
      "Epoch 18, Batch 246, Loss: 0.6482383012771606\n",
      "Epoch 18, Batch 247, Loss: 0.5299890637397766\n",
      "Epoch 18, Batch 248, Loss: 0.40483248233795166\n",
      "Epoch 18, Batch 249, Loss: 0.26244890689849854\n",
      "Epoch 18, Batch 250, Loss: 0.37503519654273987\n",
      "Epoch 18, Batch 251, Loss: 0.37436342239379883\n",
      "Epoch 18, Batch 252, Loss: 0.27415966987609863\n",
      "Epoch 18, Batch 253, Loss: 0.43938565254211426\n",
      "Epoch 18, Batch 254, Loss: 0.5577691793441772\n",
      "Epoch 18, Batch 255, Loss: 0.36941230297088623\n",
      "Epoch 18, Batch 256, Loss: 0.7048536539077759\n",
      "Epoch 18, Batch 257, Loss: 0.4189246594905853\n",
      "Epoch 18, Batch 258, Loss: 0.45127856731414795\n",
      "Epoch 18, Batch 259, Loss: 0.32179027795791626\n",
      "Epoch 18, Batch 260, Loss: 0.47387874126434326\n",
      "Epoch 18, Batch 261, Loss: 0.36892616748809814\n",
      "Epoch 18, Batch 262, Loss: 0.402631938457489\n",
      "Epoch 18, Batch 263, Loss: 0.4375882148742676\n",
      "Epoch 18, Batch 264, Loss: 0.31186410784721375\n",
      "Epoch 18, Batch 265, Loss: 0.4956676661968231\n",
      "Epoch 18, Batch 266, Loss: 0.41298168897628784\n",
      "Epoch 18, Batch 267, Loss: 0.3740275502204895\n",
      "Epoch 18, Batch 268, Loss: 0.49574291706085205\n",
      "Epoch 18, Batch 269, Loss: 0.4167352616786957\n",
      "Epoch 18, Batch 270, Loss: 0.40630704164505005\n",
      "Epoch 18, Batch 271, Loss: 0.25832417607307434\n",
      "Epoch 18, Batch 272, Loss: 0.40520942211151123\n",
      "Epoch 18, Batch 273, Loss: 0.3842679262161255\n",
      "Epoch 18, Batch 274, Loss: 0.4290475845336914\n",
      "Epoch 18, Batch 275, Loss: 0.5817843675613403\n",
      "Epoch 18, Batch 276, Loss: 0.2993546724319458\n",
      "Epoch 18, Batch 277, Loss: 0.5895757079124451\n",
      "Epoch 18, Batch 278, Loss: 0.3643934726715088\n",
      "Epoch 18, Batch 279, Loss: 0.25956225395202637\n",
      "Epoch 18, Batch 280, Loss: 0.41621580719947815\n",
      "Epoch 18, Batch 281, Loss: 0.529800534248352\n",
      "Epoch 18, Batch 282, Loss: 0.606549084186554\n",
      "Epoch 18, Batch 283, Loss: 0.6250354051589966\n",
      "Epoch 18, Batch 284, Loss: 0.38699954748153687\n",
      "Epoch 18, Batch 285, Loss: 0.2809712588787079\n",
      "Epoch 18, Batch 286, Loss: 0.5151548385620117\n",
      "Epoch 18, Batch 287, Loss: 0.5280405282974243\n",
      "Epoch 18, Batch 288, Loss: 0.472088485956192\n",
      "Epoch 18, Batch 289, Loss: 0.42405012249946594\n",
      "Epoch 18, Batch 290, Loss: 0.24790941178798676\n",
      "Epoch 18, Batch 291, Loss: 0.5016033053398132\n",
      "Epoch 18, Batch 292, Loss: 0.5641902089118958\n",
      "Epoch 18, Batch 293, Loss: 0.382478266954422\n",
      "Epoch 18, Batch 294, Loss: 0.44068047404289246\n",
      "Epoch 18, Batch 295, Loss: 0.3743332028388977\n",
      "Epoch 18, Batch 296, Loss: 0.4152736961841583\n",
      "Epoch 18, Batch 297, Loss: 0.5614109039306641\n",
      "Epoch 18, Batch 298, Loss: 0.4556829333305359\n",
      "Epoch 18, Batch 299, Loss: 0.4970736801624298\n",
      "Epoch 18, Batch 300, Loss: 0.3478188216686249\n",
      "Epoch 18, Batch 301, Loss: 0.36706459522247314\n",
      "Epoch 18, Batch 302, Loss: 0.36967694759368896\n",
      "Epoch 18, Batch 303, Loss: 0.40971794724464417\n",
      "Epoch 18, Batch 304, Loss: 0.5300910472869873\n",
      "Epoch 18, Batch 305, Loss: 0.39444682002067566\n",
      "Epoch 18, Batch 306, Loss: 0.2990913987159729\n",
      "Epoch 18, Batch 307, Loss: 0.30940958857536316\n",
      "Epoch 18, Batch 308, Loss: 0.475532591342926\n",
      "Epoch 18, Batch 309, Loss: 0.4188854992389679\n",
      "Epoch 18, Batch 310, Loss: 0.46581709384918213\n",
      "Epoch 18, Batch 311, Loss: 0.28896501660346985\n",
      "Epoch 18, Batch 312, Loss: 0.5332359671592712\n",
      "Epoch 18, Batch 313, Loss: 0.38291165232658386\n",
      "Epoch 18, Batch 314, Loss: 0.34081733226776123\n",
      "Epoch 18, Batch 315, Loss: 0.3018141984939575\n",
      "Epoch 18, Batch 316, Loss: 0.3687503933906555\n",
      "Epoch 18, Batch 317, Loss: 0.5585479140281677\n",
      "Epoch 18, Batch 318, Loss: 0.4841633439064026\n",
      "Epoch 18, Batch 319, Loss: 0.5322297215461731\n",
      "Epoch 18, Batch 320, Loss: 0.48773595690727234\n",
      "Epoch 18, Batch 321, Loss: 0.47706469893455505\n",
      "Epoch 18, Batch 322, Loss: 0.465535968542099\n",
      "Epoch 18, Batch 323, Loss: 0.2895453870296478\n",
      "Epoch 18, Batch 324, Loss: 0.38167518377304077\n",
      "Epoch 18, Batch 325, Loss: 0.36772605776786804\n",
      "Epoch 18, Batch 326, Loss: 0.3097740709781647\n",
      "Epoch 18, Batch 327, Loss: 0.35450467467308044\n",
      "Epoch 18, Batch 328, Loss: 0.44143790006637573\n",
      "Epoch 18, Batch 329, Loss: 0.5782461762428284\n",
      "Epoch 18, Batch 330, Loss: 0.49055778980255127\n",
      "Epoch 18, Batch 331, Loss: 0.5123915076255798\n",
      "Epoch 18, Batch 332, Loss: 0.389202356338501\n",
      "Epoch 18, Batch 333, Loss: 0.4946501851081848\n",
      "Epoch 18, Batch 334, Loss: 0.27052244544029236\n",
      "Epoch 18, Batch 335, Loss: 0.3838554620742798\n",
      "Epoch 18, Batch 336, Loss: 0.43753477931022644\n",
      "Epoch 18, Batch 337, Loss: 0.41323667764663696\n",
      "Epoch 18, Batch 338, Loss: 0.4032514989376068\n",
      "Epoch 18, Batch 339, Loss: 0.40851444005966187\n",
      "Epoch 18, Batch 340, Loss: 0.2609947919845581\n",
      "Epoch 18, Batch 341, Loss: 0.4548054337501526\n",
      "Epoch 18, Batch 342, Loss: 0.3438228368759155\n",
      "Epoch 18, Batch 343, Loss: 0.34585028886795044\n",
      "Epoch 18, Batch 344, Loss: 0.3394400179386139\n",
      "Epoch 18, Batch 345, Loss: 0.4576091766357422\n",
      "Epoch 18, Batch 346, Loss: 0.38243037462234497\n",
      "Epoch 18, Batch 347, Loss: 0.3685969114303589\n",
      "Epoch 18, Batch 348, Loss: 0.4923802316188812\n",
      "Epoch 18, Batch 349, Loss: 0.5158611536026001\n",
      "Epoch 18, Batch 350, Loss: 0.30702129006385803\n",
      "Epoch 18, Batch 351, Loss: 0.33025434613227844\n",
      "Epoch 18, Batch 352, Loss: 0.44358792901039124\n",
      "Epoch 18, Batch 353, Loss: 0.3867088556289673\n",
      "Epoch 18, Batch 354, Loss: 0.5078815221786499\n",
      "Epoch 18, Batch 355, Loss: 0.3418407440185547\n",
      "Epoch 18, Batch 356, Loss: 0.5666724443435669\n",
      "Epoch 18, Batch 357, Loss: 0.40034759044647217\n",
      "Epoch 18, Batch 358, Loss: 0.26565682888031006\n",
      "Epoch 18, Batch 359, Loss: 0.3079952001571655\n",
      "Epoch 18, Batch 360, Loss: 0.308134526014328\n",
      "Epoch 18, Batch 361, Loss: 0.5524385571479797\n",
      "Epoch 18, Batch 362, Loss: 0.34478288888931274\n",
      "Epoch 18, Batch 363, Loss: 0.6404370069503784\n",
      "Epoch 18, Batch 364, Loss: 0.45818668603897095\n",
      "Epoch 18, Batch 365, Loss: 0.5857376456260681\n",
      "Epoch 18, Batch 366, Loss: 0.5039465427398682\n",
      "Epoch 18, Batch 367, Loss: 0.2604256868362427\n",
      "Epoch 18, Batch 368, Loss: 0.45980024337768555\n",
      "Epoch 18, Batch 369, Loss: 0.26005589962005615\n",
      "Epoch 18, Batch 370, Loss: 0.3321497142314911\n",
      "Epoch 18, Batch 371, Loss: 0.4662967026233673\n",
      "Epoch 18, Batch 372, Loss: 0.5196220874786377\n",
      "Epoch 18, Batch 373, Loss: 0.35425153374671936\n",
      "Epoch 18, Batch 374, Loss: 0.41655227541923523\n",
      "Epoch 18, Batch 375, Loss: 0.2360815554857254\n",
      "Epoch 18, Batch 376, Loss: 0.3362048268318176\n",
      "Epoch 18, Batch 377, Loss: 0.26142358779907227\n",
      "Epoch 18, Batch 378, Loss: 0.4443725347518921\n",
      "Epoch 18, Batch 379, Loss: 0.5946776866912842\n",
      "Epoch 18, Batch 380, Loss: 0.684628963470459\n",
      "Epoch 18, Batch 381, Loss: 0.43524640798568726\n",
      "Epoch 18, Batch 382, Loss: 0.5998356938362122\n",
      "Epoch 18, Batch 383, Loss: 0.49933451414108276\n",
      "Epoch 18, Batch 384, Loss: 0.333119660615921\n",
      "Epoch 18, Batch 385, Loss: 0.3126048445701599\n",
      "Epoch 18, Batch 386, Loss: 0.41819819808006287\n",
      "Epoch 18, Batch 387, Loss: 0.4839097857475281\n",
      "Epoch 18, Batch 388, Loss: 0.32637086510658264\n",
      "Epoch 18, Batch 389, Loss: 0.3015417456626892\n",
      "Epoch 18, Batch 390, Loss: 0.42036885023117065\n",
      "Epoch 18, Batch 391, Loss: 0.4935125708580017\n",
      "Epoch 18, Batch 392, Loss: 0.3105716109275818\n",
      "Epoch 18, Batch 393, Loss: 0.3424455225467682\n",
      "Epoch 18, Batch 394, Loss: 0.38277772068977356\n",
      "Epoch 18, Batch 395, Loss: 0.45690345764160156\n",
      "Epoch 18, Batch 396, Loss: 0.7994135022163391\n",
      "Epoch 18, Batch 397, Loss: 0.43410664796829224\n",
      "Epoch 18, Batch 398, Loss: 0.5990221500396729\n",
      "Epoch 18, Batch 399, Loss: 0.34167373180389404\n",
      "Epoch 18, Batch 400, Loss: 0.43895044922828674\n",
      "Epoch 18, Batch 401, Loss: 0.43846380710601807\n",
      "Epoch 18, Batch 402, Loss: 0.4927372932434082\n",
      "Epoch 18, Batch 403, Loss: 0.40359893441200256\n",
      "Epoch 18, Batch 404, Loss: 0.30024319887161255\n",
      "Epoch 18, Batch 405, Loss: 0.47649794816970825\n",
      "Epoch 18, Batch 406, Loss: 0.4615718126296997\n",
      "Epoch 18, Batch 407, Loss: 0.38319167494773865\n",
      "Epoch 18, Batch 408, Loss: 0.2546551525592804\n",
      "Epoch 18, Batch 409, Loss: 0.3182516396045685\n",
      "Epoch 18, Batch 410, Loss: 0.3331848978996277\n",
      "Epoch 18, Batch 411, Loss: 0.48769503831863403\n",
      "Epoch 18, Batch 412, Loss: 0.5197061896324158\n",
      "Epoch 18, Batch 413, Loss: 0.5002093315124512\n",
      "Epoch 18, Batch 414, Loss: 0.49441561102867126\n",
      "Epoch 18, Batch 415, Loss: 0.6850162148475647\n",
      "Epoch 18, Batch 416, Loss: 0.31219974160194397\n",
      "Epoch 18, Batch 417, Loss: 0.3184855282306671\n",
      "Epoch 18, Batch 418, Loss: 0.5270789265632629\n",
      "Epoch 18, Batch 419, Loss: 0.5316494703292847\n",
      "Epoch 18, Batch 420, Loss: 0.4700469374656677\n",
      "Epoch 18, Batch 421, Loss: 0.3530336618423462\n",
      "Epoch 18, Batch 422, Loss: 0.3013908863067627\n",
      "Epoch 18, Batch 423, Loss: 0.3708570599555969\n",
      "Epoch 18, Batch 424, Loss: 0.40747779607772827\n",
      "Epoch 18, Batch 425, Loss: 0.3795837163925171\n",
      "Epoch 18, Batch 426, Loss: 0.5410677194595337\n",
      "Epoch 18, Batch 427, Loss: 0.36965981125831604\n",
      "Epoch 18, Batch 428, Loss: 0.3354089558124542\n",
      "Epoch 18, Batch 429, Loss: 0.30886825919151306\n",
      "Epoch 18, Batch 430, Loss: 0.4401094317436218\n",
      "Epoch 18, Batch 431, Loss: 0.3333214819431305\n",
      "Epoch 18, Batch 432, Loss: 0.6504112482070923\n",
      "Epoch 18, Batch 433, Loss: 0.458687961101532\n",
      "Epoch 18, Batch 434, Loss: 0.3045642375946045\n",
      "Epoch 18, Batch 435, Loss: 0.2792239785194397\n",
      "Epoch 18, Batch 436, Loss: 0.4246595799922943\n",
      "Epoch 18, Batch 437, Loss: 0.4865284264087677\n",
      "Epoch 18, Batch 438, Loss: 0.33322256803512573\n",
      "Epoch 18, Batch 439, Loss: 0.5059354901313782\n",
      "Epoch 18, Batch 440, Loss: 0.3616669178009033\n",
      "Epoch 18, Batch 441, Loss: 0.6150799989700317\n",
      "Epoch 18, Batch 442, Loss: 0.4003804624080658\n",
      "Epoch 18, Batch 443, Loss: 0.3804401159286499\n",
      "Epoch 18, Batch 444, Loss: 0.3451419472694397\n",
      "Epoch 18, Batch 445, Loss: 0.3959164321422577\n",
      "Epoch 18, Batch 446, Loss: 0.31789880990982056\n",
      "Epoch 18, Batch 447, Loss: 0.3774157762527466\n",
      "Epoch 18, Batch 448, Loss: 0.6612831354141235\n",
      "Epoch 18, Batch 449, Loss: 0.3195355534553528\n",
      "Epoch 18, Batch 450, Loss: 0.47880810499191284\n",
      "Epoch 18, Batch 451, Loss: 0.3773951828479767\n",
      "Epoch 18, Batch 452, Loss: 0.5239929556846619\n",
      "Epoch 18, Batch 453, Loss: 0.5867162942886353\n",
      "Epoch 18, Batch 454, Loss: 0.25279316306114197\n",
      "Epoch 18, Batch 455, Loss: 0.5327177047729492\n",
      "Epoch 18, Batch 456, Loss: 0.549002468585968\n",
      "Epoch 18, Batch 457, Loss: 0.7024169564247131\n",
      "Epoch 18, Batch 458, Loss: 0.40369901061058044\n",
      "Epoch 18, Batch 459, Loss: 0.4041910767555237\n",
      "Epoch 18, Batch 460, Loss: 0.3406393826007843\n",
      "Epoch 18, Batch 461, Loss: 0.46935173869132996\n",
      "Epoch 18, Batch 462, Loss: 0.5348972082138062\n",
      "Epoch 18, Batch 463, Loss: 0.7749632000923157\n",
      "Epoch 18, Batch 464, Loss: 0.5023598074913025\n",
      "Epoch 18, Batch 465, Loss: 0.4064238965511322\n",
      "Epoch 18, Batch 466, Loss: 0.4318784773349762\n",
      "Epoch 18, Batch 467, Loss: 0.4341905117034912\n",
      "Epoch 18, Batch 468, Loss: 0.4159191846847534\n",
      "Epoch 18, Batch 469, Loss: 0.3359501361846924\n",
      "Epoch 18, Batch 470, Loss: 0.4317013621330261\n",
      "Epoch 18, Batch 471, Loss: 0.4252588152885437\n",
      "Epoch 18, Batch 472, Loss: 0.572036862373352\n",
      "Epoch 18, Batch 473, Loss: 0.3503892719745636\n",
      "Epoch 18, Batch 474, Loss: 0.3578701615333557\n",
      "Epoch 18, Batch 475, Loss: 0.49371373653411865\n",
      "Epoch 18, Batch 476, Loss: 0.32486212253570557\n",
      "Epoch 18, Batch 477, Loss: 0.4106885492801666\n",
      "Epoch 18, Batch 478, Loss: 0.37994763255119324\n",
      "Epoch 18, Batch 479, Loss: 0.47501111030578613\n",
      "Epoch 18, Batch 480, Loss: 0.2258870005607605\n",
      "Epoch 18, Batch 481, Loss: 0.4197263717651367\n",
      "Epoch 18, Batch 482, Loss: 0.3913421034812927\n",
      "Epoch 18, Batch 483, Loss: 0.4076266884803772\n",
      "Epoch 18, Batch 484, Loss: 0.6238459348678589\n",
      "Epoch 18, Batch 485, Loss: 0.2893523573875427\n",
      "Epoch 18, Batch 486, Loss: 0.4020664095878601\n",
      "Epoch 18, Batch 487, Loss: 0.358447790145874\n",
      "Epoch 18, Batch 488, Loss: 0.5331290364265442\n",
      "Epoch 18, Batch 489, Loss: 0.5158101320266724\n",
      "Epoch 18, Batch 490, Loss: 0.4146924614906311\n",
      "Epoch 18, Batch 491, Loss: 0.48426246643066406\n",
      "Epoch 18, Batch 492, Loss: 0.4221099317073822\n",
      "Epoch 18, Batch 493, Loss: 0.3006909191608429\n",
      "Epoch 18, Batch 494, Loss: 0.5088004469871521\n",
      "Epoch 18, Batch 495, Loss: 0.473215252161026\n",
      "Epoch 18, Batch 496, Loss: 0.4810943603515625\n",
      "Epoch 18, Batch 497, Loss: 0.3611762821674347\n",
      "Epoch 18, Batch 498, Loss: 0.5166956186294556\n",
      "Epoch 18, Batch 499, Loss: 0.33450281620025635\n",
      "Epoch 18, Batch 500, Loss: 0.4924202561378479\n",
      "Epoch 18, Batch 501, Loss: 0.3906611502170563\n",
      "Epoch 18, Batch 502, Loss: 0.5078677535057068\n",
      "Epoch 18, Batch 503, Loss: 0.3441062569618225\n",
      "Epoch 18, Batch 504, Loss: 0.44913119077682495\n",
      "Epoch 18, Batch 505, Loss: 0.414883017539978\n",
      "Epoch 18, Batch 506, Loss: 0.6010932326316833\n",
      "Epoch 18, Batch 507, Loss: 0.5425161719322205\n",
      "Epoch 18, Batch 508, Loss: 0.32472050189971924\n",
      "Epoch 18, Batch 509, Loss: 0.43525299429893494\n",
      "Epoch 18, Batch 510, Loss: 0.4668148159980774\n",
      "Epoch 18, Batch 511, Loss: 0.5576615333557129\n",
      "Epoch 18, Batch 512, Loss: 0.4043963551521301\n",
      "Epoch 18, Batch 513, Loss: 0.36168986558914185\n",
      "Epoch 18, Batch 514, Loss: 0.3313828408718109\n",
      "Epoch 18, Batch 515, Loss: 0.3059593439102173\n",
      "Epoch 18, Batch 516, Loss: 0.3582669794559479\n",
      "Epoch 18, Batch 517, Loss: 0.32397186756134033\n",
      "Epoch 18, Batch 518, Loss: 0.46057552099227905\n",
      "Epoch 18, Batch 519, Loss: 0.4241310656070709\n",
      "Epoch 18, Batch 520, Loss: 0.2898447811603546\n",
      "Epoch 18, Batch 521, Loss: 0.5766045451164246\n",
      "Epoch 18, Batch 522, Loss: 0.6060632467269897\n",
      "Epoch 18, Batch 523, Loss: 0.25863030552864075\n",
      "Epoch 18, Batch 524, Loss: 0.4347505569458008\n",
      "Epoch 18, Batch 525, Loss: 0.4690227508544922\n",
      "Epoch 18, Batch 526, Loss: 0.48803597688674927\n",
      "Epoch 18, Batch 527, Loss: 0.5361675024032593\n",
      "Epoch 18, Batch 528, Loss: 0.34984251856803894\n",
      "Epoch 18, Batch 529, Loss: 0.29443293809890747\n",
      "Epoch 18, Batch 530, Loss: 0.37754425406455994\n",
      "Epoch 18, Batch 531, Loss: 0.32999876141548157\n",
      "Epoch 18, Batch 532, Loss: 0.32142531871795654\n",
      "Epoch 18, Batch 533, Loss: 0.7621340751647949\n",
      "Epoch 18, Batch 534, Loss: 0.46258607506752014\n",
      "Epoch 18, Batch 535, Loss: 0.424088716506958\n",
      "Epoch 18, Batch 536, Loss: 0.33981847763061523\n",
      "Epoch 18, Batch 537, Loss: 0.4294784963130951\n",
      "Epoch 18, Batch 538, Loss: 0.40493375062942505\n",
      "Epoch 18, Batch 539, Loss: 0.6562273502349854\n",
      "Epoch 18, Batch 540, Loss: 0.49035170674324036\n",
      "Epoch 18, Batch 541, Loss: 0.41086718440055847\n",
      "Epoch 18, Batch 542, Loss: 0.2680432200431824\n",
      "Epoch 18, Batch 543, Loss: 0.30377140641212463\n",
      "Epoch 18, Batch 544, Loss: 0.3203856348991394\n",
      "Epoch 18, Batch 545, Loss: 0.5476241111755371\n",
      "Epoch 18, Batch 546, Loss: 0.2621113061904907\n",
      "Epoch 18, Batch 547, Loss: 0.5545204877853394\n",
      "Epoch 18, Batch 548, Loss: 0.45221373438835144\n",
      "Epoch 18, Batch 549, Loss: 0.33503463864326477\n",
      "Epoch 18, Batch 550, Loss: 0.4975268840789795\n",
      "Epoch 18, Batch 551, Loss: 0.3501514494419098\n",
      "Epoch 18, Batch 552, Loss: 0.33003032207489014\n",
      "Epoch 18, Batch 553, Loss: 0.39972764253616333\n",
      "Epoch 18, Batch 554, Loss: 0.4506635069847107\n",
      "Epoch 18, Batch 555, Loss: 0.4806539714336395\n",
      "Epoch 18, Batch 556, Loss: 0.4611547887325287\n",
      "Epoch 18, Batch 557, Loss: 0.3931753635406494\n",
      "Epoch 18, Batch 558, Loss: 0.567524790763855\n",
      "Epoch 18, Batch 559, Loss: 0.3552161753177643\n",
      "Epoch 18, Batch 560, Loss: 0.3857877850532532\n",
      "Epoch 18, Batch 561, Loss: 0.4615989029407501\n",
      "Epoch 18, Batch 562, Loss: 0.3578265309333801\n",
      "Epoch 18, Batch 563, Loss: 0.7727060317993164\n",
      "Epoch 18, Batch 564, Loss: 0.39762499928474426\n",
      "Epoch 18, Batch 565, Loss: 0.34634360671043396\n",
      "Epoch 18, Batch 566, Loss: 0.603557288646698\n",
      "Epoch 18, Batch 567, Loss: 0.48458442091941833\n",
      "Epoch 18, Batch 568, Loss: 0.3069568872451782\n",
      "Epoch 18, Batch 569, Loss: 0.39782974123954773\n",
      "Epoch 18, Batch 570, Loss: 0.404537171125412\n",
      "Epoch 18, Batch 571, Loss: 0.2814854681491852\n",
      "Epoch 18, Batch 572, Loss: 0.38792258501052856\n",
      "Epoch 18, Batch 573, Loss: 0.5162582397460938\n",
      "Epoch 18, Batch 574, Loss: 0.49859973788261414\n",
      "Epoch 18, Batch 575, Loss: 0.2940233647823334\n",
      "Epoch 18, Batch 576, Loss: 0.4492122232913971\n",
      "Epoch 18, Batch 577, Loss: 0.43269163370132446\n",
      "Epoch 18, Batch 578, Loss: 0.46555274724960327\n",
      "Epoch 18, Batch 579, Loss: 0.49634942412376404\n",
      "Epoch 18, Batch 580, Loss: 0.38762587308883667\n",
      "Epoch 18, Batch 581, Loss: 0.33851146697998047\n",
      "Epoch 18, Batch 582, Loss: 0.34147870540618896\n",
      "Epoch 18, Batch 583, Loss: 0.4646108150482178\n",
      "Epoch 18, Batch 584, Loss: 0.5204563140869141\n",
      "Epoch 18, Batch 585, Loss: 0.47896525263786316\n",
      "Epoch 18, Batch 586, Loss: 0.42377373576164246\n",
      "Epoch 18, Batch 587, Loss: 0.47778385877609253\n",
      "Epoch 18, Batch 588, Loss: 0.5465027093887329\n",
      "Epoch 18, Batch 589, Loss: 0.43219882249832153\n",
      "Epoch 18, Batch 590, Loss: 0.27251845598220825\n",
      "Epoch 18, Batch 591, Loss: 0.5672498941421509\n",
      "Epoch 18, Batch 592, Loss: 0.46181702613830566\n",
      "Epoch 18, Batch 593, Loss: 0.5117337703704834\n",
      "Epoch 18, Batch 594, Loss: 0.3849329948425293\n",
      "Epoch 18, Batch 595, Loss: 0.4750947654247284\n",
      "Epoch 18, Batch 596, Loss: 0.4340418577194214\n",
      "Epoch 18, Batch 597, Loss: 0.4384004771709442\n",
      "Epoch 18, Batch 598, Loss: 0.610558807849884\n",
      "Epoch 18, Batch 599, Loss: 0.410971462726593\n",
      "Epoch 18, Batch 600, Loss: 0.3753444254398346\n",
      "Epoch 18, Batch 601, Loss: 0.3835708200931549\n",
      "Epoch 18, Batch 602, Loss: 0.32637929916381836\n",
      "Epoch 18, Batch 603, Loss: 0.19611352682113647\n",
      "Epoch 18, Batch 604, Loss: 0.3641965985298157\n",
      "Epoch 18, Batch 605, Loss: 0.39508235454559326\n",
      "Epoch 18, Batch 606, Loss: 0.5063268542289734\n",
      "Epoch 18, Batch 607, Loss: 0.2746472954750061\n",
      "Epoch 18, Batch 608, Loss: 0.47092196345329285\n",
      "Epoch 18, Batch 609, Loss: 0.34310182929039\n",
      "Epoch 18, Batch 610, Loss: 0.47859519720077515\n",
      "Epoch 18, Batch 611, Loss: 0.470132052898407\n",
      "Epoch 18, Batch 612, Loss: 0.6982594728469849\n",
      "Epoch 18, Batch 613, Loss: 0.34085240960121155\n",
      "Epoch 18, Batch 614, Loss: 0.5779474377632141\n",
      "Epoch 18, Batch 615, Loss: 0.2994970381259918\n",
      "Epoch 18, Batch 616, Loss: 0.4807230830192566\n",
      "Epoch 18, Batch 617, Loss: 0.3817848861217499\n",
      "Epoch 18, Batch 618, Loss: 0.2871456742286682\n",
      "Epoch 18, Batch 619, Loss: 0.26674801111221313\n",
      "Epoch 18, Batch 620, Loss: 0.3455042243003845\n",
      "Epoch 18, Batch 621, Loss: 0.44453123211860657\n",
      "Epoch 18, Batch 622, Loss: 0.3396379351615906\n",
      "Epoch 18, Batch 623, Loss: 0.2632274329662323\n",
      "Epoch 18, Batch 624, Loss: 0.4277627468109131\n",
      "Epoch 18, Batch 625, Loss: 0.358097106218338\n",
      "Epoch 18, Batch 626, Loss: 0.4730331003665924\n",
      "Epoch 18, Batch 627, Loss: 0.5356879234313965\n",
      "Epoch 18, Batch 628, Loss: 0.2646448016166687\n",
      "Epoch 18, Batch 629, Loss: 0.5449474453926086\n",
      "Epoch 18, Batch 630, Loss: 0.4572209119796753\n",
      "Epoch 18, Batch 631, Loss: 0.4362914562225342\n",
      "Epoch 18, Batch 632, Loss: 0.3897676467895508\n",
      "Epoch 18, Batch 633, Loss: 0.42692533135414124\n",
      "Epoch 18, Batch 634, Loss: 0.16896067559719086\n",
      "Epoch 18, Batch 635, Loss: 0.4448762536048889\n",
      "Epoch 18, Batch 636, Loss: 0.35892239212989807\n",
      "Epoch 18, Batch 637, Loss: 0.40878650546073914\n",
      "Epoch 18, Batch 638, Loss: 0.3199053108692169\n",
      "Epoch 18, Batch 639, Loss: 0.5886266231536865\n",
      "Epoch 18, Batch 640, Loss: 0.3809080123901367\n",
      "Epoch 18, Batch 641, Loss: 0.7125228047370911\n",
      "Epoch 18, Batch 642, Loss: 0.2517106831073761\n",
      "Epoch 18, Batch 643, Loss: 0.36355042457580566\n",
      "Epoch 18, Batch 644, Loss: 0.2679062485694885\n",
      "Epoch 18, Batch 645, Loss: 0.3980241119861603\n",
      "Epoch 18, Batch 646, Loss: 0.42800524830818176\n",
      "Epoch 18, Batch 647, Loss: 0.27692073583602905\n",
      "Epoch 18, Batch 648, Loss: 0.6305204629898071\n",
      "Epoch 18, Batch 649, Loss: 0.46577391028404236\n",
      "Epoch 18, Batch 650, Loss: 0.5381757020950317\n",
      "Epoch 18, Batch 651, Loss: 0.3865309953689575\n",
      "Epoch 18, Batch 652, Loss: 0.5896313190460205\n",
      "Epoch 18, Batch 653, Loss: 0.558968722820282\n",
      "Epoch 18, Batch 654, Loss: 0.37011393904685974\n",
      "Epoch 18, Batch 655, Loss: 0.36573734879493713\n",
      "Epoch 18, Batch 656, Loss: 0.5983174443244934\n",
      "Epoch 18, Batch 657, Loss: 0.6705567836761475\n",
      "Epoch 18, Batch 658, Loss: 0.5074355602264404\n",
      "Epoch 18, Batch 659, Loss: 0.43852540850639343\n",
      "Epoch 18, Batch 660, Loss: 0.5732627511024475\n",
      "Epoch 18, Batch 661, Loss: 0.5384544134140015\n",
      "Epoch 18, Batch 662, Loss: 0.41104573011398315\n",
      "Epoch 18, Batch 663, Loss: 0.5805942416191101\n",
      "Epoch 18, Batch 664, Loss: 0.3968862295150757\n",
      "Epoch 18, Batch 665, Loss: 0.3662135601043701\n",
      "Epoch 18, Batch 666, Loss: 0.4173239469528198\n",
      "Epoch 18, Batch 667, Loss: 0.44016414880752563\n",
      "Epoch 18, Batch 668, Loss: 0.34740591049194336\n",
      "Epoch 18, Batch 669, Loss: 0.4440625011920929\n",
      "Epoch 18, Batch 670, Loss: 0.3963608145713806\n",
      "Epoch 18, Batch 671, Loss: 0.49102073907852173\n",
      "Epoch 18, Batch 672, Loss: 0.43473517894744873\n",
      "Epoch 18, Batch 673, Loss: 0.2567022144794464\n",
      "Epoch 18, Batch 674, Loss: 0.4423867464065552\n",
      "Epoch 18, Batch 675, Loss: 0.4847419559955597\n",
      "Epoch 18, Batch 676, Loss: 0.3539258539676666\n",
      "Epoch 18, Batch 677, Loss: 0.46371322870254517\n",
      "Epoch 18, Batch 678, Loss: 0.48363298177719116\n",
      "Epoch 18, Batch 679, Loss: 0.5055127739906311\n",
      "Epoch 18, Batch 680, Loss: 0.4093877673149109\n",
      "Epoch 18, Batch 681, Loss: 0.6205636858940125\n",
      "Epoch 18, Batch 682, Loss: 0.3373170495033264\n",
      "Epoch 18, Batch 683, Loss: 0.35442614555358887\n",
      "Epoch 18, Batch 684, Loss: 0.38192567229270935\n",
      "Epoch 18, Batch 685, Loss: 0.4313139021396637\n",
      "Epoch 18, Batch 686, Loss: 0.43909895420074463\n",
      "Epoch 18, Batch 687, Loss: 0.6007579565048218\n",
      "Epoch 18, Batch 688, Loss: 0.4197412431240082\n",
      "Epoch 18, Batch 689, Loss: 0.4546562731266022\n",
      "Epoch 18, Batch 690, Loss: 0.46732422709465027\n",
      "Epoch 18, Batch 691, Loss: 0.3112691044807434\n",
      "Epoch 18, Batch 692, Loss: 0.46273449063301086\n",
      "Epoch 18, Batch 693, Loss: 0.35660532116889954\n",
      "Epoch 18, Batch 694, Loss: 0.462938517332077\n",
      "Epoch 18, Batch 695, Loss: 0.3363924026489258\n",
      "Epoch 18, Batch 696, Loss: 0.6258397102355957\n",
      "Epoch 18, Batch 697, Loss: 0.32568246126174927\n",
      "Epoch 18, Batch 698, Loss: 0.2903374433517456\n",
      "Epoch 18, Batch 699, Loss: 0.32341885566711426\n",
      "Epoch 18, Batch 700, Loss: 0.5731732845306396\n",
      "Epoch 18, Batch 701, Loss: 0.38755226135253906\n",
      "Epoch 18, Batch 702, Loss: 0.5725201964378357\n",
      "Epoch 18, Batch 703, Loss: 0.39456140995025635\n",
      "Epoch 18, Batch 704, Loss: 0.4578165113925934\n",
      "Epoch 18, Batch 705, Loss: 0.2410644143819809\n",
      "Epoch 18, Batch 706, Loss: 0.4550427496433258\n",
      "Epoch 18, Batch 707, Loss: 0.297495573759079\n",
      "Epoch 18, Batch 708, Loss: 0.5053687691688538\n",
      "Epoch 18, Batch 709, Loss: 0.4839603304862976\n",
      "Epoch 18, Batch 710, Loss: 0.27763229608535767\n",
      "Epoch 18, Batch 711, Loss: 0.4347788095474243\n",
      "Epoch 18, Batch 712, Loss: 0.3589087724685669\n",
      "Epoch 18, Batch 713, Loss: 0.5743451118469238\n",
      "Epoch 18, Batch 714, Loss: 0.4051889181137085\n",
      "Epoch 18, Batch 715, Loss: 0.3109060227870941\n",
      "Epoch 18, Batch 716, Loss: 0.43918663263320923\n",
      "Epoch 18, Batch 717, Loss: 0.5300024151802063\n",
      "Epoch 18, Batch 718, Loss: 0.35568058490753174\n",
      "Epoch 18, Batch 719, Loss: 0.33177345991134644\n",
      "Epoch 18, Batch 720, Loss: 0.48213571310043335\n",
      "Epoch 18, Batch 721, Loss: 0.3304927945137024\n",
      "Epoch 18, Batch 722, Loss: 0.36621272563934326\n",
      "Epoch 18, Batch 723, Loss: 0.5196254849433899\n",
      "Epoch 18, Batch 724, Loss: 0.5335624814033508\n",
      "Epoch 18, Batch 725, Loss: 0.4170447289943695\n",
      "Epoch 18, Batch 726, Loss: 0.4726792573928833\n",
      "Epoch 18, Batch 727, Loss: 0.457329660654068\n",
      "Epoch 18, Batch 728, Loss: 0.3364728093147278\n",
      "Epoch 18, Batch 729, Loss: 0.2686002552509308\n",
      "Epoch 18, Batch 730, Loss: 0.4256511926651001\n",
      "Epoch 18, Batch 731, Loss: 0.5787410140037537\n",
      "Epoch 18, Batch 732, Loss: 0.43902212381362915\n",
      "Epoch 18, Batch 733, Loss: 0.33551859855651855\n",
      "Epoch 18, Batch 734, Loss: 0.2552957534790039\n",
      "Epoch 18, Batch 735, Loss: 0.35409459471702576\n",
      "Epoch 18, Batch 736, Loss: 0.5775749087333679\n",
      "Epoch 18, Batch 737, Loss: 0.6181030869483948\n",
      "Epoch 18, Batch 738, Loss: 0.38300222158432007\n",
      "Epoch 18, Batch 739, Loss: 0.3902636766433716\n",
      "Epoch 18, Batch 740, Loss: 0.38380828499794006\n",
      "Epoch 18, Batch 741, Loss: 0.3151284456253052\n",
      "Epoch 18, Batch 742, Loss: 0.4373507797718048\n",
      "Epoch 18, Batch 743, Loss: 0.46513521671295166\n",
      "Epoch 18, Batch 744, Loss: 0.3169836103916168\n",
      "Epoch 18, Batch 745, Loss: 0.47983384132385254\n",
      "Epoch 18, Batch 746, Loss: 0.3288784623146057\n",
      "Epoch 18, Batch 747, Loss: 0.5519514083862305\n",
      "Epoch 18, Batch 748, Loss: 0.45828020572662354\n",
      "Epoch 18, Batch 749, Loss: 0.42457813024520874\n",
      "Epoch 18, Batch 750, Loss: 0.4646149277687073\n",
      "Epoch 18, Batch 751, Loss: 0.4863203763961792\n",
      "Epoch 18, Batch 752, Loss: 0.43024933338165283\n",
      "Epoch 18, Batch 753, Loss: 0.5504405498504639\n",
      "Epoch 18, Batch 754, Loss: 0.37821701169013977\n",
      "Epoch 18, Batch 755, Loss: 0.3651394546031952\n",
      "Epoch 18, Batch 756, Loss: 0.36865004897117615\n",
      "Epoch 18, Batch 757, Loss: 0.31179624795913696\n",
      "Epoch 18, Batch 758, Loss: 0.4524143934249878\n",
      "Epoch 18, Batch 759, Loss: 0.29299378395080566\n",
      "Epoch 18, Batch 760, Loss: 0.46112751960754395\n",
      "Epoch 18, Batch 761, Loss: 0.48359817266464233\n",
      "Epoch 18, Batch 762, Loss: 0.5780524611473083\n",
      "Epoch 18, Batch 763, Loss: 0.3352009057998657\n",
      "Epoch 18, Batch 764, Loss: 0.41179710626602173\n",
      "Epoch 18, Batch 765, Loss: 0.4597186744213104\n",
      "Epoch 18, Batch 766, Loss: 0.25253912806510925\n",
      "Epoch 18, Batch 767, Loss: 0.42175164818763733\n",
      "Epoch 18, Batch 768, Loss: 0.26600492000579834\n",
      "Epoch 18, Batch 769, Loss: 0.6392906904220581\n",
      "Epoch 18, Batch 770, Loss: 0.29602745175361633\n",
      "Epoch 18, Batch 771, Loss: 0.3791184425354004\n",
      "Epoch 18, Batch 772, Loss: 0.24344056844711304\n",
      "Epoch 18, Batch 773, Loss: 0.4660884737968445\n",
      "Epoch 18, Batch 774, Loss: 0.5350673198699951\n",
      "Epoch 18, Batch 775, Loss: 0.4541677236557007\n",
      "Epoch 18, Batch 776, Loss: 0.25736916065216064\n",
      "Epoch 18, Batch 777, Loss: 0.36334550380706787\n",
      "Epoch 18, Batch 778, Loss: 0.637566328048706\n",
      "Epoch 18, Batch 779, Loss: 0.5445096492767334\n",
      "Epoch 18, Batch 780, Loss: 0.6724157929420471\n",
      "Epoch 18, Batch 781, Loss: 0.40803787112236023\n",
      "Epoch 18, Batch 782, Loss: 0.6019147038459778\n",
      "Epoch 18, Batch 783, Loss: 0.36572006344795227\n",
      "Epoch 18, Batch 784, Loss: 0.4025263488292694\n",
      "Epoch 18, Batch 785, Loss: 0.31476807594299316\n",
      "Epoch 18, Batch 786, Loss: 0.5620006322860718\n",
      "Epoch 18, Batch 787, Loss: 0.5280854105949402\n",
      "Epoch 18, Batch 788, Loss: 0.4551061689853668\n",
      "Epoch 18, Batch 789, Loss: 0.44510161876678467\n",
      "Epoch 18, Batch 790, Loss: 0.4169022738933563\n",
      "Epoch 18, Batch 791, Loss: 0.583614706993103\n",
      "Epoch 18, Batch 792, Loss: 0.47640368342399597\n",
      "Epoch 18, Batch 793, Loss: 0.37584322690963745\n",
      "Epoch 18, Batch 794, Loss: 0.5632050037384033\n",
      "Epoch 18, Batch 795, Loss: 0.5028111934661865\n",
      "Epoch 18, Batch 796, Loss: 0.375830739736557\n",
      "Epoch 18, Batch 797, Loss: 0.4822053909301758\n",
      "Epoch 18, Batch 798, Loss: 0.4327055513858795\n",
      "Epoch 18, Batch 799, Loss: 0.23891402781009674\n",
      "Epoch 18, Batch 800, Loss: 0.46780306100845337\n",
      "Epoch 18, Batch 801, Loss: 0.44624361395835876\n",
      "Epoch 18, Batch 802, Loss: 0.5119706392288208\n",
      "Epoch 18, Batch 803, Loss: 0.34715843200683594\n",
      "Epoch 18, Batch 804, Loss: 0.3280198574066162\n",
      "Epoch 18, Batch 805, Loss: 0.5417070388793945\n",
      "Epoch 18, Batch 806, Loss: 0.6289758682250977\n",
      "Epoch 18, Batch 807, Loss: 0.4529964327812195\n",
      "Epoch 18, Batch 808, Loss: 0.42979562282562256\n",
      "Epoch 18, Batch 809, Loss: 0.3774554431438446\n",
      "Epoch 18, Batch 810, Loss: 0.3321093022823334\n",
      "Epoch 18, Batch 811, Loss: 0.3184010982513428\n",
      "Epoch 18, Batch 812, Loss: 0.7204814553260803\n",
      "Epoch 18, Batch 813, Loss: 0.39084306359291077\n",
      "Epoch 18, Batch 814, Loss: 0.39495038986206055\n",
      "Epoch 18, Batch 815, Loss: 0.4655902087688446\n",
      "Epoch 18, Batch 816, Loss: 0.35113072395324707\n",
      "Epoch 18, Batch 817, Loss: 0.3592991828918457\n",
      "Epoch 18, Batch 818, Loss: 0.5937580466270447\n",
      "Epoch 18, Batch 819, Loss: 0.44516804814338684\n",
      "Epoch 18, Batch 820, Loss: 0.37984007596969604\n",
      "Epoch 18, Batch 821, Loss: 0.5800639986991882\n",
      "Epoch 18, Batch 822, Loss: 0.36150962114334106\n",
      "Epoch 18, Batch 823, Loss: 0.3888818025588989\n",
      "Epoch 18, Batch 824, Loss: 0.42996975779533386\n",
      "Epoch 18, Batch 825, Loss: 0.4000765085220337\n",
      "Epoch 18, Batch 826, Loss: 0.48979678750038147\n",
      "Epoch 18, Batch 827, Loss: 0.38656023144721985\n",
      "Epoch 18, Batch 828, Loss: 0.4168245196342468\n",
      "Epoch 18, Batch 829, Loss: 0.2810470461845398\n",
      "Epoch 18, Batch 830, Loss: 0.31110477447509766\n",
      "Epoch 18, Batch 831, Loss: 0.516288697719574\n",
      "Epoch 18, Batch 832, Loss: 0.44913753867149353\n",
      "Epoch 18, Batch 833, Loss: 0.6237199902534485\n",
      "Epoch 18, Batch 834, Loss: 0.32265910506248474\n",
      "Epoch 18, Batch 835, Loss: 0.6280738711357117\n",
      "Epoch 18, Batch 836, Loss: 0.2610544264316559\n",
      "Epoch 18, Batch 837, Loss: 0.5627889633178711\n",
      "Epoch 18, Batch 838, Loss: 0.29797226190567017\n",
      "Epoch 18, Batch 839, Loss: 0.3782740831375122\n",
      "Epoch 18, Batch 840, Loss: 0.36546289920806885\n",
      "Epoch 18, Batch 841, Loss: 0.4259116053581238\n",
      "Epoch 18, Batch 842, Loss: 0.3005291819572449\n",
      "Epoch 18, Batch 843, Loss: 0.33252277970314026\n",
      "Epoch 18, Batch 844, Loss: 0.3422654867172241\n",
      "Epoch 18, Batch 845, Loss: 0.3820422291755676\n",
      "Epoch 18, Batch 846, Loss: 0.41273233294487\n",
      "Epoch 18, Batch 847, Loss: 0.453855961561203\n",
      "Epoch 18, Batch 848, Loss: 0.44106876850128174\n",
      "Epoch 18, Batch 849, Loss: 0.4334315061569214\n",
      "Epoch 18, Batch 850, Loss: 0.43526971340179443\n",
      "Epoch 18, Batch 851, Loss: 0.40345948934555054\n",
      "Epoch 18, Batch 852, Loss: 0.42532795667648315\n",
      "Epoch 18, Batch 853, Loss: 0.5642169713973999\n",
      "Epoch 18, Batch 854, Loss: 0.3706229329109192\n",
      "Epoch 18, Batch 855, Loss: 0.3021564185619354\n",
      "Epoch 18, Batch 856, Loss: 0.41585779190063477\n",
      "Epoch 18, Batch 857, Loss: 0.5590862035751343\n",
      "Epoch 18, Batch 858, Loss: 0.3738517463207245\n",
      "Epoch 18, Batch 859, Loss: 0.36915674805641174\n",
      "Epoch 18, Batch 860, Loss: 0.23547755181789398\n",
      "Epoch 18, Batch 861, Loss: 0.4096836745738983\n",
      "Epoch 18, Batch 862, Loss: 0.34869182109832764\n",
      "Epoch 18, Batch 863, Loss: 0.4863356947898865\n",
      "Epoch 18, Batch 864, Loss: 0.44029107689857483\n",
      "Epoch 18, Batch 865, Loss: 0.41577470302581787\n",
      "Epoch 18, Batch 866, Loss: 0.3685726523399353\n",
      "Epoch 18, Batch 867, Loss: 0.4390549063682556\n",
      "Epoch 18, Batch 868, Loss: 0.36067435145378113\n",
      "Epoch 18, Batch 869, Loss: 0.3265480697154999\n",
      "Epoch 18, Batch 870, Loss: 0.37253063917160034\n",
      "Epoch 18, Batch 871, Loss: 0.3223103880882263\n",
      "Epoch 18, Batch 872, Loss: 0.3065888583660126\n",
      "Epoch 18, Batch 873, Loss: 0.3600453734397888\n",
      "Epoch 18, Batch 874, Loss: 0.3940029442310333\n",
      "Epoch 18, Batch 875, Loss: 0.5479806661605835\n",
      "Epoch 18, Batch 876, Loss: 0.4029797911643982\n",
      "Epoch 18, Batch 877, Loss: 0.34627407789230347\n",
      "Epoch 18, Batch 878, Loss: 0.282355934381485\n",
      "Epoch 18, Batch 879, Loss: 0.34105122089385986\n",
      "Epoch 18, Batch 880, Loss: 0.3923203945159912\n",
      "Epoch 18, Batch 881, Loss: 0.476860374212265\n",
      "Epoch 18, Batch 882, Loss: 0.41161665320396423\n",
      "Epoch 18, Batch 883, Loss: 0.4875796139240265\n",
      "Epoch 18, Batch 884, Loss: 0.4813535213470459\n",
      "Epoch 18, Batch 885, Loss: 0.36178234219551086\n",
      "Epoch 18, Batch 886, Loss: 0.4452860653400421\n",
      "Epoch 18, Batch 887, Loss: 0.44999831914901733\n",
      "Epoch 18, Batch 888, Loss: 0.39110425114631653\n",
      "Epoch 18, Batch 889, Loss: 0.5583627820014954\n",
      "Epoch 18, Batch 890, Loss: 0.22810891270637512\n",
      "Epoch 18, Batch 891, Loss: 0.6154195070266724\n",
      "Epoch 18, Batch 892, Loss: 0.4377877116203308\n",
      "Epoch 18, Batch 893, Loss: 0.6055716872215271\n",
      "Epoch 18, Batch 894, Loss: 0.4040127396583557\n",
      "Epoch 18, Batch 895, Loss: 0.4568922221660614\n",
      "Epoch 18, Batch 896, Loss: 0.2972128987312317\n",
      "Epoch 18, Batch 897, Loss: 0.263212651014328\n",
      "Epoch 18, Batch 898, Loss: 0.6180226802825928\n",
      "Epoch 18, Batch 899, Loss: 0.49794667959213257\n",
      "Epoch 18, Batch 900, Loss: 0.46721208095550537\n",
      "Epoch 18, Batch 901, Loss: 0.30762210488319397\n",
      "Epoch 18, Batch 902, Loss: 0.35902661085128784\n",
      "Epoch 18, Batch 903, Loss: 0.5205632448196411\n",
      "Epoch 18, Batch 904, Loss: 0.35044437646865845\n",
      "Epoch 18, Batch 905, Loss: 0.3645924925804138\n",
      "Epoch 18, Batch 906, Loss: 0.43579423427581787\n",
      "Epoch 18, Batch 907, Loss: 0.3855104446411133\n",
      "Epoch 18, Batch 908, Loss: 0.3192973732948303\n",
      "Epoch 18, Batch 909, Loss: 0.43718600273132324\n",
      "Epoch 18, Batch 910, Loss: 0.37462738156318665\n",
      "Epoch 18, Batch 911, Loss: 0.6392693519592285\n",
      "Epoch 18, Batch 912, Loss: 0.6461865901947021\n",
      "Epoch 18, Batch 913, Loss: 0.41705775260925293\n",
      "Epoch 18, Batch 914, Loss: 0.36488887667655945\n",
      "Epoch 18, Batch 915, Loss: 0.3626912534236908\n",
      "Epoch 18, Batch 916, Loss: 0.4898938834667206\n",
      "Epoch 18, Batch 917, Loss: 0.5123910307884216\n",
      "Epoch 18, Batch 918, Loss: 0.28858691453933716\n",
      "Epoch 18, Batch 919, Loss: 0.36862507462501526\n",
      "Epoch 18, Batch 920, Loss: 0.6447612643241882\n",
      "Epoch 18, Batch 921, Loss: 0.3637726902961731\n",
      "Epoch 18, Batch 922, Loss: 0.35835039615631104\n",
      "Epoch 18, Batch 923, Loss: 0.4042513966560364\n",
      "Epoch 18, Batch 924, Loss: 0.30985355377197266\n",
      "Epoch 18, Batch 925, Loss: 0.4638613164424896\n",
      "Epoch 18, Batch 926, Loss: 0.42706820368766785\n",
      "Epoch 18, Batch 927, Loss: 0.4849761128425598\n",
      "Epoch 18, Batch 928, Loss: 0.6139363646507263\n",
      "Epoch 18, Batch 929, Loss: 0.4010159373283386\n",
      "Epoch 18, Batch 930, Loss: 0.2742580771446228\n",
      "Epoch 18, Batch 931, Loss: 0.365826278924942\n",
      "Epoch 18, Batch 932, Loss: 0.46631091833114624\n",
      "Epoch 18, Batch 933, Loss: 0.38566768169403076\n",
      "Epoch 18, Batch 934, Loss: 0.45410966873168945\n",
      "Epoch 18, Batch 935, Loss: 0.25661325454711914\n",
      "Epoch 18, Batch 936, Loss: 0.5129506587982178\n",
      "Epoch 18, Batch 937, Loss: 0.6208755970001221\n",
      "Epoch 18, Batch 938, Loss: 0.3207513689994812\n",
      "Accuracy of train set: 0.8488\n",
      "Epoch 18, Batch 1, Test Loss: 0.3759937882423401\n",
      "Epoch 18, Batch 2, Test Loss: 0.3223002254962921\n",
      "Epoch 18, Batch 3, Test Loss: 0.5675911903381348\n",
      "Epoch 18, Batch 4, Test Loss: 0.46478405594825745\n",
      "Epoch 18, Batch 5, Test Loss: 0.4208257496356964\n",
      "Epoch 18, Batch 6, Test Loss: 0.4049477279186249\n",
      "Epoch 18, Batch 7, Test Loss: 0.31181180477142334\n",
      "Epoch 18, Batch 8, Test Loss: 0.41860106587409973\n",
      "Epoch 18, Batch 9, Test Loss: 0.44744613766670227\n",
      "Epoch 18, Batch 10, Test Loss: 0.37529611587524414\n",
      "Epoch 18, Batch 11, Test Loss: 0.45701169967651367\n",
      "Epoch 18, Batch 12, Test Loss: 0.3378739356994629\n",
      "Epoch 18, Batch 13, Test Loss: 0.6028599739074707\n",
      "Epoch 18, Batch 14, Test Loss: 0.5083791017532349\n",
      "Epoch 18, Batch 15, Test Loss: 0.541753351688385\n",
      "Epoch 18, Batch 16, Test Loss: 0.41374096274375916\n",
      "Epoch 18, Batch 17, Test Loss: 0.4259796738624573\n",
      "Epoch 18, Batch 18, Test Loss: 0.22614850103855133\n",
      "Epoch 18, Batch 19, Test Loss: 0.3091093599796295\n",
      "Epoch 18, Batch 20, Test Loss: 0.6475618481636047\n",
      "Epoch 18, Batch 21, Test Loss: 0.4524221420288086\n",
      "Epoch 18, Batch 22, Test Loss: 0.4608582854270935\n",
      "Epoch 18, Batch 23, Test Loss: 0.5731801986694336\n",
      "Epoch 18, Batch 24, Test Loss: 0.407990425825119\n",
      "Epoch 18, Batch 25, Test Loss: 0.40533292293548584\n",
      "Epoch 18, Batch 26, Test Loss: 0.2540820240974426\n",
      "Epoch 18, Batch 27, Test Loss: 0.48061704635620117\n",
      "Epoch 18, Batch 28, Test Loss: 0.5862764120101929\n",
      "Epoch 18, Batch 29, Test Loss: 0.47626861929893494\n",
      "Epoch 18, Batch 30, Test Loss: 0.395058810710907\n",
      "Epoch 18, Batch 31, Test Loss: 0.36927011609077454\n",
      "Epoch 18, Batch 32, Test Loss: 0.4624747633934021\n",
      "Epoch 18, Batch 33, Test Loss: 0.5030182600021362\n",
      "Epoch 18, Batch 34, Test Loss: 0.4598628580570221\n",
      "Epoch 18, Batch 35, Test Loss: 0.5693709850311279\n",
      "Epoch 18, Batch 36, Test Loss: 0.3590143322944641\n",
      "Epoch 18, Batch 37, Test Loss: 0.32296884059906006\n",
      "Epoch 18, Batch 38, Test Loss: 0.32121336460113525\n",
      "Epoch 18, Batch 39, Test Loss: 0.27824702858924866\n",
      "Epoch 18, Batch 40, Test Loss: 0.47924378514289856\n",
      "Epoch 18, Batch 41, Test Loss: 0.5094469785690308\n",
      "Epoch 18, Batch 42, Test Loss: 0.3515959084033966\n",
      "Epoch 18, Batch 43, Test Loss: 0.44582101702690125\n",
      "Epoch 18, Batch 44, Test Loss: 0.3961927890777588\n",
      "Epoch 18, Batch 45, Test Loss: 0.2655373215675354\n",
      "Epoch 18, Batch 46, Test Loss: 0.36334624886512756\n",
      "Epoch 18, Batch 47, Test Loss: 0.596436083316803\n",
      "Epoch 18, Batch 48, Test Loss: 0.574019193649292\n",
      "Epoch 18, Batch 49, Test Loss: 0.3746299147605896\n",
      "Epoch 18, Batch 50, Test Loss: 0.49390822649002075\n",
      "Epoch 18, Batch 51, Test Loss: 0.36472395062446594\n",
      "Epoch 18, Batch 52, Test Loss: 0.41251909732818604\n",
      "Epoch 18, Batch 53, Test Loss: 0.1886998564004898\n",
      "Epoch 18, Batch 54, Test Loss: 0.49202045798301697\n",
      "Epoch 18, Batch 55, Test Loss: 0.42977219820022583\n",
      "Epoch 18, Batch 56, Test Loss: 0.4140405058860779\n",
      "Epoch 18, Batch 57, Test Loss: 0.5427160859107971\n",
      "Epoch 18, Batch 58, Test Loss: 0.2846587002277374\n",
      "Epoch 18, Batch 59, Test Loss: 0.24780014157295227\n",
      "Epoch 18, Batch 60, Test Loss: 0.44803643226623535\n",
      "Epoch 18, Batch 61, Test Loss: 0.27787885069847107\n",
      "Epoch 18, Batch 62, Test Loss: 0.5375031232833862\n",
      "Epoch 18, Batch 63, Test Loss: 0.49150198698043823\n",
      "Epoch 18, Batch 64, Test Loss: 0.3715550899505615\n",
      "Epoch 18, Batch 65, Test Loss: 0.32342588901519775\n",
      "Epoch 18, Batch 66, Test Loss: 0.5307484865188599\n",
      "Epoch 18, Batch 67, Test Loss: 0.6529017090797424\n",
      "Epoch 18, Batch 68, Test Loss: 0.5577039122581482\n",
      "Epoch 18, Batch 69, Test Loss: 0.35070064663887024\n",
      "Epoch 18, Batch 70, Test Loss: 0.41965368390083313\n",
      "Epoch 18, Batch 71, Test Loss: 0.35809141397476196\n",
      "Epoch 18, Batch 72, Test Loss: 0.3257179856300354\n",
      "Epoch 18, Batch 73, Test Loss: 0.4143633246421814\n",
      "Epoch 18, Batch 74, Test Loss: 0.48507094383239746\n",
      "Epoch 18, Batch 75, Test Loss: 0.38420841097831726\n",
      "Epoch 18, Batch 76, Test Loss: 0.35140374302864075\n",
      "Epoch 18, Batch 77, Test Loss: 0.43128371238708496\n",
      "Epoch 18, Batch 78, Test Loss: 0.2793157398700714\n",
      "Epoch 18, Batch 79, Test Loss: 0.445535272359848\n",
      "Epoch 18, Batch 80, Test Loss: 0.2118549942970276\n",
      "Epoch 18, Batch 81, Test Loss: 0.29064613580703735\n",
      "Epoch 18, Batch 82, Test Loss: 0.4661862254142761\n",
      "Epoch 18, Batch 83, Test Loss: 0.6642276048660278\n",
      "Epoch 18, Batch 84, Test Loss: 0.4662165939807892\n",
      "Epoch 18, Batch 85, Test Loss: 0.34165698289871216\n",
      "Epoch 18, Batch 86, Test Loss: 0.3519495725631714\n",
      "Epoch 18, Batch 87, Test Loss: 0.361439973115921\n",
      "Epoch 18, Batch 88, Test Loss: 0.1754646748304367\n",
      "Epoch 18, Batch 89, Test Loss: 0.5413620471954346\n",
      "Epoch 18, Batch 90, Test Loss: 0.43418169021606445\n",
      "Epoch 18, Batch 91, Test Loss: 0.46766242384910583\n",
      "Epoch 18, Batch 92, Test Loss: 0.3108700215816498\n",
      "Epoch 18, Batch 93, Test Loss: 0.3660171329975128\n",
      "Epoch 18, Batch 94, Test Loss: 0.5758887529373169\n",
      "Epoch 18, Batch 95, Test Loss: 0.3776188790798187\n",
      "Epoch 18, Batch 96, Test Loss: 0.3886140286922455\n",
      "Epoch 18, Batch 97, Test Loss: 0.27344194054603577\n",
      "Epoch 18, Batch 98, Test Loss: 0.20198503136634827\n",
      "Epoch 18, Batch 99, Test Loss: 0.3664169907569885\n",
      "Epoch 18, Batch 100, Test Loss: 0.4528646171092987\n",
      "Epoch 18, Batch 101, Test Loss: 0.2569640278816223\n",
      "Epoch 18, Batch 102, Test Loss: 0.4000636339187622\n",
      "Epoch 18, Batch 103, Test Loss: 0.34489649534225464\n",
      "Epoch 18, Batch 104, Test Loss: 0.40380796790122986\n",
      "Epoch 18, Batch 105, Test Loss: 0.35058116912841797\n",
      "Epoch 18, Batch 106, Test Loss: 0.39902055263519287\n",
      "Epoch 18, Batch 107, Test Loss: 0.47940754890441895\n",
      "Epoch 18, Batch 108, Test Loss: 0.3144599497318268\n",
      "Epoch 18, Batch 109, Test Loss: 0.5300763845443726\n",
      "Epoch 18, Batch 110, Test Loss: 0.5101780891418457\n",
      "Epoch 18, Batch 111, Test Loss: 0.5131300687789917\n",
      "Epoch 18, Batch 112, Test Loss: 0.2824748754501343\n",
      "Epoch 18, Batch 113, Test Loss: 0.3007371127605438\n",
      "Epoch 18, Batch 114, Test Loss: 0.353540301322937\n",
      "Epoch 18, Batch 115, Test Loss: 0.414427250623703\n",
      "Epoch 18, Batch 116, Test Loss: 0.48705315589904785\n",
      "Epoch 18, Batch 117, Test Loss: 0.40719467401504517\n",
      "Epoch 18, Batch 118, Test Loss: 0.44734179973602295\n",
      "Epoch 18, Batch 119, Test Loss: 0.3688814342021942\n",
      "Epoch 18, Batch 120, Test Loss: 0.28783631324768066\n",
      "Epoch 18, Batch 121, Test Loss: 0.4727436304092407\n",
      "Epoch 18, Batch 122, Test Loss: 0.44528651237487793\n",
      "Epoch 18, Batch 123, Test Loss: 0.4372950494289398\n",
      "Epoch 18, Batch 124, Test Loss: 0.51197350025177\n",
      "Epoch 18, Batch 125, Test Loss: 0.45402994751930237\n",
      "Epoch 18, Batch 126, Test Loss: 0.245798721909523\n",
      "Epoch 18, Batch 127, Test Loss: 0.4829563498497009\n",
      "Epoch 18, Batch 128, Test Loss: 0.4136033058166504\n",
      "Epoch 18, Batch 129, Test Loss: 0.3776925504207611\n",
      "Epoch 18, Batch 130, Test Loss: 0.30719125270843506\n",
      "Epoch 18, Batch 131, Test Loss: 0.20257781445980072\n",
      "Epoch 18, Batch 132, Test Loss: 0.3020753264427185\n",
      "Epoch 18, Batch 133, Test Loss: 0.36386755108833313\n",
      "Epoch 18, Batch 134, Test Loss: 0.6750001907348633\n",
      "Epoch 18, Batch 135, Test Loss: 0.5311755537986755\n",
      "Epoch 18, Batch 136, Test Loss: 0.44910240173339844\n",
      "Epoch 18, Batch 137, Test Loss: 0.5410576462745667\n",
      "Epoch 18, Batch 138, Test Loss: 0.382243812084198\n",
      "Epoch 18, Batch 139, Test Loss: 0.5560070872306824\n",
      "Epoch 18, Batch 140, Test Loss: 0.5928845405578613\n",
      "Epoch 18, Batch 141, Test Loss: 0.370872437953949\n",
      "Epoch 18, Batch 142, Test Loss: 0.6401225924491882\n",
      "Epoch 18, Batch 143, Test Loss: 0.36228814721107483\n",
      "Epoch 18, Batch 144, Test Loss: 0.45230811834335327\n",
      "Epoch 18, Batch 145, Test Loss: 0.3798050582408905\n",
      "Epoch 18, Batch 146, Test Loss: 0.3412740230560303\n",
      "Epoch 18, Batch 147, Test Loss: 0.38043642044067383\n",
      "Epoch 18, Batch 148, Test Loss: 0.4302399754524231\n",
      "Epoch 18, Batch 149, Test Loss: 0.558197021484375\n",
      "Epoch 18, Batch 150, Test Loss: 0.42543596029281616\n",
      "Epoch 18, Batch 151, Test Loss: 0.3969976305961609\n",
      "Epoch 18, Batch 152, Test Loss: 0.3880194425582886\n",
      "Epoch 18, Batch 153, Test Loss: 0.3838713467121124\n",
      "Epoch 18, Batch 154, Test Loss: 0.3397184908390045\n",
      "Epoch 18, Batch 155, Test Loss: 0.5751276016235352\n",
      "Epoch 18, Batch 156, Test Loss: 0.32932618260383606\n",
      "Epoch 18, Batch 157, Test Loss: 0.6412768363952637\n",
      "Epoch 18, Batch 158, Test Loss: 0.37285321950912476\n",
      "Epoch 18, Batch 159, Test Loss: 0.45968323945999146\n",
      "Epoch 18, Batch 160, Test Loss: 0.29255375266075134\n",
      "Epoch 18, Batch 161, Test Loss: 0.3699868321418762\n",
      "Epoch 18, Batch 162, Test Loss: 0.39297711849212646\n",
      "Epoch 18, Batch 163, Test Loss: 0.35542941093444824\n",
      "Epoch 18, Batch 164, Test Loss: 0.37313607335090637\n",
      "Epoch 18, Batch 165, Test Loss: 0.31693577766418457\n",
      "Epoch 18, Batch 166, Test Loss: 0.49203062057495117\n",
      "Epoch 18, Batch 167, Test Loss: 0.5099565982818604\n",
      "Epoch 18, Batch 168, Test Loss: 0.37016475200653076\n",
      "Epoch 18, Batch 169, Test Loss: 0.7030506134033203\n",
      "Epoch 18, Batch 170, Test Loss: 0.3813256621360779\n",
      "Epoch 18, Batch 171, Test Loss: 0.49318742752075195\n",
      "Epoch 18, Batch 172, Test Loss: 0.4351639747619629\n",
      "Epoch 18, Batch 173, Test Loss: 0.26643478870391846\n",
      "Epoch 18, Batch 174, Test Loss: 0.36824020743370056\n",
      "Epoch 18, Batch 175, Test Loss: 0.3007581830024719\n",
      "Epoch 18, Batch 176, Test Loss: 0.5222674012184143\n",
      "Epoch 18, Batch 177, Test Loss: 0.5199270248413086\n",
      "Epoch 18, Batch 178, Test Loss: 0.39549359679222107\n",
      "Epoch 18, Batch 179, Test Loss: 0.4289179742336273\n",
      "Epoch 18, Batch 180, Test Loss: 0.22806040942668915\n",
      "Epoch 18, Batch 181, Test Loss: 0.4816088080406189\n",
      "Epoch 18, Batch 182, Test Loss: 0.35679274797439575\n",
      "Epoch 18, Batch 183, Test Loss: 0.44545963406562805\n",
      "Epoch 18, Batch 184, Test Loss: 0.41502952575683594\n",
      "Epoch 18, Batch 185, Test Loss: 0.4448239803314209\n",
      "Epoch 18, Batch 186, Test Loss: 0.3852885961532593\n",
      "Epoch 18, Batch 187, Test Loss: 0.545803964138031\n",
      "Epoch 18, Batch 188, Test Loss: 0.3333059251308441\n",
      "Epoch 18, Batch 189, Test Loss: 0.4196527600288391\n",
      "Epoch 18, Batch 190, Test Loss: 0.43397682905197144\n",
      "Epoch 18, Batch 191, Test Loss: 0.497578501701355\n",
      "Epoch 18, Batch 192, Test Loss: 0.5259196758270264\n",
      "Epoch 18, Batch 193, Test Loss: 0.4338415265083313\n",
      "Epoch 18, Batch 194, Test Loss: 0.4128035604953766\n",
      "Epoch 18, Batch 195, Test Loss: 0.42414912581443787\n",
      "Epoch 18, Batch 196, Test Loss: 0.446518212556839\n",
      "Epoch 18, Batch 197, Test Loss: 0.3669673800468445\n",
      "Epoch 18, Batch 198, Test Loss: 0.5653681755065918\n",
      "Epoch 18, Batch 199, Test Loss: 0.6707302927970886\n",
      "Epoch 18, Batch 200, Test Loss: 0.35557276010513306\n",
      "Epoch 18, Batch 201, Test Loss: 0.4210560917854309\n",
      "Epoch 18, Batch 202, Test Loss: 0.3515776991844177\n",
      "Epoch 18, Batch 203, Test Loss: 0.2949349582195282\n",
      "Epoch 18, Batch 204, Test Loss: 0.39954131841659546\n",
      "Epoch 18, Batch 205, Test Loss: 0.44778820872306824\n",
      "Epoch 18, Batch 206, Test Loss: 0.4565843343734741\n",
      "Epoch 18, Batch 207, Test Loss: 0.32129380106925964\n",
      "Epoch 18, Batch 208, Test Loss: 0.44236427545547485\n",
      "Epoch 18, Batch 209, Test Loss: 0.4916785657405853\n",
      "Epoch 18, Batch 210, Test Loss: 0.42643818259239197\n",
      "Epoch 18, Batch 211, Test Loss: 0.5320186614990234\n",
      "Epoch 18, Batch 212, Test Loss: 0.3497096598148346\n",
      "Epoch 18, Batch 213, Test Loss: 0.29419806599617004\n",
      "Epoch 18, Batch 214, Test Loss: 0.34549832344055176\n",
      "Epoch 18, Batch 215, Test Loss: 0.44394710659980774\n",
      "Epoch 18, Batch 216, Test Loss: 0.3052634000778198\n",
      "Epoch 18, Batch 217, Test Loss: 0.5596224069595337\n",
      "Epoch 18, Batch 218, Test Loss: 0.3395865261554718\n",
      "Epoch 18, Batch 219, Test Loss: 0.42043760418891907\n",
      "Epoch 18, Batch 220, Test Loss: 0.5112472176551819\n",
      "Epoch 18, Batch 221, Test Loss: 0.40045154094696045\n",
      "Epoch 18, Batch 222, Test Loss: 0.5561253428459167\n",
      "Epoch 18, Batch 223, Test Loss: 0.33504974842071533\n",
      "Epoch 18, Batch 224, Test Loss: 0.5135424137115479\n",
      "Epoch 18, Batch 225, Test Loss: 0.3240465819835663\n",
      "Epoch 18, Batch 226, Test Loss: 0.36814171075820923\n",
      "Epoch 18, Batch 227, Test Loss: 0.38393205404281616\n",
      "Epoch 18, Batch 228, Test Loss: 0.3692789077758789\n",
      "Epoch 18, Batch 229, Test Loss: 0.5340186357498169\n",
      "Epoch 18, Batch 230, Test Loss: 0.46645015478134155\n",
      "Epoch 18, Batch 231, Test Loss: 0.5912901759147644\n",
      "Epoch 18, Batch 232, Test Loss: 0.3812006115913391\n",
      "Epoch 18, Batch 233, Test Loss: 0.3579186797142029\n",
      "Epoch 18, Batch 234, Test Loss: 0.5078890323638916\n",
      "Epoch 18, Batch 235, Test Loss: 0.25050246715545654\n",
      "Epoch 18, Batch 236, Test Loss: 0.39588016271591187\n",
      "Epoch 18, Batch 237, Test Loss: 0.3225744962692261\n",
      "Epoch 18, Batch 238, Test Loss: 0.4081205129623413\n",
      "Epoch 18, Batch 239, Test Loss: 0.41796666383743286\n",
      "Epoch 18, Batch 240, Test Loss: 0.31172072887420654\n",
      "Epoch 18, Batch 241, Test Loss: 0.3655651807785034\n",
      "Epoch 18, Batch 242, Test Loss: 0.6281155943870544\n",
      "Epoch 18, Batch 243, Test Loss: 0.5404992699623108\n",
      "Epoch 18, Batch 244, Test Loss: 0.3087306022644043\n",
      "Epoch 18, Batch 245, Test Loss: 0.49538546800613403\n",
      "Epoch 18, Batch 246, Test Loss: 0.37051936984062195\n",
      "Epoch 18, Batch 247, Test Loss: 0.4439490735530853\n",
      "Epoch 18, Batch 248, Test Loss: 0.255327433347702\n",
      "Epoch 18, Batch 249, Test Loss: 0.326188325881958\n",
      "Epoch 18, Batch 250, Test Loss: 0.3581880033016205\n",
      "Epoch 18, Batch 251, Test Loss: 0.401874303817749\n",
      "Epoch 18, Batch 252, Test Loss: 0.3795606195926666\n",
      "Epoch 18, Batch 253, Test Loss: 0.4400781989097595\n",
      "Epoch 18, Batch 254, Test Loss: 0.3588416278362274\n",
      "Epoch 18, Batch 255, Test Loss: 0.3361753523349762\n",
      "Epoch 18, Batch 256, Test Loss: 0.36291971802711487\n",
      "Epoch 18, Batch 257, Test Loss: 0.44389280676841736\n",
      "Epoch 18, Batch 258, Test Loss: 0.2632901668548584\n",
      "Epoch 18, Batch 259, Test Loss: 0.28243184089660645\n",
      "Epoch 18, Batch 260, Test Loss: 0.4638640880584717\n",
      "Epoch 18, Batch 261, Test Loss: 0.21767482161521912\n",
      "Epoch 18, Batch 262, Test Loss: 0.24244320392608643\n",
      "Epoch 18, Batch 263, Test Loss: 0.4115281403064728\n",
      "Epoch 18, Batch 264, Test Loss: 0.3116816580295563\n",
      "Epoch 18, Batch 265, Test Loss: 0.3846243917942047\n",
      "Epoch 18, Batch 266, Test Loss: 0.5165638327598572\n",
      "Epoch 18, Batch 267, Test Loss: 0.3989526629447937\n",
      "Epoch 18, Batch 268, Test Loss: 0.4187498688697815\n",
      "Epoch 18, Batch 269, Test Loss: 0.5055351853370667\n",
      "Epoch 18, Batch 270, Test Loss: 0.3811229467391968\n",
      "Epoch 18, Batch 271, Test Loss: 0.30621442198753357\n",
      "Epoch 18, Batch 272, Test Loss: 0.4442748427391052\n",
      "Epoch 18, Batch 273, Test Loss: 0.33702945709228516\n",
      "Epoch 18, Batch 274, Test Loss: 0.36563020944595337\n",
      "Epoch 18, Batch 275, Test Loss: 0.4965296685695648\n",
      "Epoch 18, Batch 276, Test Loss: 0.4800500273704529\n",
      "Epoch 18, Batch 277, Test Loss: 0.5819078683853149\n",
      "Epoch 18, Batch 278, Test Loss: 0.5764847993850708\n",
      "Epoch 18, Batch 279, Test Loss: 0.4024500548839569\n",
      "Epoch 18, Batch 280, Test Loss: 0.39796000719070435\n",
      "Epoch 18, Batch 281, Test Loss: 0.46360400319099426\n",
      "Epoch 18, Batch 282, Test Loss: 0.3472426235675812\n",
      "Epoch 18, Batch 283, Test Loss: 0.37094590067863464\n",
      "Epoch 18, Batch 284, Test Loss: 0.44214770197868347\n",
      "Epoch 18, Batch 285, Test Loss: 0.5108822584152222\n",
      "Epoch 18, Batch 286, Test Loss: 0.2589761018753052\n",
      "Epoch 18, Batch 287, Test Loss: 0.32764193415641785\n",
      "Epoch 18, Batch 288, Test Loss: 0.4765985608100891\n",
      "Epoch 18, Batch 289, Test Loss: 0.43850669264793396\n",
      "Epoch 18, Batch 290, Test Loss: 0.503828227519989\n",
      "Epoch 18, Batch 291, Test Loss: 0.42544060945510864\n",
      "Epoch 18, Batch 292, Test Loss: 0.3685898184776306\n",
      "Epoch 18, Batch 293, Test Loss: 0.7279658317565918\n",
      "Epoch 18, Batch 294, Test Loss: 0.4616642892360687\n",
      "Epoch 18, Batch 295, Test Loss: 0.5512980222702026\n",
      "Epoch 18, Batch 296, Test Loss: 0.4661288857460022\n",
      "Epoch 18, Batch 297, Test Loss: 0.46290913224220276\n",
      "Epoch 18, Batch 298, Test Loss: 0.28530967235565186\n",
      "Epoch 18, Batch 299, Test Loss: 0.3182610869407654\n",
      "Epoch 18, Batch 300, Test Loss: 0.46752509474754333\n",
      "Epoch 18, Batch 301, Test Loss: 0.30250778794288635\n",
      "Epoch 18, Batch 302, Test Loss: 0.491062194108963\n",
      "Epoch 18, Batch 303, Test Loss: 0.34008774161338806\n",
      "Epoch 18, Batch 304, Test Loss: 0.6041949987411499\n",
      "Epoch 18, Batch 305, Test Loss: 0.28963136672973633\n",
      "Epoch 18, Batch 306, Test Loss: 0.3999347388744354\n",
      "Epoch 18, Batch 307, Test Loss: 0.3803815245628357\n",
      "Epoch 18, Batch 308, Test Loss: 0.47659701108932495\n",
      "Epoch 18, Batch 309, Test Loss: 0.38831281661987305\n",
      "Epoch 18, Batch 310, Test Loss: 0.2877231538295746\n",
      "Epoch 18, Batch 311, Test Loss: 0.5648213624954224\n",
      "Epoch 18, Batch 312, Test Loss: 0.4901561141014099\n",
      "Epoch 18, Batch 313, Test Loss: 0.34908396005630493\n",
      "Epoch 18, Batch 314, Test Loss: 0.43803685903549194\n",
      "Epoch 18, Batch 315, Test Loss: 0.40798187255859375\n",
      "Epoch 18, Batch 316, Test Loss: 0.3445305824279785\n",
      "Epoch 18, Batch 317, Test Loss: 0.540645182132721\n",
      "Epoch 18, Batch 318, Test Loss: 0.44650593400001526\n",
      "Epoch 18, Batch 319, Test Loss: 0.38341736793518066\n",
      "Epoch 18, Batch 320, Test Loss: 0.4629252552986145\n",
      "Epoch 18, Batch 321, Test Loss: 0.3808647096157074\n",
      "Epoch 18, Batch 322, Test Loss: 0.33500102162361145\n",
      "Epoch 18, Batch 323, Test Loss: 0.42843398451805115\n",
      "Epoch 18, Batch 324, Test Loss: 0.27836117148399353\n",
      "Epoch 18, Batch 325, Test Loss: 0.41785258054733276\n",
      "Epoch 18, Batch 326, Test Loss: 0.2740006148815155\n",
      "Epoch 18, Batch 327, Test Loss: 0.4860832095146179\n",
      "Epoch 18, Batch 328, Test Loss: 0.3229239284992218\n",
      "Epoch 18, Batch 329, Test Loss: 0.5324423909187317\n",
      "Epoch 18, Batch 330, Test Loss: 0.3468858003616333\n",
      "Epoch 18, Batch 331, Test Loss: 0.38305166363716125\n",
      "Epoch 18, Batch 332, Test Loss: 0.43900737166404724\n",
      "Epoch 18, Batch 333, Test Loss: 0.2846744656562805\n",
      "Epoch 18, Batch 334, Test Loss: 0.4072505831718445\n",
      "Epoch 18, Batch 335, Test Loss: 0.4072284400463104\n",
      "Epoch 18, Batch 336, Test Loss: 0.2265026867389679\n",
      "Epoch 18, Batch 337, Test Loss: 0.43351754546165466\n",
      "Epoch 18, Batch 338, Test Loss: 0.37153327465057373\n",
      "Epoch 18, Batch 339, Test Loss: 0.39234694838523865\n",
      "Epoch 18, Batch 340, Test Loss: 0.3384346067905426\n",
      "Epoch 18, Batch 341, Test Loss: 0.3984798192977905\n",
      "Epoch 18, Batch 342, Test Loss: 0.47117626667022705\n",
      "Epoch 18, Batch 343, Test Loss: 0.4268965721130371\n",
      "Epoch 18, Batch 344, Test Loss: 0.31468620896339417\n",
      "Epoch 18, Batch 345, Test Loss: 0.2560853362083435\n",
      "Epoch 18, Batch 346, Test Loss: 0.35188543796539307\n",
      "Epoch 18, Batch 347, Test Loss: 0.36682021617889404\n",
      "Epoch 18, Batch 348, Test Loss: 0.6007608771324158\n",
      "Epoch 18, Batch 349, Test Loss: 0.4857925772666931\n",
      "Epoch 18, Batch 350, Test Loss: 0.4232975244522095\n",
      "Epoch 18, Batch 351, Test Loss: 0.34440043568611145\n",
      "Epoch 18, Batch 352, Test Loss: 0.5714526176452637\n",
      "Epoch 18, Batch 353, Test Loss: 0.36768850684165955\n",
      "Epoch 18, Batch 354, Test Loss: 0.6039461493492126\n",
      "Epoch 18, Batch 355, Test Loss: 0.5051039457321167\n",
      "Epoch 18, Batch 356, Test Loss: 0.36212751269340515\n",
      "Epoch 18, Batch 357, Test Loss: 0.440744549036026\n",
      "Epoch 18, Batch 358, Test Loss: 0.4111095666885376\n",
      "Epoch 18, Batch 359, Test Loss: 0.3719801902770996\n",
      "Epoch 18, Batch 360, Test Loss: 0.5140846967697144\n",
      "Epoch 18, Batch 361, Test Loss: 0.3400695025920868\n",
      "Epoch 18, Batch 362, Test Loss: 0.5474774241447449\n",
      "Epoch 18, Batch 363, Test Loss: 0.3677283823490143\n",
      "Epoch 18, Batch 364, Test Loss: 0.3081846535205841\n",
      "Epoch 18, Batch 365, Test Loss: 0.5779469013214111\n",
      "Epoch 18, Batch 366, Test Loss: 0.4604065418243408\n",
      "Epoch 18, Batch 367, Test Loss: 0.5307834148406982\n",
      "Epoch 18, Batch 368, Test Loss: 0.447216272354126\n",
      "Epoch 18, Batch 369, Test Loss: 0.5820555090904236\n",
      "Epoch 18, Batch 370, Test Loss: 0.4463609755039215\n",
      "Epoch 18, Batch 371, Test Loss: 0.474307119846344\n",
      "Epoch 18, Batch 372, Test Loss: 0.3646988272666931\n",
      "Epoch 18, Batch 373, Test Loss: 0.4344121217727661\n",
      "Epoch 18, Batch 374, Test Loss: 0.3086286187171936\n",
      "Epoch 18, Batch 375, Test Loss: 0.3702813982963562\n",
      "Epoch 18, Batch 376, Test Loss: 0.2424471378326416\n",
      "Epoch 18, Batch 377, Test Loss: 0.44783127307891846\n",
      "Epoch 18, Batch 378, Test Loss: 0.35186567902565\n",
      "Epoch 18, Batch 379, Test Loss: 0.4763523042201996\n",
      "Epoch 18, Batch 380, Test Loss: 0.2922155261039734\n",
      "Epoch 18, Batch 381, Test Loss: 0.5271602869033813\n",
      "Epoch 18, Batch 382, Test Loss: 0.464925616979599\n",
      "Epoch 18, Batch 383, Test Loss: 0.4280238449573517\n",
      "Epoch 18, Batch 384, Test Loss: 0.22838705778121948\n",
      "Epoch 18, Batch 385, Test Loss: 0.5208694338798523\n",
      "Epoch 18, Batch 386, Test Loss: 0.32573243975639343\n",
      "Epoch 18, Batch 387, Test Loss: 0.48069998621940613\n",
      "Epoch 18, Batch 388, Test Loss: 0.28471481800079346\n",
      "Epoch 18, Batch 389, Test Loss: 0.49431854486465454\n",
      "Epoch 18, Batch 390, Test Loss: 0.4681904911994934\n",
      "Epoch 18, Batch 391, Test Loss: 0.23891887068748474\n",
      "Epoch 18, Batch 392, Test Loss: 0.48558515310287476\n",
      "Epoch 18, Batch 393, Test Loss: 0.48840880393981934\n",
      "Epoch 18, Batch 394, Test Loss: 0.43916186690330505\n",
      "Epoch 18, Batch 395, Test Loss: 0.3315500020980835\n",
      "Epoch 18, Batch 396, Test Loss: 0.4048374593257904\n",
      "Epoch 18, Batch 397, Test Loss: 0.34157902002334595\n",
      "Epoch 18, Batch 398, Test Loss: 0.40018975734710693\n",
      "Epoch 18, Batch 399, Test Loss: 0.3736702799797058\n",
      "Epoch 18, Batch 400, Test Loss: 0.35244038701057434\n",
      "Epoch 18, Batch 401, Test Loss: 0.3350493013858795\n",
      "Epoch 18, Batch 402, Test Loss: 0.47902143001556396\n",
      "Epoch 18, Batch 403, Test Loss: 0.2736569941043854\n",
      "Epoch 18, Batch 404, Test Loss: 0.3585546612739563\n",
      "Epoch 18, Batch 405, Test Loss: 0.48518455028533936\n",
      "Epoch 18, Batch 406, Test Loss: 0.27361026406288147\n",
      "Epoch 18, Batch 407, Test Loss: 0.6181623935699463\n",
      "Epoch 18, Batch 408, Test Loss: 0.344862699508667\n",
      "Epoch 18, Batch 409, Test Loss: 0.5996352434158325\n",
      "Epoch 18, Batch 410, Test Loss: 0.354903906583786\n",
      "Epoch 18, Batch 411, Test Loss: 0.3543913960456848\n",
      "Epoch 18, Batch 412, Test Loss: 0.40514492988586426\n",
      "Epoch 18, Batch 413, Test Loss: 0.3522990345954895\n",
      "Epoch 18, Batch 414, Test Loss: 0.42260438203811646\n",
      "Epoch 18, Batch 415, Test Loss: 0.25739726424217224\n",
      "Epoch 18, Batch 416, Test Loss: 0.5359005928039551\n",
      "Epoch 18, Batch 417, Test Loss: 0.4484756290912628\n",
      "Epoch 18, Batch 418, Test Loss: 0.37773433327674866\n",
      "Epoch 18, Batch 419, Test Loss: 0.5602515339851379\n",
      "Epoch 18, Batch 420, Test Loss: 0.4073486328125\n",
      "Epoch 18, Batch 421, Test Loss: 0.30752038955688477\n",
      "Epoch 18, Batch 422, Test Loss: 0.5527923107147217\n",
      "Epoch 18, Batch 423, Test Loss: 0.39093315601348877\n",
      "Epoch 18, Batch 424, Test Loss: 0.49346017837524414\n",
      "Epoch 18, Batch 425, Test Loss: 0.5453035831451416\n",
      "Epoch 18, Batch 426, Test Loss: 0.4508894681930542\n",
      "Epoch 18, Batch 427, Test Loss: 0.31947875022888184\n",
      "Epoch 18, Batch 428, Test Loss: 0.4421825706958771\n",
      "Epoch 18, Batch 429, Test Loss: 0.3446638882160187\n",
      "Epoch 18, Batch 430, Test Loss: 0.6124875545501709\n",
      "Epoch 18, Batch 431, Test Loss: 0.45572465658187866\n",
      "Epoch 18, Batch 432, Test Loss: 0.38260602951049805\n",
      "Epoch 18, Batch 433, Test Loss: 0.4101904630661011\n",
      "Epoch 18, Batch 434, Test Loss: 0.5840398073196411\n",
      "Epoch 18, Batch 435, Test Loss: 0.5479507446289062\n",
      "Epoch 18, Batch 436, Test Loss: 0.4113996922969818\n",
      "Epoch 18, Batch 437, Test Loss: 0.43929973244667053\n",
      "Epoch 18, Batch 438, Test Loss: 0.382190465927124\n",
      "Epoch 18, Batch 439, Test Loss: 0.5489578247070312\n",
      "Epoch 18, Batch 440, Test Loss: 0.6517282724380493\n",
      "Epoch 18, Batch 441, Test Loss: 0.41735708713531494\n",
      "Epoch 18, Batch 442, Test Loss: 0.22805538773536682\n",
      "Epoch 18, Batch 443, Test Loss: 0.5051422715187073\n",
      "Epoch 18, Batch 444, Test Loss: 0.2559448182582855\n",
      "Epoch 18, Batch 445, Test Loss: 0.41112732887268066\n",
      "Epoch 18, Batch 446, Test Loss: 0.5126219987869263\n",
      "Epoch 18, Batch 447, Test Loss: 0.41675305366516113\n",
      "Epoch 18, Batch 448, Test Loss: 0.38416922092437744\n",
      "Epoch 18, Batch 449, Test Loss: 0.3422030210494995\n",
      "Epoch 18, Batch 450, Test Loss: 0.34299641847610474\n",
      "Epoch 18, Batch 451, Test Loss: 0.6067185997962952\n",
      "Epoch 18, Batch 452, Test Loss: 0.41110363602638245\n",
      "Epoch 18, Batch 453, Test Loss: 0.4427573084831238\n",
      "Epoch 18, Batch 454, Test Loss: 0.4452900290489197\n",
      "Epoch 18, Batch 455, Test Loss: 0.320796936750412\n",
      "Epoch 18, Batch 456, Test Loss: 0.522082507610321\n",
      "Epoch 18, Batch 457, Test Loss: 0.351641446352005\n",
      "Epoch 18, Batch 458, Test Loss: 0.400850772857666\n",
      "Epoch 18, Batch 459, Test Loss: 0.5219763517379761\n",
      "Epoch 18, Batch 460, Test Loss: 0.3337996006011963\n",
      "Epoch 18, Batch 461, Test Loss: 0.5120742321014404\n",
      "Epoch 18, Batch 462, Test Loss: 0.32585516571998596\n",
      "Epoch 18, Batch 463, Test Loss: 0.3354146480560303\n",
      "Epoch 18, Batch 464, Test Loss: 0.3604544401168823\n",
      "Epoch 18, Batch 465, Test Loss: 0.40279000997543335\n",
      "Epoch 18, Batch 466, Test Loss: 0.48593664169311523\n",
      "Epoch 18, Batch 467, Test Loss: 0.2714635133743286\n",
      "Epoch 18, Batch 468, Test Loss: 0.26513466238975525\n",
      "Epoch 18, Batch 469, Test Loss: 0.537646472454071\n",
      "Epoch 18, Batch 470, Test Loss: 0.46364691853523254\n",
      "Epoch 18, Batch 471, Test Loss: 0.20230044424533844\n",
      "Epoch 18, Batch 472, Test Loss: 0.5436127781867981\n",
      "Epoch 18, Batch 473, Test Loss: 0.47677484154701233\n",
      "Epoch 18, Batch 474, Test Loss: 0.2752544581890106\n",
      "Epoch 18, Batch 475, Test Loss: 0.45718464255332947\n",
      "Epoch 18, Batch 476, Test Loss: 0.4372081160545349\n",
      "Epoch 18, Batch 477, Test Loss: 0.3552861213684082\n",
      "Epoch 18, Batch 478, Test Loss: 0.5274854898452759\n",
      "Epoch 18, Batch 479, Test Loss: 0.6564528346061707\n",
      "Epoch 18, Batch 480, Test Loss: 0.44247835874557495\n",
      "Epoch 18, Batch 481, Test Loss: 0.4847985804080963\n",
      "Epoch 18, Batch 482, Test Loss: 0.42704448103904724\n",
      "Epoch 18, Batch 483, Test Loss: 0.3504640460014343\n",
      "Epoch 18, Batch 484, Test Loss: 0.39401447772979736\n",
      "Epoch 18, Batch 485, Test Loss: 0.3728535771369934\n",
      "Epoch 18, Batch 486, Test Loss: 0.4743947386741638\n",
      "Epoch 18, Batch 487, Test Loss: 0.37122228741645813\n",
      "Epoch 18, Batch 488, Test Loss: 0.3458141088485718\n",
      "Epoch 18, Batch 489, Test Loss: 0.5098740458488464\n",
      "Epoch 18, Batch 490, Test Loss: 0.2905339002609253\n",
      "Epoch 18, Batch 491, Test Loss: 0.4922487437725067\n",
      "Epoch 18, Batch 492, Test Loss: 0.2797958552837372\n",
      "Epoch 18, Batch 493, Test Loss: 0.51385897397995\n",
      "Epoch 18, Batch 494, Test Loss: 0.9245741963386536\n",
      "Epoch 18, Batch 495, Test Loss: 0.35741084814071655\n",
      "Epoch 18, Batch 496, Test Loss: 0.24499177932739258\n",
      "Epoch 18, Batch 497, Test Loss: 0.33260083198547363\n",
      "Epoch 18, Batch 498, Test Loss: 0.3987683951854706\n",
      "Epoch 18, Batch 499, Test Loss: 0.38285765051841736\n",
      "Epoch 18, Batch 500, Test Loss: 0.39551860094070435\n",
      "Epoch 18, Batch 501, Test Loss: 0.3393314778804779\n",
      "Epoch 18, Batch 502, Test Loss: 0.43750476837158203\n",
      "Epoch 18, Batch 503, Test Loss: 0.5145369172096252\n",
      "Epoch 18, Batch 504, Test Loss: 0.4632202684879303\n",
      "Epoch 18, Batch 505, Test Loss: 0.6548259258270264\n",
      "Epoch 18, Batch 506, Test Loss: 0.4831835627555847\n",
      "Epoch 18, Batch 507, Test Loss: 0.6268641352653503\n",
      "Epoch 18, Batch 508, Test Loss: 0.2528456747531891\n",
      "Epoch 18, Batch 509, Test Loss: 0.4745691120624542\n",
      "Epoch 18, Batch 510, Test Loss: 0.3953491449356079\n",
      "Epoch 18, Batch 511, Test Loss: 0.42675596475601196\n",
      "Epoch 18, Batch 512, Test Loss: 0.3728327751159668\n",
      "Epoch 18, Batch 513, Test Loss: 0.3920578062534332\n",
      "Epoch 18, Batch 514, Test Loss: 0.4883109927177429\n",
      "Epoch 18, Batch 515, Test Loss: 0.3194611072540283\n",
      "Epoch 18, Batch 516, Test Loss: 0.5049081444740295\n",
      "Epoch 18, Batch 517, Test Loss: 0.3905646502971649\n",
      "Epoch 18, Batch 518, Test Loss: 0.21669629216194153\n",
      "Epoch 18, Batch 519, Test Loss: 0.33980685472488403\n",
      "Epoch 18, Batch 520, Test Loss: 0.33998650312423706\n",
      "Epoch 18, Batch 521, Test Loss: 0.28720009326934814\n",
      "Epoch 18, Batch 522, Test Loss: 0.6166568994522095\n",
      "Epoch 18, Batch 523, Test Loss: 0.4831607937812805\n",
      "Epoch 18, Batch 524, Test Loss: 0.5754050016403198\n",
      "Epoch 18, Batch 525, Test Loss: 0.3981129229068756\n",
      "Epoch 18, Batch 526, Test Loss: 0.46045613288879395\n",
      "Epoch 18, Batch 527, Test Loss: 0.310825914144516\n",
      "Epoch 18, Batch 528, Test Loss: 0.4122920334339142\n",
      "Epoch 18, Batch 529, Test Loss: 0.43068447709083557\n",
      "Epoch 18, Batch 530, Test Loss: 0.456437885761261\n",
      "Epoch 18, Batch 531, Test Loss: 0.4155990481376648\n",
      "Epoch 18, Batch 532, Test Loss: 0.3991979658603668\n",
      "Epoch 18, Batch 533, Test Loss: 0.40922844409942627\n",
      "Epoch 18, Batch 534, Test Loss: 0.31638258695602417\n",
      "Epoch 18, Batch 535, Test Loss: 0.32116660475730896\n",
      "Epoch 18, Batch 536, Test Loss: 0.4043050706386566\n",
      "Epoch 18, Batch 537, Test Loss: 0.6630420684814453\n",
      "Epoch 18, Batch 538, Test Loss: 0.3552814722061157\n",
      "Epoch 18, Batch 539, Test Loss: 0.3313139081001282\n",
      "Epoch 18, Batch 540, Test Loss: 0.5082730650901794\n",
      "Epoch 18, Batch 541, Test Loss: 0.3902696371078491\n",
      "Epoch 18, Batch 542, Test Loss: 0.32435810565948486\n",
      "Epoch 18, Batch 543, Test Loss: 0.37226882576942444\n",
      "Epoch 18, Batch 544, Test Loss: 0.3716835677623749\n",
      "Epoch 18, Batch 545, Test Loss: 0.5159936547279358\n",
      "Epoch 18, Batch 546, Test Loss: 0.4152829647064209\n",
      "Epoch 18, Batch 547, Test Loss: 0.6043916344642639\n",
      "Epoch 18, Batch 548, Test Loss: 0.342393696308136\n",
      "Epoch 18, Batch 549, Test Loss: 0.34338048100471497\n",
      "Epoch 18, Batch 550, Test Loss: 0.5393944382667542\n",
      "Epoch 18, Batch 551, Test Loss: 0.41806912422180176\n",
      "Epoch 18, Batch 552, Test Loss: 0.4771832823753357\n",
      "Epoch 18, Batch 553, Test Loss: 0.5978274345397949\n",
      "Epoch 18, Batch 554, Test Loss: 0.560368001461029\n",
      "Epoch 18, Batch 555, Test Loss: 0.4525309205055237\n",
      "Epoch 18, Batch 556, Test Loss: 0.3953418433666229\n",
      "Epoch 18, Batch 557, Test Loss: 0.2677842676639557\n",
      "Epoch 18, Batch 558, Test Loss: 0.21132847666740417\n",
      "Epoch 18, Batch 559, Test Loss: 0.4056352376937866\n",
      "Epoch 18, Batch 560, Test Loss: 0.4355353116989136\n",
      "Epoch 18, Batch 561, Test Loss: 0.45727071166038513\n",
      "Epoch 18, Batch 562, Test Loss: 0.468364953994751\n",
      "Epoch 18, Batch 563, Test Loss: 0.3830004036426544\n",
      "Epoch 18, Batch 564, Test Loss: 0.22327058017253876\n",
      "Epoch 18, Batch 565, Test Loss: 0.40189090371131897\n",
      "Epoch 18, Batch 566, Test Loss: 0.27740541100502014\n",
      "Epoch 18, Batch 567, Test Loss: 0.38610830903053284\n",
      "Epoch 18, Batch 568, Test Loss: 0.48096683621406555\n",
      "Epoch 18, Batch 569, Test Loss: 0.3591546416282654\n",
      "Epoch 18, Batch 570, Test Loss: 0.3406314551830292\n",
      "Epoch 18, Batch 571, Test Loss: 0.546660304069519\n",
      "Epoch 18, Batch 572, Test Loss: 0.3595792353153229\n",
      "Epoch 18, Batch 573, Test Loss: 0.3430000841617584\n",
      "Epoch 18, Batch 574, Test Loss: 0.35725098848342896\n",
      "Epoch 18, Batch 575, Test Loss: 0.3211101293563843\n",
      "Epoch 18, Batch 576, Test Loss: 0.48685112595558167\n",
      "Epoch 18, Batch 577, Test Loss: 0.3538263142108917\n",
      "Epoch 18, Batch 578, Test Loss: 0.39304789900779724\n",
      "Epoch 18, Batch 579, Test Loss: 0.41524040699005127\n",
      "Epoch 18, Batch 580, Test Loss: 0.4785756468772888\n",
      "Epoch 18, Batch 581, Test Loss: 0.4615047872066498\n",
      "Epoch 18, Batch 582, Test Loss: 0.4527966380119324\n",
      "Epoch 18, Batch 583, Test Loss: 0.4475434720516205\n",
      "Epoch 18, Batch 584, Test Loss: 0.4087119996547699\n",
      "Epoch 18, Batch 585, Test Loss: 0.35147717595100403\n",
      "Epoch 18, Batch 586, Test Loss: 0.6842119693756104\n",
      "Epoch 18, Batch 587, Test Loss: 0.4110367000102997\n",
      "Epoch 18, Batch 588, Test Loss: 0.5033106207847595\n",
      "Epoch 18, Batch 589, Test Loss: 0.4540519118309021\n",
      "Epoch 18, Batch 590, Test Loss: 0.38593149185180664\n",
      "Epoch 18, Batch 591, Test Loss: 0.43070554733276367\n",
      "Epoch 18, Batch 592, Test Loss: 0.3689931333065033\n",
      "Epoch 18, Batch 593, Test Loss: 0.3160336911678314\n",
      "Epoch 18, Batch 594, Test Loss: 0.5159509181976318\n",
      "Epoch 18, Batch 595, Test Loss: 0.3515697717666626\n",
      "Epoch 18, Batch 596, Test Loss: 0.4773723781108856\n",
      "Epoch 18, Batch 597, Test Loss: 0.4621082842350006\n",
      "Epoch 18, Batch 598, Test Loss: 0.41134893894195557\n",
      "Epoch 18, Batch 599, Test Loss: 0.41499656438827515\n",
      "Epoch 18, Batch 600, Test Loss: 0.4174402356147766\n",
      "Epoch 18, Batch 601, Test Loss: 0.36669713258743286\n",
      "Epoch 18, Batch 602, Test Loss: 0.6548810601234436\n",
      "Epoch 18, Batch 603, Test Loss: 0.2540505528450012\n",
      "Epoch 18, Batch 604, Test Loss: 0.6414608359336853\n",
      "Epoch 18, Batch 605, Test Loss: 0.4561186134815216\n",
      "Epoch 18, Batch 606, Test Loss: 0.32509881258010864\n",
      "Epoch 18, Batch 607, Test Loss: 0.4044468104839325\n",
      "Epoch 18, Batch 608, Test Loss: 0.3443245589733124\n",
      "Epoch 18, Batch 609, Test Loss: 0.41437244415283203\n",
      "Epoch 18, Batch 610, Test Loss: 0.39993733167648315\n",
      "Epoch 18, Batch 611, Test Loss: 0.401868999004364\n",
      "Epoch 18, Batch 612, Test Loss: 0.7259272336959839\n",
      "Epoch 18, Batch 613, Test Loss: 0.2541714906692505\n",
      "Epoch 18, Batch 614, Test Loss: 0.454387903213501\n",
      "Epoch 18, Batch 615, Test Loss: 0.43289974331855774\n",
      "Epoch 18, Batch 616, Test Loss: 0.3149818480014801\n",
      "Epoch 18, Batch 617, Test Loss: 0.3469487726688385\n",
      "Epoch 18, Batch 618, Test Loss: 0.4432482421398163\n",
      "Epoch 18, Batch 619, Test Loss: 0.3168937861919403\n",
      "Epoch 18, Batch 620, Test Loss: 0.3585553765296936\n",
      "Epoch 18, Batch 621, Test Loss: 0.31463444232940674\n",
      "Epoch 18, Batch 622, Test Loss: 0.21620270609855652\n",
      "Epoch 18, Batch 623, Test Loss: 0.585568904876709\n",
      "Epoch 18, Batch 624, Test Loss: 0.31423962116241455\n",
      "Epoch 18, Batch 625, Test Loss: 0.3393254280090332\n",
      "Epoch 18, Batch 626, Test Loss: 0.3251012861728668\n",
      "Epoch 18, Batch 627, Test Loss: 0.3982568085193634\n",
      "Epoch 18, Batch 628, Test Loss: 0.27542799711227417\n",
      "Epoch 18, Batch 629, Test Loss: 0.55317622423172\n",
      "Epoch 18, Batch 630, Test Loss: 0.5241355895996094\n",
      "Epoch 18, Batch 631, Test Loss: 0.6915733218193054\n",
      "Epoch 18, Batch 632, Test Loss: 0.29894331097602844\n",
      "Epoch 18, Batch 633, Test Loss: 0.34168335795402527\n",
      "Epoch 18, Batch 634, Test Loss: 0.3226536214351654\n",
      "Epoch 18, Batch 635, Test Loss: 0.5205308198928833\n",
      "Epoch 18, Batch 636, Test Loss: 0.723233699798584\n",
      "Epoch 18, Batch 637, Test Loss: 0.4430477023124695\n",
      "Epoch 18, Batch 638, Test Loss: 0.3052632808685303\n",
      "Epoch 18, Batch 639, Test Loss: 0.4128909409046173\n",
      "Epoch 18, Batch 640, Test Loss: 0.5048344135284424\n",
      "Epoch 18, Batch 641, Test Loss: 0.5002411603927612\n",
      "Epoch 18, Batch 642, Test Loss: 0.28799426555633545\n",
      "Epoch 18, Batch 643, Test Loss: 0.3479117751121521\n",
      "Epoch 18, Batch 644, Test Loss: 0.5557709336280823\n",
      "Epoch 18, Batch 645, Test Loss: 0.46605831384658813\n",
      "Epoch 18, Batch 646, Test Loss: 0.7017598748207092\n",
      "Epoch 18, Batch 647, Test Loss: 0.5894653797149658\n",
      "Epoch 18, Batch 648, Test Loss: 0.21498991549015045\n",
      "Epoch 18, Batch 649, Test Loss: 0.23197217285633087\n",
      "Epoch 18, Batch 650, Test Loss: 0.4562206268310547\n",
      "Epoch 18, Batch 651, Test Loss: 0.45829692482948303\n",
      "Epoch 18, Batch 652, Test Loss: 0.45927897095680237\n",
      "Epoch 18, Batch 653, Test Loss: 0.4859679937362671\n",
      "Epoch 18, Batch 654, Test Loss: 0.34778672456741333\n",
      "Epoch 18, Batch 655, Test Loss: 0.4355795085430145\n",
      "Epoch 18, Batch 656, Test Loss: 0.46946072578430176\n",
      "Epoch 18, Batch 657, Test Loss: 0.287426620721817\n",
      "Epoch 18, Batch 658, Test Loss: 0.39524710178375244\n",
      "Epoch 18, Batch 659, Test Loss: 0.325716495513916\n",
      "Epoch 18, Batch 660, Test Loss: 0.4727305769920349\n",
      "Epoch 18, Batch 661, Test Loss: 0.2830479145050049\n",
      "Epoch 18, Batch 662, Test Loss: 0.5139944553375244\n",
      "Epoch 18, Batch 663, Test Loss: 0.2743492126464844\n",
      "Epoch 18, Batch 664, Test Loss: 0.4227246642112732\n",
      "Epoch 18, Batch 665, Test Loss: 0.33761316537857056\n",
      "Epoch 18, Batch 666, Test Loss: 0.2999438941478729\n",
      "Epoch 18, Batch 667, Test Loss: 0.3222782015800476\n",
      "Epoch 18, Batch 668, Test Loss: 0.4476676881313324\n",
      "Epoch 18, Batch 669, Test Loss: 0.42192402482032776\n",
      "Epoch 18, Batch 670, Test Loss: 0.3231660723686218\n",
      "Epoch 18, Batch 671, Test Loss: 0.3528411090373993\n",
      "Epoch 18, Batch 672, Test Loss: 0.35761961340904236\n",
      "Epoch 18, Batch 673, Test Loss: 0.49808892607688904\n",
      "Epoch 18, Batch 674, Test Loss: 0.4025441110134125\n",
      "Epoch 18, Batch 675, Test Loss: 0.33209940791130066\n",
      "Epoch 18, Batch 676, Test Loss: 0.558812141418457\n",
      "Epoch 18, Batch 677, Test Loss: 0.563513457775116\n",
      "Epoch 18, Batch 678, Test Loss: 0.46542099118232727\n",
      "Epoch 18, Batch 679, Test Loss: 0.41261905431747437\n",
      "Epoch 18, Batch 680, Test Loss: 0.3623324930667877\n",
      "Epoch 18, Batch 681, Test Loss: 0.36395174264907837\n",
      "Epoch 18, Batch 682, Test Loss: 0.4210198223590851\n",
      "Epoch 18, Batch 683, Test Loss: 0.4468747079372406\n",
      "Epoch 18, Batch 684, Test Loss: 0.5000576376914978\n",
      "Epoch 18, Batch 685, Test Loss: 0.27411147952079773\n",
      "Epoch 18, Batch 686, Test Loss: 0.5461827516555786\n",
      "Epoch 18, Batch 687, Test Loss: 0.5380103588104248\n",
      "Epoch 18, Batch 688, Test Loss: 0.49439889192581177\n",
      "Epoch 18, Batch 689, Test Loss: 0.3183668553829193\n",
      "Epoch 18, Batch 690, Test Loss: 0.49306294322013855\n",
      "Epoch 18, Batch 691, Test Loss: 0.45805686712265015\n",
      "Epoch 18, Batch 692, Test Loss: 0.5280894637107849\n",
      "Epoch 18, Batch 693, Test Loss: 0.43763649463653564\n",
      "Epoch 18, Batch 694, Test Loss: 0.3592160642147064\n",
      "Epoch 18, Batch 695, Test Loss: 0.3667793273925781\n",
      "Epoch 18, Batch 696, Test Loss: 0.5683100819587708\n",
      "Epoch 18, Batch 697, Test Loss: 0.27293336391448975\n",
      "Epoch 18, Batch 698, Test Loss: 0.4191526770591736\n",
      "Epoch 18, Batch 699, Test Loss: 0.3256990313529968\n",
      "Epoch 18, Batch 700, Test Loss: 0.39772510528564453\n",
      "Epoch 18, Batch 701, Test Loss: 0.41932564973831177\n",
      "Epoch 18, Batch 702, Test Loss: 0.41314077377319336\n",
      "Epoch 18, Batch 703, Test Loss: 0.484405517578125\n",
      "Epoch 18, Batch 704, Test Loss: 0.31519815325737\n",
      "Epoch 18, Batch 705, Test Loss: 0.716137170791626\n",
      "Epoch 18, Batch 706, Test Loss: 0.47473978996276855\n",
      "Epoch 18, Batch 707, Test Loss: 0.22257845103740692\n",
      "Epoch 18, Batch 708, Test Loss: 0.4824627637863159\n",
      "Epoch 18, Batch 709, Test Loss: 0.452744722366333\n",
      "Epoch 18, Batch 710, Test Loss: 0.4088931381702423\n",
      "Epoch 18, Batch 711, Test Loss: 0.1336994767189026\n",
      "Epoch 18, Batch 712, Test Loss: 0.37096425890922546\n",
      "Epoch 18, Batch 713, Test Loss: 0.5660221576690674\n",
      "Epoch 18, Batch 714, Test Loss: 0.34473830461502075\n",
      "Epoch 18, Batch 715, Test Loss: 0.47554606199264526\n",
      "Epoch 18, Batch 716, Test Loss: 0.33745020627975464\n",
      "Epoch 18, Batch 717, Test Loss: 0.27428287267684937\n",
      "Epoch 18, Batch 718, Test Loss: 0.29771432280540466\n",
      "Epoch 18, Batch 719, Test Loss: 0.2962988018989563\n",
      "Epoch 18, Batch 720, Test Loss: 0.5256394147872925\n",
      "Epoch 18, Batch 721, Test Loss: 0.4625697731971741\n",
      "Epoch 18, Batch 722, Test Loss: 0.2524033784866333\n",
      "Epoch 18, Batch 723, Test Loss: 0.48254868388175964\n",
      "Epoch 18, Batch 724, Test Loss: 0.48260101675987244\n",
      "Epoch 18, Batch 725, Test Loss: 0.3489602208137512\n",
      "Epoch 18, Batch 726, Test Loss: 0.35060882568359375\n",
      "Epoch 18, Batch 727, Test Loss: 0.2893766760826111\n",
      "Epoch 18, Batch 728, Test Loss: 0.5504388213157654\n",
      "Epoch 18, Batch 729, Test Loss: 0.477049320936203\n",
      "Epoch 18, Batch 730, Test Loss: 0.3648749589920044\n",
      "Epoch 18, Batch 731, Test Loss: 0.45197635889053345\n",
      "Epoch 18, Batch 732, Test Loss: 0.38770681619644165\n",
      "Epoch 18, Batch 733, Test Loss: 0.49926382303237915\n",
      "Epoch 18, Batch 734, Test Loss: 0.33008241653442383\n",
      "Epoch 18, Batch 735, Test Loss: 0.5202760696411133\n",
      "Epoch 18, Batch 736, Test Loss: 0.32334771752357483\n",
      "Epoch 18, Batch 737, Test Loss: 0.32442760467529297\n",
      "Epoch 18, Batch 738, Test Loss: 0.6259549856185913\n",
      "Epoch 18, Batch 739, Test Loss: 0.46635571122169495\n",
      "Epoch 18, Batch 740, Test Loss: 0.40870165824890137\n",
      "Epoch 18, Batch 741, Test Loss: 0.5556420683860779\n",
      "Epoch 18, Batch 742, Test Loss: 0.2630174458026886\n",
      "Epoch 18, Batch 743, Test Loss: 0.32444310188293457\n",
      "Epoch 18, Batch 744, Test Loss: 0.524824321269989\n",
      "Epoch 18, Batch 745, Test Loss: 0.2839738726615906\n",
      "Epoch 18, Batch 746, Test Loss: 0.33482515811920166\n",
      "Epoch 18, Batch 747, Test Loss: 0.4147893190383911\n",
      "Epoch 18, Batch 748, Test Loss: 0.4804730713367462\n",
      "Epoch 18, Batch 749, Test Loss: 0.5498402118682861\n",
      "Epoch 18, Batch 750, Test Loss: 0.5918688774108887\n",
      "Epoch 18, Batch 751, Test Loss: 0.37374135851860046\n",
      "Epoch 18, Batch 752, Test Loss: 0.4284471273422241\n",
      "Epoch 18, Batch 753, Test Loss: 0.43480443954467773\n",
      "Epoch 18, Batch 754, Test Loss: 0.35107818245887756\n",
      "Epoch 18, Batch 755, Test Loss: 0.37509122490882874\n",
      "Epoch 18, Batch 756, Test Loss: 0.45705124735832214\n",
      "Epoch 18, Batch 757, Test Loss: 0.2411247193813324\n",
      "Epoch 18, Batch 758, Test Loss: 0.4269159436225891\n",
      "Epoch 18, Batch 759, Test Loss: 0.6076197624206543\n",
      "Epoch 18, Batch 760, Test Loss: 0.5461399555206299\n",
      "Epoch 18, Batch 761, Test Loss: 0.3538219630718231\n",
      "Epoch 18, Batch 762, Test Loss: 0.2668774724006653\n",
      "Epoch 18, Batch 763, Test Loss: 0.41427919268608093\n",
      "Epoch 18, Batch 764, Test Loss: 0.3287163972854614\n",
      "Epoch 18, Batch 765, Test Loss: 0.4743610620498657\n",
      "Epoch 18, Batch 766, Test Loss: 0.6269388794898987\n",
      "Epoch 18, Batch 767, Test Loss: 0.7089722156524658\n",
      "Epoch 18, Batch 768, Test Loss: 0.3744491934776306\n",
      "Epoch 18, Batch 769, Test Loss: 0.432736873626709\n",
      "Epoch 18, Batch 770, Test Loss: 0.5759681463241577\n",
      "Epoch 18, Batch 771, Test Loss: 0.3147738575935364\n",
      "Epoch 18, Batch 772, Test Loss: 0.5053800344467163\n",
      "Epoch 18, Batch 773, Test Loss: 0.3268090486526489\n",
      "Epoch 18, Batch 774, Test Loss: 0.45884066820144653\n",
      "Epoch 18, Batch 775, Test Loss: 0.3720806837081909\n",
      "Epoch 18, Batch 776, Test Loss: 0.32392778992652893\n",
      "Epoch 18, Batch 777, Test Loss: 0.49538132548332214\n",
      "Epoch 18, Batch 778, Test Loss: 0.5375456213951111\n",
      "Epoch 18, Batch 779, Test Loss: 0.33106759190559387\n",
      "Epoch 18, Batch 780, Test Loss: 0.48744186758995056\n",
      "Epoch 18, Batch 781, Test Loss: 0.2932310700416565\n",
      "Epoch 18, Batch 782, Test Loss: 0.509405255317688\n",
      "Epoch 18, Batch 783, Test Loss: 0.39605873823165894\n",
      "Epoch 18, Batch 784, Test Loss: 0.39008277654647827\n",
      "Epoch 18, Batch 785, Test Loss: 0.4129667580127716\n",
      "Epoch 18, Batch 786, Test Loss: 0.3596546947956085\n",
      "Epoch 18, Batch 787, Test Loss: 0.34184718132019043\n",
      "Epoch 18, Batch 788, Test Loss: 0.4186984598636627\n",
      "Epoch 18, Batch 789, Test Loss: 0.47571566700935364\n",
      "Epoch 18, Batch 790, Test Loss: 0.43660908937454224\n",
      "Epoch 18, Batch 791, Test Loss: 0.40694543719291687\n",
      "Epoch 18, Batch 792, Test Loss: 0.4027794897556305\n",
      "Epoch 18, Batch 793, Test Loss: 0.5085915327072144\n",
      "Epoch 18, Batch 794, Test Loss: 0.4784058928489685\n",
      "Epoch 18, Batch 795, Test Loss: 0.43620821833610535\n",
      "Epoch 18, Batch 796, Test Loss: 0.3795177638530731\n",
      "Epoch 18, Batch 797, Test Loss: 0.3850574195384979\n",
      "Epoch 18, Batch 798, Test Loss: 0.3608543574810028\n",
      "Epoch 18, Batch 799, Test Loss: 0.3945194482803345\n",
      "Epoch 18, Batch 800, Test Loss: 0.37896060943603516\n",
      "Epoch 18, Batch 801, Test Loss: 0.3818162679672241\n",
      "Epoch 18, Batch 802, Test Loss: 0.4670684337615967\n",
      "Epoch 18, Batch 803, Test Loss: 0.5887220501899719\n",
      "Epoch 18, Batch 804, Test Loss: 0.3905094265937805\n",
      "Epoch 18, Batch 805, Test Loss: 0.3872896134853363\n",
      "Epoch 18, Batch 806, Test Loss: 0.5112245678901672\n",
      "Epoch 18, Batch 807, Test Loss: 0.5730497241020203\n",
      "Epoch 18, Batch 808, Test Loss: 0.3440245985984802\n",
      "Epoch 18, Batch 809, Test Loss: 0.3785496652126312\n",
      "Epoch 18, Batch 810, Test Loss: 0.3040749430656433\n",
      "Epoch 18, Batch 811, Test Loss: 0.41779157519340515\n",
      "Epoch 18, Batch 812, Test Loss: 0.3719932436943054\n",
      "Epoch 18, Batch 813, Test Loss: 0.5031574964523315\n",
      "Epoch 18, Batch 814, Test Loss: 0.448762983083725\n",
      "Epoch 18, Batch 815, Test Loss: 0.3200920820236206\n",
      "Epoch 18, Batch 816, Test Loss: 0.35918447375297546\n",
      "Epoch 18, Batch 817, Test Loss: 0.4333600699901581\n",
      "Epoch 18, Batch 818, Test Loss: 0.4361724853515625\n",
      "Epoch 18, Batch 819, Test Loss: 0.3756575584411621\n",
      "Epoch 18, Batch 820, Test Loss: 0.44372379779815674\n",
      "Epoch 18, Batch 821, Test Loss: 0.4798271059989929\n",
      "Epoch 18, Batch 822, Test Loss: 0.40029090642929077\n",
      "Epoch 18, Batch 823, Test Loss: 0.42130911350250244\n",
      "Epoch 18, Batch 824, Test Loss: 0.3489425778388977\n",
      "Epoch 18, Batch 825, Test Loss: 0.3375672399997711\n",
      "Epoch 18, Batch 826, Test Loss: 0.4279554486274719\n",
      "Epoch 18, Batch 827, Test Loss: 0.24198110401630402\n",
      "Epoch 18, Batch 828, Test Loss: 0.4202776253223419\n",
      "Epoch 18, Batch 829, Test Loss: 0.6453935503959656\n",
      "Epoch 18, Batch 830, Test Loss: 0.491974413394928\n",
      "Epoch 18, Batch 831, Test Loss: 0.5332193374633789\n",
      "Epoch 18, Batch 832, Test Loss: 0.2948305904865265\n",
      "Epoch 18, Batch 833, Test Loss: 0.3336613178253174\n",
      "Epoch 18, Batch 834, Test Loss: 0.18869619071483612\n",
      "Epoch 18, Batch 835, Test Loss: 0.39923301339149475\n",
      "Epoch 18, Batch 836, Test Loss: 0.3607129156589508\n",
      "Epoch 18, Batch 837, Test Loss: 0.4034387469291687\n",
      "Epoch 18, Batch 838, Test Loss: 0.2964438796043396\n",
      "Epoch 18, Batch 839, Test Loss: 0.4231610894203186\n",
      "Epoch 18, Batch 840, Test Loss: 0.28258711099624634\n",
      "Epoch 18, Batch 841, Test Loss: 0.4370303153991699\n",
      "Epoch 18, Batch 842, Test Loss: 0.41944438219070435\n",
      "Epoch 18, Batch 843, Test Loss: 0.3290797472000122\n",
      "Epoch 18, Batch 844, Test Loss: 0.6258668899536133\n",
      "Epoch 18, Batch 845, Test Loss: 0.2796674966812134\n",
      "Epoch 18, Batch 846, Test Loss: 0.403691828250885\n",
      "Epoch 18, Batch 847, Test Loss: 0.36900025606155396\n",
      "Epoch 18, Batch 848, Test Loss: 0.25058186054229736\n",
      "Epoch 18, Batch 849, Test Loss: 0.5000672936439514\n",
      "Epoch 18, Batch 850, Test Loss: 0.28511637449264526\n",
      "Epoch 18, Batch 851, Test Loss: 0.49130377173423767\n",
      "Epoch 18, Batch 852, Test Loss: 0.23411057889461517\n",
      "Epoch 18, Batch 853, Test Loss: 0.26949334144592285\n",
      "Epoch 18, Batch 854, Test Loss: 0.4620945453643799\n",
      "Epoch 18, Batch 855, Test Loss: 0.5853345394134521\n",
      "Epoch 18, Batch 856, Test Loss: 0.35897746682167053\n",
      "Epoch 18, Batch 857, Test Loss: 0.41718772053718567\n",
      "Epoch 18, Batch 858, Test Loss: 0.4012010395526886\n",
      "Epoch 18, Batch 859, Test Loss: 0.27483415603637695\n",
      "Epoch 18, Batch 860, Test Loss: 0.3158072233200073\n",
      "Epoch 18, Batch 861, Test Loss: 0.34725356101989746\n",
      "Epoch 18, Batch 862, Test Loss: 0.5678712129592896\n",
      "Epoch 18, Batch 863, Test Loss: 0.3065033555030823\n",
      "Epoch 18, Batch 864, Test Loss: 0.42016226053237915\n",
      "Epoch 18, Batch 865, Test Loss: 0.45139437913894653\n",
      "Epoch 18, Batch 866, Test Loss: 0.48687058687210083\n",
      "Epoch 18, Batch 867, Test Loss: 0.4331313669681549\n",
      "Epoch 18, Batch 868, Test Loss: 0.5434308648109436\n",
      "Epoch 18, Batch 869, Test Loss: 0.21474869549274445\n",
      "Epoch 18, Batch 870, Test Loss: 0.44835689663887024\n",
      "Epoch 18, Batch 871, Test Loss: 0.448589026927948\n",
      "Epoch 18, Batch 872, Test Loss: 0.610823392868042\n",
      "Epoch 18, Batch 873, Test Loss: 0.295788437128067\n",
      "Epoch 18, Batch 874, Test Loss: 0.37315070629119873\n",
      "Epoch 18, Batch 875, Test Loss: 0.4012807607650757\n",
      "Epoch 18, Batch 876, Test Loss: 0.3317370116710663\n",
      "Epoch 18, Batch 877, Test Loss: 0.4783603847026825\n",
      "Epoch 18, Batch 878, Test Loss: 0.40334272384643555\n",
      "Epoch 18, Batch 879, Test Loss: 0.6491395235061646\n",
      "Epoch 18, Batch 880, Test Loss: 0.34559884667396545\n",
      "Epoch 18, Batch 881, Test Loss: 0.48477062582969666\n",
      "Epoch 18, Batch 882, Test Loss: 0.3874039053916931\n",
      "Epoch 18, Batch 883, Test Loss: 0.42429763078689575\n",
      "Epoch 18, Batch 884, Test Loss: 0.40123775601387024\n",
      "Epoch 18, Batch 885, Test Loss: 0.4554094076156616\n",
      "Epoch 18, Batch 886, Test Loss: 0.4076494574546814\n",
      "Epoch 18, Batch 887, Test Loss: 0.4514835774898529\n",
      "Epoch 18, Batch 888, Test Loss: 0.2870802879333496\n",
      "Epoch 18, Batch 889, Test Loss: 0.3867679834365845\n",
      "Epoch 18, Batch 890, Test Loss: 0.3518684506416321\n",
      "Epoch 18, Batch 891, Test Loss: 0.4296403229236603\n",
      "Epoch 18, Batch 892, Test Loss: 0.428138792514801\n",
      "Epoch 18, Batch 893, Test Loss: 0.4892813265323639\n",
      "Epoch 18, Batch 894, Test Loss: 0.3563603162765503\n",
      "Epoch 18, Batch 895, Test Loss: 0.6285992860794067\n",
      "Epoch 18, Batch 896, Test Loss: 0.39521729946136475\n",
      "Epoch 18, Batch 897, Test Loss: 0.2582392394542694\n",
      "Epoch 18, Batch 898, Test Loss: 0.4329601526260376\n",
      "Epoch 18, Batch 899, Test Loss: 0.48901456594467163\n",
      "Epoch 18, Batch 900, Test Loss: 0.4321621358394623\n",
      "Epoch 18, Batch 901, Test Loss: 0.3972550630569458\n",
      "Epoch 18, Batch 902, Test Loss: 0.3266860246658325\n",
      "Epoch 18, Batch 903, Test Loss: 0.3136315643787384\n",
      "Epoch 18, Batch 904, Test Loss: 0.36901092529296875\n",
      "Epoch 18, Batch 905, Test Loss: 0.4748936891555786\n",
      "Epoch 18, Batch 906, Test Loss: 0.31963902711868286\n",
      "Epoch 18, Batch 907, Test Loss: 0.3622572720050812\n",
      "Epoch 18, Batch 908, Test Loss: 0.4581712484359741\n",
      "Epoch 18, Batch 909, Test Loss: 0.28833597898483276\n",
      "Epoch 18, Batch 910, Test Loss: 0.3907290995121002\n",
      "Epoch 18, Batch 911, Test Loss: 0.38956162333488464\n",
      "Epoch 18, Batch 912, Test Loss: 0.4089496433734894\n",
      "Epoch 18, Batch 913, Test Loss: 0.3957419991493225\n",
      "Epoch 18, Batch 914, Test Loss: 0.3531278371810913\n",
      "Epoch 18, Batch 915, Test Loss: 0.386574923992157\n",
      "Epoch 18, Batch 916, Test Loss: 0.45136645436286926\n",
      "Epoch 18, Batch 917, Test Loss: 0.35971859097480774\n",
      "Epoch 18, Batch 918, Test Loss: 0.43172773718833923\n",
      "Epoch 18, Batch 919, Test Loss: 0.4068281650543213\n",
      "Epoch 18, Batch 920, Test Loss: 0.2820987403392792\n",
      "Epoch 18, Batch 921, Test Loss: 0.620196521282196\n",
      "Epoch 18, Batch 922, Test Loss: 0.5375523567199707\n",
      "Epoch 18, Batch 923, Test Loss: 0.44916224479675293\n",
      "Epoch 18, Batch 924, Test Loss: 0.37580886483192444\n",
      "Epoch 18, Batch 925, Test Loss: 0.5589049458503723\n",
      "Epoch 18, Batch 926, Test Loss: 0.5868300199508667\n",
      "Epoch 18, Batch 927, Test Loss: 0.4717167615890503\n",
      "Epoch 18, Batch 928, Test Loss: 0.43432000279426575\n",
      "Epoch 18, Batch 929, Test Loss: 0.36861252784729004\n",
      "Epoch 18, Batch 930, Test Loss: 0.5014775991439819\n",
      "Epoch 18, Batch 931, Test Loss: 0.47105082869529724\n",
      "Epoch 18, Batch 932, Test Loss: 0.5443421602249146\n",
      "Epoch 18, Batch 933, Test Loss: 0.3697911500930786\n",
      "Epoch 18, Batch 934, Test Loss: 0.4535261392593384\n",
      "Epoch 18, Batch 935, Test Loss: 0.47554871439933777\n",
      "Epoch 18, Batch 936, Test Loss: 0.29829224944114685\n",
      "Epoch 18, Batch 937, Test Loss: 0.306530237197876\n",
      "Epoch 18, Batch 938, Test Loss: 0.5161393880844116\n",
      "Accuracy of Test set: 0.8540166666666666\n",
      "Epoch 19, Batch 1, Loss: 0.43362268805503845\n",
      "Epoch 19, Batch 2, Loss: 0.5173285007476807\n",
      "Epoch 19, Batch 3, Loss: 0.435636967420578\n",
      "Epoch 19, Batch 4, Loss: 0.3904847502708435\n",
      "Epoch 19, Batch 5, Loss: 0.3563739061355591\n",
      "Epoch 19, Batch 6, Loss: 0.5378003716468811\n",
      "Epoch 19, Batch 7, Loss: 0.5179680585861206\n",
      "Epoch 19, Batch 8, Loss: 0.43553388118743896\n",
      "Epoch 19, Batch 9, Loss: 0.3914642930030823\n",
      "Epoch 19, Batch 10, Loss: 0.5140765905380249\n",
      "Epoch 19, Batch 11, Loss: 0.3992042541503906\n",
      "Epoch 19, Batch 12, Loss: 0.6031330227851868\n",
      "Epoch 19, Batch 13, Loss: 0.34087368845939636\n",
      "Epoch 19, Batch 14, Loss: 0.4383416771888733\n",
      "Epoch 19, Batch 15, Loss: 0.35245051980018616\n",
      "Epoch 19, Batch 16, Loss: 0.35571470856666565\n",
      "Epoch 19, Batch 17, Loss: 0.34173667430877686\n",
      "Epoch 19, Batch 18, Loss: 0.4129154086112976\n",
      "Epoch 19, Batch 19, Loss: 0.5965670347213745\n",
      "Epoch 19, Batch 20, Loss: 0.3953768014907837\n",
      "Epoch 19, Batch 21, Loss: 0.5209566950798035\n",
      "Epoch 19, Batch 22, Loss: 0.2866002023220062\n",
      "Epoch 19, Batch 23, Loss: 0.37460634112358093\n",
      "Epoch 19, Batch 24, Loss: 0.38976845145225525\n",
      "Epoch 19, Batch 25, Loss: 0.5392117500305176\n",
      "Epoch 19, Batch 26, Loss: 0.40237459540367126\n",
      "Epoch 19, Batch 27, Loss: 0.3863227367401123\n",
      "Epoch 19, Batch 28, Loss: 0.4024638533592224\n",
      "Epoch 19, Batch 29, Loss: 0.38555288314819336\n",
      "Epoch 19, Batch 30, Loss: 0.36582523584365845\n",
      "Epoch 19, Batch 31, Loss: 0.5570489764213562\n",
      "Epoch 19, Batch 32, Loss: 0.3835410475730896\n",
      "Epoch 19, Batch 33, Loss: 0.5902150869369507\n",
      "Epoch 19, Batch 34, Loss: 0.4667876958847046\n",
      "Epoch 19, Batch 35, Loss: 0.25940391421318054\n",
      "Epoch 19, Batch 36, Loss: 0.6542279124259949\n",
      "Epoch 19, Batch 37, Loss: 0.3450389802455902\n",
      "Epoch 19, Batch 38, Loss: 0.5304252505302429\n",
      "Epoch 19, Batch 39, Loss: 0.35231009125709534\n",
      "Epoch 19, Batch 40, Loss: 0.27710235118865967\n",
      "Epoch 19, Batch 41, Loss: 0.5607816576957703\n",
      "Epoch 19, Batch 42, Loss: 0.4280250072479248\n",
      "Epoch 19, Batch 43, Loss: 0.4916333556175232\n",
      "Epoch 19, Batch 44, Loss: 0.394159734249115\n",
      "Epoch 19, Batch 45, Loss: 0.4504697322845459\n",
      "Epoch 19, Batch 46, Loss: 0.30078965425491333\n",
      "Epoch 19, Batch 47, Loss: 0.38218361139297485\n",
      "Epoch 19, Batch 48, Loss: 0.24386344850063324\n",
      "Epoch 19, Batch 49, Loss: 0.3455617427825928\n",
      "Epoch 19, Batch 50, Loss: 0.42325738072395325\n",
      "Epoch 19, Batch 51, Loss: 0.4267367720603943\n",
      "Epoch 19, Batch 52, Loss: 0.33901700377464294\n",
      "Epoch 19, Batch 53, Loss: 0.3075386881828308\n",
      "Epoch 19, Batch 54, Loss: 0.510301411151886\n",
      "Epoch 19, Batch 55, Loss: 0.6320174336433411\n",
      "Epoch 19, Batch 56, Loss: 0.4321819543838501\n",
      "Epoch 19, Batch 57, Loss: 0.48085498809814453\n",
      "Epoch 19, Batch 58, Loss: 0.5766379833221436\n",
      "Epoch 19, Batch 59, Loss: 0.38277652859687805\n",
      "Epoch 19, Batch 60, Loss: 0.3668137192726135\n",
      "Epoch 19, Batch 61, Loss: 0.4887908101081848\n",
      "Epoch 19, Batch 62, Loss: 0.48550623655319214\n",
      "Epoch 19, Batch 63, Loss: 0.4656417965888977\n",
      "Epoch 19, Batch 64, Loss: 0.45556098222732544\n",
      "Epoch 19, Batch 65, Loss: 0.22090500593185425\n",
      "Epoch 19, Batch 66, Loss: 0.36950746178627014\n",
      "Epoch 19, Batch 67, Loss: 0.4117925763130188\n",
      "Epoch 19, Batch 68, Loss: 0.32592833042144775\n",
      "Epoch 19, Batch 69, Loss: 0.332952082157135\n",
      "Epoch 19, Batch 70, Loss: 0.35430797934532166\n",
      "Epoch 19, Batch 71, Loss: 0.4027101993560791\n",
      "Epoch 19, Batch 72, Loss: 0.49181440472602844\n",
      "Epoch 19, Batch 73, Loss: 0.3582547903060913\n",
      "Epoch 19, Batch 74, Loss: 0.5999760031700134\n",
      "Epoch 19, Batch 75, Loss: 0.3898581862449646\n",
      "Epoch 19, Batch 76, Loss: 0.4522433578968048\n",
      "Epoch 19, Batch 77, Loss: 0.4043760597705841\n",
      "Epoch 19, Batch 78, Loss: 0.35818320512771606\n",
      "Epoch 19, Batch 79, Loss: 0.3516623079776764\n",
      "Epoch 19, Batch 80, Loss: 0.37989342212677\n",
      "Epoch 19, Batch 81, Loss: 0.3492100238800049\n",
      "Epoch 19, Batch 82, Loss: 0.4447627067565918\n",
      "Epoch 19, Batch 83, Loss: 0.34718236327171326\n",
      "Epoch 19, Batch 84, Loss: 0.6095830202102661\n",
      "Epoch 19, Batch 85, Loss: 0.6034428477287292\n",
      "Epoch 19, Batch 86, Loss: 0.3483649790287018\n",
      "Epoch 19, Batch 87, Loss: 0.5165992975234985\n",
      "Epoch 19, Batch 88, Loss: 0.6480674743652344\n",
      "Epoch 19, Batch 89, Loss: 0.44847726821899414\n",
      "Epoch 19, Batch 90, Loss: 0.518004834651947\n",
      "Epoch 19, Batch 91, Loss: 0.500483512878418\n",
      "Epoch 19, Batch 92, Loss: 0.2448367029428482\n",
      "Epoch 19, Batch 93, Loss: 0.4841981828212738\n",
      "Epoch 19, Batch 94, Loss: 0.3397104740142822\n",
      "Epoch 19, Batch 95, Loss: 0.38137340545654297\n",
      "Epoch 19, Batch 96, Loss: 0.5024827718734741\n",
      "Epoch 19, Batch 97, Loss: 0.3347015380859375\n",
      "Epoch 19, Batch 98, Loss: 0.3704335391521454\n",
      "Epoch 19, Batch 99, Loss: 0.2962005138397217\n",
      "Epoch 19, Batch 100, Loss: 0.41170966625213623\n",
      "Epoch 19, Batch 101, Loss: 0.7221264243125916\n",
      "Epoch 19, Batch 102, Loss: 0.3247523307800293\n",
      "Epoch 19, Batch 103, Loss: 0.3896483778953552\n",
      "Epoch 19, Batch 104, Loss: 0.5665212273597717\n",
      "Epoch 19, Batch 105, Loss: 0.4785894453525543\n",
      "Epoch 19, Batch 106, Loss: 0.38219255208969116\n",
      "Epoch 19, Batch 107, Loss: 0.5569631457328796\n",
      "Epoch 19, Batch 108, Loss: 0.31747525930404663\n",
      "Epoch 19, Batch 109, Loss: 0.2536146342754364\n",
      "Epoch 19, Batch 110, Loss: 0.3133898675441742\n",
      "Epoch 19, Batch 111, Loss: 0.5482839941978455\n",
      "Epoch 19, Batch 112, Loss: 0.3242400884628296\n",
      "Epoch 19, Batch 113, Loss: 0.38980239629745483\n",
      "Epoch 19, Batch 114, Loss: 0.34333908557891846\n",
      "Epoch 19, Batch 115, Loss: 0.45067915320396423\n",
      "Epoch 19, Batch 116, Loss: 0.4259740710258484\n",
      "Epoch 19, Batch 117, Loss: 0.3741929233074188\n",
      "Epoch 19, Batch 118, Loss: 0.36014828085899353\n",
      "Epoch 19, Batch 119, Loss: 0.2881428599357605\n",
      "Epoch 19, Batch 120, Loss: 0.6674079298973083\n",
      "Epoch 19, Batch 121, Loss: 0.5401162505149841\n",
      "Epoch 19, Batch 122, Loss: 0.3234940767288208\n",
      "Epoch 19, Batch 123, Loss: 0.5169031620025635\n",
      "Epoch 19, Batch 124, Loss: 0.34710219502449036\n",
      "Epoch 19, Batch 125, Loss: 0.5982679128646851\n",
      "Epoch 19, Batch 126, Loss: 0.3992045223712921\n",
      "Epoch 19, Batch 127, Loss: 0.4452928304672241\n",
      "Epoch 19, Batch 128, Loss: 0.3864699602127075\n",
      "Epoch 19, Batch 129, Loss: 0.4755752682685852\n",
      "Epoch 19, Batch 130, Loss: 0.4017317295074463\n",
      "Epoch 19, Batch 131, Loss: 0.387438029050827\n",
      "Epoch 19, Batch 132, Loss: 0.32916924357414246\n",
      "Epoch 19, Batch 133, Loss: 0.3845437467098236\n",
      "Epoch 19, Batch 134, Loss: 0.2650982737541199\n",
      "Epoch 19, Batch 135, Loss: 0.20597128570079803\n",
      "Epoch 19, Batch 136, Loss: 0.3851712942123413\n",
      "Epoch 19, Batch 137, Loss: 0.5690430402755737\n",
      "Epoch 19, Batch 138, Loss: 0.38991624116897583\n",
      "Epoch 19, Batch 139, Loss: 0.31692835688591003\n",
      "Epoch 19, Batch 140, Loss: 0.31397151947021484\n",
      "Epoch 19, Batch 141, Loss: 0.505092203617096\n",
      "Epoch 19, Batch 142, Loss: 0.4451909363269806\n",
      "Epoch 19, Batch 143, Loss: 0.4599178433418274\n",
      "Epoch 19, Batch 144, Loss: 0.6427772045135498\n",
      "Epoch 19, Batch 145, Loss: 0.420562744140625\n",
      "Epoch 19, Batch 146, Loss: 0.37073996663093567\n",
      "Epoch 19, Batch 147, Loss: 0.40479734539985657\n",
      "Epoch 19, Batch 148, Loss: 0.26935717463493347\n",
      "Epoch 19, Batch 149, Loss: 0.5300233960151672\n",
      "Epoch 19, Batch 150, Loss: 0.45975229144096375\n",
      "Epoch 19, Batch 151, Loss: 0.3480684757232666\n",
      "Epoch 19, Batch 152, Loss: 0.26822346448898315\n",
      "Epoch 19, Batch 153, Loss: 0.4102112054824829\n",
      "Epoch 19, Batch 154, Loss: 0.6025012135505676\n",
      "Epoch 19, Batch 155, Loss: 0.3729291260242462\n",
      "Epoch 19, Batch 156, Loss: 0.27057522535324097\n",
      "Epoch 19, Batch 157, Loss: 0.45920008420944214\n",
      "Epoch 19, Batch 158, Loss: 0.4282599091529846\n",
      "Epoch 19, Batch 159, Loss: 0.3081163167953491\n",
      "Epoch 19, Batch 160, Loss: 0.4894770383834839\n",
      "Epoch 19, Batch 161, Loss: 0.6336709260940552\n",
      "Epoch 19, Batch 162, Loss: 0.5719913244247437\n",
      "Epoch 19, Batch 163, Loss: 0.4842837452888489\n",
      "Epoch 19, Batch 164, Loss: 0.3973062038421631\n",
      "Epoch 19, Batch 165, Loss: 0.3857494294643402\n",
      "Epoch 19, Batch 166, Loss: 0.2803976535797119\n",
      "Epoch 19, Batch 167, Loss: 0.3288322985172272\n",
      "Epoch 19, Batch 168, Loss: 0.45112526416778564\n",
      "Epoch 19, Batch 169, Loss: 0.3179735541343689\n",
      "Epoch 19, Batch 170, Loss: 0.29239514470100403\n",
      "Epoch 19, Batch 171, Loss: 0.46792668104171753\n",
      "Epoch 19, Batch 172, Loss: 0.3376920819282532\n",
      "Epoch 19, Batch 173, Loss: 0.31430843472480774\n",
      "Epoch 19, Batch 174, Loss: 0.4481167197227478\n",
      "Epoch 19, Batch 175, Loss: 0.38309672474861145\n",
      "Epoch 19, Batch 176, Loss: 0.5996846556663513\n",
      "Epoch 19, Batch 177, Loss: 0.4563191831111908\n",
      "Epoch 19, Batch 178, Loss: 0.5882788300514221\n",
      "Epoch 19, Batch 179, Loss: 0.581659197807312\n",
      "Epoch 19, Batch 180, Loss: 0.41347989439964294\n",
      "Epoch 19, Batch 181, Loss: 0.6747790575027466\n",
      "Epoch 19, Batch 182, Loss: 0.46156981587409973\n",
      "Epoch 19, Batch 183, Loss: 0.4226704239845276\n",
      "Epoch 19, Batch 184, Loss: 0.44165360927581787\n",
      "Epoch 19, Batch 185, Loss: 0.4815853238105774\n",
      "Epoch 19, Batch 186, Loss: 0.511444628238678\n",
      "Epoch 19, Batch 187, Loss: 0.552509069442749\n",
      "Epoch 19, Batch 188, Loss: 0.406730979681015\n",
      "Epoch 19, Batch 189, Loss: 0.28763267397880554\n",
      "Epoch 19, Batch 190, Loss: 0.5878548622131348\n",
      "Epoch 19, Batch 191, Loss: 0.535729169845581\n",
      "Epoch 19, Batch 192, Loss: 0.41810157895088196\n",
      "Epoch 19, Batch 193, Loss: 0.5121008157730103\n",
      "Epoch 19, Batch 194, Loss: 0.319435715675354\n",
      "Epoch 19, Batch 195, Loss: 0.3931882083415985\n",
      "Epoch 19, Batch 196, Loss: 0.37500929832458496\n",
      "Epoch 19, Batch 197, Loss: 0.46687567234039307\n",
      "Epoch 19, Batch 198, Loss: 0.6477388143539429\n",
      "Epoch 19, Batch 199, Loss: 0.473547101020813\n",
      "Epoch 19, Batch 200, Loss: 0.36880993843078613\n",
      "Epoch 19, Batch 201, Loss: 0.5504428744316101\n",
      "Epoch 19, Batch 202, Loss: 0.4396224021911621\n",
      "Epoch 19, Batch 203, Loss: 0.4472457766532898\n",
      "Epoch 19, Batch 204, Loss: 0.351414293050766\n",
      "Epoch 19, Batch 205, Loss: 0.4923790395259857\n",
      "Epoch 19, Batch 206, Loss: 0.47447413206100464\n",
      "Epoch 19, Batch 207, Loss: 0.4384630620479584\n",
      "Epoch 19, Batch 208, Loss: 0.47465431690216064\n",
      "Epoch 19, Batch 209, Loss: 0.5342575907707214\n",
      "Epoch 19, Batch 210, Loss: 0.5740386843681335\n",
      "Epoch 19, Batch 211, Loss: 0.39580559730529785\n",
      "Epoch 19, Batch 212, Loss: 0.5983527898788452\n",
      "Epoch 19, Batch 213, Loss: 0.5946750044822693\n",
      "Epoch 19, Batch 214, Loss: 0.4615853428840637\n",
      "Epoch 19, Batch 215, Loss: 0.3951375484466553\n",
      "Epoch 19, Batch 216, Loss: 0.4847545623779297\n",
      "Epoch 19, Batch 217, Loss: 0.5380115509033203\n",
      "Epoch 19, Batch 218, Loss: 0.5560611486434937\n",
      "Epoch 19, Batch 219, Loss: 0.35023263096809387\n",
      "Epoch 19, Batch 220, Loss: 0.4205087423324585\n",
      "Epoch 19, Batch 221, Loss: 0.44449442625045776\n",
      "Epoch 19, Batch 222, Loss: 0.3394087255001068\n",
      "Epoch 19, Batch 223, Loss: 0.45031502842903137\n",
      "Epoch 19, Batch 224, Loss: 0.35773608088493347\n",
      "Epoch 19, Batch 225, Loss: 0.42209357023239136\n",
      "Epoch 19, Batch 226, Loss: 0.3791483938694\n",
      "Epoch 19, Batch 227, Loss: 0.3485153317451477\n",
      "Epoch 19, Batch 228, Loss: 0.4362263083457947\n",
      "Epoch 19, Batch 229, Loss: 0.49017971754074097\n",
      "Epoch 19, Batch 230, Loss: 0.5480653643608093\n",
      "Epoch 19, Batch 231, Loss: 0.4254588484764099\n",
      "Epoch 19, Batch 232, Loss: 0.5019506216049194\n",
      "Epoch 19, Batch 233, Loss: 0.3941159248352051\n",
      "Epoch 19, Batch 234, Loss: 0.5856298208236694\n",
      "Epoch 19, Batch 235, Loss: 0.3454352021217346\n",
      "Epoch 19, Batch 236, Loss: 0.5285354852676392\n",
      "Epoch 19, Batch 237, Loss: 0.6172995567321777\n",
      "Epoch 19, Batch 238, Loss: 0.40037572383880615\n",
      "Epoch 19, Batch 239, Loss: 0.3186812102794647\n",
      "Epoch 19, Batch 240, Loss: 0.3734476864337921\n",
      "Epoch 19, Batch 241, Loss: 0.48163315653800964\n",
      "Epoch 19, Batch 242, Loss: 0.33686861395835876\n",
      "Epoch 19, Batch 243, Loss: 0.2486860752105713\n",
      "Epoch 19, Batch 244, Loss: 0.33940786123275757\n",
      "Epoch 19, Batch 245, Loss: 0.2832469344139099\n",
      "Epoch 19, Batch 246, Loss: 0.45976680517196655\n",
      "Epoch 19, Batch 247, Loss: 0.4964587092399597\n",
      "Epoch 19, Batch 248, Loss: 0.26766809821128845\n",
      "Epoch 19, Batch 249, Loss: 0.5019679665565491\n",
      "Epoch 19, Batch 250, Loss: 0.3168749511241913\n",
      "Epoch 19, Batch 251, Loss: 0.5024883151054382\n",
      "Epoch 19, Batch 252, Loss: 0.4243966042995453\n",
      "Epoch 19, Batch 253, Loss: 0.41900867223739624\n",
      "Epoch 19, Batch 254, Loss: 0.4689778685569763\n",
      "Epoch 19, Batch 255, Loss: 0.6768933534622192\n",
      "Epoch 19, Batch 256, Loss: 0.30621078610420227\n",
      "Epoch 19, Batch 257, Loss: 0.3755050599575043\n",
      "Epoch 19, Batch 258, Loss: 0.4643968641757965\n",
      "Epoch 19, Batch 259, Loss: 0.29558470845222473\n",
      "Epoch 19, Batch 260, Loss: 0.3183934986591339\n",
      "Epoch 19, Batch 261, Loss: 0.3008526563644409\n",
      "Epoch 19, Batch 262, Loss: 0.4089317321777344\n",
      "Epoch 19, Batch 263, Loss: 0.5235278010368347\n",
      "Epoch 19, Batch 264, Loss: 0.3677738606929779\n",
      "Epoch 19, Batch 265, Loss: 0.24967972934246063\n",
      "Epoch 19, Batch 266, Loss: 0.6060014367103577\n",
      "Epoch 19, Batch 267, Loss: 0.401886910200119\n",
      "Epoch 19, Batch 268, Loss: 0.41466447710990906\n",
      "Epoch 19, Batch 269, Loss: 0.45927926898002625\n",
      "Epoch 19, Batch 270, Loss: 0.6074509620666504\n",
      "Epoch 19, Batch 271, Loss: 0.5374437570571899\n",
      "Epoch 19, Batch 272, Loss: 0.4068445563316345\n",
      "Epoch 19, Batch 273, Loss: 0.4684469997882843\n",
      "Epoch 19, Batch 274, Loss: 0.4949096739292145\n",
      "Epoch 19, Batch 275, Loss: 0.32918664813041687\n",
      "Epoch 19, Batch 276, Loss: 0.4684043228626251\n",
      "Epoch 19, Batch 277, Loss: 0.3924596905708313\n",
      "Epoch 19, Batch 278, Loss: 0.4981914162635803\n",
      "Epoch 19, Batch 279, Loss: 0.40531328320503235\n",
      "Epoch 19, Batch 280, Loss: 0.5772111415863037\n",
      "Epoch 19, Batch 281, Loss: 0.4334851801395416\n",
      "Epoch 19, Batch 282, Loss: 0.43280574679374695\n",
      "Epoch 19, Batch 283, Loss: 0.37379997968673706\n",
      "Epoch 19, Batch 284, Loss: 0.29554885625839233\n",
      "Epoch 19, Batch 285, Loss: 0.3576510548591614\n",
      "Epoch 19, Batch 286, Loss: 0.548406183719635\n",
      "Epoch 19, Batch 287, Loss: 0.36112093925476074\n",
      "Epoch 19, Batch 288, Loss: 0.2583831548690796\n",
      "Epoch 19, Batch 289, Loss: 0.5780922770500183\n",
      "Epoch 19, Batch 290, Loss: 0.3991546630859375\n",
      "Epoch 19, Batch 291, Loss: 0.5884667634963989\n",
      "Epoch 19, Batch 292, Loss: 0.2527168393135071\n",
      "Epoch 19, Batch 293, Loss: 0.4178053140640259\n",
      "Epoch 19, Batch 294, Loss: 0.5667547583580017\n",
      "Epoch 19, Batch 295, Loss: 0.5321670770645142\n",
      "Epoch 19, Batch 296, Loss: 0.494037002325058\n",
      "Epoch 19, Batch 297, Loss: 0.37912750244140625\n",
      "Epoch 19, Batch 298, Loss: 0.4032686948776245\n",
      "Epoch 19, Batch 299, Loss: 0.4461609125137329\n",
      "Epoch 19, Batch 300, Loss: 0.47901636362075806\n",
      "Epoch 19, Batch 301, Loss: 0.3095240294933319\n",
      "Epoch 19, Batch 302, Loss: 0.36780640482902527\n",
      "Epoch 19, Batch 303, Loss: 0.346223920583725\n",
      "Epoch 19, Batch 304, Loss: 0.5128889083862305\n",
      "Epoch 19, Batch 305, Loss: 0.7268404960632324\n",
      "Epoch 19, Batch 306, Loss: 0.33598923683166504\n",
      "Epoch 19, Batch 307, Loss: 0.46078425645828247\n",
      "Epoch 19, Batch 308, Loss: 0.4843337833881378\n",
      "Epoch 19, Batch 309, Loss: 0.43736782670021057\n",
      "Epoch 19, Batch 310, Loss: 0.4060323238372803\n",
      "Epoch 19, Batch 311, Loss: 0.3564443290233612\n",
      "Epoch 19, Batch 312, Loss: 0.5107147097587585\n",
      "Epoch 19, Batch 313, Loss: 0.5567692518234253\n",
      "Epoch 19, Batch 314, Loss: 0.30456021428108215\n",
      "Epoch 19, Batch 315, Loss: 0.32460567355155945\n",
      "Epoch 19, Batch 316, Loss: 0.40403643250465393\n",
      "Epoch 19, Batch 317, Loss: 0.34443381428718567\n",
      "Epoch 19, Batch 318, Loss: 0.3233647048473358\n",
      "Epoch 19, Batch 319, Loss: 0.4646117687225342\n",
      "Epoch 19, Batch 320, Loss: 0.42412540316581726\n",
      "Epoch 19, Batch 321, Loss: 0.38080304861068726\n",
      "Epoch 19, Batch 322, Loss: 0.4564984440803528\n",
      "Epoch 19, Batch 323, Loss: 0.5866255164146423\n",
      "Epoch 19, Batch 324, Loss: 0.4902840852737427\n",
      "Epoch 19, Batch 325, Loss: 0.3660734295845032\n",
      "Epoch 19, Batch 326, Loss: 0.5154511332511902\n",
      "Epoch 19, Batch 327, Loss: 0.36307552456855774\n",
      "Epoch 19, Batch 328, Loss: 0.48966366052627563\n",
      "Epoch 19, Batch 329, Loss: 0.3598514199256897\n",
      "Epoch 19, Batch 330, Loss: 0.33330437541007996\n",
      "Epoch 19, Batch 331, Loss: 0.6367999315261841\n",
      "Epoch 19, Batch 332, Loss: 0.41359421610832214\n",
      "Epoch 19, Batch 333, Loss: 0.5092981457710266\n",
      "Epoch 19, Batch 334, Loss: 0.26477742195129395\n",
      "Epoch 19, Batch 335, Loss: 0.24226287007331848\n",
      "Epoch 19, Batch 336, Loss: 0.6703615784645081\n",
      "Epoch 19, Batch 337, Loss: 0.581336259841919\n",
      "Epoch 19, Batch 338, Loss: 0.4038853049278259\n",
      "Epoch 19, Batch 339, Loss: 0.42316704988479614\n",
      "Epoch 19, Batch 340, Loss: 0.38478949666023254\n",
      "Epoch 19, Batch 341, Loss: 0.4132077097892761\n",
      "Epoch 19, Batch 342, Loss: 0.29871422052383423\n",
      "Epoch 19, Batch 343, Loss: 0.2754374146461487\n",
      "Epoch 19, Batch 344, Loss: 0.37410175800323486\n",
      "Epoch 19, Batch 345, Loss: 0.5471421480178833\n",
      "Epoch 19, Batch 346, Loss: 0.4425574243068695\n",
      "Epoch 19, Batch 347, Loss: 0.3217324912548065\n",
      "Epoch 19, Batch 348, Loss: 0.48536112904548645\n",
      "Epoch 19, Batch 349, Loss: 0.36233869194984436\n",
      "Epoch 19, Batch 350, Loss: 0.36842143535614014\n",
      "Epoch 19, Batch 351, Loss: 0.46814194321632385\n",
      "Epoch 19, Batch 352, Loss: 0.2778450548648834\n",
      "Epoch 19, Batch 353, Loss: 0.4583433270454407\n",
      "Epoch 19, Batch 354, Loss: 0.30353283882141113\n",
      "Epoch 19, Batch 355, Loss: 0.25115862488746643\n",
      "Epoch 19, Batch 356, Loss: 0.25705793499946594\n",
      "Epoch 19, Batch 357, Loss: 0.28770244121551514\n",
      "Epoch 19, Batch 358, Loss: 0.42691969871520996\n",
      "Epoch 19, Batch 359, Loss: 0.462952584028244\n",
      "Epoch 19, Batch 360, Loss: 0.361713707447052\n",
      "Epoch 19, Batch 361, Loss: 0.30154120922088623\n",
      "Epoch 19, Batch 362, Loss: 0.3531467020511627\n",
      "Epoch 19, Batch 363, Loss: 0.24282744526863098\n",
      "Epoch 19, Batch 364, Loss: 0.526033878326416\n",
      "Epoch 19, Batch 365, Loss: 0.4198911786079407\n",
      "Epoch 19, Batch 366, Loss: 0.7067986726760864\n",
      "Epoch 19, Batch 367, Loss: 0.4263651669025421\n",
      "Epoch 19, Batch 368, Loss: 0.39848849177360535\n",
      "Epoch 19, Batch 369, Loss: 0.4332194924354553\n",
      "Epoch 19, Batch 370, Loss: 0.33766379952430725\n",
      "Epoch 19, Batch 371, Loss: 0.37711241841316223\n",
      "Epoch 19, Batch 372, Loss: 0.5540789365768433\n",
      "Epoch 19, Batch 373, Loss: 0.31364190578460693\n",
      "Epoch 19, Batch 374, Loss: 0.4981880784034729\n",
      "Epoch 19, Batch 375, Loss: 0.5449590086936951\n",
      "Epoch 19, Batch 376, Loss: 0.3688278794288635\n",
      "Epoch 19, Batch 377, Loss: 0.5206043124198914\n",
      "Epoch 19, Batch 378, Loss: 0.34683728218078613\n",
      "Epoch 19, Batch 379, Loss: 0.44374218583106995\n",
      "Epoch 19, Batch 380, Loss: 0.5699702501296997\n",
      "Epoch 19, Batch 381, Loss: 0.5542146563529968\n",
      "Epoch 19, Batch 382, Loss: 0.32354381680488586\n",
      "Epoch 19, Batch 383, Loss: 0.5504816770553589\n",
      "Epoch 19, Batch 384, Loss: 0.3410819172859192\n",
      "Epoch 19, Batch 385, Loss: 0.4486062824726105\n",
      "Epoch 19, Batch 386, Loss: 0.2523828446865082\n",
      "Epoch 19, Batch 387, Loss: 0.36636459827423096\n",
      "Epoch 19, Batch 388, Loss: 0.2813968062400818\n",
      "Epoch 19, Batch 389, Loss: 0.4134502112865448\n",
      "Epoch 19, Batch 390, Loss: 0.5242884159088135\n",
      "Epoch 19, Batch 391, Loss: 0.37296900153160095\n",
      "Epoch 19, Batch 392, Loss: 0.4146088659763336\n",
      "Epoch 19, Batch 393, Loss: 0.4688273072242737\n",
      "Epoch 19, Batch 394, Loss: 0.3094366192817688\n",
      "Epoch 19, Batch 395, Loss: 0.5202093124389648\n",
      "Epoch 19, Batch 396, Loss: 0.3460099995136261\n",
      "Epoch 19, Batch 397, Loss: 0.25263455510139465\n",
      "Epoch 19, Batch 398, Loss: 0.4163234233856201\n",
      "Epoch 19, Batch 399, Loss: 0.6284105777740479\n",
      "Epoch 19, Batch 400, Loss: 0.2616298794746399\n",
      "Epoch 19, Batch 401, Loss: 0.2563541531562805\n",
      "Epoch 19, Batch 402, Loss: 0.5268056988716125\n",
      "Epoch 19, Batch 403, Loss: 0.43453577160835266\n",
      "Epoch 19, Batch 404, Loss: 0.293741375207901\n",
      "Epoch 19, Batch 405, Loss: 0.323186457157135\n",
      "Epoch 19, Batch 406, Loss: 0.46111851930618286\n",
      "Epoch 19, Batch 407, Loss: 0.39537566900253296\n",
      "Epoch 19, Batch 408, Loss: 0.5308290123939514\n",
      "Epoch 19, Batch 409, Loss: 0.4046444296836853\n",
      "Epoch 19, Batch 410, Loss: 0.46766048669815063\n",
      "Epoch 19, Batch 411, Loss: 0.4687483608722687\n",
      "Epoch 19, Batch 412, Loss: 0.396979957818985\n",
      "Epoch 19, Batch 413, Loss: 0.5121185183525085\n",
      "Epoch 19, Batch 414, Loss: 0.6604726314544678\n",
      "Epoch 19, Batch 415, Loss: 0.4948440194129944\n",
      "Epoch 19, Batch 416, Loss: 0.4266049265861511\n",
      "Epoch 19, Batch 417, Loss: 0.27905604243278503\n",
      "Epoch 19, Batch 418, Loss: 0.26586639881134033\n",
      "Epoch 19, Batch 419, Loss: 0.49911874532699585\n",
      "Epoch 19, Batch 420, Loss: 0.37775707244873047\n",
      "Epoch 19, Batch 421, Loss: 0.3790018558502197\n",
      "Epoch 19, Batch 422, Loss: 0.4293077290058136\n",
      "Epoch 19, Batch 423, Loss: 0.6025568246841431\n",
      "Epoch 19, Batch 424, Loss: 0.4759269058704376\n",
      "Epoch 19, Batch 425, Loss: 0.4947769343852997\n",
      "Epoch 19, Batch 426, Loss: 0.3768918812274933\n",
      "Epoch 19, Batch 427, Loss: 0.26044851541519165\n",
      "Epoch 19, Batch 428, Loss: 0.5245436429977417\n",
      "Epoch 19, Batch 429, Loss: 0.41721534729003906\n",
      "Epoch 19, Batch 430, Loss: 0.5613751411437988\n",
      "Epoch 19, Batch 431, Loss: 0.4301075339317322\n",
      "Epoch 19, Batch 432, Loss: 0.3987070918083191\n",
      "Epoch 19, Batch 433, Loss: 0.2690337598323822\n",
      "Epoch 19, Batch 434, Loss: 0.5382315516471863\n",
      "Epoch 19, Batch 435, Loss: 0.4751434326171875\n",
      "Epoch 19, Batch 436, Loss: 0.5827839374542236\n",
      "Epoch 19, Batch 437, Loss: 0.3498682677745819\n",
      "Epoch 19, Batch 438, Loss: 0.33215847611427307\n",
      "Epoch 19, Batch 439, Loss: 0.637962818145752\n",
      "Epoch 19, Batch 440, Loss: 0.49538612365722656\n",
      "Epoch 19, Batch 441, Loss: 0.35317423939704895\n",
      "Epoch 19, Batch 442, Loss: 0.521212100982666\n",
      "Epoch 19, Batch 443, Loss: 0.586065948009491\n",
      "Epoch 19, Batch 444, Loss: 0.5034211277961731\n",
      "Epoch 19, Batch 445, Loss: 0.2546567916870117\n",
      "Epoch 19, Batch 446, Loss: 0.5266947150230408\n",
      "Epoch 19, Batch 447, Loss: 0.4523351192474365\n",
      "Epoch 19, Batch 448, Loss: 0.2749388813972473\n",
      "Epoch 19, Batch 449, Loss: 0.5230342149734497\n",
      "Epoch 19, Batch 450, Loss: 0.47201627492904663\n",
      "Epoch 19, Batch 451, Loss: 0.2664056420326233\n",
      "Epoch 19, Batch 452, Loss: 0.42460495233535767\n",
      "Epoch 19, Batch 453, Loss: 0.4253365397453308\n",
      "Epoch 19, Batch 454, Loss: 0.5196704864501953\n",
      "Epoch 19, Batch 455, Loss: 0.3906528055667877\n",
      "Epoch 19, Batch 456, Loss: 0.37721532583236694\n",
      "Epoch 19, Batch 457, Loss: 0.4742644727230072\n",
      "Epoch 19, Batch 458, Loss: 0.6169744729995728\n",
      "Epoch 19, Batch 459, Loss: 0.5609760284423828\n",
      "Epoch 19, Batch 460, Loss: 0.40612268447875977\n",
      "Epoch 19, Batch 461, Loss: 0.28288206458091736\n",
      "Epoch 19, Batch 462, Loss: 0.5097830295562744\n",
      "Epoch 19, Batch 463, Loss: 0.37767499685287476\n",
      "Epoch 19, Batch 464, Loss: 0.585300862789154\n",
      "Epoch 19, Batch 465, Loss: 0.1569196581840515\n",
      "Epoch 19, Batch 466, Loss: 0.27355021238327026\n",
      "Epoch 19, Batch 467, Loss: 0.5002941489219666\n",
      "Epoch 19, Batch 468, Loss: 0.3456476032733917\n",
      "Epoch 19, Batch 469, Loss: 0.6463950872421265\n",
      "Epoch 19, Batch 470, Loss: 0.4727869927883148\n",
      "Epoch 19, Batch 471, Loss: 0.6169862151145935\n",
      "Epoch 19, Batch 472, Loss: 0.4718315005302429\n",
      "Epoch 19, Batch 473, Loss: 0.4205055832862854\n",
      "Epoch 19, Batch 474, Loss: 0.4825206995010376\n",
      "Epoch 19, Batch 475, Loss: 0.382196843624115\n",
      "Epoch 19, Batch 476, Loss: 0.6308490633964539\n",
      "Epoch 19, Batch 477, Loss: 0.4230227470397949\n",
      "Epoch 19, Batch 478, Loss: 0.5148521661758423\n",
      "Epoch 19, Batch 479, Loss: 0.3618199825286865\n",
      "Epoch 19, Batch 480, Loss: 0.16422583162784576\n",
      "Epoch 19, Batch 481, Loss: 0.4360407292842865\n",
      "Epoch 19, Batch 482, Loss: 0.3957420587539673\n",
      "Epoch 19, Batch 483, Loss: 0.33064937591552734\n",
      "Epoch 19, Batch 484, Loss: 0.5136759281158447\n",
      "Epoch 19, Batch 485, Loss: 0.4566997289657593\n",
      "Epoch 19, Batch 486, Loss: 0.29031357169151306\n",
      "Epoch 19, Batch 487, Loss: 0.3674181401729584\n",
      "Epoch 19, Batch 488, Loss: 0.3697390854358673\n",
      "Epoch 19, Batch 489, Loss: 0.5000231862068176\n",
      "Epoch 19, Batch 490, Loss: 0.41909292340278625\n",
      "Epoch 19, Batch 491, Loss: 0.40380528569221497\n",
      "Epoch 19, Batch 492, Loss: 0.5232399702072144\n",
      "Epoch 19, Batch 493, Loss: 0.25959452986717224\n",
      "Epoch 19, Batch 494, Loss: 0.4407164454460144\n",
      "Epoch 19, Batch 495, Loss: 0.5865871906280518\n",
      "Epoch 19, Batch 496, Loss: 0.31923937797546387\n",
      "Epoch 19, Batch 497, Loss: 0.43961426615715027\n",
      "Epoch 19, Batch 498, Loss: 0.34233713150024414\n",
      "Epoch 19, Batch 499, Loss: 0.3903772532939911\n",
      "Epoch 19, Batch 500, Loss: 0.5044552683830261\n",
      "Epoch 19, Batch 501, Loss: 0.3503403663635254\n",
      "Epoch 19, Batch 502, Loss: 0.33506274223327637\n",
      "Epoch 19, Batch 503, Loss: 0.5928382873535156\n",
      "Epoch 19, Batch 504, Loss: 0.5511903762817383\n",
      "Epoch 19, Batch 505, Loss: 0.29411646723747253\n",
      "Epoch 19, Batch 506, Loss: 0.6220782399177551\n",
      "Epoch 19, Batch 507, Loss: 0.48181837797164917\n",
      "Epoch 19, Batch 508, Loss: 0.6223106384277344\n",
      "Epoch 19, Batch 509, Loss: 0.3016780912876129\n",
      "Epoch 19, Batch 510, Loss: 0.38057857751846313\n",
      "Epoch 19, Batch 511, Loss: 0.3313252925872803\n",
      "Epoch 19, Batch 512, Loss: 0.28256165981292725\n",
      "Epoch 19, Batch 513, Loss: 0.4466509222984314\n",
      "Epoch 19, Batch 514, Loss: 0.3806389272212982\n",
      "Epoch 19, Batch 515, Loss: 0.3906777501106262\n",
      "Epoch 19, Batch 516, Loss: 0.4046596586704254\n",
      "Epoch 19, Batch 517, Loss: 0.6551284790039062\n",
      "Epoch 19, Batch 518, Loss: 0.2815139889717102\n",
      "Epoch 19, Batch 519, Loss: 0.47455406188964844\n",
      "Epoch 19, Batch 520, Loss: 0.5351964831352234\n",
      "Epoch 19, Batch 521, Loss: 0.34133440256118774\n",
      "Epoch 19, Batch 522, Loss: 0.5848179459571838\n",
      "Epoch 19, Batch 523, Loss: 0.23525933921337128\n",
      "Epoch 19, Batch 524, Loss: 0.3528270125389099\n",
      "Epoch 19, Batch 525, Loss: 0.5584291219711304\n",
      "Epoch 19, Batch 526, Loss: 0.41724053025245667\n",
      "Epoch 19, Batch 527, Loss: 0.4513911008834839\n",
      "Epoch 19, Batch 528, Loss: 0.5360700488090515\n",
      "Epoch 19, Batch 529, Loss: 0.43078476190567017\n",
      "Epoch 19, Batch 530, Loss: 0.4672046899795532\n",
      "Epoch 19, Batch 531, Loss: 0.41316378116607666\n",
      "Epoch 19, Batch 532, Loss: 0.2872837781906128\n",
      "Epoch 19, Batch 533, Loss: 0.3888508677482605\n",
      "Epoch 19, Batch 534, Loss: 0.2695942521095276\n",
      "Epoch 19, Batch 535, Loss: 0.2798360586166382\n",
      "Epoch 19, Batch 536, Loss: 0.5037305355072021\n",
      "Epoch 19, Batch 537, Loss: 0.6514223217964172\n",
      "Epoch 19, Batch 538, Loss: 0.4563218653202057\n",
      "Epoch 19, Batch 539, Loss: 0.4069133400917053\n",
      "Epoch 19, Batch 540, Loss: 0.28322499990463257\n",
      "Epoch 19, Batch 541, Loss: 0.5133001804351807\n",
      "Epoch 19, Batch 542, Loss: 0.38684746623039246\n",
      "Epoch 19, Batch 543, Loss: 0.5259870290756226\n",
      "Epoch 19, Batch 544, Loss: 0.2755270004272461\n",
      "Epoch 19, Batch 545, Loss: 0.36566245555877686\n",
      "Epoch 19, Batch 546, Loss: 0.3101070821285248\n",
      "Epoch 19, Batch 547, Loss: 0.42501556873321533\n",
      "Epoch 19, Batch 548, Loss: 0.2311447560787201\n",
      "Epoch 19, Batch 549, Loss: 0.41684892773628235\n",
      "Epoch 19, Batch 550, Loss: 0.4993687868118286\n",
      "Epoch 19, Batch 551, Loss: 0.4022584557533264\n",
      "Epoch 19, Batch 552, Loss: 0.3467289209365845\n",
      "Epoch 19, Batch 553, Loss: 0.28706982731819153\n",
      "Epoch 19, Batch 554, Loss: 0.44170650839805603\n",
      "Epoch 19, Batch 555, Loss: 0.44814229011535645\n",
      "Epoch 19, Batch 556, Loss: 0.582743227481842\n",
      "Epoch 19, Batch 557, Loss: 0.5108782649040222\n",
      "Epoch 19, Batch 558, Loss: 0.4422186613082886\n",
      "Epoch 19, Batch 559, Loss: 0.39974576234817505\n",
      "Epoch 19, Batch 560, Loss: 0.40378424525260925\n",
      "Epoch 19, Batch 561, Loss: 0.5179275870323181\n",
      "Epoch 19, Batch 562, Loss: 0.40699464082717896\n",
      "Epoch 19, Batch 563, Loss: 0.4439241290092468\n",
      "Epoch 19, Batch 564, Loss: 0.31567123532295227\n",
      "Epoch 19, Batch 565, Loss: 0.3669358193874359\n",
      "Epoch 19, Batch 566, Loss: 0.3326384425163269\n",
      "Epoch 19, Batch 567, Loss: 0.4922134578227997\n",
      "Epoch 19, Batch 568, Loss: 0.5010905265808105\n",
      "Epoch 19, Batch 569, Loss: 0.551456868648529\n",
      "Epoch 19, Batch 570, Loss: 0.46659091114997864\n",
      "Epoch 19, Batch 571, Loss: 0.3577916622161865\n",
      "Epoch 19, Batch 572, Loss: 0.21350623667240143\n",
      "Epoch 19, Batch 573, Loss: 0.5003065466880798\n",
      "Epoch 19, Batch 574, Loss: 0.5242741703987122\n",
      "Epoch 19, Batch 575, Loss: 0.27265825867652893\n",
      "Epoch 19, Batch 576, Loss: 0.7430450320243835\n",
      "Epoch 19, Batch 577, Loss: 0.2703779637813568\n",
      "Epoch 19, Batch 578, Loss: 0.5885260701179504\n",
      "Epoch 19, Batch 579, Loss: 0.5144402980804443\n",
      "Epoch 19, Batch 580, Loss: 0.1913745105266571\n",
      "Epoch 19, Batch 581, Loss: 0.42075473070144653\n",
      "Epoch 19, Batch 582, Loss: 0.4313637614250183\n",
      "Epoch 19, Batch 583, Loss: 0.37097498774528503\n",
      "Epoch 19, Batch 584, Loss: 0.5052117705345154\n",
      "Epoch 19, Batch 585, Loss: 0.38059884309768677\n",
      "Epoch 19, Batch 586, Loss: 0.4123097062110901\n",
      "Epoch 19, Batch 587, Loss: 0.3986762762069702\n",
      "Epoch 19, Batch 588, Loss: 0.20749559998512268\n",
      "Epoch 19, Batch 589, Loss: 0.4953995645046234\n",
      "Epoch 19, Batch 590, Loss: 0.37497636675834656\n",
      "Epoch 19, Batch 591, Loss: 0.30709147453308105\n",
      "Epoch 19, Batch 592, Loss: 0.46305471658706665\n",
      "Epoch 19, Batch 593, Loss: 0.4859996438026428\n",
      "Epoch 19, Batch 594, Loss: 0.44729530811309814\n",
      "Epoch 19, Batch 595, Loss: 0.3797694742679596\n",
      "Epoch 19, Batch 596, Loss: 0.3505433201789856\n",
      "Epoch 19, Batch 597, Loss: 0.27284321188926697\n",
      "Epoch 19, Batch 598, Loss: 0.3826937675476074\n",
      "Epoch 19, Batch 599, Loss: 0.34215277433395386\n",
      "Epoch 19, Batch 600, Loss: 0.40419092774391174\n",
      "Epoch 19, Batch 601, Loss: 0.3960499167442322\n",
      "Epoch 19, Batch 602, Loss: 0.34701189398765564\n",
      "Epoch 19, Batch 603, Loss: 0.42432528734207153\n",
      "Epoch 19, Batch 604, Loss: 0.40155157446861267\n",
      "Epoch 19, Batch 605, Loss: 0.50779128074646\n",
      "Epoch 19, Batch 606, Loss: 0.44901043176651\n",
      "Epoch 19, Batch 607, Loss: 0.3691336214542389\n",
      "Epoch 19, Batch 608, Loss: 0.44125568866729736\n",
      "Epoch 19, Batch 609, Loss: 0.4858505129814148\n",
      "Epoch 19, Batch 610, Loss: 0.3611021339893341\n",
      "Epoch 19, Batch 611, Loss: 0.4305577278137207\n",
      "Epoch 19, Batch 612, Loss: 0.48634180426597595\n",
      "Epoch 19, Batch 613, Loss: 0.5587156414985657\n",
      "Epoch 19, Batch 614, Loss: 0.45701009035110474\n",
      "Epoch 19, Batch 615, Loss: 0.5571798086166382\n",
      "Epoch 19, Batch 616, Loss: 0.6122921705245972\n",
      "Epoch 19, Batch 617, Loss: 0.24165470898151398\n",
      "Epoch 19, Batch 618, Loss: 0.4728422164916992\n",
      "Epoch 19, Batch 619, Loss: 0.49906957149505615\n",
      "Epoch 19, Batch 620, Loss: 0.5898414850234985\n",
      "Epoch 19, Batch 621, Loss: 0.3109871745109558\n",
      "Epoch 19, Batch 622, Loss: 0.4645324945449829\n",
      "Epoch 19, Batch 623, Loss: 0.500737190246582\n",
      "Epoch 19, Batch 624, Loss: 0.4041441082954407\n",
      "Epoch 19, Batch 625, Loss: 0.5381282567977905\n",
      "Epoch 19, Batch 626, Loss: 0.24430330097675323\n",
      "Epoch 19, Batch 627, Loss: 0.2778327167034149\n",
      "Epoch 19, Batch 628, Loss: 0.3882943093776703\n",
      "Epoch 19, Batch 629, Loss: 0.5008251070976257\n",
      "Epoch 19, Batch 630, Loss: 0.42744022607803345\n",
      "Epoch 19, Batch 631, Loss: 0.5021429657936096\n",
      "Epoch 19, Batch 632, Loss: 0.6467157006263733\n",
      "Epoch 19, Batch 633, Loss: 0.5951223373413086\n",
      "Epoch 19, Batch 634, Loss: 0.5495447516441345\n",
      "Epoch 19, Batch 635, Loss: 0.6867660880088806\n",
      "Epoch 19, Batch 636, Loss: 0.4893839359283447\n",
      "Epoch 19, Batch 637, Loss: 0.2777775824069977\n",
      "Epoch 19, Batch 638, Loss: 0.6465600728988647\n",
      "Epoch 19, Batch 639, Loss: 0.511174738407135\n",
      "Epoch 19, Batch 640, Loss: 0.6122971773147583\n",
      "Epoch 19, Batch 641, Loss: 0.5680770874023438\n",
      "Epoch 19, Batch 642, Loss: 0.42616578936576843\n",
      "Epoch 19, Batch 643, Loss: 0.32080355286598206\n",
      "Epoch 19, Batch 644, Loss: 0.5070597529411316\n",
      "Epoch 19, Batch 645, Loss: 0.42585986852645874\n",
      "Epoch 19, Batch 646, Loss: 0.40791842341423035\n",
      "Epoch 19, Batch 647, Loss: 0.5005096793174744\n",
      "Epoch 19, Batch 648, Loss: 0.3369934558868408\n",
      "Epoch 19, Batch 649, Loss: 0.6804492473602295\n",
      "Epoch 19, Batch 650, Loss: 0.27342653274536133\n",
      "Epoch 19, Batch 651, Loss: 0.48943769931793213\n",
      "Epoch 19, Batch 652, Loss: 0.4983256459236145\n",
      "Epoch 19, Batch 653, Loss: 0.35057079792022705\n",
      "Epoch 19, Batch 654, Loss: 0.30863094329833984\n",
      "Epoch 19, Batch 655, Loss: 0.441335529088974\n",
      "Epoch 19, Batch 656, Loss: 0.6453729867935181\n",
      "Epoch 19, Batch 657, Loss: 0.2808663547039032\n",
      "Epoch 19, Batch 658, Loss: 0.4571237862110138\n",
      "Epoch 19, Batch 659, Loss: 0.3462556302547455\n",
      "Epoch 19, Batch 660, Loss: 0.5602186322212219\n",
      "Epoch 19, Batch 661, Loss: 0.5044810175895691\n",
      "Epoch 19, Batch 662, Loss: 0.2476738840341568\n",
      "Epoch 19, Batch 663, Loss: 0.49376147985458374\n",
      "Epoch 19, Batch 664, Loss: 0.3795868158340454\n",
      "Epoch 19, Batch 665, Loss: 0.4254956841468811\n",
      "Epoch 19, Batch 666, Loss: 0.5148783326148987\n",
      "Epoch 19, Batch 667, Loss: 0.3368111252784729\n",
      "Epoch 19, Batch 668, Loss: 0.38374730944633484\n",
      "Epoch 19, Batch 669, Loss: 0.4677027761936188\n",
      "Epoch 19, Batch 670, Loss: 0.392406702041626\n",
      "Epoch 19, Batch 671, Loss: 0.32571807503700256\n",
      "Epoch 19, Batch 672, Loss: 0.3135029673576355\n",
      "Epoch 19, Batch 673, Loss: 0.23881608247756958\n",
      "Epoch 19, Batch 674, Loss: 0.3255164325237274\n",
      "Epoch 19, Batch 675, Loss: 0.595734715461731\n",
      "Epoch 19, Batch 676, Loss: 0.22076256573200226\n",
      "Epoch 19, Batch 677, Loss: 0.4699961245059967\n",
      "Epoch 19, Batch 678, Loss: 0.3684527277946472\n",
      "Epoch 19, Batch 679, Loss: 0.3400598168373108\n",
      "Epoch 19, Batch 680, Loss: 0.39630988240242004\n",
      "Epoch 19, Batch 681, Loss: 0.3963604271411896\n",
      "Epoch 19, Batch 682, Loss: 0.38586193323135376\n",
      "Epoch 19, Batch 683, Loss: 0.5247706770896912\n",
      "Epoch 19, Batch 684, Loss: 0.2661900520324707\n",
      "Epoch 19, Batch 685, Loss: 0.3850545883178711\n",
      "Epoch 19, Batch 686, Loss: 0.5780866146087646\n",
      "Epoch 19, Batch 687, Loss: 0.4095801115036011\n",
      "Epoch 19, Batch 688, Loss: 0.3271954357624054\n",
      "Epoch 19, Batch 689, Loss: 0.4896562993526459\n",
      "Epoch 19, Batch 690, Loss: 0.44009917974472046\n",
      "Epoch 19, Batch 691, Loss: 0.2809964120388031\n",
      "Epoch 19, Batch 692, Loss: 0.4338257312774658\n",
      "Epoch 19, Batch 693, Loss: 0.2686210870742798\n",
      "Epoch 19, Batch 694, Loss: 0.5060990452766418\n",
      "Epoch 19, Batch 695, Loss: 0.3780779242515564\n",
      "Epoch 19, Batch 696, Loss: 0.4224191904067993\n",
      "Epoch 19, Batch 697, Loss: 0.4136284291744232\n",
      "Epoch 19, Batch 698, Loss: 0.3653185963630676\n",
      "Epoch 19, Batch 699, Loss: 0.26662498712539673\n",
      "Epoch 19, Batch 700, Loss: 0.45309382677078247\n",
      "Epoch 19, Batch 701, Loss: 0.4369867444038391\n",
      "Epoch 19, Batch 702, Loss: 0.5899877548217773\n",
      "Epoch 19, Batch 703, Loss: 0.40624475479125977\n",
      "Epoch 19, Batch 704, Loss: 0.46839460730552673\n",
      "Epoch 19, Batch 705, Loss: 0.23444899916648865\n",
      "Epoch 19, Batch 706, Loss: 0.23750761151313782\n",
      "Epoch 19, Batch 707, Loss: 0.5870702266693115\n",
      "Epoch 19, Batch 708, Loss: 0.3434436023235321\n",
      "Epoch 19, Batch 709, Loss: 0.31282782554626465\n",
      "Epoch 19, Batch 710, Loss: 0.4731428027153015\n",
      "Epoch 19, Batch 711, Loss: 0.5038520693778992\n",
      "Epoch 19, Batch 712, Loss: 0.3248141407966614\n",
      "Epoch 19, Batch 713, Loss: 0.3358548581600189\n",
      "Epoch 19, Batch 714, Loss: 0.36193257570266724\n",
      "Epoch 19, Batch 715, Loss: 0.6239142417907715\n",
      "Epoch 19, Batch 716, Loss: 0.38757187128067017\n",
      "Epoch 19, Batch 717, Loss: 0.3526902496814728\n",
      "Epoch 19, Batch 718, Loss: 0.5085846781730652\n",
      "Epoch 19, Batch 719, Loss: 0.4713590443134308\n",
      "Epoch 19, Batch 720, Loss: 0.47565749287605286\n",
      "Epoch 19, Batch 721, Loss: 0.4438909590244293\n",
      "Epoch 19, Batch 722, Loss: 0.5584130883216858\n",
      "Epoch 19, Batch 723, Loss: 0.30719631910324097\n",
      "Epoch 19, Batch 724, Loss: 0.5502415895462036\n",
      "Epoch 19, Batch 725, Loss: 0.28030624985694885\n",
      "Epoch 19, Batch 726, Loss: 0.47918182611465454\n",
      "Epoch 19, Batch 727, Loss: 0.32773062586784363\n",
      "Epoch 19, Batch 728, Loss: 0.36267319321632385\n",
      "Epoch 19, Batch 729, Loss: 0.34232714772224426\n",
      "Epoch 19, Batch 730, Loss: 0.29256945848464966\n",
      "Epoch 19, Batch 731, Loss: 0.3044899106025696\n",
      "Epoch 19, Batch 732, Loss: 0.43639466166496277\n",
      "Epoch 19, Batch 733, Loss: 0.43448910117149353\n",
      "Epoch 19, Batch 734, Loss: 0.41772890090942383\n",
      "Epoch 19, Batch 735, Loss: 0.3898791968822479\n",
      "Epoch 19, Batch 736, Loss: 0.2499680370092392\n",
      "Epoch 19, Batch 737, Loss: 0.3692060708999634\n",
      "Epoch 19, Batch 738, Loss: 0.395862877368927\n",
      "Epoch 19, Batch 739, Loss: 0.46068650484085083\n",
      "Epoch 19, Batch 740, Loss: 0.26369380950927734\n",
      "Epoch 19, Batch 741, Loss: 0.40964365005493164\n",
      "Epoch 19, Batch 742, Loss: 0.6852573156356812\n",
      "Epoch 19, Batch 743, Loss: 0.21505379676818848\n",
      "Epoch 19, Batch 744, Loss: 0.5032703280448914\n",
      "Epoch 19, Batch 745, Loss: 0.4465535879135132\n",
      "Epoch 19, Batch 746, Loss: 0.44866353273391724\n",
      "Epoch 19, Batch 747, Loss: 0.5195029973983765\n",
      "Epoch 19, Batch 748, Loss: 0.2683361768722534\n",
      "Epoch 19, Batch 749, Loss: 0.48125791549682617\n",
      "Epoch 19, Batch 750, Loss: 0.3365033268928528\n",
      "Epoch 19, Batch 751, Loss: 0.36883729696273804\n",
      "Epoch 19, Batch 752, Loss: 0.5107467770576477\n",
      "Epoch 19, Batch 753, Loss: 0.24823975563049316\n",
      "Epoch 19, Batch 754, Loss: 0.3224722743034363\n",
      "Epoch 19, Batch 755, Loss: 0.6075032353401184\n",
      "Epoch 19, Batch 756, Loss: 0.5071613788604736\n",
      "Epoch 19, Batch 757, Loss: 0.5136827826499939\n",
      "Epoch 19, Batch 758, Loss: 0.32426244020462036\n",
      "Epoch 19, Batch 759, Loss: 0.38661542534828186\n",
      "Epoch 19, Batch 760, Loss: 0.504197895526886\n",
      "Epoch 19, Batch 761, Loss: 0.4620516896247864\n",
      "Epoch 19, Batch 762, Loss: 0.36996668577194214\n",
      "Epoch 19, Batch 763, Loss: 0.2247619777917862\n",
      "Epoch 19, Batch 764, Loss: 0.24262899160385132\n",
      "Epoch 19, Batch 765, Loss: 0.34967267513275146\n",
      "Epoch 19, Batch 766, Loss: 0.2516779601573944\n",
      "Epoch 19, Batch 767, Loss: 0.26232606172561646\n",
      "Epoch 19, Batch 768, Loss: 0.4615309238433838\n",
      "Epoch 19, Batch 769, Loss: 0.2915332317352295\n",
      "Epoch 19, Batch 770, Loss: 0.42801982164382935\n",
      "Epoch 19, Batch 771, Loss: 0.31048324704170227\n",
      "Epoch 19, Batch 772, Loss: 0.4390280246734619\n",
      "Epoch 19, Batch 773, Loss: 0.5496535897254944\n",
      "Epoch 19, Batch 774, Loss: 0.48078134655952454\n",
      "Epoch 19, Batch 775, Loss: 0.32823875546455383\n",
      "Epoch 19, Batch 776, Loss: 0.4320010244846344\n",
      "Epoch 19, Batch 777, Loss: 0.22919486463069916\n",
      "Epoch 19, Batch 778, Loss: 0.29063037037849426\n",
      "Epoch 19, Batch 779, Loss: 0.17893415689468384\n",
      "Epoch 19, Batch 780, Loss: 0.2826557755470276\n",
      "Epoch 19, Batch 781, Loss: 0.32130542397499084\n",
      "Epoch 19, Batch 782, Loss: 0.28690409660339355\n",
      "Epoch 19, Batch 783, Loss: 0.49511998891830444\n",
      "Epoch 19, Batch 784, Loss: 0.4001566469669342\n",
      "Epoch 19, Batch 785, Loss: 0.353162556886673\n",
      "Epoch 19, Batch 786, Loss: 0.2929047644138336\n",
      "Epoch 19, Batch 787, Loss: 0.2282036542892456\n",
      "Epoch 19, Batch 788, Loss: 0.31379473209381104\n",
      "Epoch 19, Batch 789, Loss: 0.27818602323532104\n",
      "Epoch 19, Batch 790, Loss: 0.397695392370224\n",
      "Epoch 19, Batch 791, Loss: 0.5063892602920532\n",
      "Epoch 19, Batch 792, Loss: 0.4584396481513977\n",
      "Epoch 19, Batch 793, Loss: 0.5718675255775452\n",
      "Epoch 19, Batch 794, Loss: 0.2917211055755615\n",
      "Epoch 19, Batch 795, Loss: 0.3617697060108185\n",
      "Epoch 19, Batch 796, Loss: 0.310276061296463\n",
      "Epoch 19, Batch 797, Loss: 0.3374958038330078\n",
      "Epoch 19, Batch 798, Loss: 0.3994769752025604\n",
      "Epoch 19, Batch 799, Loss: 0.26261526346206665\n",
      "Epoch 19, Batch 800, Loss: 0.4482311010360718\n",
      "Epoch 19, Batch 801, Loss: 0.2778632640838623\n",
      "Epoch 19, Batch 802, Loss: 0.24136315286159515\n",
      "Epoch 19, Batch 803, Loss: 0.4035419523715973\n",
      "Epoch 19, Batch 804, Loss: 0.5504992008209229\n",
      "Epoch 19, Batch 805, Loss: 0.7946213483810425\n",
      "Epoch 19, Batch 806, Loss: 0.3378102779388428\n",
      "Epoch 19, Batch 807, Loss: 0.4623529016971588\n",
      "Epoch 19, Batch 808, Loss: 0.3864634037017822\n",
      "Epoch 19, Batch 809, Loss: 0.4534189701080322\n",
      "Epoch 19, Batch 810, Loss: 0.26476001739501953\n",
      "Epoch 19, Batch 811, Loss: 0.34358397126197815\n",
      "Epoch 19, Batch 812, Loss: 0.4243628978729248\n",
      "Epoch 19, Batch 813, Loss: 0.42836567759513855\n",
      "Epoch 19, Batch 814, Loss: 0.7267429828643799\n",
      "Epoch 19, Batch 815, Loss: 0.36173924803733826\n",
      "Epoch 19, Batch 816, Loss: 0.4511052072048187\n",
      "Epoch 19, Batch 817, Loss: 0.29159489274024963\n",
      "Epoch 19, Batch 818, Loss: 0.36817070841789246\n",
      "Epoch 19, Batch 819, Loss: 0.5117931365966797\n",
      "Epoch 19, Batch 820, Loss: 0.36456984281539917\n",
      "Epoch 19, Batch 821, Loss: 0.5787522196769714\n",
      "Epoch 19, Batch 822, Loss: 0.45964425802230835\n",
      "Epoch 19, Batch 823, Loss: 0.4103979766368866\n",
      "Epoch 19, Batch 824, Loss: 0.2643466591835022\n",
      "Epoch 19, Batch 825, Loss: 0.2933521866798401\n",
      "Epoch 19, Batch 826, Loss: 0.4800814092159271\n",
      "Epoch 19, Batch 827, Loss: 0.4746397137641907\n",
      "Epoch 19, Batch 828, Loss: 0.3425065875053406\n",
      "Epoch 19, Batch 829, Loss: 0.33771926164627075\n",
      "Epoch 19, Batch 830, Loss: 0.5312364101409912\n",
      "Epoch 19, Batch 831, Loss: 0.31297367811203003\n",
      "Epoch 19, Batch 832, Loss: 0.3525557518005371\n",
      "Epoch 19, Batch 833, Loss: 0.41196998953819275\n",
      "Epoch 19, Batch 834, Loss: 0.19775086641311646\n",
      "Epoch 19, Batch 835, Loss: 0.37406787276268005\n",
      "Epoch 19, Batch 836, Loss: 0.4626980423927307\n",
      "Epoch 19, Batch 837, Loss: 0.42528074979782104\n",
      "Epoch 19, Batch 838, Loss: 0.4032694697380066\n",
      "Epoch 19, Batch 839, Loss: 0.5158752799034119\n",
      "Epoch 19, Batch 840, Loss: 0.561447024345398\n",
      "Epoch 19, Batch 841, Loss: 0.6145158410072327\n",
      "Epoch 19, Batch 842, Loss: 0.4127900004386902\n",
      "Epoch 19, Batch 843, Loss: 0.5245243906974792\n",
      "Epoch 19, Batch 844, Loss: 0.3481558859348297\n",
      "Epoch 19, Batch 845, Loss: 0.39862382411956787\n",
      "Epoch 19, Batch 846, Loss: 0.32532232999801636\n",
      "Epoch 19, Batch 847, Loss: 0.5966634154319763\n",
      "Epoch 19, Batch 848, Loss: 0.3278159201145172\n",
      "Epoch 19, Batch 849, Loss: 0.5090475678443909\n",
      "Epoch 19, Batch 850, Loss: 0.4795483648777008\n",
      "Epoch 19, Batch 851, Loss: 0.33642250299453735\n",
      "Epoch 19, Batch 852, Loss: 0.37891238927841187\n",
      "Epoch 19, Batch 853, Loss: 0.35638487339019775\n",
      "Epoch 19, Batch 854, Loss: 0.3901245892047882\n",
      "Epoch 19, Batch 855, Loss: 0.27287063002586365\n",
      "Epoch 19, Batch 856, Loss: 0.4089917242527008\n",
      "Epoch 19, Batch 857, Loss: 0.44195181131362915\n",
      "Epoch 19, Batch 858, Loss: 0.2784525454044342\n",
      "Epoch 19, Batch 859, Loss: 0.38228118419647217\n",
      "Epoch 19, Batch 860, Loss: 0.3027673661708832\n",
      "Epoch 19, Batch 861, Loss: 0.5507539510726929\n",
      "Epoch 19, Batch 862, Loss: 0.40174049139022827\n",
      "Epoch 19, Batch 863, Loss: 0.3783228099346161\n",
      "Epoch 19, Batch 864, Loss: 0.3127205967903137\n",
      "Epoch 19, Batch 865, Loss: 0.6211917996406555\n",
      "Epoch 19, Batch 866, Loss: 0.3675382137298584\n",
      "Epoch 19, Batch 867, Loss: 0.499485045671463\n",
      "Epoch 19, Batch 868, Loss: 0.407563179731369\n",
      "Epoch 19, Batch 869, Loss: 0.48876965045928955\n",
      "Epoch 19, Batch 870, Loss: 0.42194128036499023\n",
      "Epoch 19, Batch 871, Loss: 0.47916555404663086\n",
      "Epoch 19, Batch 872, Loss: 0.5004857182502747\n",
      "Epoch 19, Batch 873, Loss: 0.32604676485061646\n",
      "Epoch 19, Batch 874, Loss: 0.45122283697128296\n",
      "Epoch 19, Batch 875, Loss: 0.29212623834609985\n",
      "Epoch 19, Batch 876, Loss: 0.3161274492740631\n",
      "Epoch 19, Batch 877, Loss: 0.4061231017112732\n",
      "Epoch 19, Batch 878, Loss: 0.4696417450904846\n",
      "Epoch 19, Batch 879, Loss: 0.32827436923980713\n",
      "Epoch 19, Batch 880, Loss: 0.4397020936012268\n",
      "Epoch 19, Batch 881, Loss: 0.43319082260131836\n",
      "Epoch 19, Batch 882, Loss: 0.4551377296447754\n",
      "Epoch 19, Batch 883, Loss: 0.4659239947795868\n",
      "Epoch 19, Batch 884, Loss: 0.4022271931171417\n",
      "Epoch 19, Batch 885, Loss: 0.389290452003479\n",
      "Epoch 19, Batch 886, Loss: 0.3883747458457947\n",
      "Epoch 19, Batch 887, Loss: 0.32137560844421387\n",
      "Epoch 19, Batch 888, Loss: 0.3560715317726135\n",
      "Epoch 19, Batch 889, Loss: 0.4626530110836029\n",
      "Epoch 19, Batch 890, Loss: 0.37707772850990295\n",
      "Epoch 19, Batch 891, Loss: 0.4585380256175995\n",
      "Epoch 19, Batch 892, Loss: 0.3019392192363739\n",
      "Epoch 19, Batch 893, Loss: 0.2565385103225708\n",
      "Epoch 19, Batch 894, Loss: 0.3929024338722229\n",
      "Epoch 19, Batch 895, Loss: 0.4072074294090271\n",
      "Epoch 19, Batch 896, Loss: 0.3981090188026428\n",
      "Epoch 19, Batch 897, Loss: 0.39784348011016846\n",
      "Epoch 19, Batch 898, Loss: 0.3892558813095093\n",
      "Epoch 19, Batch 899, Loss: 0.45732414722442627\n",
      "Epoch 19, Batch 900, Loss: 0.3936314284801483\n",
      "Epoch 19, Batch 901, Loss: 0.22539575397968292\n",
      "Epoch 19, Batch 902, Loss: 0.2566627562046051\n",
      "Epoch 19, Batch 903, Loss: 0.46685275435447693\n",
      "Epoch 19, Batch 904, Loss: 0.4366203248500824\n",
      "Epoch 19, Batch 905, Loss: 0.5775556564331055\n",
      "Epoch 19, Batch 906, Loss: 0.42090582847595215\n",
      "Epoch 19, Batch 907, Loss: 0.43621528148651123\n",
      "Epoch 19, Batch 908, Loss: 0.5594390034675598\n",
      "Epoch 19, Batch 909, Loss: 0.36126628518104553\n",
      "Epoch 19, Batch 910, Loss: 0.3954780101776123\n",
      "Epoch 19, Batch 911, Loss: 0.2588285803794861\n",
      "Epoch 19, Batch 912, Loss: 0.4078412353992462\n",
      "Epoch 19, Batch 913, Loss: 0.3776716887950897\n",
      "Epoch 19, Batch 914, Loss: 0.35503634810447693\n",
      "Epoch 19, Batch 915, Loss: 0.37817031145095825\n",
      "Epoch 19, Batch 916, Loss: 0.41114455461502075\n",
      "Epoch 19, Batch 917, Loss: 0.32602158188819885\n",
      "Epoch 19, Batch 918, Loss: 0.3399909436702728\n",
      "Epoch 19, Batch 919, Loss: 0.5466230511665344\n",
      "Epoch 19, Batch 920, Loss: 0.33319544792175293\n",
      "Epoch 19, Batch 921, Loss: 0.5013396143913269\n",
      "Epoch 19, Batch 922, Loss: 0.6313000321388245\n",
      "Epoch 19, Batch 923, Loss: 0.22397558391094208\n",
      "Epoch 19, Batch 924, Loss: 0.37916040420532227\n",
      "Epoch 19, Batch 925, Loss: 0.5161161422729492\n",
      "Epoch 19, Batch 926, Loss: 0.5338464975357056\n",
      "Epoch 19, Batch 927, Loss: 0.4038473069667816\n",
      "Epoch 19, Batch 928, Loss: 0.548778772354126\n",
      "Epoch 19, Batch 929, Loss: 0.4744439423084259\n",
      "Epoch 19, Batch 930, Loss: 0.5772854089736938\n",
      "Epoch 19, Batch 931, Loss: 0.3712344765663147\n",
      "Epoch 19, Batch 932, Loss: 0.7117786407470703\n",
      "Epoch 19, Batch 933, Loss: 0.3812650144100189\n",
      "Epoch 19, Batch 934, Loss: 0.48998674750328064\n",
      "Epoch 19, Batch 935, Loss: 0.6206468939781189\n",
      "Epoch 19, Batch 936, Loss: 0.5751941204071045\n",
      "Epoch 19, Batch 937, Loss: 0.36589041352272034\n",
      "Epoch 19, Batch 938, Loss: 0.4884599447250366\n",
      "Accuracy of train set: 0.8510166666666666\n",
      "Epoch 19, Batch 1, Test Loss: 0.4027203321456909\n",
      "Epoch 19, Batch 2, Test Loss: 0.473101407289505\n",
      "Epoch 19, Batch 3, Test Loss: 0.49378085136413574\n",
      "Epoch 19, Batch 4, Test Loss: 0.23166415095329285\n",
      "Epoch 19, Batch 5, Test Loss: 0.33420851826667786\n",
      "Epoch 19, Batch 6, Test Loss: 0.6716183423995972\n",
      "Epoch 19, Batch 7, Test Loss: 0.387773334980011\n",
      "Epoch 19, Batch 8, Test Loss: 0.3956802785396576\n",
      "Epoch 19, Batch 9, Test Loss: 0.524662435054779\n",
      "Epoch 19, Batch 10, Test Loss: 0.29554417729377747\n",
      "Epoch 19, Batch 11, Test Loss: 0.33013665676116943\n",
      "Epoch 19, Batch 12, Test Loss: 0.42098525166511536\n",
      "Epoch 19, Batch 13, Test Loss: 0.3865028917789459\n",
      "Epoch 19, Batch 14, Test Loss: 0.6592038869857788\n",
      "Epoch 19, Batch 15, Test Loss: 0.4255649447441101\n",
      "Epoch 19, Batch 16, Test Loss: 0.3448188006877899\n",
      "Epoch 19, Batch 17, Test Loss: 0.36337506771087646\n",
      "Epoch 19, Batch 18, Test Loss: 0.5171799659729004\n",
      "Epoch 19, Batch 19, Test Loss: 0.5616519451141357\n",
      "Epoch 19, Batch 20, Test Loss: 0.3616426885128021\n",
      "Epoch 19, Batch 21, Test Loss: 0.3953573703765869\n",
      "Epoch 19, Batch 22, Test Loss: 0.502145528793335\n",
      "Epoch 19, Batch 23, Test Loss: 0.3366814851760864\n",
      "Epoch 19, Batch 24, Test Loss: 0.5136244297027588\n",
      "Epoch 19, Batch 25, Test Loss: 0.42639869451522827\n",
      "Epoch 19, Batch 26, Test Loss: 0.4080536365509033\n",
      "Epoch 19, Batch 27, Test Loss: 0.5254689455032349\n",
      "Epoch 19, Batch 28, Test Loss: 0.3467024564743042\n",
      "Epoch 19, Batch 29, Test Loss: 0.3734786808490753\n",
      "Epoch 19, Batch 30, Test Loss: 0.47747737169265747\n",
      "Epoch 19, Batch 31, Test Loss: 0.3742886483669281\n",
      "Epoch 19, Batch 32, Test Loss: 0.6335350871086121\n",
      "Epoch 19, Batch 33, Test Loss: 0.3275867700576782\n",
      "Epoch 19, Batch 34, Test Loss: 0.30274394154548645\n",
      "Epoch 19, Batch 35, Test Loss: 0.265552282333374\n",
      "Epoch 19, Batch 36, Test Loss: 0.41802918910980225\n",
      "Epoch 19, Batch 37, Test Loss: 0.5468712449073792\n",
      "Epoch 19, Batch 38, Test Loss: 0.5226038694381714\n",
      "Epoch 19, Batch 39, Test Loss: 0.5109994411468506\n",
      "Epoch 19, Batch 40, Test Loss: 0.3472176194190979\n",
      "Epoch 19, Batch 41, Test Loss: 0.4021994173526764\n",
      "Epoch 19, Batch 42, Test Loss: 0.4371080994606018\n",
      "Epoch 19, Batch 43, Test Loss: 0.4673866927623749\n",
      "Epoch 19, Batch 44, Test Loss: 0.5369635820388794\n",
      "Epoch 19, Batch 45, Test Loss: 0.5203306674957275\n",
      "Epoch 19, Batch 46, Test Loss: 0.24510164558887482\n",
      "Epoch 19, Batch 47, Test Loss: 0.6172360777854919\n",
      "Epoch 19, Batch 48, Test Loss: 0.3379063904285431\n",
      "Epoch 19, Batch 49, Test Loss: 0.27283021807670593\n",
      "Epoch 19, Batch 50, Test Loss: 0.4881289303302765\n",
      "Epoch 19, Batch 51, Test Loss: 0.36184120178222656\n",
      "Epoch 19, Batch 52, Test Loss: 0.5186643004417419\n",
      "Epoch 19, Batch 53, Test Loss: 0.5007601976394653\n",
      "Epoch 19, Batch 54, Test Loss: 0.30662697553634644\n",
      "Epoch 19, Batch 55, Test Loss: 0.3400879204273224\n",
      "Epoch 19, Batch 56, Test Loss: 0.4224764108657837\n",
      "Epoch 19, Batch 57, Test Loss: 0.5440185070037842\n",
      "Epoch 19, Batch 58, Test Loss: 0.4554697275161743\n",
      "Epoch 19, Batch 59, Test Loss: 0.6966260671615601\n",
      "Epoch 19, Batch 60, Test Loss: 0.3808908462524414\n",
      "Epoch 19, Batch 61, Test Loss: 0.4963299632072449\n",
      "Epoch 19, Batch 62, Test Loss: 0.28654903173446655\n",
      "Epoch 19, Batch 63, Test Loss: 0.38688603043556213\n",
      "Epoch 19, Batch 64, Test Loss: 0.4649781286716461\n",
      "Epoch 19, Batch 65, Test Loss: 0.6074559688568115\n",
      "Epoch 19, Batch 66, Test Loss: 0.37765535712242126\n",
      "Epoch 19, Batch 67, Test Loss: 0.3643021285533905\n",
      "Epoch 19, Batch 68, Test Loss: 0.5614547729492188\n",
      "Epoch 19, Batch 69, Test Loss: 0.37061426043510437\n",
      "Epoch 19, Batch 70, Test Loss: 0.24591164290905\n",
      "Epoch 19, Batch 71, Test Loss: 0.34148505330085754\n",
      "Epoch 19, Batch 72, Test Loss: 0.5047566890716553\n",
      "Epoch 19, Batch 73, Test Loss: 0.28457701206207275\n",
      "Epoch 19, Batch 74, Test Loss: 0.46860000491142273\n",
      "Epoch 19, Batch 75, Test Loss: 0.3904589116573334\n",
      "Epoch 19, Batch 76, Test Loss: 0.29240691661834717\n",
      "Epoch 19, Batch 77, Test Loss: 0.4470381736755371\n",
      "Epoch 19, Batch 78, Test Loss: 0.2849614918231964\n",
      "Epoch 19, Batch 79, Test Loss: 0.31181252002716064\n",
      "Epoch 19, Batch 80, Test Loss: 0.3236023485660553\n",
      "Epoch 19, Batch 81, Test Loss: 0.2656281292438507\n",
      "Epoch 19, Batch 82, Test Loss: 0.43286755681037903\n",
      "Epoch 19, Batch 83, Test Loss: 0.4336682856082916\n",
      "Epoch 19, Batch 84, Test Loss: 0.29558485746383667\n",
      "Epoch 19, Batch 85, Test Loss: 0.5480174422264099\n",
      "Epoch 19, Batch 86, Test Loss: 0.43986642360687256\n",
      "Epoch 19, Batch 87, Test Loss: 0.6408769488334656\n",
      "Epoch 19, Batch 88, Test Loss: 0.3829890787601471\n",
      "Epoch 19, Batch 89, Test Loss: 0.6126923561096191\n",
      "Epoch 19, Batch 90, Test Loss: 0.37050148844718933\n",
      "Epoch 19, Batch 91, Test Loss: 0.4662513732910156\n",
      "Epoch 19, Batch 92, Test Loss: 0.4792865812778473\n",
      "Epoch 19, Batch 93, Test Loss: 0.43360310792922974\n",
      "Epoch 19, Batch 94, Test Loss: 0.36552244424819946\n",
      "Epoch 19, Batch 95, Test Loss: 0.589036226272583\n",
      "Epoch 19, Batch 96, Test Loss: 0.4605558514595032\n",
      "Epoch 19, Batch 97, Test Loss: 0.38210129737854004\n",
      "Epoch 19, Batch 98, Test Loss: 0.5682680010795593\n",
      "Epoch 19, Batch 99, Test Loss: 0.6060944199562073\n",
      "Epoch 19, Batch 100, Test Loss: 0.46510201692581177\n",
      "Epoch 19, Batch 101, Test Loss: 0.4458233416080475\n",
      "Epoch 19, Batch 102, Test Loss: 0.4745178818702698\n",
      "Epoch 19, Batch 103, Test Loss: 0.4236227869987488\n",
      "Epoch 19, Batch 104, Test Loss: 0.3725326955318451\n",
      "Epoch 19, Batch 105, Test Loss: 0.45079511404037476\n",
      "Epoch 19, Batch 106, Test Loss: 0.5091875791549683\n",
      "Epoch 19, Batch 107, Test Loss: 0.5861825346946716\n",
      "Epoch 19, Batch 108, Test Loss: 0.4261782765388489\n",
      "Epoch 19, Batch 109, Test Loss: 0.536585807800293\n",
      "Epoch 19, Batch 110, Test Loss: 0.5589608550071716\n",
      "Epoch 19, Batch 111, Test Loss: 0.5357962250709534\n",
      "Epoch 19, Batch 112, Test Loss: 0.3152634799480438\n",
      "Epoch 19, Batch 113, Test Loss: 0.4999273121356964\n",
      "Epoch 19, Batch 114, Test Loss: 0.43597668409347534\n",
      "Epoch 19, Batch 115, Test Loss: 0.2249971628189087\n",
      "Epoch 19, Batch 116, Test Loss: 0.34166455268859863\n",
      "Epoch 19, Batch 117, Test Loss: 0.22751018404960632\n",
      "Epoch 19, Batch 118, Test Loss: 0.4727591574192047\n",
      "Epoch 19, Batch 119, Test Loss: 0.4595281481742859\n",
      "Epoch 19, Batch 120, Test Loss: 0.32575953006744385\n",
      "Epoch 19, Batch 121, Test Loss: 0.34768861532211304\n",
      "Epoch 19, Batch 122, Test Loss: 0.3277900815010071\n",
      "Epoch 19, Batch 123, Test Loss: 0.37927618622779846\n",
      "Epoch 19, Batch 124, Test Loss: 0.45890307426452637\n",
      "Epoch 19, Batch 125, Test Loss: 0.37398257851600647\n",
      "Epoch 19, Batch 126, Test Loss: 0.37928029894828796\n",
      "Epoch 19, Batch 127, Test Loss: 0.444663405418396\n",
      "Epoch 19, Batch 128, Test Loss: 0.3247052729129791\n",
      "Epoch 19, Batch 129, Test Loss: 0.565898597240448\n",
      "Epoch 19, Batch 130, Test Loss: 0.4014783501625061\n",
      "Epoch 19, Batch 131, Test Loss: 0.42617934942245483\n",
      "Epoch 19, Batch 132, Test Loss: 0.4210979640483856\n",
      "Epoch 19, Batch 133, Test Loss: 0.2796418070793152\n",
      "Epoch 19, Batch 134, Test Loss: 0.2504923641681671\n",
      "Epoch 19, Batch 135, Test Loss: 0.39798805117607117\n",
      "Epoch 19, Batch 136, Test Loss: 0.279712438583374\n",
      "Epoch 19, Batch 137, Test Loss: 0.5704554915428162\n",
      "Epoch 19, Batch 138, Test Loss: 0.4927832782268524\n",
      "Epoch 19, Batch 139, Test Loss: 0.6806499361991882\n",
      "Epoch 19, Batch 140, Test Loss: 0.3960188925266266\n",
      "Epoch 19, Batch 141, Test Loss: 0.37824445962905884\n",
      "Epoch 19, Batch 142, Test Loss: 0.3723471760749817\n",
      "Epoch 19, Batch 143, Test Loss: 0.24908462166786194\n",
      "Epoch 19, Batch 144, Test Loss: 0.42879778146743774\n",
      "Epoch 19, Batch 145, Test Loss: 0.42012009024620056\n",
      "Epoch 19, Batch 146, Test Loss: 0.45990461111068726\n",
      "Epoch 19, Batch 147, Test Loss: 0.3224623501300812\n",
      "Epoch 19, Batch 148, Test Loss: 0.6098749041557312\n",
      "Epoch 19, Batch 149, Test Loss: 0.30737248063087463\n",
      "Epoch 19, Batch 150, Test Loss: 0.2640727460384369\n",
      "Epoch 19, Batch 151, Test Loss: 0.44656503200531006\n",
      "Epoch 19, Batch 152, Test Loss: 0.2742704451084137\n",
      "Epoch 19, Batch 153, Test Loss: 0.3546285927295685\n",
      "Epoch 19, Batch 154, Test Loss: 0.5783405900001526\n",
      "Epoch 19, Batch 155, Test Loss: 0.4362560212612152\n",
      "Epoch 19, Batch 156, Test Loss: 0.47461098432540894\n",
      "Epoch 19, Batch 157, Test Loss: 0.3548857867717743\n",
      "Epoch 19, Batch 158, Test Loss: 0.622619092464447\n",
      "Epoch 19, Batch 159, Test Loss: 0.469376802444458\n",
      "Epoch 19, Batch 160, Test Loss: 0.29540935158729553\n",
      "Epoch 19, Batch 161, Test Loss: 0.4381065368652344\n",
      "Epoch 19, Batch 162, Test Loss: 0.39841634035110474\n",
      "Epoch 19, Batch 163, Test Loss: 0.6597610116004944\n",
      "Epoch 19, Batch 164, Test Loss: 0.4211617112159729\n",
      "Epoch 19, Batch 165, Test Loss: 0.4439285695552826\n",
      "Epoch 19, Batch 166, Test Loss: 0.4639488160610199\n",
      "Epoch 19, Batch 167, Test Loss: 0.3650573790073395\n",
      "Epoch 19, Batch 168, Test Loss: 0.3706090450286865\n",
      "Epoch 19, Batch 169, Test Loss: 0.4359867572784424\n",
      "Epoch 19, Batch 170, Test Loss: 0.3254263401031494\n",
      "Epoch 19, Batch 171, Test Loss: 0.36836567521095276\n",
      "Epoch 19, Batch 172, Test Loss: 0.49353957176208496\n",
      "Epoch 19, Batch 173, Test Loss: 0.3704878091812134\n",
      "Epoch 19, Batch 174, Test Loss: 0.6143583655357361\n",
      "Epoch 19, Batch 175, Test Loss: 0.521530032157898\n",
      "Epoch 19, Batch 176, Test Loss: 0.42839258909225464\n",
      "Epoch 19, Batch 177, Test Loss: 0.2667677402496338\n",
      "Epoch 19, Batch 178, Test Loss: 0.31021425127983093\n",
      "Epoch 19, Batch 179, Test Loss: 0.6159957647323608\n",
      "Epoch 19, Batch 180, Test Loss: 0.5146275758743286\n",
      "Epoch 19, Batch 181, Test Loss: 0.7750372886657715\n",
      "Epoch 19, Batch 182, Test Loss: 0.29559946060180664\n",
      "Epoch 19, Batch 183, Test Loss: 0.36973053216934204\n",
      "Epoch 19, Batch 184, Test Loss: 0.34541893005371094\n",
      "Epoch 19, Batch 185, Test Loss: 0.5153979659080505\n",
      "Epoch 19, Batch 186, Test Loss: 0.2935222089290619\n",
      "Epoch 19, Batch 187, Test Loss: 0.37344059348106384\n",
      "Epoch 19, Batch 188, Test Loss: 0.31758755445480347\n",
      "Epoch 19, Batch 189, Test Loss: 0.3514976501464844\n",
      "Epoch 19, Batch 190, Test Loss: 0.2103193998336792\n",
      "Epoch 19, Batch 191, Test Loss: 0.3920690715312958\n",
      "Epoch 19, Batch 192, Test Loss: 0.3957520127296448\n",
      "Epoch 19, Batch 193, Test Loss: 0.36504170298576355\n",
      "Epoch 19, Batch 194, Test Loss: 0.4809629023075104\n",
      "Epoch 19, Batch 195, Test Loss: 0.43086475133895874\n",
      "Epoch 19, Batch 196, Test Loss: 0.4668008089065552\n",
      "Epoch 19, Batch 197, Test Loss: 0.3769901990890503\n",
      "Epoch 19, Batch 198, Test Loss: 0.5088111162185669\n",
      "Epoch 19, Batch 199, Test Loss: 0.3522487282752991\n",
      "Epoch 19, Batch 200, Test Loss: 0.330178439617157\n",
      "Epoch 19, Batch 201, Test Loss: 0.41115495562553406\n",
      "Epoch 19, Batch 202, Test Loss: 0.5019100308418274\n",
      "Epoch 19, Batch 203, Test Loss: 0.36674025654792786\n",
      "Epoch 19, Batch 204, Test Loss: 0.5327677726745605\n",
      "Epoch 19, Batch 205, Test Loss: 0.21436436474323273\n",
      "Epoch 19, Batch 206, Test Loss: 0.44726982712745667\n",
      "Epoch 19, Batch 207, Test Loss: 0.47219106554985046\n",
      "Epoch 19, Batch 208, Test Loss: 0.5518054962158203\n",
      "Epoch 19, Batch 209, Test Loss: 0.36147016286849976\n",
      "Epoch 19, Batch 210, Test Loss: 0.4848620593547821\n",
      "Epoch 19, Batch 211, Test Loss: 0.4187615215778351\n",
      "Epoch 19, Batch 212, Test Loss: 0.6351186037063599\n",
      "Epoch 19, Batch 213, Test Loss: 0.3528375029563904\n",
      "Epoch 19, Batch 214, Test Loss: 0.43851399421691895\n",
      "Epoch 19, Batch 215, Test Loss: 0.3817770183086395\n",
      "Epoch 19, Batch 216, Test Loss: 0.34546002745628357\n",
      "Epoch 19, Batch 217, Test Loss: 0.29151004552841187\n",
      "Epoch 19, Batch 218, Test Loss: 0.4835379123687744\n",
      "Epoch 19, Batch 219, Test Loss: 0.3266024887561798\n",
      "Epoch 19, Batch 220, Test Loss: 0.4436052739620209\n",
      "Epoch 19, Batch 221, Test Loss: 0.4575954079627991\n",
      "Epoch 19, Batch 222, Test Loss: 0.37861010432243347\n",
      "Epoch 19, Batch 223, Test Loss: 0.4563194513320923\n",
      "Epoch 19, Batch 224, Test Loss: 0.46868422627449036\n",
      "Epoch 19, Batch 225, Test Loss: 0.464752197265625\n",
      "Epoch 19, Batch 226, Test Loss: 0.45766469836235046\n",
      "Epoch 19, Batch 227, Test Loss: 0.6496898531913757\n",
      "Epoch 19, Batch 228, Test Loss: 0.46582648158073425\n",
      "Epoch 19, Batch 229, Test Loss: 0.46770596504211426\n",
      "Epoch 19, Batch 230, Test Loss: 0.628261387348175\n",
      "Epoch 19, Batch 231, Test Loss: 0.2506697475910187\n",
      "Epoch 19, Batch 232, Test Loss: 0.3070986866950989\n",
      "Epoch 19, Batch 233, Test Loss: 0.5781673192977905\n",
      "Epoch 19, Batch 234, Test Loss: 0.3547123074531555\n",
      "Epoch 19, Batch 235, Test Loss: 0.39315009117126465\n",
      "Epoch 19, Batch 236, Test Loss: 0.4523211419582367\n",
      "Epoch 19, Batch 237, Test Loss: 0.48142677545547485\n",
      "Epoch 19, Batch 238, Test Loss: 0.3329155445098877\n",
      "Epoch 19, Batch 239, Test Loss: 0.4699167311191559\n",
      "Epoch 19, Batch 240, Test Loss: 0.46965840458869934\n",
      "Epoch 19, Batch 241, Test Loss: 0.6929029226303101\n",
      "Epoch 19, Batch 242, Test Loss: 0.22865994274616241\n",
      "Epoch 19, Batch 243, Test Loss: 0.43700164556503296\n",
      "Epoch 19, Batch 244, Test Loss: 0.4067568778991699\n",
      "Epoch 19, Batch 245, Test Loss: 0.44400039315223694\n",
      "Epoch 19, Batch 246, Test Loss: 0.3066691756248474\n",
      "Epoch 19, Batch 247, Test Loss: 0.4047725796699524\n",
      "Epoch 19, Batch 248, Test Loss: 0.34804344177246094\n",
      "Epoch 19, Batch 249, Test Loss: 0.3604825437068939\n",
      "Epoch 19, Batch 250, Test Loss: 0.48062577843666077\n",
      "Epoch 19, Batch 251, Test Loss: 0.4447883367538452\n",
      "Epoch 19, Batch 252, Test Loss: 0.3597122132778168\n",
      "Epoch 19, Batch 253, Test Loss: 0.3514757752418518\n",
      "Epoch 19, Batch 254, Test Loss: 0.2841655910015106\n",
      "Epoch 19, Batch 255, Test Loss: 0.32510536909103394\n",
      "Epoch 19, Batch 256, Test Loss: 0.34135928750038147\n",
      "Epoch 19, Batch 257, Test Loss: 0.3864368796348572\n",
      "Epoch 19, Batch 258, Test Loss: 0.33513712882995605\n",
      "Epoch 19, Batch 259, Test Loss: 0.6082367300987244\n",
      "Epoch 19, Batch 260, Test Loss: 0.39048728346824646\n",
      "Epoch 19, Batch 261, Test Loss: 0.30707281827926636\n",
      "Epoch 19, Batch 262, Test Loss: 0.31982436776161194\n",
      "Epoch 19, Batch 263, Test Loss: 0.4275459349155426\n",
      "Epoch 19, Batch 264, Test Loss: 0.48066481947898865\n",
      "Epoch 19, Batch 265, Test Loss: 0.3340173363685608\n",
      "Epoch 19, Batch 266, Test Loss: 0.45772141218185425\n",
      "Epoch 19, Batch 267, Test Loss: 0.534256100654602\n",
      "Epoch 19, Batch 268, Test Loss: 0.35784804821014404\n",
      "Epoch 19, Batch 269, Test Loss: 0.3969702422618866\n",
      "Epoch 19, Batch 270, Test Loss: 0.4342850148677826\n",
      "Epoch 19, Batch 271, Test Loss: 0.4845210313796997\n",
      "Epoch 19, Batch 272, Test Loss: 0.41593843698501587\n",
      "Epoch 19, Batch 273, Test Loss: 0.4520120620727539\n",
      "Epoch 19, Batch 274, Test Loss: 0.29527854919433594\n",
      "Epoch 19, Batch 275, Test Loss: 0.4815569221973419\n",
      "Epoch 19, Batch 276, Test Loss: 0.28949370980262756\n",
      "Epoch 19, Batch 277, Test Loss: 0.392604261636734\n",
      "Epoch 19, Batch 278, Test Loss: 0.3290157616138458\n",
      "Epoch 19, Batch 279, Test Loss: 0.4157145917415619\n",
      "Epoch 19, Batch 280, Test Loss: 0.29351678490638733\n",
      "Epoch 19, Batch 281, Test Loss: 0.4174141585826874\n",
      "Epoch 19, Batch 282, Test Loss: 0.3354077935218811\n",
      "Epoch 19, Batch 283, Test Loss: 0.3880070149898529\n",
      "Epoch 19, Batch 284, Test Loss: 0.5224838256835938\n",
      "Epoch 19, Batch 285, Test Loss: 0.43189510703086853\n",
      "Epoch 19, Batch 286, Test Loss: 0.5662076473236084\n",
      "Epoch 19, Batch 287, Test Loss: 0.4277677536010742\n",
      "Epoch 19, Batch 288, Test Loss: 0.37213730812072754\n",
      "Epoch 19, Batch 289, Test Loss: 0.4222968518733978\n",
      "Epoch 19, Batch 290, Test Loss: 0.31986314058303833\n",
      "Epoch 19, Batch 291, Test Loss: 0.3732897937297821\n",
      "Epoch 19, Batch 292, Test Loss: 0.5409899950027466\n",
      "Epoch 19, Batch 293, Test Loss: 0.4082554280757904\n",
      "Epoch 19, Batch 294, Test Loss: 0.4071078300476074\n",
      "Epoch 19, Batch 295, Test Loss: 0.46744394302368164\n",
      "Epoch 19, Batch 296, Test Loss: 0.5138693451881409\n",
      "Epoch 19, Batch 297, Test Loss: 0.25663870573043823\n",
      "Epoch 19, Batch 298, Test Loss: 0.3769535422325134\n",
      "Epoch 19, Batch 299, Test Loss: 0.3319706916809082\n",
      "Epoch 19, Batch 300, Test Loss: 0.3609083294868469\n",
      "Epoch 19, Batch 301, Test Loss: 0.43619415163993835\n",
      "Epoch 19, Batch 302, Test Loss: 0.2247830629348755\n",
      "Epoch 19, Batch 303, Test Loss: 0.5532125234603882\n",
      "Epoch 19, Batch 304, Test Loss: 0.4117526412010193\n",
      "Epoch 19, Batch 305, Test Loss: 0.44525355100631714\n",
      "Epoch 19, Batch 306, Test Loss: 0.39497843384742737\n",
      "Epoch 19, Batch 307, Test Loss: 0.4748968482017517\n",
      "Epoch 19, Batch 308, Test Loss: 0.5431224703788757\n",
      "Epoch 19, Batch 309, Test Loss: 0.3504393696784973\n",
      "Epoch 19, Batch 310, Test Loss: 0.5423330664634705\n",
      "Epoch 19, Batch 311, Test Loss: 0.3512449264526367\n",
      "Epoch 19, Batch 312, Test Loss: 0.3354247808456421\n",
      "Epoch 19, Batch 313, Test Loss: 0.6040877103805542\n",
      "Epoch 19, Batch 314, Test Loss: 0.38659706711769104\n",
      "Epoch 19, Batch 315, Test Loss: 0.446188747882843\n",
      "Epoch 19, Batch 316, Test Loss: 0.36539724469184875\n",
      "Epoch 19, Batch 317, Test Loss: 0.2665088474750519\n",
      "Epoch 19, Batch 318, Test Loss: 0.5239471197128296\n",
      "Epoch 19, Batch 319, Test Loss: 0.2598530650138855\n",
      "Epoch 19, Batch 320, Test Loss: 0.5948859453201294\n",
      "Epoch 19, Batch 321, Test Loss: 0.43253204226493835\n",
      "Epoch 19, Batch 322, Test Loss: 0.2575853765010834\n",
      "Epoch 19, Batch 323, Test Loss: 0.34852999448776245\n",
      "Epoch 19, Batch 324, Test Loss: 0.5353904962539673\n",
      "Epoch 19, Batch 325, Test Loss: 0.4044289290904999\n",
      "Epoch 19, Batch 326, Test Loss: 0.4035200774669647\n",
      "Epoch 19, Batch 327, Test Loss: 0.3235110640525818\n",
      "Epoch 19, Batch 328, Test Loss: 0.3065004348754883\n",
      "Epoch 19, Batch 329, Test Loss: 0.6628195643424988\n",
      "Epoch 19, Batch 330, Test Loss: 0.42363107204437256\n",
      "Epoch 19, Batch 331, Test Loss: 0.5627371072769165\n",
      "Epoch 19, Batch 332, Test Loss: 0.4651227295398712\n",
      "Epoch 19, Batch 333, Test Loss: 0.5889018774032593\n",
      "Epoch 19, Batch 334, Test Loss: 0.36430034041404724\n",
      "Epoch 19, Batch 335, Test Loss: 0.314981609582901\n",
      "Epoch 19, Batch 336, Test Loss: 0.4527687430381775\n",
      "Epoch 19, Batch 337, Test Loss: 0.47266048192977905\n",
      "Epoch 19, Batch 338, Test Loss: 0.32979896664619446\n",
      "Epoch 19, Batch 339, Test Loss: 0.4640157222747803\n",
      "Epoch 19, Batch 340, Test Loss: 0.324116587638855\n",
      "Epoch 19, Batch 341, Test Loss: 0.3374813497066498\n",
      "Epoch 19, Batch 342, Test Loss: 0.486447811126709\n",
      "Epoch 19, Batch 343, Test Loss: 0.3262806832790375\n",
      "Epoch 19, Batch 344, Test Loss: 0.35534727573394775\n",
      "Epoch 19, Batch 345, Test Loss: 0.6161062717437744\n",
      "Epoch 19, Batch 346, Test Loss: 0.3730230927467346\n",
      "Epoch 19, Batch 347, Test Loss: 0.5447188019752502\n",
      "Epoch 19, Batch 348, Test Loss: 0.4409208297729492\n",
      "Epoch 19, Batch 349, Test Loss: 0.4267662763595581\n",
      "Epoch 19, Batch 350, Test Loss: 0.40361884236335754\n",
      "Epoch 19, Batch 351, Test Loss: 0.28601014614105225\n",
      "Epoch 19, Batch 352, Test Loss: 0.4987412095069885\n",
      "Epoch 19, Batch 353, Test Loss: 0.2968834638595581\n",
      "Epoch 19, Batch 354, Test Loss: 0.3483113944530487\n",
      "Epoch 19, Batch 355, Test Loss: 0.36590251326560974\n",
      "Epoch 19, Batch 356, Test Loss: 0.35019630193710327\n",
      "Epoch 19, Batch 357, Test Loss: 0.4140496253967285\n",
      "Epoch 19, Batch 358, Test Loss: 0.24980983138084412\n",
      "Epoch 19, Batch 359, Test Loss: 0.2996872663497925\n",
      "Epoch 19, Batch 360, Test Loss: 0.228907972574234\n",
      "Epoch 19, Batch 361, Test Loss: 0.4556417167186737\n",
      "Epoch 19, Batch 362, Test Loss: 0.3715253472328186\n",
      "Epoch 19, Batch 363, Test Loss: 0.5103408694267273\n",
      "Epoch 19, Batch 364, Test Loss: 0.5418566465377808\n",
      "Epoch 19, Batch 365, Test Loss: 0.3297201693058014\n",
      "Epoch 19, Batch 366, Test Loss: 0.5034273862838745\n",
      "Epoch 19, Batch 367, Test Loss: 0.4726307988166809\n",
      "Epoch 19, Batch 368, Test Loss: 0.4686557352542877\n",
      "Epoch 19, Batch 369, Test Loss: 0.6181743144989014\n",
      "Epoch 19, Batch 370, Test Loss: 0.3519671559333801\n",
      "Epoch 19, Batch 371, Test Loss: 0.445183664560318\n",
      "Epoch 19, Batch 372, Test Loss: 0.44974520802497864\n",
      "Epoch 19, Batch 373, Test Loss: 0.4731988310813904\n",
      "Epoch 19, Batch 374, Test Loss: 0.41383054852485657\n",
      "Epoch 19, Batch 375, Test Loss: 0.3604739308357239\n",
      "Epoch 19, Batch 376, Test Loss: 0.35072287917137146\n",
      "Epoch 19, Batch 377, Test Loss: 0.3273479640483856\n",
      "Epoch 19, Batch 378, Test Loss: 0.5296691656112671\n",
      "Epoch 19, Batch 379, Test Loss: 0.3820401430130005\n",
      "Epoch 19, Batch 380, Test Loss: 0.382664293050766\n",
      "Epoch 19, Batch 381, Test Loss: 0.33457452058792114\n",
      "Epoch 19, Batch 382, Test Loss: 0.3837193250656128\n",
      "Epoch 19, Batch 383, Test Loss: 0.3728622496128082\n",
      "Epoch 19, Batch 384, Test Loss: 0.503298819065094\n",
      "Epoch 19, Batch 385, Test Loss: 0.5080226063728333\n",
      "Epoch 19, Batch 386, Test Loss: 0.28645089268684387\n",
      "Epoch 19, Batch 387, Test Loss: 0.3949016332626343\n",
      "Epoch 19, Batch 388, Test Loss: 0.41236063838005066\n",
      "Epoch 19, Batch 389, Test Loss: 0.3094072937965393\n",
      "Epoch 19, Batch 390, Test Loss: 0.47831806540489197\n",
      "Epoch 19, Batch 391, Test Loss: 0.307387113571167\n",
      "Epoch 19, Batch 392, Test Loss: 0.31400588154792786\n",
      "Epoch 19, Batch 393, Test Loss: 0.4827911853790283\n",
      "Epoch 19, Batch 394, Test Loss: 0.3999664783477783\n",
      "Epoch 19, Batch 395, Test Loss: 0.3267210125923157\n",
      "Epoch 19, Batch 396, Test Loss: 0.23930475115776062\n",
      "Epoch 19, Batch 397, Test Loss: 0.45352548360824585\n",
      "Epoch 19, Batch 398, Test Loss: 0.22993400692939758\n",
      "Epoch 19, Batch 399, Test Loss: 0.40728315711021423\n",
      "Epoch 19, Batch 400, Test Loss: 0.31339508295059204\n",
      "Epoch 19, Batch 401, Test Loss: 0.34885603189468384\n",
      "Epoch 19, Batch 402, Test Loss: 0.2205587774515152\n",
      "Epoch 19, Batch 403, Test Loss: 0.44619277119636536\n",
      "Epoch 19, Batch 404, Test Loss: 0.39913952350616455\n",
      "Epoch 19, Batch 405, Test Loss: 0.3087623417377472\n",
      "Epoch 19, Batch 406, Test Loss: 0.610011637210846\n",
      "Epoch 19, Batch 407, Test Loss: 0.4316123425960541\n",
      "Epoch 19, Batch 408, Test Loss: 0.5392752885818481\n",
      "Epoch 19, Batch 409, Test Loss: 0.384988009929657\n",
      "Epoch 19, Batch 410, Test Loss: 0.2847140431404114\n",
      "Epoch 19, Batch 411, Test Loss: 0.3101901412010193\n",
      "Epoch 19, Batch 412, Test Loss: 0.3006824851036072\n",
      "Epoch 19, Batch 413, Test Loss: 0.4522415101528168\n",
      "Epoch 19, Batch 414, Test Loss: 0.31728076934814453\n",
      "Epoch 19, Batch 415, Test Loss: 0.4870976507663727\n",
      "Epoch 19, Batch 416, Test Loss: 0.5933597087860107\n",
      "Epoch 19, Batch 417, Test Loss: 0.4924298822879791\n",
      "Epoch 19, Batch 418, Test Loss: 0.40964260697364807\n",
      "Epoch 19, Batch 419, Test Loss: 0.4742942452430725\n",
      "Epoch 19, Batch 420, Test Loss: 0.48957374691963196\n",
      "Epoch 19, Batch 421, Test Loss: 0.25786086916923523\n",
      "Epoch 19, Batch 422, Test Loss: 0.4077956974506378\n",
      "Epoch 19, Batch 423, Test Loss: 0.3372076153755188\n",
      "Epoch 19, Batch 424, Test Loss: 0.3570319414138794\n",
      "Epoch 19, Batch 425, Test Loss: 0.5264959335327148\n",
      "Epoch 19, Batch 426, Test Loss: 0.47717753052711487\n",
      "Epoch 19, Batch 427, Test Loss: 0.24909299612045288\n",
      "Epoch 19, Batch 428, Test Loss: 0.41219425201416016\n",
      "Epoch 19, Batch 429, Test Loss: 0.3554264307022095\n",
      "Epoch 19, Batch 430, Test Loss: 0.401082843542099\n",
      "Epoch 19, Batch 431, Test Loss: 0.6098721623420715\n",
      "Epoch 19, Batch 432, Test Loss: 0.3606971204280853\n",
      "Epoch 19, Batch 433, Test Loss: 0.2638230323791504\n",
      "Epoch 19, Batch 434, Test Loss: 0.4919849932193756\n",
      "Epoch 19, Batch 435, Test Loss: 0.29357439279556274\n",
      "Epoch 19, Batch 436, Test Loss: 0.33148252964019775\n",
      "Epoch 19, Batch 437, Test Loss: 0.29151684045791626\n",
      "Epoch 19, Batch 438, Test Loss: 0.5340384840965271\n",
      "Epoch 19, Batch 439, Test Loss: 0.3418320417404175\n",
      "Epoch 19, Batch 440, Test Loss: 0.4957925081253052\n",
      "Epoch 19, Batch 441, Test Loss: 0.35224419832229614\n",
      "Epoch 19, Batch 442, Test Loss: 0.40912482142448425\n",
      "Epoch 19, Batch 443, Test Loss: 0.4107968211174011\n",
      "Epoch 19, Batch 444, Test Loss: 0.3297417163848877\n",
      "Epoch 19, Batch 445, Test Loss: 0.3957459032535553\n",
      "Epoch 19, Batch 446, Test Loss: 0.33507823944091797\n",
      "Epoch 19, Batch 447, Test Loss: 0.46912452578544617\n",
      "Epoch 19, Batch 448, Test Loss: 0.4246225357055664\n",
      "Epoch 19, Batch 449, Test Loss: 0.5803201794624329\n",
      "Epoch 19, Batch 450, Test Loss: 0.4305950105190277\n",
      "Epoch 19, Batch 451, Test Loss: 0.3045276403427124\n",
      "Epoch 19, Batch 452, Test Loss: 0.6105456352233887\n",
      "Epoch 19, Batch 453, Test Loss: 0.32609111070632935\n",
      "Epoch 19, Batch 454, Test Loss: 0.4372956156730652\n",
      "Epoch 19, Batch 455, Test Loss: 0.49935275316238403\n",
      "Epoch 19, Batch 456, Test Loss: 0.4813248813152313\n",
      "Epoch 19, Batch 457, Test Loss: 0.49918678402900696\n",
      "Epoch 19, Batch 458, Test Loss: 0.44318079948425293\n",
      "Epoch 19, Batch 459, Test Loss: 0.27584734559059143\n",
      "Epoch 19, Batch 460, Test Loss: 0.2821124494075775\n",
      "Epoch 19, Batch 461, Test Loss: 0.44192013144493103\n",
      "Epoch 19, Batch 462, Test Loss: 0.5403652191162109\n",
      "Epoch 19, Batch 463, Test Loss: 0.45847588777542114\n",
      "Epoch 19, Batch 464, Test Loss: 0.3559633493423462\n",
      "Epoch 19, Batch 465, Test Loss: 0.2136644721031189\n",
      "Epoch 19, Batch 466, Test Loss: 0.3461382985115051\n",
      "Epoch 19, Batch 467, Test Loss: 0.2489742934703827\n",
      "Epoch 19, Batch 468, Test Loss: 0.35643476247787476\n",
      "Epoch 19, Batch 469, Test Loss: 0.29575854539871216\n",
      "Epoch 19, Batch 470, Test Loss: 0.38846346735954285\n",
      "Epoch 19, Batch 471, Test Loss: 0.48786938190460205\n",
      "Epoch 19, Batch 472, Test Loss: 0.339499831199646\n",
      "Epoch 19, Batch 473, Test Loss: 0.504638135433197\n",
      "Epoch 19, Batch 474, Test Loss: 0.46018174290657043\n",
      "Epoch 19, Batch 475, Test Loss: 0.45556890964508057\n",
      "Epoch 19, Batch 476, Test Loss: 0.5013566017150879\n",
      "Epoch 19, Batch 477, Test Loss: 0.37063735723495483\n",
      "Epoch 19, Batch 478, Test Loss: 0.5932708382606506\n",
      "Epoch 19, Batch 479, Test Loss: 0.4576929807662964\n",
      "Epoch 19, Batch 480, Test Loss: 0.43770384788513184\n",
      "Epoch 19, Batch 481, Test Loss: 0.48312342166900635\n",
      "Epoch 19, Batch 482, Test Loss: 0.27710962295532227\n",
      "Epoch 19, Batch 483, Test Loss: 0.3360934853553772\n",
      "Epoch 19, Batch 484, Test Loss: 0.4643537700176239\n",
      "Epoch 19, Batch 485, Test Loss: 0.38965171575546265\n",
      "Epoch 19, Batch 486, Test Loss: 0.37024182081222534\n",
      "Epoch 19, Batch 487, Test Loss: 0.45005834102630615\n",
      "Epoch 19, Batch 488, Test Loss: 0.390755832195282\n",
      "Epoch 19, Batch 489, Test Loss: 0.4578932821750641\n",
      "Epoch 19, Batch 490, Test Loss: 0.48015323281288147\n",
      "Epoch 19, Batch 491, Test Loss: 0.3644273281097412\n",
      "Epoch 19, Batch 492, Test Loss: 0.5262656211853027\n",
      "Epoch 19, Batch 493, Test Loss: 0.4945636987686157\n",
      "Epoch 19, Batch 494, Test Loss: 0.31785035133361816\n",
      "Epoch 19, Batch 495, Test Loss: 0.5401298403739929\n",
      "Epoch 19, Batch 496, Test Loss: 0.4266207218170166\n",
      "Epoch 19, Batch 497, Test Loss: 0.47600796818733215\n",
      "Epoch 19, Batch 498, Test Loss: 0.4987068772315979\n",
      "Epoch 19, Batch 499, Test Loss: 0.22804245352745056\n",
      "Epoch 19, Batch 500, Test Loss: 0.45040053129196167\n",
      "Epoch 19, Batch 501, Test Loss: 0.7018817067146301\n",
      "Epoch 19, Batch 502, Test Loss: 0.4284425377845764\n",
      "Epoch 19, Batch 503, Test Loss: 0.4099901020526886\n",
      "Epoch 19, Batch 504, Test Loss: 0.41794583201408386\n",
      "Epoch 19, Batch 505, Test Loss: 0.39465945959091187\n",
      "Epoch 19, Batch 506, Test Loss: 0.3315853178501129\n",
      "Epoch 19, Batch 507, Test Loss: 0.37272438406944275\n",
      "Epoch 19, Batch 508, Test Loss: 0.6035104990005493\n",
      "Epoch 19, Batch 509, Test Loss: 0.38343343138694763\n",
      "Epoch 19, Batch 510, Test Loss: 0.34836551547050476\n",
      "Epoch 19, Batch 511, Test Loss: 0.33180883526802063\n",
      "Epoch 19, Batch 512, Test Loss: 0.32491108775138855\n",
      "Epoch 19, Batch 513, Test Loss: 0.31692543625831604\n",
      "Epoch 19, Batch 514, Test Loss: 0.30590325593948364\n",
      "Epoch 19, Batch 515, Test Loss: 0.5939821600914001\n",
      "Epoch 19, Batch 516, Test Loss: 0.3767743706703186\n",
      "Epoch 19, Batch 517, Test Loss: 0.6065832376480103\n",
      "Epoch 19, Batch 518, Test Loss: 0.5799870491027832\n",
      "Epoch 19, Batch 519, Test Loss: 0.4885377287864685\n",
      "Epoch 19, Batch 520, Test Loss: 0.5791260600090027\n",
      "Epoch 19, Batch 521, Test Loss: 0.4076566994190216\n",
      "Epoch 19, Batch 522, Test Loss: 0.6040180921554565\n",
      "Epoch 19, Batch 523, Test Loss: 0.3849353492259979\n",
      "Epoch 19, Batch 524, Test Loss: 0.6168931126594543\n",
      "Epoch 19, Batch 525, Test Loss: 0.2222270518541336\n",
      "Epoch 19, Batch 526, Test Loss: 0.22035002708435059\n",
      "Epoch 19, Batch 527, Test Loss: 0.58066326379776\n",
      "Epoch 19, Batch 528, Test Loss: 0.5020949840545654\n",
      "Epoch 19, Batch 529, Test Loss: 0.27306732535362244\n",
      "Epoch 19, Batch 530, Test Loss: 0.6462992429733276\n",
      "Epoch 19, Batch 531, Test Loss: 0.4693678915500641\n",
      "Epoch 19, Batch 532, Test Loss: 0.3503631055355072\n",
      "Epoch 19, Batch 533, Test Loss: 0.4804762303829193\n",
      "Epoch 19, Batch 534, Test Loss: 0.4364946484565735\n",
      "Epoch 19, Batch 535, Test Loss: 0.38533005118370056\n",
      "Epoch 19, Batch 536, Test Loss: 0.2312457263469696\n",
      "Epoch 19, Batch 537, Test Loss: 0.5326311588287354\n",
      "Epoch 19, Batch 538, Test Loss: 0.5579186081886292\n",
      "Epoch 19, Batch 539, Test Loss: 0.3112039864063263\n",
      "Epoch 19, Batch 540, Test Loss: 0.5038766860961914\n",
      "Epoch 19, Batch 541, Test Loss: 0.4894714057445526\n",
      "Epoch 19, Batch 542, Test Loss: 0.6570618748664856\n",
      "Epoch 19, Batch 543, Test Loss: 0.45894089341163635\n",
      "Epoch 19, Batch 544, Test Loss: 0.1900041550397873\n",
      "Epoch 19, Batch 545, Test Loss: 0.4208492636680603\n",
      "Epoch 19, Batch 546, Test Loss: 0.3418426513671875\n",
      "Epoch 19, Batch 547, Test Loss: 0.5497825145721436\n",
      "Epoch 19, Batch 548, Test Loss: 0.4668008089065552\n",
      "Epoch 19, Batch 549, Test Loss: 0.5171741843223572\n",
      "Epoch 19, Batch 550, Test Loss: 0.3872290849685669\n",
      "Epoch 19, Batch 551, Test Loss: 0.4155060052871704\n",
      "Epoch 19, Batch 552, Test Loss: 0.36448919773101807\n",
      "Epoch 19, Batch 553, Test Loss: 0.387649804353714\n",
      "Epoch 19, Batch 554, Test Loss: 0.5538107752799988\n",
      "Epoch 19, Batch 555, Test Loss: 0.6379197835922241\n",
      "Epoch 19, Batch 556, Test Loss: 0.6029162406921387\n",
      "Epoch 19, Batch 557, Test Loss: 0.38444066047668457\n",
      "Epoch 19, Batch 558, Test Loss: 0.3978874683380127\n",
      "Epoch 19, Batch 559, Test Loss: 0.41597625613212585\n",
      "Epoch 19, Batch 560, Test Loss: 0.3855549395084381\n",
      "Epoch 19, Batch 561, Test Loss: 0.2645033895969391\n",
      "Epoch 19, Batch 562, Test Loss: 0.4279249310493469\n",
      "Epoch 19, Batch 563, Test Loss: 0.42423295974731445\n",
      "Epoch 19, Batch 564, Test Loss: 0.32155972719192505\n",
      "Epoch 19, Batch 565, Test Loss: 0.35574355721473694\n",
      "Epoch 19, Batch 566, Test Loss: 0.36704689264297485\n",
      "Epoch 19, Batch 567, Test Loss: 0.48138943314552307\n",
      "Epoch 19, Batch 568, Test Loss: 0.3605019450187683\n",
      "Epoch 19, Batch 569, Test Loss: 0.4548681378364563\n",
      "Epoch 19, Batch 570, Test Loss: 0.3896499276161194\n",
      "Epoch 19, Batch 571, Test Loss: 0.45533835887908936\n",
      "Epoch 19, Batch 572, Test Loss: 0.3504227101802826\n",
      "Epoch 19, Batch 573, Test Loss: 0.24044620990753174\n",
      "Epoch 19, Batch 574, Test Loss: 0.2679102122783661\n",
      "Epoch 19, Batch 575, Test Loss: 0.5727042555809021\n",
      "Epoch 19, Batch 576, Test Loss: 0.4454268217086792\n",
      "Epoch 19, Batch 577, Test Loss: 0.42453569173812866\n",
      "Epoch 19, Batch 578, Test Loss: 0.3794732093811035\n",
      "Epoch 19, Batch 579, Test Loss: 0.3909607529640198\n",
      "Epoch 19, Batch 580, Test Loss: 0.36731061339378357\n",
      "Epoch 19, Batch 581, Test Loss: 0.4311245083808899\n",
      "Epoch 19, Batch 582, Test Loss: 0.5750970244407654\n",
      "Epoch 19, Batch 583, Test Loss: 0.43070530891418457\n",
      "Epoch 19, Batch 584, Test Loss: 0.3708118200302124\n",
      "Epoch 19, Batch 585, Test Loss: 0.3787935674190521\n",
      "Epoch 19, Batch 586, Test Loss: 0.4803962707519531\n",
      "Epoch 19, Batch 587, Test Loss: 0.4148927927017212\n",
      "Epoch 19, Batch 588, Test Loss: 0.4112932085990906\n",
      "Epoch 19, Batch 589, Test Loss: 0.3155202865600586\n",
      "Epoch 19, Batch 590, Test Loss: 0.3516496419906616\n",
      "Epoch 19, Batch 591, Test Loss: 0.5781832933425903\n",
      "Epoch 19, Batch 592, Test Loss: 0.355937659740448\n",
      "Epoch 19, Batch 593, Test Loss: 0.4818376898765564\n",
      "Epoch 19, Batch 594, Test Loss: 0.46074044704437256\n",
      "Epoch 19, Batch 595, Test Loss: 0.4005065858364105\n",
      "Epoch 19, Batch 596, Test Loss: 0.37398454546928406\n",
      "Epoch 19, Batch 597, Test Loss: 0.4253273606300354\n",
      "Epoch 19, Batch 598, Test Loss: 0.45630159974098206\n",
      "Epoch 19, Batch 599, Test Loss: 0.5378766655921936\n",
      "Epoch 19, Batch 600, Test Loss: 0.3835758566856384\n",
      "Epoch 19, Batch 601, Test Loss: 0.44081398844718933\n",
      "Epoch 19, Batch 602, Test Loss: 0.33964109420776367\n",
      "Epoch 19, Batch 603, Test Loss: 0.4539259374141693\n",
      "Epoch 19, Batch 604, Test Loss: 0.382858544588089\n",
      "Epoch 19, Batch 605, Test Loss: 0.43617352843284607\n",
      "Epoch 19, Batch 606, Test Loss: 0.3273701071739197\n",
      "Epoch 19, Batch 607, Test Loss: 0.22726765275001526\n",
      "Epoch 19, Batch 608, Test Loss: 0.41089048981666565\n",
      "Epoch 19, Batch 609, Test Loss: 0.7446638941764832\n",
      "Epoch 19, Batch 610, Test Loss: 0.34712857007980347\n",
      "Epoch 19, Batch 611, Test Loss: 0.35779523849487305\n",
      "Epoch 19, Batch 612, Test Loss: 0.37219175696372986\n",
      "Epoch 19, Batch 613, Test Loss: 0.5899456739425659\n",
      "Epoch 19, Batch 614, Test Loss: 0.5820896625518799\n",
      "Epoch 19, Batch 615, Test Loss: 0.6448360681533813\n",
      "Epoch 19, Batch 616, Test Loss: 0.38571855425834656\n",
      "Epoch 19, Batch 617, Test Loss: 0.3855289816856384\n",
      "Epoch 19, Batch 618, Test Loss: 0.24751171469688416\n",
      "Epoch 19, Batch 619, Test Loss: 0.4076399803161621\n",
      "Epoch 19, Batch 620, Test Loss: 0.5073704719543457\n",
      "Epoch 19, Batch 621, Test Loss: 0.3526000380516052\n",
      "Epoch 19, Batch 622, Test Loss: 0.2660147547721863\n",
      "Epoch 19, Batch 623, Test Loss: 0.2550047039985657\n",
      "Epoch 19, Batch 624, Test Loss: 0.502819299697876\n",
      "Epoch 19, Batch 625, Test Loss: 0.3998965919017792\n",
      "Epoch 19, Batch 626, Test Loss: 0.4812561869621277\n",
      "Epoch 19, Batch 627, Test Loss: 0.37529832124710083\n",
      "Epoch 19, Batch 628, Test Loss: 0.3782571256160736\n",
      "Epoch 19, Batch 629, Test Loss: 0.34296271204948425\n",
      "Epoch 19, Batch 630, Test Loss: 0.3949793577194214\n",
      "Epoch 19, Batch 631, Test Loss: 0.45987468957901\n",
      "Epoch 19, Batch 632, Test Loss: 0.6287990212440491\n",
      "Epoch 19, Batch 633, Test Loss: 0.4887869358062744\n",
      "Epoch 19, Batch 634, Test Loss: 0.3844510614871979\n",
      "Epoch 19, Batch 635, Test Loss: 0.6573841571807861\n",
      "Epoch 19, Batch 636, Test Loss: 0.3886836767196655\n",
      "Epoch 19, Batch 637, Test Loss: 0.3901444971561432\n",
      "Epoch 19, Batch 638, Test Loss: 0.476828932762146\n",
      "Epoch 19, Batch 639, Test Loss: 0.36920619010925293\n",
      "Epoch 19, Batch 640, Test Loss: 0.19787564873695374\n",
      "Epoch 19, Batch 641, Test Loss: 0.3898341953754425\n",
      "Epoch 19, Batch 642, Test Loss: 0.4506397843360901\n",
      "Epoch 19, Batch 643, Test Loss: 0.42569488286972046\n",
      "Epoch 19, Batch 644, Test Loss: 0.4155130982398987\n",
      "Epoch 19, Batch 645, Test Loss: 0.5238297581672668\n",
      "Epoch 19, Batch 646, Test Loss: 0.39821749925613403\n",
      "Epoch 19, Batch 647, Test Loss: 0.4237671196460724\n",
      "Epoch 19, Batch 648, Test Loss: 0.4752030670642853\n",
      "Epoch 19, Batch 649, Test Loss: 0.20519012212753296\n",
      "Epoch 19, Batch 650, Test Loss: 0.32098203897476196\n",
      "Epoch 19, Batch 651, Test Loss: 0.5379132628440857\n",
      "Epoch 19, Batch 652, Test Loss: 0.5137813687324524\n",
      "Epoch 19, Batch 653, Test Loss: 0.3869878947734833\n",
      "Epoch 19, Batch 654, Test Loss: 0.44448545575141907\n",
      "Epoch 19, Batch 655, Test Loss: 0.38048210740089417\n",
      "Epoch 19, Batch 656, Test Loss: 0.6880868077278137\n",
      "Epoch 19, Batch 657, Test Loss: 0.4807555675506592\n",
      "Epoch 19, Batch 658, Test Loss: 0.4246242046356201\n",
      "Epoch 19, Batch 659, Test Loss: 0.5245409607887268\n",
      "Epoch 19, Batch 660, Test Loss: 0.46168363094329834\n",
      "Epoch 19, Batch 661, Test Loss: 0.31099382042884827\n",
      "Epoch 19, Batch 662, Test Loss: 0.4513276219367981\n",
      "Epoch 19, Batch 663, Test Loss: 0.20454224944114685\n",
      "Epoch 19, Batch 664, Test Loss: 0.5203128457069397\n",
      "Epoch 19, Batch 665, Test Loss: 0.4025152921676636\n",
      "Epoch 19, Batch 666, Test Loss: 0.4027153253555298\n",
      "Epoch 19, Batch 667, Test Loss: 0.3489623963832855\n",
      "Epoch 19, Batch 668, Test Loss: 0.4280241131782532\n",
      "Epoch 19, Batch 669, Test Loss: 0.4455890357494354\n",
      "Epoch 19, Batch 670, Test Loss: 0.34026437997817993\n",
      "Epoch 19, Batch 671, Test Loss: 0.31442007422447205\n",
      "Epoch 19, Batch 672, Test Loss: 0.4245935380458832\n",
      "Epoch 19, Batch 673, Test Loss: 0.8184725642204285\n",
      "Epoch 19, Batch 674, Test Loss: 0.18284010887145996\n",
      "Epoch 19, Batch 675, Test Loss: 0.423204243183136\n",
      "Epoch 19, Batch 676, Test Loss: 0.32680898904800415\n",
      "Epoch 19, Batch 677, Test Loss: 0.31994637846946716\n",
      "Epoch 19, Batch 678, Test Loss: 0.33260297775268555\n",
      "Epoch 19, Batch 679, Test Loss: 0.34619855880737305\n",
      "Epoch 19, Batch 680, Test Loss: 0.5003290176391602\n",
      "Epoch 19, Batch 681, Test Loss: 0.3967238664627075\n",
      "Epoch 19, Batch 682, Test Loss: 0.47654759883880615\n",
      "Epoch 19, Batch 683, Test Loss: 0.58100426197052\n",
      "Epoch 19, Batch 684, Test Loss: 0.45283958315849304\n",
      "Epoch 19, Batch 685, Test Loss: 0.35978075861930847\n",
      "Epoch 19, Batch 686, Test Loss: 0.36605918407440186\n",
      "Epoch 19, Batch 687, Test Loss: 0.5823754072189331\n",
      "Epoch 19, Batch 688, Test Loss: 0.22459957003593445\n",
      "Epoch 19, Batch 689, Test Loss: 0.5746227502822876\n",
      "Epoch 19, Batch 690, Test Loss: 0.4056646227836609\n",
      "Epoch 19, Batch 691, Test Loss: 0.32482579350471497\n",
      "Epoch 19, Batch 692, Test Loss: 0.2287352830171585\n",
      "Epoch 19, Batch 693, Test Loss: 0.4315938651561737\n",
      "Epoch 19, Batch 694, Test Loss: 0.2599964439868927\n",
      "Epoch 19, Batch 695, Test Loss: 0.4090711176395416\n",
      "Epoch 19, Batch 696, Test Loss: 0.35393601655960083\n",
      "Epoch 19, Batch 697, Test Loss: 0.4147486388683319\n",
      "Epoch 19, Batch 698, Test Loss: 0.39319297671318054\n",
      "Epoch 19, Batch 699, Test Loss: 0.38838115334510803\n",
      "Epoch 19, Batch 700, Test Loss: 0.5019490718841553\n",
      "Epoch 19, Batch 701, Test Loss: 0.4630221128463745\n",
      "Epoch 19, Batch 702, Test Loss: 0.5765747427940369\n",
      "Epoch 19, Batch 703, Test Loss: 0.5365965962409973\n",
      "Epoch 19, Batch 704, Test Loss: 0.6619341969490051\n",
      "Epoch 19, Batch 705, Test Loss: 0.39607709646224976\n",
      "Epoch 19, Batch 706, Test Loss: 0.4188767075538635\n",
      "Epoch 19, Batch 707, Test Loss: 0.3158141076564789\n",
      "Epoch 19, Batch 708, Test Loss: 0.4810503423213959\n",
      "Epoch 19, Batch 709, Test Loss: 0.2637804448604584\n",
      "Epoch 19, Batch 710, Test Loss: 0.3603311777114868\n",
      "Epoch 19, Batch 711, Test Loss: 0.5310134291648865\n",
      "Epoch 19, Batch 712, Test Loss: 0.47110530734062195\n",
      "Epoch 19, Batch 713, Test Loss: 0.5178439617156982\n",
      "Epoch 19, Batch 714, Test Loss: 0.6026398539543152\n",
      "Epoch 19, Batch 715, Test Loss: 0.41707608103752136\n",
      "Epoch 19, Batch 716, Test Loss: 0.24831970036029816\n",
      "Epoch 19, Batch 717, Test Loss: 0.6021819114685059\n",
      "Epoch 19, Batch 718, Test Loss: 0.4813927114009857\n",
      "Epoch 19, Batch 719, Test Loss: 0.30844977498054504\n",
      "Epoch 19, Batch 720, Test Loss: 0.3295135498046875\n",
      "Epoch 19, Batch 721, Test Loss: 0.3463499844074249\n",
      "Epoch 19, Batch 722, Test Loss: 0.5229238271713257\n",
      "Epoch 19, Batch 723, Test Loss: 0.36727702617645264\n",
      "Epoch 19, Batch 724, Test Loss: 0.4278199374675751\n",
      "Epoch 19, Batch 725, Test Loss: 0.3529374301433563\n",
      "Epoch 19, Batch 726, Test Loss: 0.5478560328483582\n",
      "Epoch 19, Batch 727, Test Loss: 0.47020697593688965\n",
      "Epoch 19, Batch 728, Test Loss: 0.2591191530227661\n",
      "Epoch 19, Batch 729, Test Loss: 0.3652377128601074\n",
      "Epoch 19, Batch 730, Test Loss: 0.4255869686603546\n",
      "Epoch 19, Batch 731, Test Loss: 0.3556632399559021\n",
      "Epoch 19, Batch 732, Test Loss: 0.3002592921257019\n",
      "Epoch 19, Batch 733, Test Loss: 0.3662627935409546\n",
      "Epoch 19, Batch 734, Test Loss: 0.44359180331230164\n",
      "Epoch 19, Batch 735, Test Loss: 0.2997366189956665\n",
      "Epoch 19, Batch 736, Test Loss: 0.41195935010910034\n",
      "Epoch 19, Batch 737, Test Loss: 0.4989316463470459\n",
      "Epoch 19, Batch 738, Test Loss: 0.3695881962776184\n",
      "Epoch 19, Batch 739, Test Loss: 0.32706931233406067\n",
      "Epoch 19, Batch 740, Test Loss: 0.40582647919654846\n",
      "Epoch 19, Batch 741, Test Loss: 0.4317745268344879\n",
      "Epoch 19, Batch 742, Test Loss: 0.2818886935710907\n",
      "Epoch 19, Batch 743, Test Loss: 0.4315887987613678\n",
      "Epoch 19, Batch 744, Test Loss: 0.38014405965805054\n",
      "Epoch 19, Batch 745, Test Loss: 0.4329756498336792\n",
      "Epoch 19, Batch 746, Test Loss: 0.4307647943496704\n",
      "Epoch 19, Batch 747, Test Loss: 0.46458330750465393\n",
      "Epoch 19, Batch 748, Test Loss: 0.32862091064453125\n",
      "Epoch 19, Batch 749, Test Loss: 0.4790016710758209\n",
      "Epoch 19, Batch 750, Test Loss: 0.4666854739189148\n",
      "Epoch 19, Batch 751, Test Loss: 0.7658137083053589\n",
      "Epoch 19, Batch 752, Test Loss: 0.4901251792907715\n",
      "Epoch 19, Batch 753, Test Loss: 0.40712401270866394\n",
      "Epoch 19, Batch 754, Test Loss: 0.39968839287757874\n",
      "Epoch 19, Batch 755, Test Loss: 0.5230898857116699\n",
      "Epoch 19, Batch 756, Test Loss: 0.5943921804428101\n",
      "Epoch 19, Batch 757, Test Loss: 0.5638021230697632\n",
      "Epoch 19, Batch 758, Test Loss: 0.5620900988578796\n",
      "Epoch 19, Batch 759, Test Loss: 0.44108808040618896\n",
      "Epoch 19, Batch 760, Test Loss: 0.47198358178138733\n",
      "Epoch 19, Batch 761, Test Loss: 0.2937092185020447\n",
      "Epoch 19, Batch 762, Test Loss: 0.3362204432487488\n",
      "Epoch 19, Batch 763, Test Loss: 0.4115505516529083\n",
      "Epoch 19, Batch 764, Test Loss: 0.48768186569213867\n",
      "Epoch 19, Batch 765, Test Loss: 0.4118988513946533\n",
      "Epoch 19, Batch 766, Test Loss: 0.25388675928115845\n",
      "Epoch 19, Batch 767, Test Loss: 0.533366322517395\n",
      "Epoch 19, Batch 768, Test Loss: 0.4171483814716339\n",
      "Epoch 19, Batch 769, Test Loss: 0.35108956694602966\n",
      "Epoch 19, Batch 770, Test Loss: 0.34395039081573486\n",
      "Epoch 19, Batch 771, Test Loss: 0.39595434069633484\n",
      "Epoch 19, Batch 772, Test Loss: 0.3729374408721924\n",
      "Epoch 19, Batch 773, Test Loss: 0.29092684388160706\n",
      "Epoch 19, Batch 774, Test Loss: 0.4891807436943054\n",
      "Epoch 19, Batch 775, Test Loss: 0.5643424987792969\n",
      "Epoch 19, Batch 776, Test Loss: 0.3593360483646393\n",
      "Epoch 19, Batch 777, Test Loss: 0.33589184284210205\n",
      "Epoch 19, Batch 778, Test Loss: 0.321433424949646\n",
      "Epoch 19, Batch 779, Test Loss: 0.5343096256256104\n",
      "Epoch 19, Batch 780, Test Loss: 0.3439425230026245\n",
      "Epoch 19, Batch 781, Test Loss: 0.36761796474456787\n",
      "Epoch 19, Batch 782, Test Loss: 0.42989468574523926\n",
      "Epoch 19, Batch 783, Test Loss: 0.39272090792655945\n",
      "Epoch 19, Batch 784, Test Loss: 0.3642902374267578\n",
      "Epoch 19, Batch 785, Test Loss: 0.5271967053413391\n",
      "Epoch 19, Batch 786, Test Loss: 0.5203108787536621\n",
      "Epoch 19, Batch 787, Test Loss: 0.5513566136360168\n",
      "Epoch 19, Batch 788, Test Loss: 0.28618499636650085\n",
      "Epoch 19, Batch 789, Test Loss: 0.2949253022670746\n",
      "Epoch 19, Batch 790, Test Loss: 0.2362457662820816\n",
      "Epoch 19, Batch 791, Test Loss: 0.5306300520896912\n",
      "Epoch 19, Batch 792, Test Loss: 0.32827264070510864\n",
      "Epoch 19, Batch 793, Test Loss: 0.5085111260414124\n",
      "Epoch 19, Batch 794, Test Loss: 0.3270077407360077\n",
      "Epoch 19, Batch 795, Test Loss: 0.506436824798584\n",
      "Epoch 19, Batch 796, Test Loss: 0.5291792750358582\n",
      "Epoch 19, Batch 797, Test Loss: 0.3634238541126251\n",
      "Epoch 19, Batch 798, Test Loss: 0.2727088928222656\n",
      "Epoch 19, Batch 799, Test Loss: 0.5154265761375427\n",
      "Epoch 19, Batch 800, Test Loss: 0.5785293579101562\n",
      "Epoch 19, Batch 801, Test Loss: 0.6386172771453857\n",
      "Epoch 19, Batch 802, Test Loss: 0.28141576051712036\n",
      "Epoch 19, Batch 803, Test Loss: 0.4157900810241699\n",
      "Epoch 19, Batch 804, Test Loss: 0.5560888648033142\n",
      "Epoch 19, Batch 805, Test Loss: 0.3958745002746582\n",
      "Epoch 19, Batch 806, Test Loss: 0.41334617137908936\n",
      "Epoch 19, Batch 807, Test Loss: 0.3214566111564636\n",
      "Epoch 19, Batch 808, Test Loss: 0.5042770504951477\n",
      "Epoch 19, Batch 809, Test Loss: 0.5225434303283691\n",
      "Epoch 19, Batch 810, Test Loss: 0.36827629804611206\n",
      "Epoch 19, Batch 811, Test Loss: 0.5209333896636963\n",
      "Epoch 19, Batch 812, Test Loss: 0.4994925558567047\n",
      "Epoch 19, Batch 813, Test Loss: 0.3239819407463074\n",
      "Epoch 19, Batch 814, Test Loss: 0.44653597474098206\n",
      "Epoch 19, Batch 815, Test Loss: 0.381499707698822\n",
      "Epoch 19, Batch 816, Test Loss: 0.3950253129005432\n",
      "Epoch 19, Batch 817, Test Loss: 0.2062445729970932\n",
      "Epoch 19, Batch 818, Test Loss: 0.2696971595287323\n",
      "Epoch 19, Batch 819, Test Loss: 0.3991576135158539\n",
      "Epoch 19, Batch 820, Test Loss: 0.4450908899307251\n",
      "Epoch 19, Batch 821, Test Loss: 0.27741119265556335\n",
      "Epoch 19, Batch 822, Test Loss: 0.4532518684864044\n",
      "Epoch 19, Batch 823, Test Loss: 0.40486156940460205\n",
      "Epoch 19, Batch 824, Test Loss: 0.5000050663948059\n",
      "Epoch 19, Batch 825, Test Loss: 0.49306267499923706\n",
      "Epoch 19, Batch 826, Test Loss: 0.6687125563621521\n",
      "Epoch 19, Batch 827, Test Loss: 0.47515448927879333\n",
      "Epoch 19, Batch 828, Test Loss: 0.5910548567771912\n",
      "Epoch 19, Batch 829, Test Loss: 0.3556680381298065\n",
      "Epoch 19, Batch 830, Test Loss: 0.5467008948326111\n",
      "Epoch 19, Batch 831, Test Loss: 0.4982553720474243\n",
      "Epoch 19, Batch 832, Test Loss: 0.4997089207172394\n",
      "Epoch 19, Batch 833, Test Loss: 0.6650403738021851\n",
      "Epoch 19, Batch 834, Test Loss: 0.4794691503047943\n",
      "Epoch 19, Batch 835, Test Loss: 0.31802669167518616\n",
      "Epoch 19, Batch 836, Test Loss: 0.3202700614929199\n",
      "Epoch 19, Batch 837, Test Loss: 0.41562482714653015\n",
      "Epoch 19, Batch 838, Test Loss: 0.3568500578403473\n",
      "Epoch 19, Batch 839, Test Loss: 0.5938434600830078\n",
      "Epoch 19, Batch 840, Test Loss: 0.4217067062854767\n",
      "Epoch 19, Batch 841, Test Loss: 0.36099302768707275\n",
      "Epoch 19, Batch 842, Test Loss: 0.5238166451454163\n",
      "Epoch 19, Batch 843, Test Loss: 0.4930279850959778\n",
      "Epoch 19, Batch 844, Test Loss: 0.5570279955863953\n",
      "Epoch 19, Batch 845, Test Loss: 0.35770878195762634\n",
      "Epoch 19, Batch 846, Test Loss: 0.4828665256500244\n",
      "Epoch 19, Batch 847, Test Loss: 0.31186503171920776\n",
      "Epoch 19, Batch 848, Test Loss: 0.5036479234695435\n",
      "Epoch 19, Batch 849, Test Loss: 0.3063402473926544\n",
      "Epoch 19, Batch 850, Test Loss: 0.5129410028457642\n",
      "Epoch 19, Batch 851, Test Loss: 0.42650097608566284\n",
      "Epoch 19, Batch 852, Test Loss: 0.46890905499458313\n",
      "Epoch 19, Batch 853, Test Loss: 0.2545718252658844\n",
      "Epoch 19, Batch 854, Test Loss: 0.3840654790401459\n",
      "Epoch 19, Batch 855, Test Loss: 0.38486671447753906\n",
      "Epoch 19, Batch 856, Test Loss: 0.27409133315086365\n",
      "Epoch 19, Batch 857, Test Loss: 0.4854716658592224\n",
      "Epoch 19, Batch 858, Test Loss: 0.6699737310409546\n",
      "Epoch 19, Batch 859, Test Loss: 0.3880380392074585\n",
      "Epoch 19, Batch 860, Test Loss: 0.2855461835861206\n",
      "Epoch 19, Batch 861, Test Loss: 0.610197126865387\n",
      "Epoch 19, Batch 862, Test Loss: 0.40457046031951904\n",
      "Epoch 19, Batch 863, Test Loss: 0.3589034080505371\n",
      "Epoch 19, Batch 864, Test Loss: 0.5991398096084595\n",
      "Epoch 19, Batch 865, Test Loss: 0.442962646484375\n",
      "Epoch 19, Batch 866, Test Loss: 0.2557213306427002\n",
      "Epoch 19, Batch 867, Test Loss: 0.4131631553173065\n",
      "Epoch 19, Batch 868, Test Loss: 0.27720123529434204\n",
      "Epoch 19, Batch 869, Test Loss: 0.31979963183403015\n",
      "Epoch 19, Batch 870, Test Loss: 0.4630809724330902\n",
      "Epoch 19, Batch 871, Test Loss: 0.41573238372802734\n",
      "Epoch 19, Batch 872, Test Loss: 0.30691295862197876\n",
      "Epoch 19, Batch 873, Test Loss: 0.5264304876327515\n",
      "Epoch 19, Batch 874, Test Loss: 0.34746450185775757\n",
      "Epoch 19, Batch 875, Test Loss: 0.39057591557502747\n",
      "Epoch 19, Batch 876, Test Loss: 0.299801766872406\n",
      "Epoch 19, Batch 877, Test Loss: 0.3763406574726105\n",
      "Epoch 19, Batch 878, Test Loss: 0.38315051794052124\n",
      "Epoch 19, Batch 879, Test Loss: 0.4014928936958313\n",
      "Epoch 19, Batch 880, Test Loss: 0.4580395221710205\n",
      "Epoch 19, Batch 881, Test Loss: 0.32860445976257324\n",
      "Epoch 19, Batch 882, Test Loss: 0.39783430099487305\n",
      "Epoch 19, Batch 883, Test Loss: 0.4761597216129303\n",
      "Epoch 19, Batch 884, Test Loss: 0.5242288112640381\n",
      "Epoch 19, Batch 885, Test Loss: 0.345760315656662\n",
      "Epoch 19, Batch 886, Test Loss: 0.22698307037353516\n",
      "Epoch 19, Batch 887, Test Loss: 0.39165836572647095\n",
      "Epoch 19, Batch 888, Test Loss: 0.6572932004928589\n",
      "Epoch 19, Batch 889, Test Loss: 0.23672345280647278\n",
      "Epoch 19, Batch 890, Test Loss: 0.5334059596061707\n",
      "Epoch 19, Batch 891, Test Loss: 0.46423909068107605\n",
      "Epoch 19, Batch 892, Test Loss: 0.433693528175354\n",
      "Epoch 19, Batch 893, Test Loss: 0.2407534420490265\n",
      "Epoch 19, Batch 894, Test Loss: 0.30837583541870117\n",
      "Epoch 19, Batch 895, Test Loss: 0.3807838261127472\n",
      "Epoch 19, Batch 896, Test Loss: 0.4284113645553589\n",
      "Epoch 19, Batch 897, Test Loss: 0.29880213737487793\n",
      "Epoch 19, Batch 898, Test Loss: 0.4676606357097626\n",
      "Epoch 19, Batch 899, Test Loss: 0.5617741942405701\n",
      "Epoch 19, Batch 900, Test Loss: 0.37926357984542847\n",
      "Epoch 19, Batch 901, Test Loss: 0.43806374073028564\n",
      "Epoch 19, Batch 902, Test Loss: 0.5199159383773804\n",
      "Epoch 19, Batch 903, Test Loss: 0.2627914547920227\n",
      "Epoch 19, Batch 904, Test Loss: 0.5245750546455383\n",
      "Epoch 19, Batch 905, Test Loss: 0.3731439709663391\n",
      "Epoch 19, Batch 906, Test Loss: 0.4161716103553772\n",
      "Epoch 19, Batch 907, Test Loss: 0.6774062514305115\n",
      "Epoch 19, Batch 908, Test Loss: 0.2824718952178955\n",
      "Epoch 19, Batch 909, Test Loss: 0.44118979573249817\n",
      "Epoch 19, Batch 910, Test Loss: 0.393535852432251\n",
      "Epoch 19, Batch 911, Test Loss: 0.32417646050453186\n",
      "Epoch 19, Batch 912, Test Loss: 0.6583081483840942\n",
      "Epoch 19, Batch 913, Test Loss: 0.47121119499206543\n",
      "Epoch 19, Batch 914, Test Loss: 0.4552741050720215\n",
      "Epoch 19, Batch 915, Test Loss: 0.625400722026825\n",
      "Epoch 19, Batch 916, Test Loss: 0.341602623462677\n",
      "Epoch 19, Batch 917, Test Loss: 0.4377417266368866\n",
      "Epoch 19, Batch 918, Test Loss: 0.4597376883029938\n",
      "Epoch 19, Batch 919, Test Loss: 0.40653395652770996\n",
      "Epoch 19, Batch 920, Test Loss: 0.3227189779281616\n",
      "Epoch 19, Batch 921, Test Loss: 0.3040393590927124\n",
      "Epoch 19, Batch 922, Test Loss: 0.36088624596595764\n",
      "Epoch 19, Batch 923, Test Loss: 0.7317131161689758\n",
      "Epoch 19, Batch 924, Test Loss: 0.354815274477005\n",
      "Epoch 19, Batch 925, Test Loss: 0.35615038871765137\n",
      "Epoch 19, Batch 926, Test Loss: 0.40709957480430603\n",
      "Epoch 19, Batch 927, Test Loss: 0.36755669116973877\n",
      "Epoch 19, Batch 928, Test Loss: 0.8059480786323547\n",
      "Epoch 19, Batch 929, Test Loss: 0.4011772871017456\n",
      "Epoch 19, Batch 930, Test Loss: 0.2937883734703064\n",
      "Epoch 19, Batch 931, Test Loss: 0.322317898273468\n",
      "Epoch 19, Batch 932, Test Loss: 0.44214925169944763\n",
      "Epoch 19, Batch 933, Test Loss: 0.38896051049232483\n",
      "Epoch 19, Batch 934, Test Loss: 0.33697646856307983\n",
      "Epoch 19, Batch 935, Test Loss: 0.38254380226135254\n",
      "Epoch 19, Batch 936, Test Loss: 0.32887694239616394\n",
      "Epoch 19, Batch 937, Test Loss: 0.48420923948287964\n",
      "Epoch 19, Batch 938, Test Loss: 0.6020126938819885\n",
      "Accuracy of Test set: 0.8523166666666666\n",
      "Epoch 20, Batch 1, Loss: 0.7162232398986816\n",
      "Epoch 20, Batch 2, Loss: 0.3431382477283478\n",
      "Epoch 20, Batch 3, Loss: 0.45300889015197754\n",
      "Epoch 20, Batch 4, Loss: 0.5216376781463623\n",
      "Epoch 20, Batch 5, Loss: 0.38039758801460266\n",
      "Epoch 20, Batch 6, Loss: 0.29693639278411865\n",
      "Epoch 20, Batch 7, Loss: 0.3730016350746155\n",
      "Epoch 20, Batch 8, Loss: 0.5688187479972839\n",
      "Epoch 20, Batch 9, Loss: 0.30420419573783875\n",
      "Epoch 20, Batch 10, Loss: 0.49129271507263184\n",
      "Epoch 20, Batch 11, Loss: 0.20597636699676514\n",
      "Epoch 20, Batch 12, Loss: 0.44448912143707275\n",
      "Epoch 20, Batch 13, Loss: 0.4576537609100342\n",
      "Epoch 20, Batch 14, Loss: 0.2530229091644287\n",
      "Epoch 20, Batch 15, Loss: 0.34123918414115906\n",
      "Epoch 20, Batch 16, Loss: 0.3415868580341339\n",
      "Epoch 20, Batch 17, Loss: 0.4395005404949188\n",
      "Epoch 20, Batch 18, Loss: 0.4840238094329834\n",
      "Epoch 20, Batch 19, Loss: 0.5489916801452637\n",
      "Epoch 20, Batch 20, Loss: 0.40562522411346436\n",
      "Epoch 20, Batch 21, Loss: 0.3507825434207916\n",
      "Epoch 20, Batch 22, Loss: 0.4085359275341034\n",
      "Epoch 20, Batch 23, Loss: 0.456600546836853\n",
      "Epoch 20, Batch 24, Loss: 0.3048930764198303\n",
      "Epoch 20, Batch 25, Loss: 0.38831838965415955\n",
      "Epoch 20, Batch 26, Loss: 0.4123288094997406\n",
      "Epoch 20, Batch 27, Loss: 0.45166897773742676\n",
      "Epoch 20, Batch 28, Loss: 0.47716832160949707\n",
      "Epoch 20, Batch 29, Loss: 0.5535085797309875\n",
      "Epoch 20, Batch 30, Loss: 0.5443813800811768\n",
      "Epoch 20, Batch 31, Loss: 0.4418400526046753\n",
      "Epoch 20, Batch 32, Loss: 0.2649911046028137\n",
      "Epoch 20, Batch 33, Loss: 0.39297646284103394\n",
      "Epoch 20, Batch 34, Loss: 0.3906654715538025\n",
      "Epoch 20, Batch 35, Loss: 0.35305655002593994\n",
      "Epoch 20, Batch 36, Loss: 0.3903443515300751\n",
      "Epoch 20, Batch 37, Loss: 0.33664482831954956\n",
      "Epoch 20, Batch 38, Loss: 0.3630463480949402\n",
      "Epoch 20, Batch 39, Loss: 0.5721372365951538\n",
      "Epoch 20, Batch 40, Loss: 0.479682594537735\n",
      "Epoch 20, Batch 41, Loss: 0.3781965672969818\n",
      "Epoch 20, Batch 42, Loss: 0.4590335488319397\n",
      "Epoch 20, Batch 43, Loss: 0.41379520297050476\n",
      "Epoch 20, Batch 44, Loss: 0.3612561821937561\n",
      "Epoch 20, Batch 45, Loss: 0.509217381477356\n",
      "Epoch 20, Batch 46, Loss: 0.484577476978302\n",
      "Epoch 20, Batch 47, Loss: 0.3328300416469574\n",
      "Epoch 20, Batch 48, Loss: 0.24121345579624176\n",
      "Epoch 20, Batch 49, Loss: 0.4487188458442688\n",
      "Epoch 20, Batch 50, Loss: 0.42588382959365845\n",
      "Epoch 20, Batch 51, Loss: 0.45769235491752625\n",
      "Epoch 20, Batch 52, Loss: 0.3791268467903137\n",
      "Epoch 20, Batch 53, Loss: 0.3901057839393616\n",
      "Epoch 20, Batch 54, Loss: 0.4425865709781647\n",
      "Epoch 20, Batch 55, Loss: 0.30672189593315125\n",
      "Epoch 20, Batch 56, Loss: 0.5265110731124878\n",
      "Epoch 20, Batch 57, Loss: 0.30030032992362976\n",
      "Epoch 20, Batch 58, Loss: 0.28950589895248413\n",
      "Epoch 20, Batch 59, Loss: 0.42365434765815735\n",
      "Epoch 20, Batch 60, Loss: 0.43081802129745483\n",
      "Epoch 20, Batch 61, Loss: 0.3725225627422333\n",
      "Epoch 20, Batch 62, Loss: 0.5578906536102295\n",
      "Epoch 20, Batch 63, Loss: 0.37345629930496216\n",
      "Epoch 20, Batch 64, Loss: 0.21071192622184753\n",
      "Epoch 20, Batch 65, Loss: 0.35958367586135864\n",
      "Epoch 20, Batch 66, Loss: 0.5664700269699097\n",
      "Epoch 20, Batch 67, Loss: 0.494550496339798\n",
      "Epoch 20, Batch 68, Loss: 0.4423072338104248\n",
      "Epoch 20, Batch 69, Loss: 0.2513364851474762\n",
      "Epoch 20, Batch 70, Loss: 0.42349618673324585\n",
      "Epoch 20, Batch 71, Loss: 0.5699293613433838\n",
      "Epoch 20, Batch 72, Loss: 0.22625470161437988\n",
      "Epoch 20, Batch 73, Loss: 0.3327101171016693\n",
      "Epoch 20, Batch 74, Loss: 0.37804266810417175\n",
      "Epoch 20, Batch 75, Loss: 0.3957396447658539\n",
      "Epoch 20, Batch 76, Loss: 0.3349367380142212\n",
      "Epoch 20, Batch 77, Loss: 0.326260507106781\n",
      "Epoch 20, Batch 78, Loss: 0.5547018647193909\n",
      "Epoch 20, Batch 79, Loss: 0.5232852697372437\n",
      "Epoch 20, Batch 80, Loss: 0.3616836667060852\n",
      "Epoch 20, Batch 81, Loss: 0.48456451296806335\n",
      "Epoch 20, Batch 82, Loss: 0.7816823720932007\n",
      "Epoch 20, Batch 83, Loss: 0.40628424286842346\n",
      "Epoch 20, Batch 84, Loss: 0.40612244606018066\n",
      "Epoch 20, Batch 85, Loss: 0.5723319053649902\n",
      "Epoch 20, Batch 86, Loss: 0.4127562940120697\n",
      "Epoch 20, Batch 87, Loss: 0.40350794792175293\n",
      "Epoch 20, Batch 88, Loss: 0.47832241654396057\n",
      "Epoch 20, Batch 89, Loss: 0.541717529296875\n",
      "Epoch 20, Batch 90, Loss: 0.4859783351421356\n",
      "Epoch 20, Batch 91, Loss: 0.23589029908180237\n",
      "Epoch 20, Batch 92, Loss: 0.3893949091434479\n",
      "Epoch 20, Batch 93, Loss: 0.38954174518585205\n",
      "Epoch 20, Batch 94, Loss: 0.32778051495552063\n",
      "Epoch 20, Batch 95, Loss: 0.4250086843967438\n",
      "Epoch 20, Batch 96, Loss: 0.51058030128479\n",
      "Epoch 20, Batch 97, Loss: 0.4918456971645355\n",
      "Epoch 20, Batch 98, Loss: 0.20649679005146027\n",
      "Epoch 20, Batch 99, Loss: 0.3485608696937561\n",
      "Epoch 20, Batch 100, Loss: 0.41714638471603394\n",
      "Epoch 20, Batch 101, Loss: 0.3622112274169922\n",
      "Epoch 20, Batch 102, Loss: 0.35317355394363403\n",
      "Epoch 20, Batch 103, Loss: 0.6386874914169312\n",
      "Epoch 20, Batch 104, Loss: 0.3119773864746094\n",
      "Epoch 20, Batch 105, Loss: 0.3630727529525757\n",
      "Epoch 20, Batch 106, Loss: 0.6803243160247803\n",
      "Epoch 20, Batch 107, Loss: 0.42409807443618774\n",
      "Epoch 20, Batch 108, Loss: 0.3744761645793915\n",
      "Epoch 20, Batch 109, Loss: 0.378463476896286\n",
      "Epoch 20, Batch 110, Loss: 0.46448343992233276\n",
      "Epoch 20, Batch 111, Loss: 0.48339134454727173\n",
      "Epoch 20, Batch 112, Loss: 0.280180960893631\n",
      "Epoch 20, Batch 113, Loss: 0.45323503017425537\n",
      "Epoch 20, Batch 114, Loss: 0.3778017461299896\n",
      "Epoch 20, Batch 115, Loss: 0.5498368144035339\n",
      "Epoch 20, Batch 116, Loss: 0.3598925769329071\n",
      "Epoch 20, Batch 117, Loss: 0.35695961117744446\n",
      "Epoch 20, Batch 118, Loss: 0.2380216270685196\n",
      "Epoch 20, Batch 119, Loss: 0.39035558700561523\n",
      "Epoch 20, Batch 120, Loss: 0.42766836285591125\n",
      "Epoch 20, Batch 121, Loss: 0.4085281789302826\n",
      "Epoch 20, Batch 122, Loss: 0.2696816325187683\n",
      "Epoch 20, Batch 123, Loss: 0.6799945831298828\n",
      "Epoch 20, Batch 124, Loss: 0.23735621571540833\n",
      "Epoch 20, Batch 125, Loss: 0.2230217605829239\n",
      "Epoch 20, Batch 126, Loss: 0.4631122052669525\n",
      "Epoch 20, Batch 127, Loss: 0.6475638151168823\n",
      "Epoch 20, Batch 128, Loss: 0.22660693526268005\n",
      "Epoch 20, Batch 129, Loss: 0.34556710720062256\n",
      "Epoch 20, Batch 130, Loss: 0.4220454692840576\n",
      "Epoch 20, Batch 131, Loss: 0.281236469745636\n",
      "Epoch 20, Batch 132, Loss: 0.4196544289588928\n",
      "Epoch 20, Batch 133, Loss: 0.5838900804519653\n",
      "Epoch 20, Batch 134, Loss: 0.28788456320762634\n",
      "Epoch 20, Batch 135, Loss: 0.33217334747314453\n",
      "Epoch 20, Batch 136, Loss: 0.3007287383079529\n",
      "Epoch 20, Batch 137, Loss: 0.3597252070903778\n",
      "Epoch 20, Batch 138, Loss: 0.3982308804988861\n",
      "Epoch 20, Batch 139, Loss: 0.4141136705875397\n",
      "Epoch 20, Batch 140, Loss: 0.6559872627258301\n",
      "Epoch 20, Batch 141, Loss: 0.5026168823242188\n",
      "Epoch 20, Batch 142, Loss: 0.3628399968147278\n",
      "Epoch 20, Batch 143, Loss: 0.5189694762229919\n",
      "Epoch 20, Batch 144, Loss: 0.45683708786964417\n",
      "Epoch 20, Batch 145, Loss: 0.42759355902671814\n",
      "Epoch 20, Batch 146, Loss: 0.3563309907913208\n",
      "Epoch 20, Batch 147, Loss: 0.4673682451248169\n",
      "Epoch 20, Batch 148, Loss: 0.39049479365348816\n",
      "Epoch 20, Batch 149, Loss: 0.35505378246307373\n",
      "Epoch 20, Batch 150, Loss: 0.32343822717666626\n",
      "Epoch 20, Batch 151, Loss: 0.35895678400993347\n",
      "Epoch 20, Batch 152, Loss: 0.45951515436172485\n",
      "Epoch 20, Batch 153, Loss: 0.5123631358146667\n",
      "Epoch 20, Batch 154, Loss: 0.6209526658058167\n",
      "Epoch 20, Batch 155, Loss: 0.38946306705474854\n",
      "Epoch 20, Batch 156, Loss: 0.5364212989807129\n",
      "Epoch 20, Batch 157, Loss: 0.8663612604141235\n",
      "Epoch 20, Batch 158, Loss: 0.5107557773590088\n",
      "Epoch 20, Batch 159, Loss: 0.4379633963108063\n",
      "Epoch 20, Batch 160, Loss: 0.4247702658176422\n",
      "Epoch 20, Batch 161, Loss: 0.2567808926105499\n",
      "Epoch 20, Batch 162, Loss: 0.33890074491500854\n",
      "Epoch 20, Batch 163, Loss: 0.25881093740463257\n",
      "Epoch 20, Batch 164, Loss: 0.3906620442867279\n",
      "Epoch 20, Batch 165, Loss: 0.29012730717658997\n",
      "Epoch 20, Batch 166, Loss: 0.4617569148540497\n",
      "Epoch 20, Batch 167, Loss: 0.26439157128334045\n",
      "Epoch 20, Batch 168, Loss: 0.46922552585601807\n",
      "Epoch 20, Batch 169, Loss: 0.2632942795753479\n",
      "Epoch 20, Batch 170, Loss: 0.5667769908905029\n",
      "Epoch 20, Batch 171, Loss: 0.45311713218688965\n",
      "Epoch 20, Batch 172, Loss: 0.5061883926391602\n",
      "Epoch 20, Batch 173, Loss: 0.5741697549819946\n",
      "Epoch 20, Batch 174, Loss: 0.33320868015289307\n",
      "Epoch 20, Batch 175, Loss: 0.3284417390823364\n",
      "Epoch 20, Batch 176, Loss: 0.3753141760826111\n",
      "Epoch 20, Batch 177, Loss: 0.27667436003685\n",
      "Epoch 20, Batch 178, Loss: 0.2868969440460205\n",
      "Epoch 20, Batch 179, Loss: 0.4369688630104065\n",
      "Epoch 20, Batch 180, Loss: 0.331691712141037\n",
      "Epoch 20, Batch 181, Loss: 0.5069178938865662\n",
      "Epoch 20, Batch 182, Loss: 0.4731045663356781\n",
      "Epoch 20, Batch 183, Loss: 0.4015955626964569\n",
      "Epoch 20, Batch 184, Loss: 0.6064941883087158\n",
      "Epoch 20, Batch 185, Loss: 0.45632249116897583\n",
      "Epoch 20, Batch 186, Loss: 0.2778598964214325\n",
      "Epoch 20, Batch 187, Loss: 0.28368905186653137\n",
      "Epoch 20, Batch 188, Loss: 0.34728002548217773\n",
      "Epoch 20, Batch 189, Loss: 0.4151342809200287\n",
      "Epoch 20, Batch 190, Loss: 0.447488933801651\n",
      "Epoch 20, Batch 191, Loss: 0.42134204506874084\n",
      "Epoch 20, Batch 192, Loss: 0.4746338129043579\n",
      "Epoch 20, Batch 193, Loss: 0.30930936336517334\n",
      "Epoch 20, Batch 194, Loss: 0.2870917320251465\n",
      "Epoch 20, Batch 195, Loss: 0.36698710918426514\n",
      "Epoch 20, Batch 196, Loss: 0.21385741233825684\n",
      "Epoch 20, Batch 197, Loss: 0.3379434645175934\n",
      "Epoch 20, Batch 198, Loss: 0.31868621706962585\n",
      "Epoch 20, Batch 199, Loss: 0.3061533272266388\n",
      "Epoch 20, Batch 200, Loss: 0.6093864440917969\n",
      "Epoch 20, Batch 201, Loss: 0.2425646185874939\n",
      "Epoch 20, Batch 202, Loss: 0.41134965419769287\n",
      "Epoch 20, Batch 203, Loss: 0.24337562918663025\n",
      "Epoch 20, Batch 204, Loss: 0.4594076871871948\n",
      "Epoch 20, Batch 205, Loss: 0.457203209400177\n",
      "Epoch 20, Batch 206, Loss: 0.48665422201156616\n",
      "Epoch 20, Batch 207, Loss: 0.3116089105606079\n",
      "Epoch 20, Batch 208, Loss: 0.3650779724121094\n",
      "Epoch 20, Batch 209, Loss: 0.42937156558036804\n",
      "Epoch 20, Batch 210, Loss: 0.3672753572463989\n",
      "Epoch 20, Batch 211, Loss: 0.34910374879837036\n",
      "Epoch 20, Batch 212, Loss: 0.39575546979904175\n",
      "Epoch 20, Batch 213, Loss: 0.41535937786102295\n",
      "Epoch 20, Batch 214, Loss: 0.5168849229812622\n",
      "Epoch 20, Batch 215, Loss: 0.3471176028251648\n",
      "Epoch 20, Batch 216, Loss: 0.42343705892562866\n",
      "Epoch 20, Batch 217, Loss: 0.18961772322654724\n",
      "Epoch 20, Batch 218, Loss: 0.7231235504150391\n",
      "Epoch 20, Batch 219, Loss: 0.5276179909706116\n",
      "Epoch 20, Batch 220, Loss: 0.34473729133605957\n",
      "Epoch 20, Batch 221, Loss: 0.419445276260376\n",
      "Epoch 20, Batch 222, Loss: 0.3113110363483429\n",
      "Epoch 20, Batch 223, Loss: 0.35368770360946655\n",
      "Epoch 20, Batch 224, Loss: 0.5088365077972412\n",
      "Epoch 20, Batch 225, Loss: 0.408181369304657\n",
      "Epoch 20, Batch 226, Loss: 0.30961281061172485\n",
      "Epoch 20, Batch 227, Loss: 0.39537331461906433\n",
      "Epoch 20, Batch 228, Loss: 0.49554985761642456\n",
      "Epoch 20, Batch 229, Loss: 0.30584725737571716\n",
      "Epoch 20, Batch 230, Loss: 0.35270869731903076\n",
      "Epoch 20, Batch 231, Loss: 0.24976299703121185\n",
      "Epoch 20, Batch 232, Loss: 0.3447071313858032\n",
      "Epoch 20, Batch 233, Loss: 0.34206387400627136\n",
      "Epoch 20, Batch 234, Loss: 0.3670867681503296\n",
      "Epoch 20, Batch 235, Loss: 0.34731966257095337\n",
      "Epoch 20, Batch 236, Loss: 0.2993789613246918\n",
      "Epoch 20, Batch 237, Loss: 0.33435192704200745\n",
      "Epoch 20, Batch 238, Loss: 0.33052724599838257\n",
      "Epoch 20, Batch 239, Loss: 0.38067111372947693\n",
      "Epoch 20, Batch 240, Loss: 0.6877967715263367\n",
      "Epoch 20, Batch 241, Loss: 0.5388248562812805\n",
      "Epoch 20, Batch 242, Loss: 0.4407748878002167\n",
      "Epoch 20, Batch 243, Loss: 0.3937217593193054\n",
      "Epoch 20, Batch 244, Loss: 0.4264831840991974\n",
      "Epoch 20, Batch 245, Loss: 0.6437092423439026\n",
      "Epoch 20, Batch 246, Loss: 0.3097601532936096\n",
      "Epoch 20, Batch 247, Loss: 0.38365113735198975\n",
      "Epoch 20, Batch 248, Loss: 0.4623483717441559\n",
      "Epoch 20, Batch 249, Loss: 0.5269070267677307\n",
      "Epoch 20, Batch 250, Loss: 0.4760743975639343\n",
      "Epoch 20, Batch 251, Loss: 0.4524572193622589\n",
      "Epoch 20, Batch 252, Loss: 0.3032078742980957\n",
      "Epoch 20, Batch 253, Loss: 0.5844196081161499\n",
      "Epoch 20, Batch 254, Loss: 0.3662169277667999\n",
      "Epoch 20, Batch 255, Loss: 0.45932042598724365\n",
      "Epoch 20, Batch 256, Loss: 0.6127219200134277\n",
      "Epoch 20, Batch 257, Loss: 0.43712130188941956\n",
      "Epoch 20, Batch 258, Loss: 0.4465433955192566\n",
      "Epoch 20, Batch 259, Loss: 0.5946352481842041\n",
      "Epoch 20, Batch 260, Loss: 0.4025108814239502\n",
      "Epoch 20, Batch 261, Loss: 0.2899435758590698\n",
      "Epoch 20, Batch 262, Loss: 0.37519657611846924\n",
      "Epoch 20, Batch 263, Loss: 0.4701221287250519\n",
      "Epoch 20, Batch 264, Loss: 0.45567786693573\n",
      "Epoch 20, Batch 265, Loss: 0.3982012867927551\n",
      "Epoch 20, Batch 266, Loss: 0.33698004484176636\n",
      "Epoch 20, Batch 267, Loss: 0.37443339824676514\n",
      "Epoch 20, Batch 268, Loss: 0.3353269696235657\n",
      "Epoch 20, Batch 269, Loss: 0.4238670766353607\n",
      "Epoch 20, Batch 270, Loss: 0.44601428508758545\n",
      "Epoch 20, Batch 271, Loss: 0.30946362018585205\n",
      "Epoch 20, Batch 272, Loss: 0.45830240845680237\n",
      "Epoch 20, Batch 273, Loss: 0.38372012972831726\n",
      "Epoch 20, Batch 274, Loss: 0.44884517788887024\n",
      "Epoch 20, Batch 275, Loss: 0.2656491696834564\n",
      "Epoch 20, Batch 276, Loss: 0.2961711287498474\n",
      "Epoch 20, Batch 277, Loss: 0.39266741275787354\n",
      "Epoch 20, Batch 278, Loss: 0.3415011465549469\n",
      "Epoch 20, Batch 279, Loss: 0.24731196463108063\n",
      "Epoch 20, Batch 280, Loss: 0.2553388476371765\n",
      "Epoch 20, Batch 281, Loss: 0.4697811007499695\n",
      "Epoch 20, Batch 282, Loss: 0.3342787027359009\n",
      "Epoch 20, Batch 283, Loss: 0.39397525787353516\n",
      "Epoch 20, Batch 284, Loss: 0.41011103987693787\n",
      "Epoch 20, Batch 285, Loss: 0.31843411922454834\n",
      "Epoch 20, Batch 286, Loss: 0.3735658526420593\n",
      "Epoch 20, Batch 287, Loss: 0.32187333703041077\n",
      "Epoch 20, Batch 288, Loss: 0.4479959011077881\n",
      "Epoch 20, Batch 289, Loss: 0.26233139634132385\n",
      "Epoch 20, Batch 290, Loss: 0.6063442230224609\n",
      "Epoch 20, Batch 291, Loss: 0.3926413059234619\n",
      "Epoch 20, Batch 292, Loss: 0.5279131531715393\n",
      "Epoch 20, Batch 293, Loss: 0.43221113085746765\n",
      "Epoch 20, Batch 294, Loss: 0.21563369035720825\n",
      "Epoch 20, Batch 295, Loss: 0.4356289505958557\n",
      "Epoch 20, Batch 296, Loss: 0.4776598811149597\n",
      "Epoch 20, Batch 297, Loss: 0.29433688521385193\n",
      "Epoch 20, Batch 298, Loss: 0.6072770953178406\n",
      "Epoch 20, Batch 299, Loss: 0.25407126545906067\n",
      "Epoch 20, Batch 300, Loss: 0.43468576669692993\n",
      "Epoch 20, Batch 301, Loss: 0.2748883366584778\n",
      "Epoch 20, Batch 302, Loss: 0.3567177653312683\n",
      "Epoch 20, Batch 303, Loss: 0.35140666365623474\n",
      "Epoch 20, Batch 304, Loss: 0.22992706298828125\n",
      "Epoch 20, Batch 305, Loss: 0.4902190566062927\n",
      "Epoch 20, Batch 306, Loss: 0.39375296235084534\n",
      "Epoch 20, Batch 307, Loss: 0.4782533347606659\n",
      "Epoch 20, Batch 308, Loss: 0.6976802349090576\n",
      "Epoch 20, Batch 309, Loss: 0.5717097520828247\n",
      "Epoch 20, Batch 310, Loss: 0.25501757860183716\n",
      "Epoch 20, Batch 311, Loss: 0.4719184339046478\n",
      "Epoch 20, Batch 312, Loss: 0.6606266498565674\n",
      "Epoch 20, Batch 313, Loss: 0.31696078181266785\n",
      "Epoch 20, Batch 314, Loss: 0.38451820611953735\n",
      "Epoch 20, Batch 315, Loss: 0.330342173576355\n",
      "Epoch 20, Batch 316, Loss: 0.6792597770690918\n",
      "Epoch 20, Batch 317, Loss: 0.2667187452316284\n",
      "Epoch 20, Batch 318, Loss: 0.4041198790073395\n",
      "Epoch 20, Batch 319, Loss: 0.5112230777740479\n",
      "Epoch 20, Batch 320, Loss: 0.5198841094970703\n",
      "Epoch 20, Batch 321, Loss: 0.49872252345085144\n",
      "Epoch 20, Batch 322, Loss: 0.3155841529369354\n",
      "Epoch 20, Batch 323, Loss: 0.4434230327606201\n",
      "Epoch 20, Batch 324, Loss: 0.5183339715003967\n",
      "Epoch 20, Batch 325, Loss: 0.7866195440292358\n",
      "Epoch 20, Batch 326, Loss: 0.2824331223964691\n",
      "Epoch 20, Batch 327, Loss: 0.4301198422908783\n",
      "Epoch 20, Batch 328, Loss: 0.3729882836341858\n",
      "Epoch 20, Batch 329, Loss: 0.33881253004074097\n",
      "Epoch 20, Batch 330, Loss: 0.44868868589401245\n",
      "Epoch 20, Batch 331, Loss: 0.4182378053665161\n",
      "Epoch 20, Batch 332, Loss: 0.38597697019577026\n",
      "Epoch 20, Batch 333, Loss: 0.41318199038505554\n",
      "Epoch 20, Batch 334, Loss: 0.39824217557907104\n",
      "Epoch 20, Batch 335, Loss: 0.34037747979164124\n",
      "Epoch 20, Batch 336, Loss: 0.29820573329925537\n",
      "Epoch 20, Batch 337, Loss: 0.37198352813720703\n",
      "Epoch 20, Batch 338, Loss: 0.4894963800907135\n",
      "Epoch 20, Batch 339, Loss: 0.4470604360103607\n",
      "Epoch 20, Batch 340, Loss: 0.36839696764945984\n",
      "Epoch 20, Batch 341, Loss: 0.3980121612548828\n",
      "Epoch 20, Batch 342, Loss: 0.3541249930858612\n",
      "Epoch 20, Batch 343, Loss: 0.3259326219558716\n",
      "Epoch 20, Batch 344, Loss: 0.30718010663986206\n",
      "Epoch 20, Batch 345, Loss: 0.3343048691749573\n",
      "Epoch 20, Batch 346, Loss: 0.3395955264568329\n",
      "Epoch 20, Batch 347, Loss: 0.4117812216281891\n",
      "Epoch 20, Batch 348, Loss: 0.33083075284957886\n",
      "Epoch 20, Batch 349, Loss: 0.41393643617630005\n",
      "Epoch 20, Batch 350, Loss: 0.6032600402832031\n",
      "Epoch 20, Batch 351, Loss: 0.5173333883285522\n",
      "Epoch 20, Batch 352, Loss: 0.4674915671348572\n",
      "Epoch 20, Batch 353, Loss: 0.36003947257995605\n",
      "Epoch 20, Batch 354, Loss: 0.4584468603134155\n",
      "Epoch 20, Batch 355, Loss: 0.431329607963562\n",
      "Epoch 20, Batch 356, Loss: 0.411626398563385\n",
      "Epoch 20, Batch 357, Loss: 0.4118034839630127\n",
      "Epoch 20, Batch 358, Loss: 0.48247846961021423\n",
      "Epoch 20, Batch 359, Loss: 0.5228166580200195\n",
      "Epoch 20, Batch 360, Loss: 0.4057389497756958\n",
      "Epoch 20, Batch 361, Loss: 0.34958863258361816\n",
      "Epoch 20, Batch 362, Loss: 0.3840915858745575\n",
      "Epoch 20, Batch 363, Loss: 0.34452399611473083\n",
      "Epoch 20, Batch 364, Loss: 0.25659337639808655\n",
      "Epoch 20, Batch 365, Loss: 0.4021385610103607\n",
      "Epoch 20, Batch 366, Loss: 0.5545824766159058\n",
      "Epoch 20, Batch 367, Loss: 0.3437002897262573\n",
      "Epoch 20, Batch 368, Loss: 0.43827420473098755\n",
      "Epoch 20, Batch 369, Loss: 0.4170534014701843\n",
      "Epoch 20, Batch 370, Loss: 0.25306519865989685\n",
      "Epoch 20, Batch 371, Loss: 0.44041866064071655\n",
      "Epoch 20, Batch 372, Loss: 0.31420886516571045\n",
      "Epoch 20, Batch 373, Loss: 0.2635989487171173\n",
      "Epoch 20, Batch 374, Loss: 0.33528199791908264\n",
      "Epoch 20, Batch 375, Loss: 0.2760525047779083\n",
      "Epoch 20, Batch 376, Loss: 0.3499875068664551\n",
      "Epoch 20, Batch 377, Loss: 0.4262296259403229\n",
      "Epoch 20, Batch 378, Loss: 0.379747211933136\n",
      "Epoch 20, Batch 379, Loss: 0.49804848432540894\n",
      "Epoch 20, Batch 380, Loss: 0.5590568780899048\n",
      "Epoch 20, Batch 381, Loss: 0.4499603807926178\n",
      "Epoch 20, Batch 382, Loss: 0.49748241901397705\n",
      "Epoch 20, Batch 383, Loss: 0.38213422894477844\n",
      "Epoch 20, Batch 384, Loss: 0.4501635432243347\n",
      "Epoch 20, Batch 385, Loss: 0.4055052399635315\n",
      "Epoch 20, Batch 386, Loss: 0.2594856917858124\n",
      "Epoch 20, Batch 387, Loss: 0.5760751366615295\n",
      "Epoch 20, Batch 388, Loss: 0.4259492754936218\n",
      "Epoch 20, Batch 389, Loss: 0.3710216283798218\n",
      "Epoch 20, Batch 390, Loss: 0.40617436170578003\n",
      "Epoch 20, Batch 391, Loss: 0.561211884021759\n",
      "Epoch 20, Batch 392, Loss: 0.7113723158836365\n",
      "Epoch 20, Batch 393, Loss: 0.4003082811832428\n",
      "Epoch 20, Batch 394, Loss: 0.551979660987854\n",
      "Epoch 20, Batch 395, Loss: 0.40492427349090576\n",
      "Epoch 20, Batch 396, Loss: 0.3328215479850769\n",
      "Epoch 20, Batch 397, Loss: 0.34305259585380554\n",
      "Epoch 20, Batch 398, Loss: 0.48634418845176697\n",
      "Epoch 20, Batch 399, Loss: 0.4687419533729553\n",
      "Epoch 20, Batch 400, Loss: 0.3532159626483917\n",
      "Epoch 20, Batch 401, Loss: 0.6139273047447205\n",
      "Epoch 20, Batch 402, Loss: 0.4143156409263611\n",
      "Epoch 20, Batch 403, Loss: 0.36363598704338074\n",
      "Epoch 20, Batch 404, Loss: 0.6317641735076904\n",
      "Epoch 20, Batch 405, Loss: 0.4693240821361542\n",
      "Epoch 20, Batch 406, Loss: 0.42051267623901367\n",
      "Epoch 20, Batch 407, Loss: 0.5410550832748413\n",
      "Epoch 20, Batch 408, Loss: 0.590828001499176\n",
      "Epoch 20, Batch 409, Loss: 0.6378985643386841\n",
      "Epoch 20, Batch 410, Loss: 0.5853185653686523\n",
      "Epoch 20, Batch 411, Loss: 0.45162707567214966\n",
      "Epoch 20, Batch 412, Loss: 0.3427579998970032\n",
      "Epoch 20, Batch 413, Loss: 0.38056010007858276\n",
      "Epoch 20, Batch 414, Loss: 0.38058164715766907\n",
      "Epoch 20, Batch 415, Loss: 0.43196263909339905\n",
      "Epoch 20, Batch 416, Loss: 0.47913914918899536\n",
      "Epoch 20, Batch 417, Loss: 0.4365541636943817\n",
      "Epoch 20, Batch 418, Loss: 0.35429319739341736\n",
      "Epoch 20, Batch 419, Loss: 0.29808446764945984\n",
      "Epoch 20, Batch 420, Loss: 0.2025810182094574\n",
      "Epoch 20, Batch 421, Loss: 0.3274613618850708\n",
      "Epoch 20, Batch 422, Loss: 0.31918865442276\n",
      "Epoch 20, Batch 423, Loss: 0.4036771357059479\n",
      "Epoch 20, Batch 424, Loss: 0.2987435758113861\n",
      "Epoch 20, Batch 425, Loss: 0.47829848527908325\n",
      "Epoch 20, Batch 426, Loss: 0.39226025342941284\n",
      "Epoch 20, Batch 427, Loss: 0.46826040744781494\n",
      "Epoch 20, Batch 428, Loss: 0.5011208057403564\n",
      "Epoch 20, Batch 429, Loss: 0.5393403768539429\n",
      "Epoch 20, Batch 430, Loss: 0.5497882962226868\n",
      "Epoch 20, Batch 431, Loss: 0.535560667514801\n",
      "Epoch 20, Batch 432, Loss: 0.48019540309906006\n",
      "Epoch 20, Batch 433, Loss: 0.36408692598342896\n",
      "Epoch 20, Batch 434, Loss: 0.45185407996177673\n",
      "Epoch 20, Batch 435, Loss: 0.40650251507759094\n",
      "Epoch 20, Batch 436, Loss: 0.334432452917099\n",
      "Epoch 20, Batch 437, Loss: 0.3441918194293976\n",
      "Epoch 20, Batch 438, Loss: 0.5336018204689026\n",
      "Epoch 20, Batch 439, Loss: 0.371491014957428\n",
      "Epoch 20, Batch 440, Loss: 0.4927687644958496\n",
      "Epoch 20, Batch 441, Loss: 0.6518953442573547\n",
      "Epoch 20, Batch 442, Loss: 0.3711509108543396\n",
      "Epoch 20, Batch 443, Loss: 0.5659952163696289\n",
      "Epoch 20, Batch 444, Loss: 0.43666037917137146\n",
      "Epoch 20, Batch 445, Loss: 0.42429158091545105\n",
      "Epoch 20, Batch 446, Loss: 0.3694049119949341\n",
      "Epoch 20, Batch 447, Loss: 0.3662986755371094\n",
      "Epoch 20, Batch 448, Loss: 0.47646498680114746\n",
      "Epoch 20, Batch 449, Loss: 0.507084846496582\n",
      "Epoch 20, Batch 450, Loss: 0.4452877342700958\n",
      "Epoch 20, Batch 451, Loss: 0.5775659680366516\n",
      "Epoch 20, Batch 452, Loss: 0.25555455684661865\n",
      "Epoch 20, Batch 453, Loss: 0.5355272889137268\n",
      "Epoch 20, Batch 454, Loss: 0.41291433572769165\n",
      "Epoch 20, Batch 455, Loss: 0.5383579730987549\n",
      "Epoch 20, Batch 456, Loss: 0.5724872946739197\n",
      "Epoch 20, Batch 457, Loss: 0.4396874010562897\n",
      "Epoch 20, Batch 458, Loss: 0.47825291752815247\n",
      "Epoch 20, Batch 459, Loss: 0.28196981549263\n",
      "Epoch 20, Batch 460, Loss: 0.36189374327659607\n",
      "Epoch 20, Batch 461, Loss: 0.3273776173591614\n",
      "Epoch 20, Batch 462, Loss: 0.6241205930709839\n",
      "Epoch 20, Batch 463, Loss: 0.42719411849975586\n",
      "Epoch 20, Batch 464, Loss: 0.27225354313850403\n",
      "Epoch 20, Batch 465, Loss: 0.3587772250175476\n",
      "Epoch 20, Batch 466, Loss: 0.3457077741622925\n",
      "Epoch 20, Batch 467, Loss: 0.45455560088157654\n",
      "Epoch 20, Batch 468, Loss: 0.36944496631622314\n",
      "Epoch 20, Batch 469, Loss: 0.5645538568496704\n",
      "Epoch 20, Batch 470, Loss: 0.2549028992652893\n",
      "Epoch 20, Batch 471, Loss: 0.3333400785923004\n",
      "Epoch 20, Batch 472, Loss: 0.26444685459136963\n",
      "Epoch 20, Batch 473, Loss: 0.4259493350982666\n",
      "Epoch 20, Batch 474, Loss: 0.5014779567718506\n",
      "Epoch 20, Batch 475, Loss: 0.2736511528491974\n",
      "Epoch 20, Batch 476, Loss: 0.657210111618042\n",
      "Epoch 20, Batch 477, Loss: 0.3743542432785034\n",
      "Epoch 20, Batch 478, Loss: 0.27825847268104553\n",
      "Epoch 20, Batch 479, Loss: 0.46476638317108154\n",
      "Epoch 20, Batch 480, Loss: 0.41381895542144775\n",
      "Epoch 20, Batch 481, Loss: 0.34908100962638855\n",
      "Epoch 20, Batch 482, Loss: 0.47097527980804443\n",
      "Epoch 20, Batch 483, Loss: 0.47671788930892944\n",
      "Epoch 20, Batch 484, Loss: 0.45159831643104553\n",
      "Epoch 20, Batch 485, Loss: 0.5168836712837219\n",
      "Epoch 20, Batch 486, Loss: 0.5178038477897644\n",
      "Epoch 20, Batch 487, Loss: 0.41075819730758667\n",
      "Epoch 20, Batch 488, Loss: 0.28226029872894287\n",
      "Epoch 20, Batch 489, Loss: 0.30136412382125854\n",
      "Epoch 20, Batch 490, Loss: 0.5688810348510742\n",
      "Epoch 20, Batch 491, Loss: 0.32206034660339355\n",
      "Epoch 20, Batch 492, Loss: 0.5466703176498413\n",
      "Epoch 20, Batch 493, Loss: 0.27639511227607727\n",
      "Epoch 20, Batch 494, Loss: 0.29402685165405273\n",
      "Epoch 20, Batch 495, Loss: 0.6019654273986816\n",
      "Epoch 20, Batch 496, Loss: 0.28448766469955444\n",
      "Epoch 20, Batch 497, Loss: 0.3810657560825348\n",
      "Epoch 20, Batch 498, Loss: 0.41870614886283875\n",
      "Epoch 20, Batch 499, Loss: 0.32965999841690063\n",
      "Epoch 20, Batch 500, Loss: 0.35342496633529663\n",
      "Epoch 20, Batch 501, Loss: 0.6013846397399902\n",
      "Epoch 20, Batch 502, Loss: 0.49123409390449524\n",
      "Epoch 20, Batch 503, Loss: 0.36928272247314453\n",
      "Epoch 20, Batch 504, Loss: 0.3482157588005066\n",
      "Epoch 20, Batch 505, Loss: 0.44676142930984497\n",
      "Epoch 20, Batch 506, Loss: 0.4263434410095215\n",
      "Epoch 20, Batch 507, Loss: 0.4436664283275604\n",
      "Epoch 20, Batch 508, Loss: 0.3963901698589325\n",
      "Epoch 20, Batch 509, Loss: 0.32148030400276184\n",
      "Epoch 20, Batch 510, Loss: 0.3962515592575073\n",
      "Epoch 20, Batch 511, Loss: 0.3729764223098755\n",
      "Epoch 20, Batch 512, Loss: 0.43228259682655334\n",
      "Epoch 20, Batch 513, Loss: 0.46634045243263245\n",
      "Epoch 20, Batch 514, Loss: 0.35341140627861023\n",
      "Epoch 20, Batch 515, Loss: 0.3390427231788635\n",
      "Epoch 20, Batch 516, Loss: 0.6142722368240356\n",
      "Epoch 20, Batch 517, Loss: 0.3663555681705475\n",
      "Epoch 20, Batch 518, Loss: 0.6826055645942688\n",
      "Epoch 20, Batch 519, Loss: 0.3389420807361603\n",
      "Epoch 20, Batch 520, Loss: 0.3861980736255646\n",
      "Epoch 20, Batch 521, Loss: 0.46152597665786743\n",
      "Epoch 20, Batch 522, Loss: 0.44405344128608704\n",
      "Epoch 20, Batch 523, Loss: 0.31587687134742737\n",
      "Epoch 20, Batch 524, Loss: 0.24517905712127686\n",
      "Epoch 20, Batch 525, Loss: 0.49931761622428894\n",
      "Epoch 20, Batch 526, Loss: 0.25302767753601074\n",
      "Epoch 20, Batch 527, Loss: 0.38872167468070984\n",
      "Epoch 20, Batch 528, Loss: 0.4466129243373871\n",
      "Epoch 20, Batch 529, Loss: 0.447274774312973\n",
      "Epoch 20, Batch 530, Loss: 0.3693578541278839\n",
      "Epoch 20, Batch 531, Loss: 0.40109431743621826\n",
      "Epoch 20, Batch 532, Loss: 0.5309682488441467\n",
      "Epoch 20, Batch 533, Loss: 0.31261491775512695\n",
      "Epoch 20, Batch 534, Loss: 0.41445112228393555\n",
      "Epoch 20, Batch 535, Loss: 0.3596550524234772\n",
      "Epoch 20, Batch 536, Loss: 0.4556034803390503\n",
      "Epoch 20, Batch 537, Loss: 0.20072954893112183\n",
      "Epoch 20, Batch 538, Loss: 0.5119775533676147\n",
      "Epoch 20, Batch 539, Loss: 0.5469166040420532\n",
      "Epoch 20, Batch 540, Loss: 0.4631410241127014\n",
      "Epoch 20, Batch 541, Loss: 0.5135192275047302\n",
      "Epoch 20, Batch 542, Loss: 0.31534984707832336\n",
      "Epoch 20, Batch 543, Loss: 0.3363490700721741\n",
      "Epoch 20, Batch 544, Loss: 0.3577306568622589\n",
      "Epoch 20, Batch 545, Loss: 0.39584124088287354\n",
      "Epoch 20, Batch 546, Loss: 0.5772993564605713\n",
      "Epoch 20, Batch 547, Loss: 0.37916266918182373\n",
      "Epoch 20, Batch 548, Loss: 0.42416930198669434\n",
      "Epoch 20, Batch 549, Loss: 0.6565214395523071\n",
      "Epoch 20, Batch 550, Loss: 0.43047618865966797\n",
      "Epoch 20, Batch 551, Loss: 0.6692599654197693\n",
      "Epoch 20, Batch 552, Loss: 0.3779942989349365\n",
      "Epoch 20, Batch 553, Loss: 0.5550242066383362\n",
      "Epoch 20, Batch 554, Loss: 0.6853936910629272\n",
      "Epoch 20, Batch 555, Loss: 0.35404345393180847\n",
      "Epoch 20, Batch 556, Loss: 0.4553491473197937\n",
      "Epoch 20, Batch 557, Loss: 0.4489463269710541\n",
      "Epoch 20, Batch 558, Loss: 0.33240383863449097\n",
      "Epoch 20, Batch 559, Loss: 0.29936468601226807\n",
      "Epoch 20, Batch 560, Loss: 0.2659282386302948\n",
      "Epoch 20, Batch 561, Loss: 0.2600388824939728\n",
      "Epoch 20, Batch 562, Loss: 0.44449174404144287\n",
      "Epoch 20, Batch 563, Loss: 0.7017056941986084\n",
      "Epoch 20, Batch 564, Loss: 0.440652459859848\n",
      "Epoch 20, Batch 565, Loss: 0.4670034945011139\n",
      "Epoch 20, Batch 566, Loss: 0.4111066162586212\n",
      "Epoch 20, Batch 567, Loss: 0.512852668762207\n",
      "Epoch 20, Batch 568, Loss: 0.3021341562271118\n",
      "Epoch 20, Batch 569, Loss: 0.4187382459640503\n",
      "Epoch 20, Batch 570, Loss: 0.3008759319782257\n",
      "Epoch 20, Batch 571, Loss: 0.4390242099761963\n",
      "Epoch 20, Batch 572, Loss: 0.4965957999229431\n",
      "Epoch 20, Batch 573, Loss: 0.42986807227134705\n",
      "Epoch 20, Batch 574, Loss: 0.3580052852630615\n",
      "Epoch 20, Batch 575, Loss: 0.7583359479904175\n",
      "Epoch 20, Batch 576, Loss: 0.24609223008155823\n",
      "Epoch 20, Batch 577, Loss: 0.37257859110832214\n",
      "Epoch 20, Batch 578, Loss: 0.3354494273662567\n",
      "Epoch 20, Batch 579, Loss: 0.33407819271087646\n",
      "Epoch 20, Batch 580, Loss: 0.5747256875038147\n",
      "Epoch 20, Batch 581, Loss: 0.5923517346382141\n",
      "Epoch 20, Batch 582, Loss: 0.4041406512260437\n",
      "Epoch 20, Batch 583, Loss: 0.4641651511192322\n",
      "Epoch 20, Batch 584, Loss: 0.2540704607963562\n",
      "Epoch 20, Batch 585, Loss: 0.40310636162757874\n",
      "Epoch 20, Batch 586, Loss: 0.23965135216712952\n",
      "Epoch 20, Batch 587, Loss: 0.32527992129325867\n",
      "Epoch 20, Batch 588, Loss: 0.5496450066566467\n",
      "Epoch 20, Batch 589, Loss: 0.31349286437034607\n",
      "Epoch 20, Batch 590, Loss: 0.30110687017440796\n",
      "Epoch 20, Batch 591, Loss: 0.26734375953674316\n",
      "Epoch 20, Batch 592, Loss: 0.5608687996864319\n",
      "Epoch 20, Batch 593, Loss: 0.45329684019088745\n",
      "Epoch 20, Batch 594, Loss: 0.3091542720794678\n",
      "Epoch 20, Batch 595, Loss: 0.3633734881877899\n",
      "Epoch 20, Batch 596, Loss: 0.7831243276596069\n",
      "Epoch 20, Batch 597, Loss: 0.3881770670413971\n",
      "Epoch 20, Batch 598, Loss: 0.4377549886703491\n",
      "Epoch 20, Batch 599, Loss: 0.551847517490387\n",
      "Epoch 20, Batch 600, Loss: 0.48589399456977844\n",
      "Epoch 20, Batch 601, Loss: 0.5104967951774597\n",
      "Epoch 20, Batch 602, Loss: 0.40764886140823364\n",
      "Epoch 20, Batch 603, Loss: 0.4317284822463989\n",
      "Epoch 20, Batch 604, Loss: 0.3433634042739868\n",
      "Epoch 20, Batch 605, Loss: 0.29371535778045654\n",
      "Epoch 20, Batch 606, Loss: 0.30082249641418457\n",
      "Epoch 20, Batch 607, Loss: 0.4064963161945343\n",
      "Epoch 20, Batch 608, Loss: 0.3805696368217468\n",
      "Epoch 20, Batch 609, Loss: 0.42910945415496826\n",
      "Epoch 20, Batch 610, Loss: 0.3385292589664459\n",
      "Epoch 20, Batch 611, Loss: 0.5064966678619385\n",
      "Epoch 20, Batch 612, Loss: 0.6312853097915649\n",
      "Epoch 20, Batch 613, Loss: 0.368179589509964\n",
      "Epoch 20, Batch 614, Loss: 0.3354355990886688\n",
      "Epoch 20, Batch 615, Loss: 0.5926509499549866\n",
      "Epoch 20, Batch 616, Loss: 0.4616098403930664\n",
      "Epoch 20, Batch 617, Loss: 0.33879807591438293\n",
      "Epoch 20, Batch 618, Loss: 0.36480453610420227\n",
      "Epoch 20, Batch 619, Loss: 0.3438659906387329\n",
      "Epoch 20, Batch 620, Loss: 0.40936124324798584\n",
      "Epoch 20, Batch 621, Loss: 0.466863751411438\n",
      "Epoch 20, Batch 622, Loss: 0.3454170525074005\n",
      "Epoch 20, Batch 623, Loss: 0.2573331594467163\n",
      "Epoch 20, Batch 624, Loss: 0.5021558403968811\n",
      "Epoch 20, Batch 625, Loss: 0.33239999413490295\n",
      "Epoch 20, Batch 626, Loss: 0.463603138923645\n",
      "Epoch 20, Batch 627, Loss: 0.4668276607990265\n",
      "Epoch 20, Batch 628, Loss: 0.41275906562805176\n",
      "Epoch 20, Batch 629, Loss: 0.49801313877105713\n",
      "Epoch 20, Batch 630, Loss: 0.3566019535064697\n",
      "Epoch 20, Batch 631, Loss: 0.33694738149642944\n",
      "Epoch 20, Batch 632, Loss: 0.3144489824771881\n",
      "Epoch 20, Batch 633, Loss: 0.48294350504875183\n",
      "Epoch 20, Batch 634, Loss: 0.4301518499851227\n",
      "Epoch 20, Batch 635, Loss: 0.3871684670448303\n",
      "Epoch 20, Batch 636, Loss: 0.4746663272380829\n",
      "Epoch 20, Batch 637, Loss: 0.4004756808280945\n",
      "Epoch 20, Batch 638, Loss: 0.4366110563278198\n",
      "Epoch 20, Batch 639, Loss: 0.5724182724952698\n",
      "Epoch 20, Batch 640, Loss: 0.2839586138725281\n",
      "Epoch 20, Batch 641, Loss: 0.37035611271858215\n",
      "Epoch 20, Batch 642, Loss: 0.2398393452167511\n",
      "Epoch 20, Batch 643, Loss: 0.34338098764419556\n",
      "Epoch 20, Batch 644, Loss: 0.5941604971885681\n",
      "Epoch 20, Batch 645, Loss: 0.34705474972724915\n",
      "Epoch 20, Batch 646, Loss: 0.360027015209198\n",
      "Epoch 20, Batch 647, Loss: 0.36315008997917175\n",
      "Epoch 20, Batch 648, Loss: 0.3230818808078766\n",
      "Epoch 20, Batch 649, Loss: 0.3287844955921173\n",
      "Epoch 20, Batch 650, Loss: 0.43203893303871155\n",
      "Epoch 20, Batch 651, Loss: 0.3638038635253906\n",
      "Epoch 20, Batch 652, Loss: 0.42016878724098206\n",
      "Epoch 20, Batch 653, Loss: 0.43269214034080505\n",
      "Epoch 20, Batch 654, Loss: 0.3539856970310211\n",
      "Epoch 20, Batch 655, Loss: 0.5777304172515869\n",
      "Epoch 20, Batch 656, Loss: 0.40108853578567505\n",
      "Epoch 20, Batch 657, Loss: 0.44907131791114807\n",
      "Epoch 20, Batch 658, Loss: 0.22023004293441772\n",
      "Epoch 20, Batch 659, Loss: 0.5484145879745483\n",
      "Epoch 20, Batch 660, Loss: 0.31877198815345764\n",
      "Epoch 20, Batch 661, Loss: 0.31926029920578003\n",
      "Epoch 20, Batch 662, Loss: 0.312205046415329\n",
      "Epoch 20, Batch 663, Loss: 0.3173016607761383\n",
      "Epoch 20, Batch 664, Loss: 0.5259784460067749\n",
      "Epoch 20, Batch 665, Loss: 0.6110917329788208\n",
      "Epoch 20, Batch 666, Loss: 0.3124013841152191\n",
      "Epoch 20, Batch 667, Loss: 0.3771647810935974\n",
      "Epoch 20, Batch 668, Loss: 0.4072273075580597\n",
      "Epoch 20, Batch 669, Loss: 0.3686371445655823\n",
      "Epoch 20, Batch 670, Loss: 0.47410905361175537\n",
      "Epoch 20, Batch 671, Loss: 0.3442624807357788\n",
      "Epoch 20, Batch 672, Loss: 0.4775065779685974\n",
      "Epoch 20, Batch 673, Loss: 0.2838548719882965\n",
      "Epoch 20, Batch 674, Loss: 0.3680310845375061\n",
      "Epoch 20, Batch 675, Loss: 0.500827968120575\n",
      "Epoch 20, Batch 676, Loss: 0.3956633508205414\n",
      "Epoch 20, Batch 677, Loss: 0.32808804512023926\n",
      "Epoch 20, Batch 678, Loss: 0.3362589478492737\n",
      "Epoch 20, Batch 679, Loss: 0.3453536331653595\n",
      "Epoch 20, Batch 680, Loss: 0.38347795605659485\n",
      "Epoch 20, Batch 681, Loss: 0.48152944445610046\n",
      "Epoch 20, Batch 682, Loss: 0.3544897139072418\n",
      "Epoch 20, Batch 683, Loss: 0.2185235172510147\n",
      "Epoch 20, Batch 684, Loss: 0.5135536789894104\n",
      "Epoch 20, Batch 685, Loss: 0.42975541949272156\n",
      "Epoch 20, Batch 686, Loss: 0.3518443703651428\n",
      "Epoch 20, Batch 687, Loss: 0.25325924158096313\n",
      "Epoch 20, Batch 688, Loss: 0.4592144787311554\n",
      "Epoch 20, Batch 689, Loss: 0.3679822087287903\n",
      "Epoch 20, Batch 690, Loss: 0.3763328194618225\n",
      "Epoch 20, Batch 691, Loss: 0.49643731117248535\n",
      "Epoch 20, Batch 692, Loss: 0.6178846955299377\n",
      "Epoch 20, Batch 693, Loss: 0.4723094701766968\n",
      "Epoch 20, Batch 694, Loss: 0.26725369691848755\n",
      "Epoch 20, Batch 695, Loss: 0.41356346011161804\n",
      "Epoch 20, Batch 696, Loss: 0.4180593490600586\n",
      "Epoch 20, Batch 697, Loss: 0.42996254563331604\n",
      "Epoch 20, Batch 698, Loss: 0.3580326735973358\n",
      "Epoch 20, Batch 699, Loss: 0.655509352684021\n",
      "Epoch 20, Batch 700, Loss: 0.13433150947093964\n",
      "Epoch 20, Batch 701, Loss: 0.5170100331306458\n",
      "Epoch 20, Batch 702, Loss: 0.40451323986053467\n",
      "Epoch 20, Batch 703, Loss: 0.44700887799263\n",
      "Epoch 20, Batch 704, Loss: 0.5557399988174438\n",
      "Epoch 20, Batch 705, Loss: 0.3669430613517761\n",
      "Epoch 20, Batch 706, Loss: 0.3961726725101471\n",
      "Epoch 20, Batch 707, Loss: 0.44799837470054626\n",
      "Epoch 20, Batch 708, Loss: 0.2521454989910126\n",
      "Epoch 20, Batch 709, Loss: 0.2977209985256195\n",
      "Epoch 20, Batch 710, Loss: 0.5879666805267334\n",
      "Epoch 20, Batch 711, Loss: 0.3661113381385803\n",
      "Epoch 20, Batch 712, Loss: 0.4212033152580261\n",
      "Epoch 20, Batch 713, Loss: 0.5751161575317383\n",
      "Epoch 20, Batch 714, Loss: 0.36687180399894714\n",
      "Epoch 20, Batch 715, Loss: 0.6769539713859558\n",
      "Epoch 20, Batch 716, Loss: 0.3427579700946808\n",
      "Epoch 20, Batch 717, Loss: 0.4428790509700775\n",
      "Epoch 20, Batch 718, Loss: 0.4087649881839752\n",
      "Epoch 20, Batch 719, Loss: 0.5510622262954712\n",
      "Epoch 20, Batch 720, Loss: 0.3281983435153961\n",
      "Epoch 20, Batch 721, Loss: 0.2790297567844391\n",
      "Epoch 20, Batch 722, Loss: 0.2799831032752991\n",
      "Epoch 20, Batch 723, Loss: 0.28815075755119324\n",
      "Epoch 20, Batch 724, Loss: 0.6203697919845581\n",
      "Epoch 20, Batch 725, Loss: 0.571410059928894\n",
      "Epoch 20, Batch 726, Loss: 0.3815975487232208\n",
      "Epoch 20, Batch 727, Loss: 0.29789453744888306\n",
      "Epoch 20, Batch 728, Loss: 0.506620466709137\n",
      "Epoch 20, Batch 729, Loss: 0.5169496536254883\n",
      "Epoch 20, Batch 730, Loss: 0.6649525761604309\n",
      "Epoch 20, Batch 731, Loss: 0.44330650568008423\n",
      "Epoch 20, Batch 732, Loss: 0.34521764516830444\n",
      "Epoch 20, Batch 733, Loss: 0.372701495885849\n",
      "Epoch 20, Batch 734, Loss: 0.41735130548477173\n",
      "Epoch 20, Batch 735, Loss: 0.36153656244277954\n",
      "Epoch 20, Batch 736, Loss: 0.6073289513587952\n",
      "Epoch 20, Batch 737, Loss: 0.4714973270893097\n",
      "Epoch 20, Batch 738, Loss: 0.5247430205345154\n",
      "Epoch 20, Batch 739, Loss: 0.43788906931877136\n",
      "Epoch 20, Batch 740, Loss: 0.4659308195114136\n",
      "Epoch 20, Batch 741, Loss: 0.3578720986843109\n",
      "Epoch 20, Batch 742, Loss: 0.3661019802093506\n",
      "Epoch 20, Batch 743, Loss: 0.28675317764282227\n",
      "Epoch 20, Batch 744, Loss: 0.37670642137527466\n",
      "Epoch 20, Batch 745, Loss: 0.4621778726577759\n",
      "Epoch 20, Batch 746, Loss: 0.2918976843357086\n",
      "Epoch 20, Batch 747, Loss: 0.40006598830223083\n",
      "Epoch 20, Batch 748, Loss: 0.39799630641937256\n",
      "Epoch 20, Batch 749, Loss: 0.34854656457901\n",
      "Epoch 20, Batch 750, Loss: 0.35210177302360535\n",
      "Epoch 20, Batch 751, Loss: 0.45758920907974243\n",
      "Epoch 20, Batch 752, Loss: 0.36428511142730713\n",
      "Epoch 20, Batch 753, Loss: 0.5382447838783264\n",
      "Epoch 20, Batch 754, Loss: 0.5503883361816406\n",
      "Epoch 20, Batch 755, Loss: 0.3621682822704315\n",
      "Epoch 20, Batch 756, Loss: 0.32600170373916626\n",
      "Epoch 20, Batch 757, Loss: 0.3828284740447998\n",
      "Epoch 20, Batch 758, Loss: 0.4127086102962494\n",
      "Epoch 20, Batch 759, Loss: 0.5612102150917053\n",
      "Epoch 20, Batch 760, Loss: 0.34296783804893494\n",
      "Epoch 20, Batch 761, Loss: 0.4537968039512634\n",
      "Epoch 20, Batch 762, Loss: 0.3020390570163727\n",
      "Epoch 20, Batch 763, Loss: 0.5349185466766357\n",
      "Epoch 20, Batch 764, Loss: 0.2507973909378052\n",
      "Epoch 20, Batch 765, Loss: 0.32064762711524963\n",
      "Epoch 20, Batch 766, Loss: 0.46647271513938904\n",
      "Epoch 20, Batch 767, Loss: 0.3359481692314148\n",
      "Epoch 20, Batch 768, Loss: 0.22748035192489624\n",
      "Epoch 20, Batch 769, Loss: 0.38204923272132874\n",
      "Epoch 20, Batch 770, Loss: 0.43737128376960754\n",
      "Epoch 20, Batch 771, Loss: 0.3404276371002197\n",
      "Epoch 20, Batch 772, Loss: 0.26841437816619873\n",
      "Epoch 20, Batch 773, Loss: 0.49915963411331177\n",
      "Epoch 20, Batch 774, Loss: 0.3500610888004303\n",
      "Epoch 20, Batch 775, Loss: 0.2653619349002838\n",
      "Epoch 20, Batch 776, Loss: 0.41490304470062256\n",
      "Epoch 20, Batch 777, Loss: 0.4859205186367035\n",
      "Epoch 20, Batch 778, Loss: 0.4522119462490082\n",
      "Epoch 20, Batch 779, Loss: 0.567940890789032\n",
      "Epoch 20, Batch 780, Loss: 0.49730154871940613\n",
      "Epoch 20, Batch 781, Loss: 0.5244840383529663\n",
      "Epoch 20, Batch 782, Loss: 0.41261106729507446\n",
      "Epoch 20, Batch 783, Loss: 0.3941998779773712\n",
      "Epoch 20, Batch 784, Loss: 0.3748604655265808\n",
      "Epoch 20, Batch 785, Loss: 0.5308773517608643\n",
      "Epoch 20, Batch 786, Loss: 0.349867045879364\n",
      "Epoch 20, Batch 787, Loss: 0.4004926085472107\n",
      "Epoch 20, Batch 788, Loss: 0.38038551807403564\n",
      "Epoch 20, Batch 789, Loss: 0.4369756579399109\n",
      "Epoch 20, Batch 790, Loss: 0.3089278042316437\n",
      "Epoch 20, Batch 791, Loss: 0.39225608110427856\n",
      "Epoch 20, Batch 792, Loss: 0.3270764648914337\n",
      "Epoch 20, Batch 793, Loss: 0.3939078450202942\n",
      "Epoch 20, Batch 794, Loss: 0.3101988434791565\n",
      "Epoch 20, Batch 795, Loss: 0.4440925419330597\n",
      "Epoch 20, Batch 796, Loss: 0.6414905786514282\n",
      "Epoch 20, Batch 797, Loss: 0.3131864368915558\n",
      "Epoch 20, Batch 798, Loss: 0.4392445683479309\n",
      "Epoch 20, Batch 799, Loss: 0.31617850065231323\n",
      "Epoch 20, Batch 800, Loss: 0.2068040519952774\n",
      "Epoch 20, Batch 801, Loss: 0.6045262217521667\n",
      "Epoch 20, Batch 802, Loss: 0.4225710928440094\n",
      "Epoch 20, Batch 803, Loss: 0.397106796503067\n",
      "Epoch 20, Batch 804, Loss: 0.43486228585243225\n",
      "Epoch 20, Batch 805, Loss: 0.3695574998855591\n",
      "Epoch 20, Batch 806, Loss: 0.30823156237602234\n",
      "Epoch 20, Batch 807, Loss: 0.38125818967819214\n",
      "Epoch 20, Batch 808, Loss: 0.37338727712631226\n",
      "Epoch 20, Batch 809, Loss: 0.5282692313194275\n",
      "Epoch 20, Batch 810, Loss: 0.3316308557987213\n",
      "Epoch 20, Batch 811, Loss: 0.32591748237609863\n",
      "Epoch 20, Batch 812, Loss: 0.2790718972682953\n",
      "Epoch 20, Batch 813, Loss: 0.5841182470321655\n",
      "Epoch 20, Batch 814, Loss: 0.35567784309387207\n",
      "Epoch 20, Batch 815, Loss: 0.4147856533527374\n",
      "Epoch 20, Batch 816, Loss: 0.733554482460022\n",
      "Epoch 20, Batch 817, Loss: 0.4097704589366913\n",
      "Epoch 20, Batch 818, Loss: 0.44097527861595154\n",
      "Epoch 20, Batch 819, Loss: 0.5398516058921814\n",
      "Epoch 20, Batch 820, Loss: 0.42390239238739014\n",
      "Epoch 20, Batch 821, Loss: 0.2741481065750122\n",
      "Epoch 20, Batch 822, Loss: 0.4710017442703247\n",
      "Epoch 20, Batch 823, Loss: 0.46838533878326416\n",
      "Epoch 20, Batch 824, Loss: 0.31111088395118713\n",
      "Epoch 20, Batch 825, Loss: 0.34210729598999023\n",
      "Epoch 20, Batch 826, Loss: 0.5167475938796997\n",
      "Epoch 20, Batch 827, Loss: 0.4790092706680298\n",
      "Epoch 20, Batch 828, Loss: 0.3206433951854706\n",
      "Epoch 20, Batch 829, Loss: 0.3640146255493164\n",
      "Epoch 20, Batch 830, Loss: 0.3400496244430542\n",
      "Epoch 20, Batch 831, Loss: 0.4152350127696991\n",
      "Epoch 20, Batch 832, Loss: 0.4381183981895447\n",
      "Epoch 20, Batch 833, Loss: 0.3422999083995819\n",
      "Epoch 20, Batch 834, Loss: 0.20480242371559143\n",
      "Epoch 20, Batch 835, Loss: 0.3766341507434845\n",
      "Epoch 20, Batch 836, Loss: 0.4877777695655823\n",
      "Epoch 20, Batch 837, Loss: 0.39312589168548584\n",
      "Epoch 20, Batch 838, Loss: 0.49921026825904846\n",
      "Epoch 20, Batch 839, Loss: 0.4446576237678528\n",
      "Epoch 20, Batch 840, Loss: 0.3198551535606384\n",
      "Epoch 20, Batch 841, Loss: 0.35605353116989136\n",
      "Epoch 20, Batch 842, Loss: 0.3586902618408203\n",
      "Epoch 20, Batch 843, Loss: 0.34774747490882874\n",
      "Epoch 20, Batch 844, Loss: 0.4395924508571625\n",
      "Epoch 20, Batch 845, Loss: 0.24002104997634888\n",
      "Epoch 20, Batch 846, Loss: 0.38926565647125244\n",
      "Epoch 20, Batch 847, Loss: 0.3622450828552246\n",
      "Epoch 20, Batch 848, Loss: 0.32851356267929077\n",
      "Epoch 20, Batch 849, Loss: 0.2298317849636078\n",
      "Epoch 20, Batch 850, Loss: 0.3856939375400543\n",
      "Epoch 20, Batch 851, Loss: 0.549599289894104\n",
      "Epoch 20, Batch 852, Loss: 0.3056598901748657\n",
      "Epoch 20, Batch 853, Loss: 0.25774404406547546\n",
      "Epoch 20, Batch 854, Loss: 0.3817569613456726\n",
      "Epoch 20, Batch 855, Loss: 0.36464810371398926\n",
      "Epoch 20, Batch 856, Loss: 0.4113442301750183\n",
      "Epoch 20, Batch 857, Loss: 0.34895288944244385\n",
      "Epoch 20, Batch 858, Loss: 0.2920636534690857\n",
      "Epoch 20, Batch 859, Loss: 0.23946261405944824\n",
      "Epoch 20, Batch 860, Loss: 0.3219459652900696\n",
      "Epoch 20, Batch 861, Loss: 0.4897049069404602\n",
      "Epoch 20, Batch 862, Loss: 0.28044259548187256\n",
      "Epoch 20, Batch 863, Loss: 0.3635452091693878\n",
      "Epoch 20, Batch 864, Loss: 0.27377188205718994\n",
      "Epoch 20, Batch 865, Loss: 0.4854164719581604\n",
      "Epoch 20, Batch 866, Loss: 0.3873727321624756\n",
      "Epoch 20, Batch 867, Loss: 0.4037126898765564\n",
      "Epoch 20, Batch 868, Loss: 0.2939048409461975\n",
      "Epoch 20, Batch 869, Loss: 0.4760737419128418\n",
      "Epoch 20, Batch 870, Loss: 0.46275046467781067\n",
      "Epoch 20, Batch 871, Loss: 0.22463344037532806\n",
      "Epoch 20, Batch 872, Loss: 0.5740214586257935\n",
      "Epoch 20, Batch 873, Loss: 0.27665385603904724\n",
      "Epoch 20, Batch 874, Loss: 0.36039143800735474\n",
      "Epoch 20, Batch 875, Loss: 0.5646260380744934\n",
      "Epoch 20, Batch 876, Loss: 0.5927194356918335\n",
      "Epoch 20, Batch 877, Loss: 0.4053431451320648\n",
      "Epoch 20, Batch 878, Loss: 0.31773895025253296\n",
      "Epoch 20, Batch 879, Loss: 0.5545381307601929\n",
      "Epoch 20, Batch 880, Loss: 0.4518459439277649\n",
      "Epoch 20, Batch 881, Loss: 0.44563546776771545\n",
      "Epoch 20, Batch 882, Loss: 0.35164645314216614\n",
      "Epoch 20, Batch 883, Loss: 0.3306156098842621\n",
      "Epoch 20, Batch 884, Loss: 0.36661630868911743\n",
      "Epoch 20, Batch 885, Loss: 0.36017045378685\n",
      "Epoch 20, Batch 886, Loss: 0.4655725955963135\n",
      "Epoch 20, Batch 887, Loss: 0.3338935077190399\n",
      "Epoch 20, Batch 888, Loss: 0.5672430992126465\n",
      "Epoch 20, Batch 889, Loss: 0.497806578874588\n",
      "Epoch 20, Batch 890, Loss: 0.43983176350593567\n",
      "Epoch 20, Batch 891, Loss: 0.3060654401779175\n",
      "Epoch 20, Batch 892, Loss: 0.5519979596138\n",
      "Epoch 20, Batch 893, Loss: 0.3565968871116638\n",
      "Epoch 20, Batch 894, Loss: 0.3536373972892761\n",
      "Epoch 20, Batch 895, Loss: 0.37936121225357056\n",
      "Epoch 20, Batch 896, Loss: 0.6073071360588074\n",
      "Epoch 20, Batch 897, Loss: 0.6351016759872437\n",
      "Epoch 20, Batch 898, Loss: 0.25828224420547485\n",
      "Epoch 20, Batch 899, Loss: 0.3579346537590027\n",
      "Epoch 20, Batch 900, Loss: 0.3999846577644348\n",
      "Epoch 20, Batch 901, Loss: 0.322003036737442\n",
      "Epoch 20, Batch 902, Loss: 0.2998849153518677\n",
      "Epoch 20, Batch 903, Loss: 0.4852336347103119\n",
      "Epoch 20, Batch 904, Loss: 0.6051520705223083\n",
      "Epoch 20, Batch 905, Loss: 0.5050968527793884\n",
      "Epoch 20, Batch 906, Loss: 0.2919531762599945\n",
      "Epoch 20, Batch 907, Loss: 0.4240390658378601\n",
      "Epoch 20, Batch 908, Loss: 0.33931636810302734\n",
      "Epoch 20, Batch 909, Loss: 0.5400731563568115\n",
      "Epoch 20, Batch 910, Loss: 0.5410060286521912\n",
      "Epoch 20, Batch 911, Loss: 0.4542405605316162\n",
      "Epoch 20, Batch 912, Loss: 0.25826799869537354\n",
      "Epoch 20, Batch 913, Loss: 0.40071091055870056\n",
      "Epoch 20, Batch 914, Loss: 0.2884337902069092\n",
      "Epoch 20, Batch 915, Loss: 0.43679293990135193\n",
      "Epoch 20, Batch 916, Loss: 0.3930678963661194\n",
      "Epoch 20, Batch 917, Loss: 0.3566063940525055\n",
      "Epoch 20, Batch 918, Loss: 0.35840731859207153\n",
      "Epoch 20, Batch 919, Loss: 0.5190150737762451\n",
      "Epoch 20, Batch 920, Loss: 0.42871129512786865\n",
      "Epoch 20, Batch 921, Loss: 0.49980923533439636\n",
      "Epoch 20, Batch 922, Loss: 0.27239301800727844\n",
      "Epoch 20, Batch 923, Loss: 0.2713991105556488\n",
      "Epoch 20, Batch 924, Loss: 0.23273319005966187\n",
      "Epoch 20, Batch 925, Loss: 0.4328968822956085\n",
      "Epoch 20, Batch 926, Loss: 0.3297024369239807\n",
      "Epoch 20, Batch 927, Loss: 0.2960546016693115\n",
      "Epoch 20, Batch 928, Loss: 0.5817151069641113\n",
      "Epoch 20, Batch 929, Loss: 0.30292758345603943\n",
      "Epoch 20, Batch 930, Loss: 0.353595495223999\n",
      "Epoch 20, Batch 931, Loss: 0.3729810118675232\n",
      "Epoch 20, Batch 932, Loss: 0.4280936121940613\n",
      "Epoch 20, Batch 933, Loss: 0.5165901184082031\n",
      "Epoch 20, Batch 934, Loss: 0.43062856793403625\n",
      "Epoch 20, Batch 935, Loss: 0.6306636929512024\n",
      "Epoch 20, Batch 936, Loss: 0.5047515630722046\n",
      "Epoch 20, Batch 937, Loss: 0.3476144075393677\n",
      "Epoch 20, Batch 938, Loss: 0.412173330783844\n",
      "Accuracy of train set: 0.85505\n",
      "Epoch 20, Batch 1, Test Loss: 0.43665048480033875\n",
      "Epoch 20, Batch 2, Test Loss: 0.38361772894859314\n",
      "Epoch 20, Batch 3, Test Loss: 0.47017428278923035\n",
      "Epoch 20, Batch 4, Test Loss: 0.5743240118026733\n",
      "Epoch 20, Batch 5, Test Loss: 0.5167743563652039\n",
      "Epoch 20, Batch 6, Test Loss: 0.1732301265001297\n",
      "Epoch 20, Batch 7, Test Loss: 0.3894420862197876\n",
      "Epoch 20, Batch 8, Test Loss: 0.4870879054069519\n",
      "Epoch 20, Batch 9, Test Loss: 0.32625702023506165\n",
      "Epoch 20, Batch 10, Test Loss: 0.4214983880519867\n",
      "Epoch 20, Batch 11, Test Loss: 0.5161498188972473\n",
      "Epoch 20, Batch 12, Test Loss: 0.25187766551971436\n",
      "Epoch 20, Batch 13, Test Loss: 0.3957676589488983\n",
      "Epoch 20, Batch 14, Test Loss: 0.37090152502059937\n",
      "Epoch 20, Batch 15, Test Loss: 0.5003365278244019\n",
      "Epoch 20, Batch 16, Test Loss: 0.5391943454742432\n",
      "Epoch 20, Batch 17, Test Loss: 0.3918169438838959\n",
      "Epoch 20, Batch 18, Test Loss: 0.3208601474761963\n",
      "Epoch 20, Batch 19, Test Loss: 0.3704177439212799\n",
      "Epoch 20, Batch 20, Test Loss: 0.4793720245361328\n",
      "Epoch 20, Batch 21, Test Loss: 0.2992532551288605\n",
      "Epoch 20, Batch 22, Test Loss: 0.7038071155548096\n",
      "Epoch 20, Batch 23, Test Loss: 0.24806776642799377\n",
      "Epoch 20, Batch 24, Test Loss: 0.34200340509414673\n",
      "Epoch 20, Batch 25, Test Loss: 0.3925572633743286\n",
      "Epoch 20, Batch 26, Test Loss: 0.29965242743492126\n",
      "Epoch 20, Batch 27, Test Loss: 0.48643168807029724\n",
      "Epoch 20, Batch 28, Test Loss: 0.27965500950813293\n",
      "Epoch 20, Batch 29, Test Loss: 0.22218231856822968\n",
      "Epoch 20, Batch 30, Test Loss: 0.46811017394065857\n",
      "Epoch 20, Batch 31, Test Loss: 0.38604727387428284\n",
      "Epoch 20, Batch 32, Test Loss: 0.3354935646057129\n",
      "Epoch 20, Batch 33, Test Loss: 0.42349904775619507\n",
      "Epoch 20, Batch 34, Test Loss: 0.33289095759391785\n",
      "Epoch 20, Batch 35, Test Loss: 0.47237586975097656\n",
      "Epoch 20, Batch 36, Test Loss: 0.4197201132774353\n",
      "Epoch 20, Batch 37, Test Loss: 0.33842408657073975\n",
      "Epoch 20, Batch 38, Test Loss: 0.6023187041282654\n",
      "Epoch 20, Batch 39, Test Loss: 0.47812172770500183\n",
      "Epoch 20, Batch 40, Test Loss: 0.4258185625076294\n",
      "Epoch 20, Batch 41, Test Loss: 0.3096238970756531\n",
      "Epoch 20, Batch 42, Test Loss: 0.4002337157726288\n",
      "Epoch 20, Batch 43, Test Loss: 0.4053953289985657\n",
      "Epoch 20, Batch 44, Test Loss: 0.48934707045555115\n",
      "Epoch 20, Batch 45, Test Loss: 0.42089253664016724\n",
      "Epoch 20, Batch 46, Test Loss: 0.18706440925598145\n",
      "Epoch 20, Batch 47, Test Loss: 0.4467412233352661\n",
      "Epoch 20, Batch 48, Test Loss: 0.41861477494239807\n",
      "Epoch 20, Batch 49, Test Loss: 0.5055599808692932\n",
      "Epoch 20, Batch 50, Test Loss: 0.3585038483142853\n",
      "Epoch 20, Batch 51, Test Loss: 0.5946605205535889\n",
      "Epoch 20, Batch 52, Test Loss: 0.5484597682952881\n",
      "Epoch 20, Batch 53, Test Loss: 0.41597849130630493\n",
      "Epoch 20, Batch 54, Test Loss: 0.41772082448005676\n",
      "Epoch 20, Batch 55, Test Loss: 0.28992733359336853\n",
      "Epoch 20, Batch 56, Test Loss: 0.47130706906318665\n",
      "Epoch 20, Batch 57, Test Loss: 0.37321263551712036\n",
      "Epoch 20, Batch 58, Test Loss: 0.35395026206970215\n",
      "Epoch 20, Batch 59, Test Loss: 0.5419473648071289\n",
      "Epoch 20, Batch 60, Test Loss: 0.39673423767089844\n",
      "Epoch 20, Batch 61, Test Loss: 0.29357022047042847\n",
      "Epoch 20, Batch 62, Test Loss: 0.3101564943790436\n",
      "Epoch 20, Batch 63, Test Loss: 0.444993257522583\n",
      "Epoch 20, Batch 64, Test Loss: 0.33987778425216675\n",
      "Epoch 20, Batch 65, Test Loss: 0.33762127161026\n",
      "Epoch 20, Batch 66, Test Loss: 0.4906001091003418\n",
      "Epoch 20, Batch 67, Test Loss: 0.3997191786766052\n",
      "Epoch 20, Batch 68, Test Loss: 0.43649616837501526\n",
      "Epoch 20, Batch 69, Test Loss: 0.31762439012527466\n",
      "Epoch 20, Batch 70, Test Loss: 0.5459601283073425\n",
      "Epoch 20, Batch 71, Test Loss: 0.3973480761051178\n",
      "Epoch 20, Batch 72, Test Loss: 0.3978569209575653\n",
      "Epoch 20, Batch 73, Test Loss: 0.3901481330394745\n",
      "Epoch 20, Batch 74, Test Loss: 0.32742318511009216\n",
      "Epoch 20, Batch 75, Test Loss: 0.48674899339675903\n",
      "Epoch 20, Batch 76, Test Loss: 0.4100419580936432\n",
      "Epoch 20, Batch 77, Test Loss: 0.43832868337631226\n",
      "Epoch 20, Batch 78, Test Loss: 0.4104764759540558\n",
      "Epoch 20, Batch 79, Test Loss: 0.4028361141681671\n",
      "Epoch 20, Batch 80, Test Loss: 0.5302734971046448\n",
      "Epoch 20, Batch 81, Test Loss: 0.5435153245925903\n",
      "Epoch 20, Batch 82, Test Loss: 0.4140320420265198\n",
      "Epoch 20, Batch 83, Test Loss: 0.39149966835975647\n",
      "Epoch 20, Batch 84, Test Loss: 0.41131171584129333\n",
      "Epoch 20, Batch 85, Test Loss: 0.28868040442466736\n",
      "Epoch 20, Batch 86, Test Loss: 0.37089723348617554\n",
      "Epoch 20, Batch 87, Test Loss: 0.39184021949768066\n",
      "Epoch 20, Batch 88, Test Loss: 0.42331820726394653\n",
      "Epoch 20, Batch 89, Test Loss: 0.45012035965919495\n",
      "Epoch 20, Batch 90, Test Loss: 0.3623402714729309\n",
      "Epoch 20, Batch 91, Test Loss: 0.43295320868492126\n",
      "Epoch 20, Batch 92, Test Loss: 0.4030514657497406\n",
      "Epoch 20, Batch 93, Test Loss: 0.4228443503379822\n",
      "Epoch 20, Batch 94, Test Loss: 0.4815097451210022\n",
      "Epoch 20, Batch 95, Test Loss: 0.31780630350112915\n",
      "Epoch 20, Batch 96, Test Loss: 0.5373092293739319\n",
      "Epoch 20, Batch 97, Test Loss: 0.47375282645225525\n",
      "Epoch 20, Batch 98, Test Loss: 0.40936294198036194\n",
      "Epoch 20, Batch 99, Test Loss: 0.32395094633102417\n",
      "Epoch 20, Batch 100, Test Loss: 0.34130939841270447\n",
      "Epoch 20, Batch 101, Test Loss: 0.4723146855831146\n",
      "Epoch 20, Batch 102, Test Loss: 0.4207497537136078\n",
      "Epoch 20, Batch 103, Test Loss: 0.4738173484802246\n",
      "Epoch 20, Batch 104, Test Loss: 0.34361645579338074\n",
      "Epoch 20, Batch 105, Test Loss: 0.45299237966537476\n",
      "Epoch 20, Batch 106, Test Loss: 0.5661056041717529\n",
      "Epoch 20, Batch 107, Test Loss: 0.3436693847179413\n",
      "Epoch 20, Batch 108, Test Loss: 0.39314693212509155\n",
      "Epoch 20, Batch 109, Test Loss: 0.4155356287956238\n",
      "Epoch 20, Batch 110, Test Loss: 0.487606406211853\n",
      "Epoch 20, Batch 111, Test Loss: 0.5762157440185547\n",
      "Epoch 20, Batch 112, Test Loss: 0.30554550886154175\n",
      "Epoch 20, Batch 113, Test Loss: 0.3754707872867584\n",
      "Epoch 20, Batch 114, Test Loss: 0.5423387289047241\n",
      "Epoch 20, Batch 115, Test Loss: 0.36427056789398193\n",
      "Epoch 20, Batch 116, Test Loss: 0.31307581067085266\n",
      "Epoch 20, Batch 117, Test Loss: 0.29282107949256897\n",
      "Epoch 20, Batch 118, Test Loss: 0.3479602634906769\n",
      "Epoch 20, Batch 119, Test Loss: 0.5367590188980103\n",
      "Epoch 20, Batch 120, Test Loss: 0.38394489884376526\n",
      "Epoch 20, Batch 121, Test Loss: 0.35129857063293457\n",
      "Epoch 20, Batch 122, Test Loss: 0.4907850921154022\n",
      "Epoch 20, Batch 123, Test Loss: 0.5472172498703003\n",
      "Epoch 20, Batch 124, Test Loss: 0.3802598714828491\n",
      "Epoch 20, Batch 125, Test Loss: 0.4696677327156067\n",
      "Epoch 20, Batch 126, Test Loss: 0.48941105604171753\n",
      "Epoch 20, Batch 127, Test Loss: 0.3215506970882416\n",
      "Epoch 20, Batch 128, Test Loss: 0.2908332347869873\n",
      "Epoch 20, Batch 129, Test Loss: 0.37069424986839294\n",
      "Epoch 20, Batch 130, Test Loss: 0.45729658007621765\n",
      "Epoch 20, Batch 131, Test Loss: 0.3262943625450134\n",
      "Epoch 20, Batch 132, Test Loss: 0.44533562660217285\n",
      "Epoch 20, Batch 133, Test Loss: 0.3955085873603821\n",
      "Epoch 20, Batch 134, Test Loss: 0.36766815185546875\n",
      "Epoch 20, Batch 135, Test Loss: 0.3869950473308563\n",
      "Epoch 20, Batch 136, Test Loss: 0.4181850850582123\n",
      "Epoch 20, Batch 137, Test Loss: 0.3347005248069763\n",
      "Epoch 20, Batch 138, Test Loss: 0.5520222187042236\n",
      "Epoch 20, Batch 139, Test Loss: 0.3413114547729492\n",
      "Epoch 20, Batch 140, Test Loss: 0.4594680666923523\n",
      "Epoch 20, Batch 141, Test Loss: 0.5574755668640137\n",
      "Epoch 20, Batch 142, Test Loss: 0.43396851420402527\n",
      "Epoch 20, Batch 143, Test Loss: 0.6128591895103455\n",
      "Epoch 20, Batch 144, Test Loss: 0.528690755367279\n",
      "Epoch 20, Batch 145, Test Loss: 0.5068680644035339\n",
      "Epoch 20, Batch 146, Test Loss: 0.3838912546634674\n",
      "Epoch 20, Batch 147, Test Loss: 0.5642850399017334\n",
      "Epoch 20, Batch 148, Test Loss: 0.5520948171615601\n",
      "Epoch 20, Batch 149, Test Loss: 0.3852965831756592\n",
      "Epoch 20, Batch 150, Test Loss: 0.27295058965682983\n",
      "Epoch 20, Batch 151, Test Loss: 0.8361930251121521\n",
      "Epoch 20, Batch 152, Test Loss: 0.40138813853263855\n",
      "Epoch 20, Batch 153, Test Loss: 0.25596675276756287\n",
      "Epoch 20, Batch 154, Test Loss: 0.39854615926742554\n",
      "Epoch 20, Batch 155, Test Loss: 0.34639763832092285\n",
      "Epoch 20, Batch 156, Test Loss: 0.23253658413887024\n",
      "Epoch 20, Batch 157, Test Loss: 0.39225083589553833\n",
      "Epoch 20, Batch 158, Test Loss: 0.48947957158088684\n",
      "Epoch 20, Batch 159, Test Loss: 0.3512832522392273\n",
      "Epoch 20, Batch 160, Test Loss: 0.3741466999053955\n",
      "Epoch 20, Batch 161, Test Loss: 0.3853352963924408\n",
      "Epoch 20, Batch 162, Test Loss: 0.47411784529685974\n",
      "Epoch 20, Batch 163, Test Loss: 0.36358535289764404\n",
      "Epoch 20, Batch 164, Test Loss: 0.28179866075515747\n",
      "Epoch 20, Batch 165, Test Loss: 0.2579978406429291\n",
      "Epoch 20, Batch 166, Test Loss: 0.6401256918907166\n",
      "Epoch 20, Batch 167, Test Loss: 0.47069114446640015\n",
      "Epoch 20, Batch 168, Test Loss: 0.46524617075920105\n",
      "Epoch 20, Batch 169, Test Loss: 0.2388143539428711\n",
      "Epoch 20, Batch 170, Test Loss: 0.399834007024765\n",
      "Epoch 20, Batch 171, Test Loss: 0.374563992023468\n",
      "Epoch 20, Batch 172, Test Loss: 0.4396249055862427\n",
      "Epoch 20, Batch 173, Test Loss: 0.5796616077423096\n",
      "Epoch 20, Batch 174, Test Loss: 0.24223962426185608\n",
      "Epoch 20, Batch 175, Test Loss: 0.5595328211784363\n",
      "Epoch 20, Batch 176, Test Loss: 0.25565657019615173\n",
      "Epoch 20, Batch 177, Test Loss: 0.28175151348114014\n",
      "Epoch 20, Batch 178, Test Loss: 0.4677351713180542\n",
      "Epoch 20, Batch 179, Test Loss: 0.397210955619812\n",
      "Epoch 20, Batch 180, Test Loss: 0.5781728029251099\n",
      "Epoch 20, Batch 181, Test Loss: 0.36817488074302673\n",
      "Epoch 20, Batch 182, Test Loss: 0.3792971670627594\n",
      "Epoch 20, Batch 183, Test Loss: 0.3971284031867981\n",
      "Epoch 20, Batch 184, Test Loss: 0.48913347721099854\n",
      "Epoch 20, Batch 185, Test Loss: 0.3037184178829193\n",
      "Epoch 20, Batch 186, Test Loss: 0.3012990355491638\n",
      "Epoch 20, Batch 187, Test Loss: 0.5952568650245667\n",
      "Epoch 20, Batch 188, Test Loss: 0.6886399984359741\n",
      "Epoch 20, Batch 189, Test Loss: 0.4189855456352234\n",
      "Epoch 20, Batch 190, Test Loss: 0.3288418650627136\n",
      "Epoch 20, Batch 191, Test Loss: 0.3071410357952118\n",
      "Epoch 20, Batch 192, Test Loss: 0.4489474892616272\n",
      "Epoch 20, Batch 193, Test Loss: 0.44688111543655396\n",
      "Epoch 20, Batch 194, Test Loss: 0.39774543046951294\n",
      "Epoch 20, Batch 195, Test Loss: 0.48823848366737366\n",
      "Epoch 20, Batch 196, Test Loss: 0.3780844807624817\n",
      "Epoch 20, Batch 197, Test Loss: 0.25546348094940186\n",
      "Epoch 20, Batch 198, Test Loss: 0.6705542802810669\n",
      "Epoch 20, Batch 199, Test Loss: 0.3185505270957947\n",
      "Epoch 20, Batch 200, Test Loss: 0.34960150718688965\n",
      "Epoch 20, Batch 201, Test Loss: 0.4293256103992462\n",
      "Epoch 20, Batch 202, Test Loss: 0.38142114877700806\n",
      "Epoch 20, Batch 203, Test Loss: 0.5336467623710632\n",
      "Epoch 20, Batch 204, Test Loss: 0.3851017951965332\n",
      "Epoch 20, Batch 205, Test Loss: 0.2979702949523926\n",
      "Epoch 20, Batch 206, Test Loss: 0.27935710549354553\n",
      "Epoch 20, Batch 207, Test Loss: 0.4236558973789215\n",
      "Epoch 20, Batch 208, Test Loss: 0.3212401866912842\n",
      "Epoch 20, Batch 209, Test Loss: 0.3494543731212616\n",
      "Epoch 20, Batch 210, Test Loss: 0.3615151643753052\n",
      "Epoch 20, Batch 211, Test Loss: 0.35998502373695374\n",
      "Epoch 20, Batch 212, Test Loss: 0.4092598557472229\n",
      "Epoch 20, Batch 213, Test Loss: 0.30953967571258545\n",
      "Epoch 20, Batch 214, Test Loss: 0.4620729088783264\n",
      "Epoch 20, Batch 215, Test Loss: 0.5994749069213867\n",
      "Epoch 20, Batch 216, Test Loss: 0.5147910118103027\n",
      "Epoch 20, Batch 217, Test Loss: 0.3216250538825989\n",
      "Epoch 20, Batch 218, Test Loss: 0.32687461376190186\n",
      "Epoch 20, Batch 219, Test Loss: 0.44855549931526184\n",
      "Epoch 20, Batch 220, Test Loss: 0.5336833000183105\n",
      "Epoch 20, Batch 221, Test Loss: 0.19537532329559326\n",
      "Epoch 20, Batch 222, Test Loss: 0.3613399565219879\n",
      "Epoch 20, Batch 223, Test Loss: 0.46092554926872253\n",
      "Epoch 20, Batch 224, Test Loss: 0.45128607749938965\n",
      "Epoch 20, Batch 225, Test Loss: 0.45100587606430054\n",
      "Epoch 20, Batch 226, Test Loss: 0.3961934447288513\n",
      "Epoch 20, Batch 227, Test Loss: 0.46947652101516724\n",
      "Epoch 20, Batch 228, Test Loss: 0.41240155696868896\n",
      "Epoch 20, Batch 229, Test Loss: 0.4935252368450165\n",
      "Epoch 20, Batch 230, Test Loss: 0.41522157192230225\n",
      "Epoch 20, Batch 231, Test Loss: 0.2362394630908966\n",
      "Epoch 20, Batch 232, Test Loss: 0.28640657663345337\n",
      "Epoch 20, Batch 233, Test Loss: 0.4010149836540222\n",
      "Epoch 20, Batch 234, Test Loss: 0.41476184129714966\n",
      "Epoch 20, Batch 235, Test Loss: 0.5246075391769409\n",
      "Epoch 20, Batch 236, Test Loss: 0.3437713086605072\n",
      "Epoch 20, Batch 237, Test Loss: 0.6124283671379089\n",
      "Epoch 20, Batch 238, Test Loss: 0.40936312079429626\n",
      "Epoch 20, Batch 239, Test Loss: 0.45474690198898315\n",
      "Epoch 20, Batch 240, Test Loss: 0.5631716251373291\n",
      "Epoch 20, Batch 241, Test Loss: 0.6226953864097595\n",
      "Epoch 20, Batch 242, Test Loss: 0.3679302930831909\n",
      "Epoch 20, Batch 243, Test Loss: 0.6140429377555847\n",
      "Epoch 20, Batch 244, Test Loss: 0.4925724267959595\n",
      "Epoch 20, Batch 245, Test Loss: 0.489159494638443\n",
      "Epoch 20, Batch 246, Test Loss: 0.5203744173049927\n",
      "Epoch 20, Batch 247, Test Loss: 0.4015190601348877\n",
      "Epoch 20, Batch 248, Test Loss: 0.4401596188545227\n",
      "Epoch 20, Batch 249, Test Loss: 0.3025197386741638\n",
      "Epoch 20, Batch 250, Test Loss: 0.33113402128219604\n",
      "Epoch 20, Batch 251, Test Loss: 0.3067510724067688\n",
      "Epoch 20, Batch 252, Test Loss: 0.40338680148124695\n",
      "Epoch 20, Batch 253, Test Loss: 0.37053096294403076\n",
      "Epoch 20, Batch 254, Test Loss: 0.2661131024360657\n",
      "Epoch 20, Batch 255, Test Loss: 0.5221968293190002\n",
      "Epoch 20, Batch 256, Test Loss: 0.3187062442302704\n",
      "Epoch 20, Batch 257, Test Loss: 0.495047390460968\n",
      "Epoch 20, Batch 258, Test Loss: 0.4723614454269409\n",
      "Epoch 20, Batch 259, Test Loss: 0.46950864791870117\n",
      "Epoch 20, Batch 260, Test Loss: 0.43052810430526733\n",
      "Epoch 20, Batch 261, Test Loss: 0.2976609766483307\n",
      "Epoch 20, Batch 262, Test Loss: 0.5599732995033264\n",
      "Epoch 20, Batch 263, Test Loss: 0.4965707063674927\n",
      "Epoch 20, Batch 264, Test Loss: 0.35041189193725586\n",
      "Epoch 20, Batch 265, Test Loss: 0.3552612364292145\n",
      "Epoch 20, Batch 266, Test Loss: 0.4696677029132843\n",
      "Epoch 20, Batch 267, Test Loss: 0.47594425082206726\n",
      "Epoch 20, Batch 268, Test Loss: 0.5707554221153259\n",
      "Epoch 20, Batch 269, Test Loss: 0.25466281175613403\n",
      "Epoch 20, Batch 270, Test Loss: 0.3452352285385132\n",
      "Epoch 20, Batch 271, Test Loss: 0.35916662216186523\n",
      "Epoch 20, Batch 272, Test Loss: 0.43864086270332336\n",
      "Epoch 20, Batch 273, Test Loss: 0.40934306383132935\n",
      "Epoch 20, Batch 274, Test Loss: 0.4198441803455353\n",
      "Epoch 20, Batch 275, Test Loss: 0.5113996267318726\n",
      "Epoch 20, Batch 276, Test Loss: 0.3229230046272278\n",
      "Epoch 20, Batch 277, Test Loss: 0.5007969737052917\n",
      "Epoch 20, Batch 278, Test Loss: 0.3782012462615967\n",
      "Epoch 20, Batch 279, Test Loss: 0.33515122532844543\n",
      "Epoch 20, Batch 280, Test Loss: 0.3192918598651886\n",
      "Epoch 20, Batch 281, Test Loss: 0.4381563067436218\n",
      "Epoch 20, Batch 282, Test Loss: 0.4917643964290619\n",
      "Epoch 20, Batch 283, Test Loss: 0.5605734586715698\n",
      "Epoch 20, Batch 284, Test Loss: 0.3173755705356598\n",
      "Epoch 20, Batch 285, Test Loss: 0.4635515511035919\n",
      "Epoch 20, Batch 286, Test Loss: 0.2372988760471344\n",
      "Epoch 20, Batch 287, Test Loss: 0.318019837141037\n",
      "Epoch 20, Batch 288, Test Loss: 0.4906911849975586\n",
      "Epoch 20, Batch 289, Test Loss: 0.3107883334159851\n",
      "Epoch 20, Batch 290, Test Loss: 0.585334300994873\n",
      "Epoch 20, Batch 291, Test Loss: 0.6596879959106445\n",
      "Epoch 20, Batch 292, Test Loss: 0.4999654293060303\n",
      "Epoch 20, Batch 293, Test Loss: 0.3833868205547333\n",
      "Epoch 20, Batch 294, Test Loss: 0.4909842610359192\n",
      "Epoch 20, Batch 295, Test Loss: 0.36478596925735474\n",
      "Epoch 20, Batch 296, Test Loss: 0.3302134573459625\n",
      "Epoch 20, Batch 297, Test Loss: 0.6144458651542664\n",
      "Epoch 20, Batch 298, Test Loss: 0.2909233570098877\n",
      "Epoch 20, Batch 299, Test Loss: 0.7245166301727295\n",
      "Epoch 20, Batch 300, Test Loss: 0.5419385433197021\n",
      "Epoch 20, Batch 301, Test Loss: 0.3606794476509094\n",
      "Epoch 20, Batch 302, Test Loss: 0.70174241065979\n",
      "Epoch 20, Batch 303, Test Loss: 0.43136027455329895\n",
      "Epoch 20, Batch 304, Test Loss: 0.40777650475502014\n",
      "Epoch 20, Batch 305, Test Loss: 0.314138799905777\n",
      "Epoch 20, Batch 306, Test Loss: 0.6967622637748718\n",
      "Epoch 20, Batch 307, Test Loss: 0.3784535527229309\n",
      "Epoch 20, Batch 308, Test Loss: 0.4894257187843323\n",
      "Epoch 20, Batch 309, Test Loss: 0.3737855553627014\n",
      "Epoch 20, Batch 310, Test Loss: 0.4310027062892914\n",
      "Epoch 20, Batch 311, Test Loss: 0.4569912850856781\n",
      "Epoch 20, Batch 312, Test Loss: 0.3500504493713379\n",
      "Epoch 20, Batch 313, Test Loss: 0.3892708718776703\n",
      "Epoch 20, Batch 314, Test Loss: 0.5707324147224426\n",
      "Epoch 20, Batch 315, Test Loss: 0.41257280111312866\n",
      "Epoch 20, Batch 316, Test Loss: 0.4411693811416626\n",
      "Epoch 20, Batch 317, Test Loss: 0.44528478384017944\n",
      "Epoch 20, Batch 318, Test Loss: 0.3296085596084595\n",
      "Epoch 20, Batch 319, Test Loss: 0.5895415544509888\n",
      "Epoch 20, Batch 320, Test Loss: 0.3379397690296173\n",
      "Epoch 20, Batch 321, Test Loss: 0.30403298139572144\n",
      "Epoch 20, Batch 322, Test Loss: 0.3701741397380829\n",
      "Epoch 20, Batch 323, Test Loss: 0.3780578672885895\n",
      "Epoch 20, Batch 324, Test Loss: 0.4464256465435028\n",
      "Epoch 20, Batch 325, Test Loss: 0.3837323784828186\n",
      "Epoch 20, Batch 326, Test Loss: 0.3930385112762451\n",
      "Epoch 20, Batch 327, Test Loss: 0.33987951278686523\n",
      "Epoch 20, Batch 328, Test Loss: 0.38930898904800415\n",
      "Epoch 20, Batch 329, Test Loss: 0.4238051176071167\n",
      "Epoch 20, Batch 330, Test Loss: 0.3300900161266327\n",
      "Epoch 20, Batch 331, Test Loss: 0.4381347596645355\n",
      "Epoch 20, Batch 332, Test Loss: 0.3840920329093933\n",
      "Epoch 20, Batch 333, Test Loss: 0.5255618691444397\n",
      "Epoch 20, Batch 334, Test Loss: 0.5341760516166687\n",
      "Epoch 20, Batch 335, Test Loss: 0.3113905191421509\n",
      "Epoch 20, Batch 336, Test Loss: 0.37338271737098694\n",
      "Epoch 20, Batch 337, Test Loss: 0.37721604108810425\n",
      "Epoch 20, Batch 338, Test Loss: 0.4509359300136566\n",
      "Epoch 20, Batch 339, Test Loss: 0.2962506413459778\n",
      "Epoch 20, Batch 340, Test Loss: 0.4539034366607666\n",
      "Epoch 20, Batch 341, Test Loss: 0.4895085096359253\n",
      "Epoch 20, Batch 342, Test Loss: 0.2872035801410675\n",
      "Epoch 20, Batch 343, Test Loss: 0.25377896428108215\n",
      "Epoch 20, Batch 344, Test Loss: 0.4026428163051605\n",
      "Epoch 20, Batch 345, Test Loss: 0.3606569170951843\n",
      "Epoch 20, Batch 346, Test Loss: 0.4926128089427948\n",
      "Epoch 20, Batch 347, Test Loss: 0.3402830958366394\n",
      "Epoch 20, Batch 348, Test Loss: 0.26665371656417847\n",
      "Epoch 20, Batch 349, Test Loss: 0.4423304498195648\n",
      "Epoch 20, Batch 350, Test Loss: 0.34418100118637085\n",
      "Epoch 20, Batch 351, Test Loss: 0.45334750413894653\n",
      "Epoch 20, Batch 352, Test Loss: 0.6527814269065857\n",
      "Epoch 20, Batch 353, Test Loss: 0.6200548410415649\n",
      "Epoch 20, Batch 354, Test Loss: 0.5587143301963806\n",
      "Epoch 20, Batch 355, Test Loss: 0.5136168003082275\n",
      "Epoch 20, Batch 356, Test Loss: 0.4285942018032074\n",
      "Epoch 20, Batch 357, Test Loss: 0.4900282025337219\n",
      "Epoch 20, Batch 358, Test Loss: 0.38514962792396545\n",
      "Epoch 20, Batch 359, Test Loss: 0.5090186595916748\n",
      "Epoch 20, Batch 360, Test Loss: 0.2592637538909912\n",
      "Epoch 20, Batch 361, Test Loss: 0.45961061120033264\n",
      "Epoch 20, Batch 362, Test Loss: 0.39455899596214294\n",
      "Epoch 20, Batch 363, Test Loss: 0.4606734812259674\n",
      "Epoch 20, Batch 364, Test Loss: 0.43292471766471863\n",
      "Epoch 20, Batch 365, Test Loss: 0.5642824172973633\n",
      "Epoch 20, Batch 366, Test Loss: 0.4517945647239685\n",
      "Epoch 20, Batch 367, Test Loss: 0.370788037776947\n",
      "Epoch 20, Batch 368, Test Loss: 0.19527249038219452\n",
      "Epoch 20, Batch 369, Test Loss: 0.3061915934085846\n",
      "Epoch 20, Batch 370, Test Loss: 0.4120112359523773\n",
      "Epoch 20, Batch 371, Test Loss: 0.3414877653121948\n",
      "Epoch 20, Batch 372, Test Loss: 0.39551660418510437\n",
      "Epoch 20, Batch 373, Test Loss: 0.5147857666015625\n",
      "Epoch 20, Batch 374, Test Loss: 0.32874196767807007\n",
      "Epoch 20, Batch 375, Test Loss: 0.38269734382629395\n",
      "Epoch 20, Batch 376, Test Loss: 0.44423553347587585\n",
      "Epoch 20, Batch 377, Test Loss: 0.5114911794662476\n",
      "Epoch 20, Batch 378, Test Loss: 0.33087825775146484\n",
      "Epoch 20, Batch 379, Test Loss: 0.4805504083633423\n",
      "Epoch 20, Batch 380, Test Loss: 0.5326007008552551\n",
      "Epoch 20, Batch 381, Test Loss: 0.4550362825393677\n",
      "Epoch 20, Batch 382, Test Loss: 0.5959710478782654\n",
      "Epoch 20, Batch 383, Test Loss: 0.48979538679122925\n",
      "Epoch 20, Batch 384, Test Loss: 0.5602137446403503\n",
      "Epoch 20, Batch 385, Test Loss: 0.43082621693611145\n",
      "Epoch 20, Batch 386, Test Loss: 0.582493245601654\n",
      "Epoch 20, Batch 387, Test Loss: 0.4224799573421478\n",
      "Epoch 20, Batch 388, Test Loss: 0.5352398753166199\n",
      "Epoch 20, Batch 389, Test Loss: 0.5667274594306946\n",
      "Epoch 20, Batch 390, Test Loss: 0.4067874848842621\n",
      "Epoch 20, Batch 391, Test Loss: 0.3131623864173889\n",
      "Epoch 20, Batch 392, Test Loss: 0.3672751784324646\n",
      "Epoch 20, Batch 393, Test Loss: 0.3855886459350586\n",
      "Epoch 20, Batch 394, Test Loss: 0.5078482031822205\n",
      "Epoch 20, Batch 395, Test Loss: 0.35654181241989136\n",
      "Epoch 20, Batch 396, Test Loss: 0.31053513288497925\n",
      "Epoch 20, Batch 397, Test Loss: 0.4509609341621399\n",
      "Epoch 20, Batch 398, Test Loss: 0.5389626026153564\n",
      "Epoch 20, Batch 399, Test Loss: 0.45647257566452026\n",
      "Epoch 20, Batch 400, Test Loss: 0.2787586450576782\n",
      "Epoch 20, Batch 401, Test Loss: 0.41238921880722046\n",
      "Epoch 20, Batch 402, Test Loss: 0.36237937211990356\n",
      "Epoch 20, Batch 403, Test Loss: 0.4662511646747589\n",
      "Epoch 20, Batch 404, Test Loss: 0.40186765789985657\n",
      "Epoch 20, Batch 405, Test Loss: 0.5489783883094788\n",
      "Epoch 20, Batch 406, Test Loss: 0.43878674507141113\n",
      "Epoch 20, Batch 407, Test Loss: 0.38757431507110596\n",
      "Epoch 20, Batch 408, Test Loss: 0.3117307126522064\n",
      "Epoch 20, Batch 409, Test Loss: 0.5438056588172913\n",
      "Epoch 20, Batch 410, Test Loss: 0.36850816011428833\n",
      "Epoch 20, Batch 411, Test Loss: 0.48899078369140625\n",
      "Epoch 20, Batch 412, Test Loss: 0.5231926441192627\n",
      "Epoch 20, Batch 413, Test Loss: 0.4561034142971039\n",
      "Epoch 20, Batch 414, Test Loss: 0.3631696403026581\n",
      "Epoch 20, Batch 415, Test Loss: 0.3735450506210327\n",
      "Epoch 20, Batch 416, Test Loss: 0.3519115447998047\n",
      "Epoch 20, Batch 417, Test Loss: 0.3459019362926483\n",
      "Epoch 20, Batch 418, Test Loss: 0.36043232679367065\n",
      "Epoch 20, Batch 419, Test Loss: 0.3320468068122864\n",
      "Epoch 20, Batch 420, Test Loss: 0.4404035210609436\n",
      "Epoch 20, Batch 421, Test Loss: 0.4570939540863037\n",
      "Epoch 20, Batch 422, Test Loss: 0.5243989825248718\n",
      "Epoch 20, Batch 423, Test Loss: 0.45885807275772095\n",
      "Epoch 20, Batch 424, Test Loss: 0.5694584250450134\n",
      "Epoch 20, Batch 425, Test Loss: 0.5860249996185303\n",
      "Epoch 20, Batch 426, Test Loss: 0.5458911061286926\n",
      "Epoch 20, Batch 427, Test Loss: 0.5006539225578308\n",
      "Epoch 20, Batch 428, Test Loss: 0.4092751741409302\n",
      "Epoch 20, Batch 429, Test Loss: 0.6088496446609497\n",
      "Epoch 20, Batch 430, Test Loss: 0.48936980962753296\n",
      "Epoch 20, Batch 431, Test Loss: 0.3586730659008026\n",
      "Epoch 20, Batch 432, Test Loss: 0.4426983594894409\n",
      "Epoch 20, Batch 433, Test Loss: 0.4138454496860504\n",
      "Epoch 20, Batch 434, Test Loss: 0.3722425401210785\n",
      "Epoch 20, Batch 435, Test Loss: 0.4931892454624176\n",
      "Epoch 20, Batch 436, Test Loss: 0.42911088466644287\n",
      "Epoch 20, Batch 437, Test Loss: 0.5426328778266907\n",
      "Epoch 20, Batch 438, Test Loss: 0.5686641931533813\n",
      "Epoch 20, Batch 439, Test Loss: 0.2990211546421051\n",
      "Epoch 20, Batch 440, Test Loss: 0.3073198199272156\n",
      "Epoch 20, Batch 441, Test Loss: 0.4910638928413391\n",
      "Epoch 20, Batch 442, Test Loss: 0.5187644958496094\n",
      "Epoch 20, Batch 443, Test Loss: 0.5230284929275513\n",
      "Epoch 20, Batch 444, Test Loss: 0.5729948878288269\n",
      "Epoch 20, Batch 445, Test Loss: 0.4050360321998596\n",
      "Epoch 20, Batch 446, Test Loss: 0.33244138956069946\n",
      "Epoch 20, Batch 447, Test Loss: 0.4258035719394684\n",
      "Epoch 20, Batch 448, Test Loss: 0.5220835208892822\n",
      "Epoch 20, Batch 449, Test Loss: 0.44789761304855347\n",
      "Epoch 20, Batch 450, Test Loss: 0.28946858644485474\n",
      "Epoch 20, Batch 451, Test Loss: 0.5511026382446289\n",
      "Epoch 20, Batch 452, Test Loss: 0.5147596001625061\n",
      "Epoch 20, Batch 453, Test Loss: 0.4812917709350586\n",
      "Epoch 20, Batch 454, Test Loss: 0.35727444291114807\n",
      "Epoch 20, Batch 455, Test Loss: 0.3595712184906006\n",
      "Epoch 20, Batch 456, Test Loss: 0.43872392177581787\n",
      "Epoch 20, Batch 457, Test Loss: 0.3375042974948883\n",
      "Epoch 20, Batch 458, Test Loss: 0.49599045515060425\n",
      "Epoch 20, Batch 459, Test Loss: 0.29428133368492126\n",
      "Epoch 20, Batch 460, Test Loss: 0.31972819566726685\n",
      "Epoch 20, Batch 461, Test Loss: 0.4041367173194885\n",
      "Epoch 20, Batch 462, Test Loss: 0.3708324134349823\n",
      "Epoch 20, Batch 463, Test Loss: 0.34068357944488525\n",
      "Epoch 20, Batch 464, Test Loss: 0.5380098223686218\n",
      "Epoch 20, Batch 465, Test Loss: 0.4321233630180359\n",
      "Epoch 20, Batch 466, Test Loss: 0.3897276222705841\n",
      "Epoch 20, Batch 467, Test Loss: 0.4640563130378723\n",
      "Epoch 20, Batch 468, Test Loss: 0.27080294489860535\n",
      "Epoch 20, Batch 469, Test Loss: 0.47072410583496094\n",
      "Epoch 20, Batch 470, Test Loss: 0.39416009187698364\n",
      "Epoch 20, Batch 471, Test Loss: 0.4847901165485382\n",
      "Epoch 20, Batch 472, Test Loss: 0.3740355372428894\n",
      "Epoch 20, Batch 473, Test Loss: 0.6545460224151611\n",
      "Epoch 20, Batch 474, Test Loss: 0.7789519429206848\n",
      "Epoch 20, Batch 475, Test Loss: 0.292412132024765\n",
      "Epoch 20, Batch 476, Test Loss: 0.43493223190307617\n",
      "Epoch 20, Batch 477, Test Loss: 0.47463783621788025\n",
      "Epoch 20, Batch 478, Test Loss: 0.490427166223526\n",
      "Epoch 20, Batch 479, Test Loss: 0.354132741689682\n",
      "Epoch 20, Batch 480, Test Loss: 0.2979121804237366\n",
      "Epoch 20, Batch 481, Test Loss: 0.389181911945343\n",
      "Epoch 20, Batch 482, Test Loss: 0.24233320355415344\n",
      "Epoch 20, Batch 483, Test Loss: 0.6185973286628723\n",
      "Epoch 20, Batch 484, Test Loss: 0.4636736512184143\n",
      "Epoch 20, Batch 485, Test Loss: 0.3193909823894501\n",
      "Epoch 20, Batch 486, Test Loss: 0.43060874938964844\n",
      "Epoch 20, Batch 487, Test Loss: 0.37484827637672424\n",
      "Epoch 20, Batch 488, Test Loss: 0.31233352422714233\n",
      "Epoch 20, Batch 489, Test Loss: 0.4757154583930969\n",
      "Epoch 20, Batch 490, Test Loss: 0.3947255611419678\n",
      "Epoch 20, Batch 491, Test Loss: 0.4309597611427307\n",
      "Epoch 20, Batch 492, Test Loss: 0.549052894115448\n",
      "Epoch 20, Batch 493, Test Loss: 0.42809441685676575\n",
      "Epoch 20, Batch 494, Test Loss: 0.3433615565299988\n",
      "Epoch 20, Batch 495, Test Loss: 0.3547840118408203\n",
      "Epoch 20, Batch 496, Test Loss: 0.5555640459060669\n",
      "Epoch 20, Batch 497, Test Loss: 0.46008536219596863\n",
      "Epoch 20, Batch 498, Test Loss: 0.5431269407272339\n",
      "Epoch 20, Batch 499, Test Loss: 0.5524066686630249\n",
      "Epoch 20, Batch 500, Test Loss: 0.38979634642601013\n",
      "Epoch 20, Batch 501, Test Loss: 0.3414152264595032\n",
      "Epoch 20, Batch 502, Test Loss: 0.3209264278411865\n",
      "Epoch 20, Batch 503, Test Loss: 0.37447378039360046\n",
      "Epoch 20, Batch 504, Test Loss: 0.4400283396244049\n",
      "Epoch 20, Batch 505, Test Loss: 0.46700796484947205\n",
      "Epoch 20, Batch 506, Test Loss: 0.7483125925064087\n",
      "Epoch 20, Batch 507, Test Loss: 0.3451381027698517\n",
      "Epoch 20, Batch 508, Test Loss: 0.3417622745037079\n",
      "Epoch 20, Batch 509, Test Loss: 0.42027002573013306\n",
      "Epoch 20, Batch 510, Test Loss: 0.422890305519104\n",
      "Epoch 20, Batch 511, Test Loss: 0.5196077227592468\n",
      "Epoch 20, Batch 512, Test Loss: 0.3292118012905121\n",
      "Epoch 20, Batch 513, Test Loss: 0.30130165815353394\n",
      "Epoch 20, Batch 514, Test Loss: 0.5272412300109863\n",
      "Epoch 20, Batch 515, Test Loss: 0.36223724484443665\n",
      "Epoch 20, Batch 516, Test Loss: 0.5643961429595947\n",
      "Epoch 20, Batch 517, Test Loss: 0.45626100897789\n",
      "Epoch 20, Batch 518, Test Loss: 0.3098829686641693\n",
      "Epoch 20, Batch 519, Test Loss: 0.21963730454444885\n",
      "Epoch 20, Batch 520, Test Loss: 0.5715455412864685\n",
      "Epoch 20, Batch 521, Test Loss: 0.3474735915660858\n",
      "Epoch 20, Batch 522, Test Loss: 0.39564937353134155\n",
      "Epoch 20, Batch 523, Test Loss: 0.43766117095947266\n",
      "Epoch 20, Batch 524, Test Loss: 0.3946671485900879\n",
      "Epoch 20, Batch 525, Test Loss: 0.3039308786392212\n",
      "Epoch 20, Batch 526, Test Loss: 0.6113014221191406\n",
      "Epoch 20, Batch 527, Test Loss: 0.5752542018890381\n",
      "Epoch 20, Batch 528, Test Loss: 0.4322669208049774\n",
      "Epoch 20, Batch 529, Test Loss: 0.4940793514251709\n",
      "Epoch 20, Batch 530, Test Loss: 0.38538476824760437\n",
      "Epoch 20, Batch 531, Test Loss: 0.5187773108482361\n",
      "Epoch 20, Batch 532, Test Loss: 0.5107688903808594\n",
      "Epoch 20, Batch 533, Test Loss: 0.6091359853744507\n",
      "Epoch 20, Batch 534, Test Loss: 0.494173139333725\n",
      "Epoch 20, Batch 535, Test Loss: 0.4631805121898651\n",
      "Epoch 20, Batch 536, Test Loss: 0.539089560508728\n",
      "Epoch 20, Batch 537, Test Loss: 0.40614694356918335\n",
      "Epoch 20, Batch 538, Test Loss: 0.5937297940254211\n",
      "Epoch 20, Batch 539, Test Loss: 0.2601270079612732\n",
      "Epoch 20, Batch 540, Test Loss: 0.39176908135414124\n",
      "Epoch 20, Batch 541, Test Loss: 0.5396305322647095\n",
      "Epoch 20, Batch 542, Test Loss: 0.3402191698551178\n",
      "Epoch 20, Batch 543, Test Loss: 0.3160836398601532\n",
      "Epoch 20, Batch 544, Test Loss: 0.359319269657135\n",
      "Epoch 20, Batch 545, Test Loss: 0.5821676254272461\n",
      "Epoch 20, Batch 546, Test Loss: 0.5036741495132446\n",
      "Epoch 20, Batch 547, Test Loss: 0.4269905388355255\n",
      "Epoch 20, Batch 548, Test Loss: 0.5520347952842712\n",
      "Epoch 20, Batch 549, Test Loss: 0.5443032383918762\n",
      "Epoch 20, Batch 550, Test Loss: 0.4632216691970825\n",
      "Epoch 20, Batch 551, Test Loss: 0.562747597694397\n",
      "Epoch 20, Batch 552, Test Loss: 0.4185336232185364\n",
      "Epoch 20, Batch 553, Test Loss: 0.4588330388069153\n",
      "Epoch 20, Batch 554, Test Loss: 0.22392302751541138\n",
      "Epoch 20, Batch 555, Test Loss: 0.51167231798172\n",
      "Epoch 20, Batch 556, Test Loss: 0.4536728262901306\n",
      "Epoch 20, Batch 557, Test Loss: 0.39380255341529846\n",
      "Epoch 20, Batch 558, Test Loss: 0.32545483112335205\n",
      "Epoch 20, Batch 559, Test Loss: 0.4693261981010437\n",
      "Epoch 20, Batch 560, Test Loss: 0.3510817587375641\n",
      "Epoch 20, Batch 561, Test Loss: 0.592498779296875\n",
      "Epoch 20, Batch 562, Test Loss: 0.4669939875602722\n",
      "Epoch 20, Batch 563, Test Loss: 0.44030916690826416\n",
      "Epoch 20, Batch 564, Test Loss: 0.3437580466270447\n",
      "Epoch 20, Batch 565, Test Loss: 0.43596136569976807\n",
      "Epoch 20, Batch 566, Test Loss: 0.46594494581222534\n",
      "Epoch 20, Batch 567, Test Loss: 0.42333143949508667\n",
      "Epoch 20, Batch 568, Test Loss: 0.6414299607276917\n",
      "Epoch 20, Batch 569, Test Loss: 0.3727405369281769\n",
      "Epoch 20, Batch 570, Test Loss: 0.4533056616783142\n",
      "Epoch 20, Batch 571, Test Loss: 0.3665146231651306\n",
      "Epoch 20, Batch 572, Test Loss: 0.24970977008342743\n",
      "Epoch 20, Batch 573, Test Loss: 0.3515986502170563\n",
      "Epoch 20, Batch 574, Test Loss: 0.6160370111465454\n",
      "Epoch 20, Batch 575, Test Loss: 0.48424816131591797\n",
      "Epoch 20, Batch 576, Test Loss: 0.42597582936286926\n",
      "Epoch 20, Batch 577, Test Loss: 0.514440655708313\n",
      "Epoch 20, Batch 578, Test Loss: 0.24185313284397125\n",
      "Epoch 20, Batch 579, Test Loss: 0.45004045963287354\n",
      "Epoch 20, Batch 580, Test Loss: 0.3298793137073517\n",
      "Epoch 20, Batch 581, Test Loss: 0.3553057312965393\n",
      "Epoch 20, Batch 582, Test Loss: 0.44100746512413025\n",
      "Epoch 20, Batch 583, Test Loss: 0.23843832314014435\n",
      "Epoch 20, Batch 584, Test Loss: 0.6317592859268188\n",
      "Epoch 20, Batch 585, Test Loss: 0.32194554805755615\n",
      "Epoch 20, Batch 586, Test Loss: 0.43520259857177734\n",
      "Epoch 20, Batch 587, Test Loss: 0.8410313129425049\n",
      "Epoch 20, Batch 588, Test Loss: 0.2806408107280731\n",
      "Epoch 20, Batch 589, Test Loss: 0.3611033260822296\n",
      "Epoch 20, Batch 590, Test Loss: 0.242646262049675\n",
      "Epoch 20, Batch 591, Test Loss: 0.3433015048503876\n",
      "Epoch 20, Batch 592, Test Loss: 0.454492449760437\n",
      "Epoch 20, Batch 593, Test Loss: 0.3805236518383026\n",
      "Epoch 20, Batch 594, Test Loss: 0.4345949590206146\n",
      "Epoch 20, Batch 595, Test Loss: 0.49142763018608093\n",
      "Epoch 20, Batch 596, Test Loss: 0.3458212912082672\n",
      "Epoch 20, Batch 597, Test Loss: 0.5146104097366333\n",
      "Epoch 20, Batch 598, Test Loss: 0.3004645109176636\n",
      "Epoch 20, Batch 599, Test Loss: 0.3559964895248413\n",
      "Epoch 20, Batch 600, Test Loss: 0.36359646916389465\n",
      "Epoch 20, Batch 601, Test Loss: 0.340142160654068\n",
      "Epoch 20, Batch 602, Test Loss: 0.3806872069835663\n",
      "Epoch 20, Batch 603, Test Loss: 0.4482991099357605\n",
      "Epoch 20, Batch 604, Test Loss: 0.5463025569915771\n",
      "Epoch 20, Batch 605, Test Loss: 0.3123776614665985\n",
      "Epoch 20, Batch 606, Test Loss: 0.4494760036468506\n",
      "Epoch 20, Batch 607, Test Loss: 0.3171862065792084\n",
      "Epoch 20, Batch 608, Test Loss: 0.3321872055530548\n",
      "Epoch 20, Batch 609, Test Loss: 0.4035396873950958\n",
      "Epoch 20, Batch 610, Test Loss: 0.5675562620162964\n",
      "Epoch 20, Batch 611, Test Loss: 0.33098718523979187\n",
      "Epoch 20, Batch 612, Test Loss: 0.3142542243003845\n",
      "Epoch 20, Batch 613, Test Loss: 0.32394951581954956\n",
      "Epoch 20, Batch 614, Test Loss: 0.4403073787689209\n",
      "Epoch 20, Batch 615, Test Loss: 0.7499389052391052\n",
      "Epoch 20, Batch 616, Test Loss: 0.5782594680786133\n",
      "Epoch 20, Batch 617, Test Loss: 0.3629828095436096\n",
      "Epoch 20, Batch 618, Test Loss: 0.3958430886268616\n",
      "Epoch 20, Batch 619, Test Loss: 0.5717456936836243\n",
      "Epoch 20, Batch 620, Test Loss: 0.49386435747146606\n",
      "Epoch 20, Batch 621, Test Loss: 0.43787315487861633\n",
      "Epoch 20, Batch 622, Test Loss: 0.555707573890686\n",
      "Epoch 20, Batch 623, Test Loss: 0.5368946194648743\n",
      "Epoch 20, Batch 624, Test Loss: 0.3466148376464844\n",
      "Epoch 20, Batch 625, Test Loss: 0.3674708902835846\n",
      "Epoch 20, Batch 626, Test Loss: 0.41632014513015747\n",
      "Epoch 20, Batch 627, Test Loss: 0.3344409465789795\n",
      "Epoch 20, Batch 628, Test Loss: 0.6200609803199768\n",
      "Epoch 20, Batch 629, Test Loss: 0.3281959295272827\n",
      "Epoch 20, Batch 630, Test Loss: 0.38378241658210754\n",
      "Epoch 20, Batch 631, Test Loss: 0.39932340383529663\n",
      "Epoch 20, Batch 632, Test Loss: 0.37785202264785767\n",
      "Epoch 20, Batch 633, Test Loss: 0.4964805543422699\n",
      "Epoch 20, Batch 634, Test Loss: 0.2975119650363922\n",
      "Epoch 20, Batch 635, Test Loss: 0.4245629608631134\n",
      "Epoch 20, Batch 636, Test Loss: 0.41676220297813416\n",
      "Epoch 20, Batch 637, Test Loss: 0.552784264087677\n",
      "Epoch 20, Batch 638, Test Loss: 0.39722582697868347\n",
      "Epoch 20, Batch 639, Test Loss: 0.31638821959495544\n",
      "Epoch 20, Batch 640, Test Loss: 0.5258700847625732\n",
      "Epoch 20, Batch 641, Test Loss: 0.3495592772960663\n",
      "Epoch 20, Batch 642, Test Loss: 0.5286738872528076\n",
      "Epoch 20, Batch 643, Test Loss: 0.3857574760913849\n",
      "Epoch 20, Batch 644, Test Loss: 0.548456072807312\n",
      "Epoch 20, Batch 645, Test Loss: 0.5056548714637756\n",
      "Epoch 20, Batch 646, Test Loss: 0.3350487947463989\n",
      "Epoch 20, Batch 647, Test Loss: 0.37672996520996094\n",
      "Epoch 20, Batch 648, Test Loss: 0.6225440502166748\n",
      "Epoch 20, Batch 649, Test Loss: 0.37219393253326416\n",
      "Epoch 20, Batch 650, Test Loss: 0.6726052761077881\n",
      "Epoch 20, Batch 651, Test Loss: 0.4172656536102295\n",
      "Epoch 20, Batch 652, Test Loss: 0.5593850016593933\n",
      "Epoch 20, Batch 653, Test Loss: 0.3563896119594574\n",
      "Epoch 20, Batch 654, Test Loss: 0.36026662588119507\n",
      "Epoch 20, Batch 655, Test Loss: 0.2669949233531952\n",
      "Epoch 20, Batch 656, Test Loss: 0.3818727433681488\n",
      "Epoch 20, Batch 657, Test Loss: 0.3769770562648773\n",
      "Epoch 20, Batch 658, Test Loss: 0.4074937105178833\n",
      "Epoch 20, Batch 659, Test Loss: 0.280346155166626\n",
      "Epoch 20, Batch 660, Test Loss: 0.5433552265167236\n",
      "Epoch 20, Batch 661, Test Loss: 0.368438720703125\n",
      "Epoch 20, Batch 662, Test Loss: 0.3770025968551636\n",
      "Epoch 20, Batch 663, Test Loss: 0.40054380893707275\n",
      "Epoch 20, Batch 664, Test Loss: 0.442989706993103\n",
      "Epoch 20, Batch 665, Test Loss: 0.3018404245376587\n",
      "Epoch 20, Batch 666, Test Loss: 0.4867061972618103\n",
      "Epoch 20, Batch 667, Test Loss: 0.3424481451511383\n",
      "Epoch 20, Batch 668, Test Loss: 0.36941733956336975\n",
      "Epoch 20, Batch 669, Test Loss: 0.4961724281311035\n",
      "Epoch 20, Batch 670, Test Loss: 0.5120866298675537\n",
      "Epoch 20, Batch 671, Test Loss: 0.3410862684249878\n",
      "Epoch 20, Batch 672, Test Loss: 0.4737817645072937\n",
      "Epoch 20, Batch 673, Test Loss: 0.1867286115884781\n",
      "Epoch 20, Batch 674, Test Loss: 0.45946627855300903\n",
      "Epoch 20, Batch 675, Test Loss: 0.25207778811454773\n",
      "Epoch 20, Batch 676, Test Loss: 0.533677339553833\n",
      "Epoch 20, Batch 677, Test Loss: 0.47378626465797424\n",
      "Epoch 20, Batch 678, Test Loss: 0.3152230679988861\n",
      "Epoch 20, Batch 679, Test Loss: 0.36150410771369934\n",
      "Epoch 20, Batch 680, Test Loss: 0.4495949447154999\n",
      "Epoch 20, Batch 681, Test Loss: 0.4137950539588928\n",
      "Epoch 20, Batch 682, Test Loss: 0.3333280384540558\n",
      "Epoch 20, Batch 683, Test Loss: 0.4367033839225769\n",
      "Epoch 20, Batch 684, Test Loss: 0.5073590874671936\n",
      "Epoch 20, Batch 685, Test Loss: 0.4652307629585266\n",
      "Epoch 20, Batch 686, Test Loss: 0.48169365525245667\n",
      "Epoch 20, Batch 687, Test Loss: 0.4829244911670685\n",
      "Epoch 20, Batch 688, Test Loss: 0.711237907409668\n",
      "Epoch 20, Batch 689, Test Loss: 0.5574308037757874\n",
      "Epoch 20, Batch 690, Test Loss: 0.4825296401977539\n",
      "Epoch 20, Batch 691, Test Loss: 0.4519856572151184\n",
      "Epoch 20, Batch 692, Test Loss: 0.593787670135498\n",
      "Epoch 20, Batch 693, Test Loss: 0.3403915762901306\n",
      "Epoch 20, Batch 694, Test Loss: 0.5281111001968384\n",
      "Epoch 20, Batch 695, Test Loss: 0.3031848967075348\n",
      "Epoch 20, Batch 696, Test Loss: 0.5196563601493835\n",
      "Epoch 20, Batch 697, Test Loss: 0.49660369753837585\n",
      "Epoch 20, Batch 698, Test Loss: 0.4409741163253784\n",
      "Epoch 20, Batch 699, Test Loss: 0.3816262483596802\n",
      "Epoch 20, Batch 700, Test Loss: 0.36116814613342285\n",
      "Epoch 20, Batch 701, Test Loss: 0.33912336826324463\n",
      "Epoch 20, Batch 702, Test Loss: 0.4415750205516815\n",
      "Epoch 20, Batch 703, Test Loss: 0.4429764151573181\n",
      "Epoch 20, Batch 704, Test Loss: 0.4632752239704132\n",
      "Epoch 20, Batch 705, Test Loss: 0.40066075325012207\n",
      "Epoch 20, Batch 706, Test Loss: 0.4509292542934418\n",
      "Epoch 20, Batch 707, Test Loss: 0.5636865496635437\n",
      "Epoch 20, Batch 708, Test Loss: 0.40523189306259155\n",
      "Epoch 20, Batch 709, Test Loss: 0.37558767199516296\n",
      "Epoch 20, Batch 710, Test Loss: 0.3718445897102356\n",
      "Epoch 20, Batch 711, Test Loss: 0.2944827377796173\n",
      "Epoch 20, Batch 712, Test Loss: 0.5035461783409119\n",
      "Epoch 20, Batch 713, Test Loss: 0.6153787970542908\n",
      "Epoch 20, Batch 714, Test Loss: 0.42990434169769287\n",
      "Epoch 20, Batch 715, Test Loss: 0.4042322635650635\n",
      "Epoch 20, Batch 716, Test Loss: 0.28786712884902954\n",
      "Epoch 20, Batch 717, Test Loss: 0.39897891879081726\n",
      "Epoch 20, Batch 718, Test Loss: 0.5519459247589111\n",
      "Epoch 20, Batch 719, Test Loss: 0.41721367835998535\n",
      "Epoch 20, Batch 720, Test Loss: 0.4597655236721039\n",
      "Epoch 20, Batch 721, Test Loss: 0.459266722202301\n",
      "Epoch 20, Batch 722, Test Loss: 0.3152892589569092\n",
      "Epoch 20, Batch 723, Test Loss: 0.44854605197906494\n",
      "Epoch 20, Batch 724, Test Loss: 0.33831116557121277\n",
      "Epoch 20, Batch 725, Test Loss: 0.341097891330719\n",
      "Epoch 20, Batch 726, Test Loss: 0.431633323431015\n",
      "Epoch 20, Batch 727, Test Loss: 0.5007623434066772\n",
      "Epoch 20, Batch 728, Test Loss: 0.33808910846710205\n",
      "Epoch 20, Batch 729, Test Loss: 0.40449345111846924\n",
      "Epoch 20, Batch 730, Test Loss: 0.5242838263511658\n",
      "Epoch 20, Batch 731, Test Loss: 0.5581550598144531\n",
      "Epoch 20, Batch 732, Test Loss: 0.392170786857605\n",
      "Epoch 20, Batch 733, Test Loss: 0.5353678464889526\n",
      "Epoch 20, Batch 734, Test Loss: 0.6826460957527161\n",
      "Epoch 20, Batch 735, Test Loss: 0.49437105655670166\n",
      "Epoch 20, Batch 736, Test Loss: 0.2741439640522003\n",
      "Epoch 20, Batch 737, Test Loss: 0.4818088412284851\n",
      "Epoch 20, Batch 738, Test Loss: 0.6302535533905029\n",
      "Epoch 20, Batch 739, Test Loss: 0.4709493815898895\n",
      "Epoch 20, Batch 740, Test Loss: 0.3563846945762634\n",
      "Epoch 20, Batch 741, Test Loss: 0.46257907152175903\n",
      "Epoch 20, Batch 742, Test Loss: 0.4039783775806427\n",
      "Epoch 20, Batch 743, Test Loss: 0.45504850149154663\n",
      "Epoch 20, Batch 744, Test Loss: 0.3715679943561554\n",
      "Epoch 20, Batch 745, Test Loss: 0.5012561082839966\n",
      "Epoch 20, Batch 746, Test Loss: 0.3007734715938568\n",
      "Epoch 20, Batch 747, Test Loss: 0.4650709629058838\n",
      "Epoch 20, Batch 748, Test Loss: 0.2870122790336609\n",
      "Epoch 20, Batch 749, Test Loss: 0.4450746774673462\n",
      "Epoch 20, Batch 750, Test Loss: 0.3384742736816406\n",
      "Epoch 20, Batch 751, Test Loss: 0.2503312826156616\n",
      "Epoch 20, Batch 752, Test Loss: 0.677793025970459\n",
      "Epoch 20, Batch 753, Test Loss: 0.23020482063293457\n",
      "Epoch 20, Batch 754, Test Loss: 0.4492945671081543\n",
      "Epoch 20, Batch 755, Test Loss: 0.4139753580093384\n",
      "Epoch 20, Batch 756, Test Loss: 0.42472559213638306\n",
      "Epoch 20, Batch 757, Test Loss: 0.44359320402145386\n",
      "Epoch 20, Batch 758, Test Loss: 0.36041000485420227\n",
      "Epoch 20, Batch 759, Test Loss: 0.4528254568576813\n",
      "Epoch 20, Batch 760, Test Loss: 0.5016350150108337\n",
      "Epoch 20, Batch 761, Test Loss: 0.3342297077178955\n",
      "Epoch 20, Batch 762, Test Loss: 0.5222876667976379\n",
      "Epoch 20, Batch 763, Test Loss: 0.3537946343421936\n",
      "Epoch 20, Batch 764, Test Loss: 0.4390813708305359\n",
      "Epoch 20, Batch 765, Test Loss: 0.465779185295105\n",
      "Epoch 20, Batch 766, Test Loss: 0.4712107181549072\n",
      "Epoch 20, Batch 767, Test Loss: 0.42660701274871826\n",
      "Epoch 20, Batch 768, Test Loss: 0.4559595286846161\n",
      "Epoch 20, Batch 769, Test Loss: 0.5450963973999023\n",
      "Epoch 20, Batch 770, Test Loss: 0.5663926005363464\n",
      "Epoch 20, Batch 771, Test Loss: 0.33922305703163147\n",
      "Epoch 20, Batch 772, Test Loss: 0.5058217644691467\n",
      "Epoch 20, Batch 773, Test Loss: 0.629267692565918\n",
      "Epoch 20, Batch 774, Test Loss: 0.23189181089401245\n",
      "Epoch 20, Batch 775, Test Loss: 0.29062849283218384\n",
      "Epoch 20, Batch 776, Test Loss: 0.46819180250167847\n",
      "Epoch 20, Batch 777, Test Loss: 0.439131498336792\n",
      "Epoch 20, Batch 778, Test Loss: 0.46232277154922485\n",
      "Epoch 20, Batch 779, Test Loss: 0.4837016761302948\n",
      "Epoch 20, Batch 780, Test Loss: 0.40490856766700745\n",
      "Epoch 20, Batch 781, Test Loss: 0.23144644498825073\n",
      "Epoch 20, Batch 782, Test Loss: 0.361514151096344\n",
      "Epoch 20, Batch 783, Test Loss: 0.5811843872070312\n",
      "Epoch 20, Batch 784, Test Loss: 0.33395814895629883\n",
      "Epoch 20, Batch 785, Test Loss: 0.4776342213153839\n",
      "Epoch 20, Batch 786, Test Loss: 0.33706119656562805\n",
      "Epoch 20, Batch 787, Test Loss: 0.33546727895736694\n",
      "Epoch 20, Batch 788, Test Loss: 0.38365182280540466\n",
      "Epoch 20, Batch 789, Test Loss: 0.5675452947616577\n",
      "Epoch 20, Batch 790, Test Loss: 0.38401952385902405\n",
      "Epoch 20, Batch 791, Test Loss: 0.40529125928878784\n",
      "Epoch 20, Batch 792, Test Loss: 0.5457782745361328\n",
      "Epoch 20, Batch 793, Test Loss: 0.3132503628730774\n",
      "Epoch 20, Batch 794, Test Loss: 0.3838665783405304\n",
      "Epoch 20, Batch 795, Test Loss: 0.3929367661476135\n",
      "Epoch 20, Batch 796, Test Loss: 0.3416917622089386\n",
      "Epoch 20, Batch 797, Test Loss: 0.5128123164176941\n",
      "Epoch 20, Batch 798, Test Loss: 0.4684385359287262\n",
      "Epoch 20, Batch 799, Test Loss: 0.40249237418174744\n",
      "Epoch 20, Batch 800, Test Loss: 0.3810800313949585\n",
      "Epoch 20, Batch 801, Test Loss: 0.3606030344963074\n",
      "Epoch 20, Batch 802, Test Loss: 0.5409607887268066\n",
      "Epoch 20, Batch 803, Test Loss: 0.37103649973869324\n",
      "Epoch 20, Batch 804, Test Loss: 0.2862563729286194\n",
      "Epoch 20, Batch 805, Test Loss: 0.3249959945678711\n",
      "Epoch 20, Batch 806, Test Loss: 0.5920348167419434\n",
      "Epoch 20, Batch 807, Test Loss: 0.41452598571777344\n",
      "Epoch 20, Batch 808, Test Loss: 0.4243660867214203\n",
      "Epoch 20, Batch 809, Test Loss: 0.5019720792770386\n",
      "Epoch 20, Batch 810, Test Loss: 0.7320707440376282\n",
      "Epoch 20, Batch 811, Test Loss: 0.4938051104545593\n",
      "Epoch 20, Batch 812, Test Loss: 0.431090772151947\n",
      "Epoch 20, Batch 813, Test Loss: 0.43543773889541626\n",
      "Epoch 20, Batch 814, Test Loss: 0.44104576110839844\n",
      "Epoch 20, Batch 815, Test Loss: 0.4062480926513672\n",
      "Epoch 20, Batch 816, Test Loss: 0.5158069133758545\n",
      "Epoch 20, Batch 817, Test Loss: 0.38908490538597107\n",
      "Epoch 20, Batch 818, Test Loss: 0.277045875787735\n",
      "Epoch 20, Batch 819, Test Loss: 0.376440167427063\n",
      "Epoch 20, Batch 820, Test Loss: 0.47000885009765625\n",
      "Epoch 20, Batch 821, Test Loss: 0.6087810397148132\n",
      "Epoch 20, Batch 822, Test Loss: 0.26232805848121643\n",
      "Epoch 20, Batch 823, Test Loss: 0.29802876710891724\n",
      "Epoch 20, Batch 824, Test Loss: 0.41909047961235046\n",
      "Epoch 20, Batch 825, Test Loss: 0.3186665177345276\n",
      "Epoch 20, Batch 826, Test Loss: 0.3534221053123474\n",
      "Epoch 20, Batch 827, Test Loss: 0.4502807557582855\n",
      "Epoch 20, Batch 828, Test Loss: 0.3203619718551636\n",
      "Epoch 20, Batch 829, Test Loss: 0.3483218848705292\n",
      "Epoch 20, Batch 830, Test Loss: 0.5714712142944336\n",
      "Epoch 20, Batch 831, Test Loss: 0.3446723222732544\n",
      "Epoch 20, Batch 832, Test Loss: 0.49249565601348877\n",
      "Epoch 20, Batch 833, Test Loss: 0.5844043493270874\n",
      "Epoch 20, Batch 834, Test Loss: 0.30629172921180725\n",
      "Epoch 20, Batch 835, Test Loss: 0.5196696519851685\n",
      "Epoch 20, Batch 836, Test Loss: 0.33704596757888794\n",
      "Epoch 20, Batch 837, Test Loss: 0.3081270158290863\n",
      "Epoch 20, Batch 838, Test Loss: 0.3690238296985626\n",
      "Epoch 20, Batch 839, Test Loss: 0.3944288492202759\n",
      "Epoch 20, Batch 840, Test Loss: 0.47657662630081177\n",
      "Epoch 20, Batch 841, Test Loss: 0.4321253001689911\n",
      "Epoch 20, Batch 842, Test Loss: 0.3859149217605591\n",
      "Epoch 20, Batch 843, Test Loss: 0.5254255533218384\n",
      "Epoch 20, Batch 844, Test Loss: 0.329754114151001\n",
      "Epoch 20, Batch 845, Test Loss: 0.3520989418029785\n",
      "Epoch 20, Batch 846, Test Loss: 0.38961440324783325\n",
      "Epoch 20, Batch 847, Test Loss: 0.2919673025608063\n",
      "Epoch 20, Batch 848, Test Loss: 0.5016171336174011\n",
      "Epoch 20, Batch 849, Test Loss: 0.54034423828125\n",
      "Epoch 20, Batch 850, Test Loss: 0.3310549557209015\n",
      "Epoch 20, Batch 851, Test Loss: 0.49805769324302673\n",
      "Epoch 20, Batch 852, Test Loss: 0.363349050283432\n",
      "Epoch 20, Batch 853, Test Loss: 0.5450384020805359\n",
      "Epoch 20, Batch 854, Test Loss: 0.37891778349876404\n",
      "Epoch 20, Batch 855, Test Loss: 0.3445652425289154\n",
      "Epoch 20, Batch 856, Test Loss: 0.3148465156555176\n",
      "Epoch 20, Batch 857, Test Loss: 0.5213718414306641\n",
      "Epoch 20, Batch 858, Test Loss: 0.34519749879837036\n",
      "Epoch 20, Batch 859, Test Loss: 0.4643855690956116\n",
      "Epoch 20, Batch 860, Test Loss: 0.40734127163887024\n",
      "Epoch 20, Batch 861, Test Loss: 0.5760740041732788\n",
      "Epoch 20, Batch 862, Test Loss: 0.3925958573818207\n",
      "Epoch 20, Batch 863, Test Loss: 0.41296762228012085\n",
      "Epoch 20, Batch 864, Test Loss: 0.6965125799179077\n",
      "Epoch 20, Batch 865, Test Loss: 0.3706269860267639\n",
      "Epoch 20, Batch 866, Test Loss: 0.45462262630462646\n",
      "Epoch 20, Batch 867, Test Loss: 0.3818512558937073\n",
      "Epoch 20, Batch 868, Test Loss: 0.43626832962036133\n",
      "Epoch 20, Batch 869, Test Loss: 0.5281285643577576\n",
      "Epoch 20, Batch 870, Test Loss: 0.4608193337917328\n",
      "Epoch 20, Batch 871, Test Loss: 0.3531835675239563\n",
      "Epoch 20, Batch 872, Test Loss: 0.4060174524784088\n",
      "Epoch 20, Batch 873, Test Loss: 0.606292188167572\n",
      "Epoch 20, Batch 874, Test Loss: 0.5644443035125732\n",
      "Epoch 20, Batch 875, Test Loss: 0.256226509809494\n",
      "Epoch 20, Batch 876, Test Loss: 0.3476067781448364\n",
      "Epoch 20, Batch 877, Test Loss: 0.2845149040222168\n",
      "Epoch 20, Batch 878, Test Loss: 0.5241127610206604\n",
      "Epoch 20, Batch 879, Test Loss: 0.49226051568984985\n",
      "Epoch 20, Batch 880, Test Loss: 0.38145095109939575\n",
      "Epoch 20, Batch 881, Test Loss: 0.4928370714187622\n",
      "Epoch 20, Batch 882, Test Loss: 0.3314160108566284\n",
      "Epoch 20, Batch 883, Test Loss: 0.416625052690506\n",
      "Epoch 20, Batch 884, Test Loss: 0.3850310444831848\n",
      "Epoch 20, Batch 885, Test Loss: 0.30026957392692566\n",
      "Epoch 20, Batch 886, Test Loss: 0.7252725958824158\n",
      "Epoch 20, Batch 887, Test Loss: 0.40907326340675354\n",
      "Epoch 20, Batch 888, Test Loss: 0.4458562433719635\n",
      "Epoch 20, Batch 889, Test Loss: 0.3910353183746338\n",
      "Epoch 20, Batch 890, Test Loss: 0.3625858724117279\n",
      "Epoch 20, Batch 891, Test Loss: 0.29231762886047363\n",
      "Epoch 20, Batch 892, Test Loss: 0.256400465965271\n",
      "Epoch 20, Batch 893, Test Loss: 0.4318145215511322\n",
      "Epoch 20, Batch 894, Test Loss: 0.508264422416687\n",
      "Epoch 20, Batch 895, Test Loss: 0.39040660858154297\n",
      "Epoch 20, Batch 896, Test Loss: 0.3137721121311188\n",
      "Epoch 20, Batch 897, Test Loss: 0.37924593687057495\n",
      "Epoch 20, Batch 898, Test Loss: 0.4718746542930603\n",
      "Epoch 20, Batch 899, Test Loss: 0.4422696828842163\n",
      "Epoch 20, Batch 900, Test Loss: 0.36101794242858887\n",
      "Epoch 20, Batch 901, Test Loss: 0.36067983508110046\n",
      "Epoch 20, Batch 902, Test Loss: 0.545753538608551\n",
      "Epoch 20, Batch 903, Test Loss: 0.34293556213378906\n",
      "Epoch 20, Batch 904, Test Loss: 0.276011198759079\n",
      "Epoch 20, Batch 905, Test Loss: 0.3522900640964508\n",
      "Epoch 20, Batch 906, Test Loss: 0.3302339017391205\n",
      "Epoch 20, Batch 907, Test Loss: 0.3027283847332001\n",
      "Epoch 20, Batch 908, Test Loss: 0.5111971497535706\n",
      "Epoch 20, Batch 909, Test Loss: 0.31885504722595215\n",
      "Epoch 20, Batch 910, Test Loss: 0.4820196330547333\n",
      "Epoch 20, Batch 911, Test Loss: 0.4337448477745056\n",
      "Epoch 20, Batch 912, Test Loss: 0.43662744760513306\n",
      "Epoch 20, Batch 913, Test Loss: 0.25637754797935486\n",
      "Epoch 20, Batch 914, Test Loss: 0.3546614646911621\n",
      "Epoch 20, Batch 915, Test Loss: 0.31559252738952637\n",
      "Epoch 20, Batch 916, Test Loss: 0.32706040143966675\n",
      "Epoch 20, Batch 917, Test Loss: 0.48612552881240845\n",
      "Epoch 20, Batch 918, Test Loss: 0.4976590871810913\n",
      "Epoch 20, Batch 919, Test Loss: 0.398253470659256\n",
      "Epoch 20, Batch 920, Test Loss: 0.5058468580245972\n",
      "Epoch 20, Batch 921, Test Loss: 0.3778890371322632\n",
      "Epoch 20, Batch 922, Test Loss: 0.2993815243244171\n",
      "Epoch 20, Batch 923, Test Loss: 0.4105842709541321\n",
      "Epoch 20, Batch 924, Test Loss: 0.4216810166835785\n",
      "Epoch 20, Batch 925, Test Loss: 0.3302120864391327\n",
      "Epoch 20, Batch 926, Test Loss: 0.32550081610679626\n",
      "Epoch 20, Batch 927, Test Loss: 0.4317769408226013\n",
      "Epoch 20, Batch 928, Test Loss: 0.41227924823760986\n",
      "Epoch 20, Batch 929, Test Loss: 0.40909644961357117\n",
      "Epoch 20, Batch 930, Test Loss: 0.4355253577232361\n",
      "Epoch 20, Batch 931, Test Loss: 0.35874855518341064\n",
      "Epoch 20, Batch 932, Test Loss: 0.5649399757385254\n",
      "Epoch 20, Batch 933, Test Loss: 0.4650965929031372\n",
      "Epoch 20, Batch 934, Test Loss: 0.4822273254394531\n",
      "Epoch 20, Batch 935, Test Loss: 0.253889262676239\n",
      "Epoch 20, Batch 936, Test Loss: 0.4200296700000763\n",
      "Epoch 20, Batch 937, Test Loss: 0.30303195118904114\n",
      "Epoch 20, Batch 938, Test Loss: 0.25172144174575806\n",
      "Accuracy of Test set: 0.8480666666666666\n",
      "Epoch 21, Batch 1, Loss: 0.3347957730293274\n",
      "Epoch 21, Batch 2, Loss: 0.491627037525177\n",
      "Epoch 21, Batch 3, Loss: 0.33461737632751465\n",
      "Epoch 21, Batch 4, Loss: 0.4130869507789612\n",
      "Epoch 21, Batch 5, Loss: 0.19317424297332764\n",
      "Epoch 21, Batch 6, Loss: 0.5236579775810242\n",
      "Epoch 21, Batch 7, Loss: 0.4818074107170105\n",
      "Epoch 21, Batch 8, Loss: 0.45757243037223816\n",
      "Epoch 21, Batch 9, Loss: 0.364231139421463\n",
      "Epoch 21, Batch 10, Loss: 0.5150150060653687\n",
      "Epoch 21, Batch 11, Loss: 0.5123530030250549\n",
      "Epoch 21, Batch 12, Loss: 0.46951088309288025\n",
      "Epoch 21, Batch 13, Loss: 0.5685741901397705\n",
      "Epoch 21, Batch 14, Loss: 0.44092807173728943\n",
      "Epoch 21, Batch 15, Loss: 0.4831552505493164\n",
      "Epoch 21, Batch 16, Loss: 0.31585031747817993\n",
      "Epoch 21, Batch 17, Loss: 0.2817028760910034\n",
      "Epoch 21, Batch 18, Loss: 0.48456358909606934\n",
      "Epoch 21, Batch 19, Loss: 0.32613545656204224\n",
      "Epoch 21, Batch 20, Loss: 0.40045109391212463\n",
      "Epoch 21, Batch 21, Loss: 0.3894861936569214\n",
      "Epoch 21, Batch 22, Loss: 0.37728726863861084\n",
      "Epoch 21, Batch 23, Loss: 0.39314034581184387\n",
      "Epoch 21, Batch 24, Loss: 0.3862406015396118\n",
      "Epoch 21, Batch 25, Loss: 0.5277705788612366\n",
      "Epoch 21, Batch 26, Loss: 0.3759134113788605\n",
      "Epoch 21, Batch 27, Loss: 0.382208913564682\n",
      "Epoch 21, Batch 28, Loss: 0.6539034843444824\n",
      "Epoch 21, Batch 29, Loss: 0.5301758646965027\n",
      "Epoch 21, Batch 30, Loss: 0.43515634536743164\n",
      "Epoch 21, Batch 31, Loss: 0.3213507831096649\n",
      "Epoch 21, Batch 32, Loss: 0.36973312497138977\n",
      "Epoch 21, Batch 33, Loss: 0.40823668241500854\n",
      "Epoch 21, Batch 34, Loss: 0.45422399044036865\n",
      "Epoch 21, Batch 35, Loss: 0.4169203042984009\n",
      "Epoch 21, Batch 36, Loss: 0.5196289420127869\n",
      "Epoch 21, Batch 37, Loss: 0.3690015375614166\n",
      "Epoch 21, Batch 38, Loss: 0.4453090727329254\n",
      "Epoch 21, Batch 39, Loss: 0.48490434885025024\n",
      "Epoch 21, Batch 40, Loss: 0.36551156640052795\n",
      "Epoch 21, Batch 41, Loss: 0.5616613030433655\n",
      "Epoch 21, Batch 42, Loss: 0.470639169216156\n",
      "Epoch 21, Batch 43, Loss: 0.39607298374176025\n",
      "Epoch 21, Batch 44, Loss: 0.6115933656692505\n",
      "Epoch 21, Batch 45, Loss: 0.333117812871933\n",
      "Epoch 21, Batch 46, Loss: 0.34076666831970215\n",
      "Epoch 21, Batch 47, Loss: 0.3875414729118347\n",
      "Epoch 21, Batch 48, Loss: 0.3183545172214508\n",
      "Epoch 21, Batch 49, Loss: 0.4896630346775055\n",
      "Epoch 21, Batch 50, Loss: 0.4672963321208954\n",
      "Epoch 21, Batch 51, Loss: 0.28702035546302795\n",
      "Epoch 21, Batch 52, Loss: 0.3110271692276001\n",
      "Epoch 21, Batch 53, Loss: 0.39438697695732117\n",
      "Epoch 21, Batch 54, Loss: 0.2744046151638031\n",
      "Epoch 21, Batch 55, Loss: 0.39398133754730225\n",
      "Epoch 21, Batch 56, Loss: 0.39288946986198425\n",
      "Epoch 21, Batch 57, Loss: 0.5256475806236267\n",
      "Epoch 21, Batch 58, Loss: 0.19151084125041962\n",
      "Epoch 21, Batch 59, Loss: 0.3671218454837799\n",
      "Epoch 21, Batch 60, Loss: 0.4089336395263672\n",
      "Epoch 21, Batch 61, Loss: 0.5704677700996399\n",
      "Epoch 21, Batch 62, Loss: 0.4152177572250366\n",
      "Epoch 21, Batch 63, Loss: 0.43479517102241516\n",
      "Epoch 21, Batch 64, Loss: 0.3670337200164795\n",
      "Epoch 21, Batch 65, Loss: 0.19101639091968536\n",
      "Epoch 21, Batch 66, Loss: 0.3447555899620056\n",
      "Epoch 21, Batch 67, Loss: 0.5616698861122131\n",
      "Epoch 21, Batch 68, Loss: 0.37005531787872314\n",
      "Epoch 21, Batch 69, Loss: 0.34025830030441284\n",
      "Epoch 21, Batch 70, Loss: 0.44049444794654846\n",
      "Epoch 21, Batch 71, Loss: 0.24215078353881836\n",
      "Epoch 21, Batch 72, Loss: 0.35944706201553345\n",
      "Epoch 21, Batch 73, Loss: 0.5159392356872559\n",
      "Epoch 21, Batch 74, Loss: 0.5906869173049927\n",
      "Epoch 21, Batch 75, Loss: 0.452758252620697\n",
      "Epoch 21, Batch 76, Loss: 0.4108377993106842\n",
      "Epoch 21, Batch 77, Loss: 0.42440134286880493\n",
      "Epoch 21, Batch 78, Loss: 0.4354400634765625\n",
      "Epoch 21, Batch 79, Loss: 0.2915765345096588\n",
      "Epoch 21, Batch 80, Loss: 0.3542778193950653\n",
      "Epoch 21, Batch 81, Loss: 0.6115584373474121\n",
      "Epoch 21, Batch 82, Loss: 0.521751880645752\n",
      "Epoch 21, Batch 83, Loss: 0.3607490658760071\n",
      "Epoch 21, Batch 84, Loss: 0.5643924474716187\n",
      "Epoch 21, Batch 85, Loss: 0.5494450330734253\n",
      "Epoch 21, Batch 86, Loss: 0.5051836371421814\n",
      "Epoch 21, Batch 87, Loss: 0.4032767117023468\n",
      "Epoch 21, Batch 88, Loss: 0.43110448122024536\n",
      "Epoch 21, Batch 89, Loss: 0.5326284170150757\n",
      "Epoch 21, Batch 90, Loss: 0.7861302495002747\n",
      "Epoch 21, Batch 91, Loss: 0.18164822459220886\n",
      "Epoch 21, Batch 92, Loss: 0.47282612323760986\n",
      "Epoch 21, Batch 93, Loss: 0.6259758472442627\n",
      "Epoch 21, Batch 94, Loss: 0.4228302836418152\n",
      "Epoch 21, Batch 95, Loss: 0.4123470187187195\n",
      "Epoch 21, Batch 96, Loss: 0.2753271460533142\n",
      "Epoch 21, Batch 97, Loss: 0.29977089166641235\n",
      "Epoch 21, Batch 98, Loss: 0.33218440413475037\n",
      "Epoch 21, Batch 99, Loss: 0.22375769913196564\n",
      "Epoch 21, Batch 100, Loss: 0.3508681058883667\n",
      "Epoch 21, Batch 101, Loss: 0.3614407479763031\n",
      "Epoch 21, Batch 102, Loss: 0.5911961793899536\n",
      "Epoch 21, Batch 103, Loss: 0.37558266520500183\n",
      "Epoch 21, Batch 104, Loss: 0.4349188208580017\n",
      "Epoch 21, Batch 105, Loss: 0.2436908781528473\n",
      "Epoch 21, Batch 106, Loss: 0.5218870639801025\n",
      "Epoch 21, Batch 107, Loss: 0.6861628293991089\n",
      "Epoch 21, Batch 108, Loss: 0.39737188816070557\n",
      "Epoch 21, Batch 109, Loss: 0.7254176139831543\n",
      "Epoch 21, Batch 110, Loss: 0.35112860798835754\n",
      "Epoch 21, Batch 111, Loss: 0.5151869058609009\n",
      "Epoch 21, Batch 112, Loss: 0.4280192255973816\n",
      "Epoch 21, Batch 113, Loss: 0.4455578923225403\n",
      "Epoch 21, Batch 114, Loss: 0.22071205079555511\n",
      "Epoch 21, Batch 115, Loss: 0.3212633430957794\n",
      "Epoch 21, Batch 116, Loss: 0.3089190423488617\n",
      "Epoch 21, Batch 117, Loss: 0.581966757774353\n",
      "Epoch 21, Batch 118, Loss: 0.5111334919929504\n",
      "Epoch 21, Batch 119, Loss: 0.517989456653595\n",
      "Epoch 21, Batch 120, Loss: 0.5889670848846436\n",
      "Epoch 21, Batch 121, Loss: 0.46782729029655457\n",
      "Epoch 21, Batch 122, Loss: 0.622351348400116\n",
      "Epoch 21, Batch 123, Loss: 0.351886510848999\n",
      "Epoch 21, Batch 124, Loss: 0.40383997559547424\n",
      "Epoch 21, Batch 125, Loss: 0.3460005819797516\n",
      "Epoch 21, Batch 126, Loss: 0.4260788559913635\n",
      "Epoch 21, Batch 127, Loss: 0.3724502921104431\n",
      "Epoch 21, Batch 128, Loss: 0.43354377150535583\n",
      "Epoch 21, Batch 129, Loss: 0.44459646940231323\n",
      "Epoch 21, Batch 130, Loss: 0.3426734507083893\n",
      "Epoch 21, Batch 131, Loss: 0.5156381726264954\n",
      "Epoch 21, Batch 132, Loss: 0.3535294234752655\n",
      "Epoch 21, Batch 133, Loss: 0.2462897002696991\n",
      "Epoch 21, Batch 134, Loss: 0.5008953809738159\n",
      "Epoch 21, Batch 135, Loss: 0.4710257053375244\n",
      "Epoch 21, Batch 136, Loss: 0.5062789916992188\n",
      "Epoch 21, Batch 137, Loss: 0.40810999274253845\n",
      "Epoch 21, Batch 138, Loss: 0.3685767352581024\n",
      "Epoch 21, Batch 139, Loss: 0.26900094747543335\n",
      "Epoch 21, Batch 140, Loss: 0.4917339086532593\n",
      "Epoch 21, Batch 141, Loss: 0.42673978209495544\n",
      "Epoch 21, Batch 142, Loss: 0.3658396601676941\n",
      "Epoch 21, Batch 143, Loss: 0.3622412383556366\n",
      "Epoch 21, Batch 144, Loss: 0.3364471197128296\n",
      "Epoch 21, Batch 145, Loss: 0.2946620583534241\n",
      "Epoch 21, Batch 146, Loss: 0.5513231158256531\n",
      "Epoch 21, Batch 147, Loss: 0.41364964842796326\n",
      "Epoch 21, Batch 148, Loss: 0.34254780411720276\n",
      "Epoch 21, Batch 149, Loss: 0.5411244630813599\n",
      "Epoch 21, Batch 150, Loss: 0.2889626622200012\n",
      "Epoch 21, Batch 151, Loss: 0.5270953178405762\n",
      "Epoch 21, Batch 152, Loss: 0.5944861173629761\n",
      "Epoch 21, Batch 153, Loss: 0.3694223165512085\n",
      "Epoch 21, Batch 154, Loss: 0.5193832516670227\n",
      "Epoch 21, Batch 155, Loss: 0.45200470089912415\n",
      "Epoch 21, Batch 156, Loss: 0.6637926697731018\n",
      "Epoch 21, Batch 157, Loss: 0.31842395663261414\n",
      "Epoch 21, Batch 158, Loss: 0.5511105060577393\n",
      "Epoch 21, Batch 159, Loss: 0.4002816677093506\n",
      "Epoch 21, Batch 160, Loss: 0.3588913381099701\n",
      "Epoch 21, Batch 161, Loss: 0.3720878064632416\n",
      "Epoch 21, Batch 162, Loss: 0.38922178745269775\n",
      "Epoch 21, Batch 163, Loss: 0.29985782504081726\n",
      "Epoch 21, Batch 164, Loss: 0.3338581919670105\n",
      "Epoch 21, Batch 165, Loss: 0.2566259801387787\n",
      "Epoch 21, Batch 166, Loss: 0.3673756718635559\n",
      "Epoch 21, Batch 167, Loss: 0.509330153465271\n",
      "Epoch 21, Batch 168, Loss: 0.3235889971256256\n",
      "Epoch 21, Batch 169, Loss: 0.33007773756980896\n",
      "Epoch 21, Batch 170, Loss: 0.33161282539367676\n",
      "Epoch 21, Batch 171, Loss: 0.5857858657836914\n",
      "Epoch 21, Batch 172, Loss: 0.39717623591423035\n",
      "Epoch 21, Batch 173, Loss: 0.23611615598201752\n",
      "Epoch 21, Batch 174, Loss: 0.381612628698349\n",
      "Epoch 21, Batch 175, Loss: 0.3032657206058502\n",
      "Epoch 21, Batch 176, Loss: 0.30792051553726196\n",
      "Epoch 21, Batch 177, Loss: 0.4542102813720703\n",
      "Epoch 21, Batch 178, Loss: 0.3798438012599945\n",
      "Epoch 21, Batch 179, Loss: 0.43537968397140503\n",
      "Epoch 21, Batch 180, Loss: 0.3772009611129761\n",
      "Epoch 21, Batch 181, Loss: 0.3300645649433136\n",
      "Epoch 21, Batch 182, Loss: 0.4726535379886627\n",
      "Epoch 21, Batch 183, Loss: 0.34148842096328735\n",
      "Epoch 21, Batch 184, Loss: 0.4456700086593628\n",
      "Epoch 21, Batch 185, Loss: 0.37793079018592834\n",
      "Epoch 21, Batch 186, Loss: 0.4475434720516205\n",
      "Epoch 21, Batch 187, Loss: 0.2665971517562866\n",
      "Epoch 21, Batch 188, Loss: 0.4143894612789154\n",
      "Epoch 21, Batch 189, Loss: 0.4550063908100128\n",
      "Epoch 21, Batch 190, Loss: 0.43943166732788086\n",
      "Epoch 21, Batch 191, Loss: 0.6314461827278137\n",
      "Epoch 21, Batch 192, Loss: 0.38470548391342163\n",
      "Epoch 21, Batch 193, Loss: 0.3767118752002716\n",
      "Epoch 21, Batch 194, Loss: 0.35692939162254333\n",
      "Epoch 21, Batch 195, Loss: 0.5113242268562317\n",
      "Epoch 21, Batch 196, Loss: 0.517673134803772\n",
      "Epoch 21, Batch 197, Loss: 0.3446921110153198\n",
      "Epoch 21, Batch 198, Loss: 0.5192432403564453\n",
      "Epoch 21, Batch 199, Loss: 0.5277131199836731\n",
      "Epoch 21, Batch 200, Loss: 0.31172090768814087\n",
      "Epoch 21, Batch 201, Loss: 0.41890811920166016\n",
      "Epoch 21, Batch 202, Loss: 0.4181913733482361\n",
      "Epoch 21, Batch 203, Loss: 0.4571528434753418\n",
      "Epoch 21, Batch 204, Loss: 0.2791016101837158\n",
      "Epoch 21, Batch 205, Loss: 0.3122635781764984\n",
      "Epoch 21, Batch 206, Loss: 0.35069209337234497\n",
      "Epoch 21, Batch 207, Loss: 0.5097446441650391\n",
      "Epoch 21, Batch 208, Loss: 0.1906288117170334\n",
      "Epoch 21, Batch 209, Loss: 0.4354669153690338\n",
      "Epoch 21, Batch 210, Loss: 0.32740750908851624\n",
      "Epoch 21, Batch 211, Loss: 0.5658165812492371\n",
      "Epoch 21, Batch 212, Loss: 0.25853103399276733\n",
      "Epoch 21, Batch 213, Loss: 0.1829877644777298\n",
      "Epoch 21, Batch 214, Loss: 0.3554563820362091\n",
      "Epoch 21, Batch 215, Loss: 0.26083841919898987\n",
      "Epoch 21, Batch 216, Loss: 0.42150747776031494\n",
      "Epoch 21, Batch 217, Loss: 0.41196149587631226\n",
      "Epoch 21, Batch 218, Loss: 0.5066161155700684\n",
      "Epoch 21, Batch 219, Loss: 0.38396209478378296\n",
      "Epoch 21, Batch 220, Loss: 0.265945166349411\n",
      "Epoch 21, Batch 221, Loss: 0.38212645053863525\n",
      "Epoch 21, Batch 222, Loss: 0.30699920654296875\n",
      "Epoch 21, Batch 223, Loss: 0.3800554871559143\n",
      "Epoch 21, Batch 224, Loss: 0.5184710621833801\n",
      "Epoch 21, Batch 225, Loss: 0.25789719820022583\n",
      "Epoch 21, Batch 226, Loss: 0.3514648973941803\n",
      "Epoch 21, Batch 227, Loss: 0.3258343040943146\n",
      "Epoch 21, Batch 228, Loss: 0.47028544545173645\n",
      "Epoch 21, Batch 229, Loss: 0.48338794708251953\n",
      "Epoch 21, Batch 230, Loss: 0.4372550845146179\n",
      "Epoch 21, Batch 231, Loss: 0.5147393345832825\n",
      "Epoch 21, Batch 232, Loss: 0.6518046855926514\n",
      "Epoch 21, Batch 233, Loss: 0.2749704122543335\n",
      "Epoch 21, Batch 234, Loss: 0.522040843963623\n",
      "Epoch 21, Batch 235, Loss: 0.5576652884483337\n",
      "Epoch 21, Batch 236, Loss: 0.4094325304031372\n",
      "Epoch 21, Batch 237, Loss: 0.33615341782569885\n",
      "Epoch 21, Batch 238, Loss: 0.3944626450538635\n",
      "Epoch 21, Batch 239, Loss: 0.36715590953826904\n",
      "Epoch 21, Batch 240, Loss: 0.259576678276062\n",
      "Epoch 21, Batch 241, Loss: 0.39132067561149597\n",
      "Epoch 21, Batch 242, Loss: 0.44086435437202454\n",
      "Epoch 21, Batch 243, Loss: 0.3474994897842407\n",
      "Epoch 21, Batch 244, Loss: 0.4372293949127197\n",
      "Epoch 21, Batch 245, Loss: 0.3972395062446594\n",
      "Epoch 21, Batch 246, Loss: 0.30281180143356323\n",
      "Epoch 21, Batch 247, Loss: 0.3302179276943207\n",
      "Epoch 21, Batch 248, Loss: 0.49902665615081787\n",
      "Epoch 21, Batch 249, Loss: 0.423059344291687\n",
      "Epoch 21, Batch 250, Loss: 0.43075495958328247\n",
      "Epoch 21, Batch 251, Loss: 0.6170439124107361\n",
      "Epoch 21, Batch 252, Loss: 0.3184053897857666\n",
      "Epoch 21, Batch 253, Loss: 0.3672533929347992\n",
      "Epoch 21, Batch 254, Loss: 0.3066059947013855\n",
      "Epoch 21, Batch 255, Loss: 0.29303064942359924\n",
      "Epoch 21, Batch 256, Loss: 0.4584343135356903\n",
      "Epoch 21, Batch 257, Loss: 0.4030799865722656\n",
      "Epoch 21, Batch 258, Loss: 0.33854085206985474\n",
      "Epoch 21, Batch 259, Loss: 0.4005484879016876\n",
      "Epoch 21, Batch 260, Loss: 0.33360517024993896\n",
      "Epoch 21, Batch 261, Loss: 0.24830150604248047\n",
      "Epoch 21, Batch 262, Loss: 0.40557482838630676\n",
      "Epoch 21, Batch 263, Loss: 0.6104561686515808\n",
      "Epoch 21, Batch 264, Loss: 0.3662782311439514\n",
      "Epoch 21, Batch 265, Loss: 0.2614840865135193\n",
      "Epoch 21, Batch 266, Loss: 0.5085843205451965\n",
      "Epoch 21, Batch 267, Loss: 0.5977340936660767\n",
      "Epoch 21, Batch 268, Loss: 0.36378413438796997\n",
      "Epoch 21, Batch 269, Loss: 0.3547060489654541\n",
      "Epoch 21, Batch 270, Loss: 0.40273016691207886\n",
      "Epoch 21, Batch 271, Loss: 0.32199883460998535\n",
      "Epoch 21, Batch 272, Loss: 0.31111085414886475\n",
      "Epoch 21, Batch 273, Loss: 0.45986247062683105\n",
      "Epoch 21, Batch 274, Loss: 0.5138827562332153\n",
      "Epoch 21, Batch 275, Loss: 0.5141534805297852\n",
      "Epoch 21, Batch 276, Loss: 0.419551283121109\n",
      "Epoch 21, Batch 277, Loss: 0.45400574803352356\n",
      "Epoch 21, Batch 278, Loss: 0.4459935426712036\n",
      "Epoch 21, Batch 279, Loss: 0.36734429001808167\n",
      "Epoch 21, Batch 280, Loss: 0.25859472155570984\n",
      "Epoch 21, Batch 281, Loss: 0.4043448567390442\n",
      "Epoch 21, Batch 282, Loss: 0.5060869455337524\n",
      "Epoch 21, Batch 283, Loss: 0.3816479742527008\n",
      "Epoch 21, Batch 284, Loss: 0.36412277817726135\n",
      "Epoch 21, Batch 285, Loss: 0.302073210477829\n",
      "Epoch 21, Batch 286, Loss: 0.43687495589256287\n",
      "Epoch 21, Batch 287, Loss: 0.30990105867385864\n",
      "Epoch 21, Batch 288, Loss: 0.42389124631881714\n",
      "Epoch 21, Batch 289, Loss: 0.3668276071548462\n",
      "Epoch 21, Batch 290, Loss: 0.39503705501556396\n",
      "Epoch 21, Batch 291, Loss: 0.6696940660476685\n",
      "Epoch 21, Batch 292, Loss: 0.2567967474460602\n",
      "Epoch 21, Batch 293, Loss: 0.3657211661338806\n",
      "Epoch 21, Batch 294, Loss: 0.28476694226264954\n",
      "Epoch 21, Batch 295, Loss: 0.4747461974620819\n",
      "Epoch 21, Batch 296, Loss: 0.7082237005233765\n",
      "Epoch 21, Batch 297, Loss: 0.34129559993743896\n",
      "Epoch 21, Batch 298, Loss: 0.3214508295059204\n",
      "Epoch 21, Batch 299, Loss: 0.5401978492736816\n",
      "Epoch 21, Batch 300, Loss: 0.3352903127670288\n",
      "Epoch 21, Batch 301, Loss: 0.3818637728691101\n",
      "Epoch 21, Batch 302, Loss: 0.3781891465187073\n",
      "Epoch 21, Batch 303, Loss: 0.3575436770915985\n",
      "Epoch 21, Batch 304, Loss: 0.5174497365951538\n",
      "Epoch 21, Batch 305, Loss: 0.2677531838417053\n",
      "Epoch 21, Batch 306, Loss: 0.35006874799728394\n",
      "Epoch 21, Batch 307, Loss: 0.6319302320480347\n",
      "Epoch 21, Batch 308, Loss: 0.5220654606819153\n",
      "Epoch 21, Batch 309, Loss: 0.27954596281051636\n",
      "Epoch 21, Batch 310, Loss: 0.5328110456466675\n",
      "Epoch 21, Batch 311, Loss: 0.530742883682251\n",
      "Epoch 21, Batch 312, Loss: 0.3992162346839905\n",
      "Epoch 21, Batch 313, Loss: 0.43766796588897705\n",
      "Epoch 21, Batch 314, Loss: 0.4410048723220825\n",
      "Epoch 21, Batch 315, Loss: 0.3186708688735962\n",
      "Epoch 21, Batch 316, Loss: 0.34837114810943604\n",
      "Epoch 21, Batch 317, Loss: 0.4000372290611267\n",
      "Epoch 21, Batch 318, Loss: 0.2756432890892029\n",
      "Epoch 21, Batch 319, Loss: 0.45500466227531433\n",
      "Epoch 21, Batch 320, Loss: 0.3706602454185486\n",
      "Epoch 21, Batch 321, Loss: 0.32607778906822205\n",
      "Epoch 21, Batch 322, Loss: 0.3872029483318329\n",
      "Epoch 21, Batch 323, Loss: 0.3134942352771759\n",
      "Epoch 21, Batch 324, Loss: 0.3792475461959839\n",
      "Epoch 21, Batch 325, Loss: 0.32356545329093933\n",
      "Epoch 21, Batch 326, Loss: 0.5393290519714355\n",
      "Epoch 21, Batch 327, Loss: 0.28173407912254333\n",
      "Epoch 21, Batch 328, Loss: 0.5970163345336914\n",
      "Epoch 21, Batch 329, Loss: 0.2911299467086792\n",
      "Epoch 21, Batch 330, Loss: 0.29043829441070557\n",
      "Epoch 21, Batch 331, Loss: 0.32232922315597534\n",
      "Epoch 21, Batch 332, Loss: 0.39956536889076233\n",
      "Epoch 21, Batch 333, Loss: 0.4098106920719147\n",
      "Epoch 21, Batch 334, Loss: 0.3265507221221924\n",
      "Epoch 21, Batch 335, Loss: 0.24807807803153992\n",
      "Epoch 21, Batch 336, Loss: 0.5430722832679749\n",
      "Epoch 21, Batch 337, Loss: 0.31130820512771606\n",
      "Epoch 21, Batch 338, Loss: 0.26968714594841003\n",
      "Epoch 21, Batch 339, Loss: 0.38673603534698486\n",
      "Epoch 21, Batch 340, Loss: 0.4190356433391571\n",
      "Epoch 21, Batch 341, Loss: 0.3430602550506592\n",
      "Epoch 21, Batch 342, Loss: 0.536881148815155\n",
      "Epoch 21, Batch 343, Loss: 0.47371649742126465\n",
      "Epoch 21, Batch 344, Loss: 0.34019502997398376\n",
      "Epoch 21, Batch 345, Loss: 0.4877525269985199\n",
      "Epoch 21, Batch 346, Loss: 0.42814183235168457\n",
      "Epoch 21, Batch 347, Loss: 0.44624677300453186\n",
      "Epoch 21, Batch 348, Loss: 0.4065149128437042\n",
      "Epoch 21, Batch 349, Loss: 0.4062941074371338\n",
      "Epoch 21, Batch 350, Loss: 0.46726900339126587\n",
      "Epoch 21, Batch 351, Loss: 0.450821191072464\n",
      "Epoch 21, Batch 352, Loss: 0.340655654668808\n",
      "Epoch 21, Batch 353, Loss: 0.3363129794597626\n",
      "Epoch 21, Batch 354, Loss: 0.5151486992835999\n",
      "Epoch 21, Batch 355, Loss: 0.22384493052959442\n",
      "Epoch 21, Batch 356, Loss: 0.24047532677650452\n",
      "Epoch 21, Batch 357, Loss: 0.5312725901603699\n",
      "Epoch 21, Batch 358, Loss: 0.5461000800132751\n",
      "Epoch 21, Batch 359, Loss: 0.598676860332489\n",
      "Epoch 21, Batch 360, Loss: 0.39145416021347046\n",
      "Epoch 21, Batch 361, Loss: 0.4031260907649994\n",
      "Epoch 21, Batch 362, Loss: 0.31779977679252625\n",
      "Epoch 21, Batch 363, Loss: 0.6802858114242554\n",
      "Epoch 21, Batch 364, Loss: 0.5142799019813538\n",
      "Epoch 21, Batch 365, Loss: 0.323907732963562\n",
      "Epoch 21, Batch 366, Loss: 0.5019470453262329\n",
      "Epoch 21, Batch 367, Loss: 0.22606298327445984\n",
      "Epoch 21, Batch 368, Loss: 0.5115439891815186\n",
      "Epoch 21, Batch 369, Loss: 0.4052036702632904\n",
      "Epoch 21, Batch 370, Loss: 0.2943057119846344\n",
      "Epoch 21, Batch 371, Loss: 0.36454683542251587\n",
      "Epoch 21, Batch 372, Loss: 0.2901776432991028\n",
      "Epoch 21, Batch 373, Loss: 0.32722437381744385\n",
      "Epoch 21, Batch 374, Loss: 0.5104938745498657\n",
      "Epoch 21, Batch 375, Loss: 0.47297123074531555\n",
      "Epoch 21, Batch 376, Loss: 0.40034788846969604\n",
      "Epoch 21, Batch 377, Loss: 0.45687374472618103\n",
      "Epoch 21, Batch 378, Loss: 0.3300643265247345\n",
      "Epoch 21, Batch 379, Loss: 0.37019047141075134\n",
      "Epoch 21, Batch 380, Loss: 0.3495248258113861\n",
      "Epoch 21, Batch 381, Loss: 0.41762271523475647\n",
      "Epoch 21, Batch 382, Loss: 0.3060756027698517\n",
      "Epoch 21, Batch 383, Loss: 0.2566833794116974\n",
      "Epoch 21, Batch 384, Loss: 0.37909984588623047\n",
      "Epoch 21, Batch 385, Loss: 0.4100886881351471\n",
      "Epoch 21, Batch 386, Loss: 0.5508213043212891\n",
      "Epoch 21, Batch 387, Loss: 0.48659536242485046\n",
      "Epoch 21, Batch 388, Loss: 0.3562045097351074\n",
      "Epoch 21, Batch 389, Loss: 0.6471736431121826\n",
      "Epoch 21, Batch 390, Loss: 0.35009992122650146\n",
      "Epoch 21, Batch 391, Loss: 0.35657185316085815\n",
      "Epoch 21, Batch 392, Loss: 0.4435511827468872\n",
      "Epoch 21, Batch 393, Loss: 0.36378079652786255\n",
      "Epoch 21, Batch 394, Loss: 0.34922492504119873\n",
      "Epoch 21, Batch 395, Loss: 0.3850773572921753\n",
      "Epoch 21, Batch 396, Loss: 0.5784326791763306\n",
      "Epoch 21, Batch 397, Loss: 0.3285585343837738\n",
      "Epoch 21, Batch 398, Loss: 0.5220816731452942\n",
      "Epoch 21, Batch 399, Loss: 0.2686571776866913\n",
      "Epoch 21, Batch 400, Loss: 0.5362461805343628\n",
      "Epoch 21, Batch 401, Loss: 0.4548303186893463\n",
      "Epoch 21, Batch 402, Loss: 0.23305872082710266\n",
      "Epoch 21, Batch 403, Loss: 0.2808668911457062\n",
      "Epoch 21, Batch 404, Loss: 0.2830394208431244\n",
      "Epoch 21, Batch 405, Loss: 0.3605891168117523\n",
      "Epoch 21, Batch 406, Loss: 0.20885224640369415\n",
      "Epoch 21, Batch 407, Loss: 0.47045063972473145\n",
      "Epoch 21, Batch 408, Loss: 0.4087086319923401\n",
      "Epoch 21, Batch 409, Loss: 0.42638179659843445\n",
      "Epoch 21, Batch 410, Loss: 0.30449962615966797\n",
      "Epoch 21, Batch 411, Loss: 0.378525972366333\n",
      "Epoch 21, Batch 412, Loss: 0.6138403415679932\n",
      "Epoch 21, Batch 413, Loss: 0.5296725034713745\n",
      "Epoch 21, Batch 414, Loss: 0.5287551879882812\n",
      "Epoch 21, Batch 415, Loss: 0.3603743612766266\n",
      "Epoch 21, Batch 416, Loss: 0.34728240966796875\n",
      "Epoch 21, Batch 417, Loss: 0.44893842935562134\n",
      "Epoch 21, Batch 418, Loss: 0.400553435087204\n",
      "Epoch 21, Batch 419, Loss: 0.45229393243789673\n",
      "Epoch 21, Batch 420, Loss: 0.32035762071609497\n",
      "Epoch 21, Batch 421, Loss: 0.3838624060153961\n",
      "Epoch 21, Batch 422, Loss: 0.38548940420150757\n",
      "Epoch 21, Batch 423, Loss: 0.40906232595443726\n",
      "Epoch 21, Batch 424, Loss: 0.43323761224746704\n",
      "Epoch 21, Batch 425, Loss: 0.3780324459075928\n",
      "Epoch 21, Batch 426, Loss: 0.3586626648902893\n",
      "Epoch 21, Batch 427, Loss: 0.373876690864563\n",
      "Epoch 21, Batch 428, Loss: 0.2967245578765869\n",
      "Epoch 21, Batch 429, Loss: 0.4241180717945099\n",
      "Epoch 21, Batch 430, Loss: 0.34088045358657837\n",
      "Epoch 21, Batch 431, Loss: 0.42643025517463684\n",
      "Epoch 21, Batch 432, Loss: 0.30003997683525085\n",
      "Epoch 21, Batch 433, Loss: 0.7624459862709045\n",
      "Epoch 21, Batch 434, Loss: 0.33588728308677673\n",
      "Epoch 21, Batch 435, Loss: 0.29968804121017456\n",
      "Epoch 21, Batch 436, Loss: 0.49342554807662964\n",
      "Epoch 21, Batch 437, Loss: 0.48679491877555847\n",
      "Epoch 21, Batch 438, Loss: 0.41287946701049805\n",
      "Epoch 21, Batch 439, Loss: 0.24814373254776\n",
      "Epoch 21, Batch 440, Loss: 0.36691272258758545\n",
      "Epoch 21, Batch 441, Loss: 0.4085685610771179\n",
      "Epoch 21, Batch 442, Loss: 0.6169455051422119\n",
      "Epoch 21, Batch 443, Loss: 0.43636542558670044\n",
      "Epoch 21, Batch 444, Loss: 0.5154499411582947\n",
      "Epoch 21, Batch 445, Loss: 0.38600194454193115\n",
      "Epoch 21, Batch 446, Loss: 0.30518755316734314\n",
      "Epoch 21, Batch 447, Loss: 0.23518750071525574\n",
      "Epoch 21, Batch 448, Loss: 0.4232032299041748\n",
      "Epoch 21, Batch 449, Loss: 0.249578595161438\n",
      "Epoch 21, Batch 450, Loss: 0.352927029132843\n",
      "Epoch 21, Batch 451, Loss: 0.32970699667930603\n",
      "Epoch 21, Batch 452, Loss: 0.26461151242256165\n",
      "Epoch 21, Batch 453, Loss: 0.35765692591667175\n",
      "Epoch 21, Batch 454, Loss: 0.23274001479148865\n",
      "Epoch 21, Batch 455, Loss: 0.2759911119937897\n",
      "Epoch 21, Batch 456, Loss: 0.2796061635017395\n",
      "Epoch 21, Batch 457, Loss: 0.4958699345588684\n",
      "Epoch 21, Batch 458, Loss: 0.4747442603111267\n",
      "Epoch 21, Batch 459, Loss: 0.4303135573863983\n",
      "Epoch 21, Batch 460, Loss: 0.23285353183746338\n",
      "Epoch 21, Batch 461, Loss: 0.5075246691703796\n",
      "Epoch 21, Batch 462, Loss: 0.27504754066467285\n",
      "Epoch 21, Batch 463, Loss: 0.2476094365119934\n",
      "Epoch 21, Batch 464, Loss: 0.4229373037815094\n",
      "Epoch 21, Batch 465, Loss: 0.30261853337287903\n",
      "Epoch 21, Batch 466, Loss: 0.4071906805038452\n",
      "Epoch 21, Batch 467, Loss: 0.3826214373111725\n",
      "Epoch 21, Batch 468, Loss: 0.3029298782348633\n",
      "Epoch 21, Batch 469, Loss: 0.27059897780418396\n",
      "Epoch 21, Batch 470, Loss: 0.3229006230831146\n",
      "Epoch 21, Batch 471, Loss: 0.3846183717250824\n",
      "Epoch 21, Batch 472, Loss: 0.4771297574043274\n",
      "Epoch 21, Batch 473, Loss: 0.5031477808952332\n",
      "Epoch 21, Batch 474, Loss: 0.43486133217811584\n",
      "Epoch 21, Batch 475, Loss: 0.43092063069343567\n",
      "Epoch 21, Batch 476, Loss: 0.34320592880249023\n",
      "Epoch 21, Batch 477, Loss: 0.41413867473602295\n",
      "Epoch 21, Batch 478, Loss: 0.37492233514785767\n",
      "Epoch 21, Batch 479, Loss: 0.3783610463142395\n",
      "Epoch 21, Batch 480, Loss: 0.3322741687297821\n",
      "Epoch 21, Batch 481, Loss: 0.3484886586666107\n",
      "Epoch 21, Batch 482, Loss: 0.3477078080177307\n",
      "Epoch 21, Batch 483, Loss: 0.25276026129722595\n",
      "Epoch 21, Batch 484, Loss: 0.5609084963798523\n",
      "Epoch 21, Batch 485, Loss: 0.36904218792915344\n",
      "Epoch 21, Batch 486, Loss: 0.2631247341632843\n",
      "Epoch 21, Batch 487, Loss: 0.557503342628479\n",
      "Epoch 21, Batch 488, Loss: 0.22567583620548248\n",
      "Epoch 21, Batch 489, Loss: 0.3591378927230835\n",
      "Epoch 21, Batch 490, Loss: 0.46681147813796997\n",
      "Epoch 21, Batch 491, Loss: 0.5423907041549683\n",
      "Epoch 21, Batch 492, Loss: 0.47985202074050903\n",
      "Epoch 21, Batch 493, Loss: 0.2387806475162506\n",
      "Epoch 21, Batch 494, Loss: 0.5898939371109009\n",
      "Epoch 21, Batch 495, Loss: 0.3989815413951874\n",
      "Epoch 21, Batch 496, Loss: 0.3383325934410095\n",
      "Epoch 21, Batch 497, Loss: 0.23264667391777039\n",
      "Epoch 21, Batch 498, Loss: 0.3380199074745178\n",
      "Epoch 21, Batch 499, Loss: 0.5437741875648499\n",
      "Epoch 21, Batch 500, Loss: 0.282700777053833\n",
      "Epoch 21, Batch 501, Loss: 0.45475950837135315\n",
      "Epoch 21, Batch 502, Loss: 0.3333304524421692\n",
      "Epoch 21, Batch 503, Loss: 0.4484784007072449\n",
      "Epoch 21, Batch 504, Loss: 0.6221396327018738\n",
      "Epoch 21, Batch 505, Loss: 0.32767871022224426\n",
      "Epoch 21, Batch 506, Loss: 0.3549858033657074\n",
      "Epoch 21, Batch 507, Loss: 0.24038609862327576\n",
      "Epoch 21, Batch 508, Loss: 0.3674595355987549\n",
      "Epoch 21, Batch 509, Loss: 0.4780285060405731\n",
      "Epoch 21, Batch 510, Loss: 0.40730899572372437\n",
      "Epoch 21, Batch 511, Loss: 0.3700244426727295\n",
      "Epoch 21, Batch 512, Loss: 0.3750459849834442\n",
      "Epoch 21, Batch 513, Loss: 0.37676727771759033\n",
      "Epoch 21, Batch 514, Loss: 0.34585124254226685\n",
      "Epoch 21, Batch 515, Loss: 0.33771345019340515\n",
      "Epoch 21, Batch 516, Loss: 0.33910810947418213\n",
      "Epoch 21, Batch 517, Loss: 0.3957115411758423\n",
      "Epoch 21, Batch 518, Loss: 0.45410940051078796\n",
      "Epoch 21, Batch 519, Loss: 0.4124584197998047\n",
      "Epoch 21, Batch 520, Loss: 0.6837337017059326\n",
      "Epoch 21, Batch 521, Loss: 0.3445998430252075\n",
      "Epoch 21, Batch 522, Loss: 0.5035359263420105\n",
      "Epoch 21, Batch 523, Loss: 0.39281126856803894\n",
      "Epoch 21, Batch 524, Loss: 0.42596113681793213\n",
      "Epoch 21, Batch 525, Loss: 0.4239969551563263\n",
      "Epoch 21, Batch 526, Loss: 0.2769090235233307\n",
      "Epoch 21, Batch 527, Loss: 0.4260411262512207\n",
      "Epoch 21, Batch 528, Loss: 0.42401930689811707\n",
      "Epoch 21, Batch 529, Loss: 0.3917444348335266\n",
      "Epoch 21, Batch 530, Loss: 0.3474731147289276\n",
      "Epoch 21, Batch 531, Loss: 0.36781778931617737\n",
      "Epoch 21, Batch 532, Loss: 0.44692206382751465\n",
      "Epoch 21, Batch 533, Loss: 0.5223466157913208\n",
      "Epoch 21, Batch 534, Loss: 0.35758155584335327\n",
      "Epoch 21, Batch 535, Loss: 0.2914767563343048\n",
      "Epoch 21, Batch 536, Loss: 0.494209885597229\n",
      "Epoch 21, Batch 537, Loss: 0.36181092262268066\n",
      "Epoch 21, Batch 538, Loss: 0.4083923101425171\n",
      "Epoch 21, Batch 539, Loss: 0.45968422293663025\n",
      "Epoch 21, Batch 540, Loss: 0.3554237186908722\n",
      "Epoch 21, Batch 541, Loss: 0.43490710854530334\n",
      "Epoch 21, Batch 542, Loss: 0.30507519841194153\n",
      "Epoch 21, Batch 543, Loss: 0.41654273867607117\n",
      "Epoch 21, Batch 544, Loss: 0.40762194991111755\n",
      "Epoch 21, Batch 545, Loss: 0.3596517741680145\n",
      "Epoch 21, Batch 546, Loss: 0.35323435068130493\n",
      "Epoch 21, Batch 547, Loss: 0.2250838279724121\n",
      "Epoch 21, Batch 548, Loss: 0.5515641570091248\n",
      "Epoch 21, Batch 549, Loss: 0.3791104853153229\n",
      "Epoch 21, Batch 550, Loss: 0.4119477868080139\n",
      "Epoch 21, Batch 551, Loss: 0.4646218419075012\n",
      "Epoch 21, Batch 552, Loss: 0.29150933027267456\n",
      "Epoch 21, Batch 553, Loss: 0.27856895327568054\n",
      "Epoch 21, Batch 554, Loss: 0.6342493295669556\n",
      "Epoch 21, Batch 555, Loss: 0.3384357988834381\n",
      "Epoch 21, Batch 556, Loss: 0.5312575101852417\n",
      "Epoch 21, Batch 557, Loss: 0.6378559470176697\n",
      "Epoch 21, Batch 558, Loss: 0.35386088490486145\n",
      "Epoch 21, Batch 559, Loss: 0.33180513978004456\n",
      "Epoch 21, Batch 560, Loss: 0.4137287735939026\n",
      "Epoch 21, Batch 561, Loss: 0.13385029137134552\n",
      "Epoch 21, Batch 562, Loss: 0.45801857113838196\n",
      "Epoch 21, Batch 563, Loss: 0.38609859347343445\n",
      "Epoch 21, Batch 564, Loss: 0.3044506311416626\n",
      "Epoch 21, Batch 565, Loss: 0.4961106777191162\n",
      "Epoch 21, Batch 566, Loss: 0.36521971225738525\n",
      "Epoch 21, Batch 567, Loss: 0.3505065143108368\n",
      "Epoch 21, Batch 568, Loss: 0.522004246711731\n",
      "Epoch 21, Batch 569, Loss: 0.4519004225730896\n",
      "Epoch 21, Batch 570, Loss: 0.4066730737686157\n",
      "Epoch 21, Batch 571, Loss: 0.3493673801422119\n",
      "Epoch 21, Batch 572, Loss: 0.4325798451900482\n",
      "Epoch 21, Batch 573, Loss: 0.40487414598464966\n",
      "Epoch 21, Batch 574, Loss: 0.280532568693161\n",
      "Epoch 21, Batch 575, Loss: 0.3624534010887146\n",
      "Epoch 21, Batch 576, Loss: 0.3322402834892273\n",
      "Epoch 21, Batch 577, Loss: 0.47940754890441895\n",
      "Epoch 21, Batch 578, Loss: 0.4040515720844269\n",
      "Epoch 21, Batch 579, Loss: 0.34659552574157715\n",
      "Epoch 21, Batch 580, Loss: 0.24546785652637482\n",
      "Epoch 21, Batch 581, Loss: 0.30494043231010437\n",
      "Epoch 21, Batch 582, Loss: 0.34623590111732483\n",
      "Epoch 21, Batch 583, Loss: 0.5377513766288757\n",
      "Epoch 21, Batch 584, Loss: 0.18284787237644196\n",
      "Epoch 21, Batch 585, Loss: 0.3812321424484253\n",
      "Epoch 21, Batch 586, Loss: 0.3926616907119751\n",
      "Epoch 21, Batch 587, Loss: 0.34225136041641235\n",
      "Epoch 21, Batch 588, Loss: 0.3437715172767639\n",
      "Epoch 21, Batch 589, Loss: 0.48856881260871887\n",
      "Epoch 21, Batch 590, Loss: 0.36271288990974426\n",
      "Epoch 21, Batch 591, Loss: 0.4266631007194519\n",
      "Epoch 21, Batch 592, Loss: 0.5546363592147827\n",
      "Epoch 21, Batch 593, Loss: 0.4434817135334015\n",
      "Epoch 21, Batch 594, Loss: 0.4701370596885681\n",
      "Epoch 21, Batch 595, Loss: 0.43982934951782227\n",
      "Epoch 21, Batch 596, Loss: 0.46246612071990967\n",
      "Epoch 21, Batch 597, Loss: 0.47717079520225525\n",
      "Epoch 21, Batch 598, Loss: 0.3160640597343445\n",
      "Epoch 21, Batch 599, Loss: 0.35698631405830383\n",
      "Epoch 21, Batch 600, Loss: 0.5107506513595581\n",
      "Epoch 21, Batch 601, Loss: 0.2896987199783325\n",
      "Epoch 21, Batch 602, Loss: 0.3787804841995239\n",
      "Epoch 21, Batch 603, Loss: 0.3097617030143738\n",
      "Epoch 21, Batch 604, Loss: 0.5142061710357666\n",
      "Epoch 21, Batch 605, Loss: 0.5656832456588745\n",
      "Epoch 21, Batch 606, Loss: 0.4055796265602112\n",
      "Epoch 21, Batch 607, Loss: 0.26763737201690674\n",
      "Epoch 21, Batch 608, Loss: 0.42199426889419556\n",
      "Epoch 21, Batch 609, Loss: 0.22363178431987762\n",
      "Epoch 21, Batch 610, Loss: 0.4704950451850891\n",
      "Epoch 21, Batch 611, Loss: 0.501078724861145\n",
      "Epoch 21, Batch 612, Loss: 0.3611748218536377\n",
      "Epoch 21, Batch 613, Loss: 0.3867343068122864\n",
      "Epoch 21, Batch 614, Loss: 0.35361459851264954\n",
      "Epoch 21, Batch 615, Loss: 0.3937589228153229\n",
      "Epoch 21, Batch 616, Loss: 0.43330833315849304\n",
      "Epoch 21, Batch 617, Loss: 0.30202850699424744\n",
      "Epoch 21, Batch 618, Loss: 0.41488954424858093\n",
      "Epoch 21, Batch 619, Loss: 0.352800190448761\n",
      "Epoch 21, Batch 620, Loss: 0.39000657200813293\n",
      "Epoch 21, Batch 621, Loss: 0.36462002992630005\n",
      "Epoch 21, Batch 622, Loss: 0.23187308013439178\n",
      "Epoch 21, Batch 623, Loss: 0.6138966083526611\n",
      "Epoch 21, Batch 624, Loss: 0.4617650508880615\n",
      "Epoch 21, Batch 625, Loss: 0.4816998243331909\n",
      "Epoch 21, Batch 626, Loss: 0.3962394893169403\n",
      "Epoch 21, Batch 627, Loss: 0.5130292773246765\n",
      "Epoch 21, Batch 628, Loss: 0.4043314754962921\n",
      "Epoch 21, Batch 629, Loss: 0.47082197666168213\n",
      "Epoch 21, Batch 630, Loss: 0.6021470427513123\n",
      "Epoch 21, Batch 631, Loss: 0.4120114743709564\n",
      "Epoch 21, Batch 632, Loss: 0.39721640944480896\n",
      "Epoch 21, Batch 633, Loss: 0.2968710660934448\n",
      "Epoch 21, Batch 634, Loss: 0.6105427742004395\n",
      "Epoch 21, Batch 635, Loss: 0.48452210426330566\n",
      "Epoch 21, Batch 636, Loss: 0.4303795397281647\n",
      "Epoch 21, Batch 637, Loss: 0.3166969418525696\n",
      "Epoch 21, Batch 638, Loss: 0.43737027049064636\n",
      "Epoch 21, Batch 639, Loss: 0.30952006578445435\n",
      "Epoch 21, Batch 640, Loss: 0.41245412826538086\n",
      "Epoch 21, Batch 641, Loss: 0.2203313410282135\n",
      "Epoch 21, Batch 642, Loss: 0.2727830410003662\n",
      "Epoch 21, Batch 643, Loss: 0.36925220489501953\n",
      "Epoch 21, Batch 644, Loss: 0.40153464674949646\n",
      "Epoch 21, Batch 645, Loss: 0.3002343773841858\n",
      "Epoch 21, Batch 646, Loss: 0.22807225584983826\n",
      "Epoch 21, Batch 647, Loss: 0.4108152389526367\n",
      "Epoch 21, Batch 648, Loss: 0.5468035340309143\n",
      "Epoch 21, Batch 649, Loss: 0.46389931440353394\n",
      "Epoch 21, Batch 650, Loss: 0.37425798177719116\n",
      "Epoch 21, Batch 651, Loss: 0.38742709159851074\n",
      "Epoch 21, Batch 652, Loss: 0.39493659138679504\n",
      "Epoch 21, Batch 653, Loss: 0.34084317088127136\n",
      "Epoch 21, Batch 654, Loss: 0.2509975731372833\n",
      "Epoch 21, Batch 655, Loss: 0.43830758333206177\n",
      "Epoch 21, Batch 656, Loss: 0.3775683343410492\n",
      "Epoch 21, Batch 657, Loss: 0.45781779289245605\n",
      "Epoch 21, Batch 658, Loss: 0.35174551606178284\n",
      "Epoch 21, Batch 659, Loss: 0.26771920919418335\n",
      "Epoch 21, Batch 660, Loss: 0.2783495783805847\n",
      "Epoch 21, Batch 661, Loss: 0.4965592622756958\n",
      "Epoch 21, Batch 662, Loss: 0.5266870260238647\n",
      "Epoch 21, Batch 663, Loss: 0.3004833161830902\n",
      "Epoch 21, Batch 664, Loss: 0.3018115758895874\n",
      "Epoch 21, Batch 665, Loss: 0.33794209361076355\n",
      "Epoch 21, Batch 666, Loss: 0.41806522011756897\n",
      "Epoch 21, Batch 667, Loss: 0.3741721510887146\n",
      "Epoch 21, Batch 668, Loss: 0.498263418674469\n",
      "Epoch 21, Batch 669, Loss: 0.3014894127845764\n",
      "Epoch 21, Batch 670, Loss: 0.48204973340034485\n",
      "Epoch 21, Batch 671, Loss: 0.42734265327453613\n",
      "Epoch 21, Batch 672, Loss: 0.572874903678894\n",
      "Epoch 21, Batch 673, Loss: 0.36064884066581726\n",
      "Epoch 21, Batch 674, Loss: 0.3294272720813751\n",
      "Epoch 21, Batch 675, Loss: 0.3470131456851959\n",
      "Epoch 21, Batch 676, Loss: 0.23219019174575806\n",
      "Epoch 21, Batch 677, Loss: 0.4853874444961548\n",
      "Epoch 21, Batch 678, Loss: 0.3195096552371979\n",
      "Epoch 21, Batch 679, Loss: 0.3574508726596832\n",
      "Epoch 21, Batch 680, Loss: 0.4297640025615692\n",
      "Epoch 21, Batch 681, Loss: 0.45916953682899475\n",
      "Epoch 21, Batch 682, Loss: 0.4468846917152405\n",
      "Epoch 21, Batch 683, Loss: 0.29649344086647034\n",
      "Epoch 21, Batch 684, Loss: 0.31641894578933716\n",
      "Epoch 21, Batch 685, Loss: 0.29265153408050537\n",
      "Epoch 21, Batch 686, Loss: 0.43576332926750183\n",
      "Epoch 21, Batch 687, Loss: 0.32136642932891846\n",
      "Epoch 21, Batch 688, Loss: 0.7568131685256958\n",
      "Epoch 21, Batch 689, Loss: 0.43885523080825806\n",
      "Epoch 21, Batch 690, Loss: 0.3982527256011963\n",
      "Epoch 21, Batch 691, Loss: 0.4854934811592102\n",
      "Epoch 21, Batch 692, Loss: 0.4204155504703522\n",
      "Epoch 21, Batch 693, Loss: 0.41267362236976624\n",
      "Epoch 21, Batch 694, Loss: 0.494830459356308\n",
      "Epoch 21, Batch 695, Loss: 0.3986285924911499\n",
      "Epoch 21, Batch 696, Loss: 0.34035974740982056\n",
      "Epoch 21, Batch 697, Loss: 0.437887579202652\n",
      "Epoch 21, Batch 698, Loss: 0.3455391824245453\n",
      "Epoch 21, Batch 699, Loss: 0.3739546835422516\n",
      "Epoch 21, Batch 700, Loss: 0.3667556941509247\n",
      "Epoch 21, Batch 701, Loss: 0.29547709226608276\n",
      "Epoch 21, Batch 702, Loss: 0.3716546893119812\n",
      "Epoch 21, Batch 703, Loss: 0.48390719294548035\n",
      "Epoch 21, Batch 704, Loss: 0.2776377201080322\n",
      "Epoch 21, Batch 705, Loss: 0.5350807905197144\n",
      "Epoch 21, Batch 706, Loss: 0.45072075724601746\n",
      "Epoch 21, Batch 707, Loss: 0.31225714087486267\n",
      "Epoch 21, Batch 708, Loss: 0.29716581106185913\n",
      "Epoch 21, Batch 709, Loss: 0.3836543560028076\n",
      "Epoch 21, Batch 710, Loss: 0.5095113515853882\n",
      "Epoch 21, Batch 711, Loss: 0.6607937216758728\n",
      "Epoch 21, Batch 712, Loss: 0.21991632878780365\n",
      "Epoch 21, Batch 713, Loss: 0.2858014702796936\n",
      "Epoch 21, Batch 714, Loss: 0.49323832988739014\n",
      "Epoch 21, Batch 715, Loss: 0.7545639276504517\n",
      "Epoch 21, Batch 716, Loss: 0.3268094062805176\n",
      "Epoch 21, Batch 717, Loss: 0.2674211263656616\n",
      "Epoch 21, Batch 718, Loss: 0.45737409591674805\n",
      "Epoch 21, Batch 719, Loss: 0.3581901788711548\n",
      "Epoch 21, Batch 720, Loss: 0.29037895798683167\n",
      "Epoch 21, Batch 721, Loss: 0.4782504141330719\n",
      "Epoch 21, Batch 722, Loss: 0.35957667231559753\n",
      "Epoch 21, Batch 723, Loss: 0.3919409215450287\n",
      "Epoch 21, Batch 724, Loss: 0.5619101524353027\n",
      "Epoch 21, Batch 725, Loss: 0.5607353448867798\n",
      "Epoch 21, Batch 726, Loss: 0.3939651846885681\n",
      "Epoch 21, Batch 727, Loss: 0.40162768959999084\n",
      "Epoch 21, Batch 728, Loss: 0.491423100233078\n",
      "Epoch 21, Batch 729, Loss: 0.44277703762054443\n",
      "Epoch 21, Batch 730, Loss: 0.3302251398563385\n",
      "Epoch 21, Batch 731, Loss: 0.4838108420372009\n",
      "Epoch 21, Batch 732, Loss: 0.5461519360542297\n",
      "Epoch 21, Batch 733, Loss: 0.3749614953994751\n",
      "Epoch 21, Batch 734, Loss: 0.4762963056564331\n",
      "Epoch 21, Batch 735, Loss: 0.31843307614326477\n",
      "Epoch 21, Batch 736, Loss: 0.43492987751960754\n",
      "Epoch 21, Batch 737, Loss: 0.4038080871105194\n",
      "Epoch 21, Batch 738, Loss: 0.4298804700374603\n",
      "Epoch 21, Batch 739, Loss: 0.3819597363471985\n",
      "Epoch 21, Batch 740, Loss: 0.33934158086776733\n",
      "Epoch 21, Batch 741, Loss: 0.4286789298057556\n",
      "Epoch 21, Batch 742, Loss: 0.3747206926345825\n",
      "Epoch 21, Batch 743, Loss: 0.46703222393989563\n",
      "Epoch 21, Batch 744, Loss: 0.25702640414237976\n",
      "Epoch 21, Batch 745, Loss: 0.2061205953359604\n",
      "Epoch 21, Batch 746, Loss: 0.5035910606384277\n",
      "Epoch 21, Batch 747, Loss: 0.2109958380460739\n",
      "Epoch 21, Batch 748, Loss: 0.42513155937194824\n",
      "Epoch 21, Batch 749, Loss: 0.342766135931015\n",
      "Epoch 21, Batch 750, Loss: 0.4543052315711975\n",
      "Epoch 21, Batch 751, Loss: 0.5322790741920471\n",
      "Epoch 21, Batch 752, Loss: 0.4946620762348175\n",
      "Epoch 21, Batch 753, Loss: 0.4451759457588196\n",
      "Epoch 21, Batch 754, Loss: 0.5129612684249878\n",
      "Epoch 21, Batch 755, Loss: 0.5309903621673584\n",
      "Epoch 21, Batch 756, Loss: 0.4152279496192932\n",
      "Epoch 21, Batch 757, Loss: 0.40352004766464233\n",
      "Epoch 21, Batch 758, Loss: 0.4985181391239166\n",
      "Epoch 21, Batch 759, Loss: 0.3964800238609314\n",
      "Epoch 21, Batch 760, Loss: 0.41172781586647034\n",
      "Epoch 21, Batch 761, Loss: 0.4566086530685425\n",
      "Epoch 21, Batch 762, Loss: 0.28736263513565063\n",
      "Epoch 21, Batch 763, Loss: 0.3884861469268799\n",
      "Epoch 21, Batch 764, Loss: 0.45972201228141785\n",
      "Epoch 21, Batch 765, Loss: 0.4114800989627838\n",
      "Epoch 21, Batch 766, Loss: 0.30505526065826416\n",
      "Epoch 21, Batch 767, Loss: 0.33067119121551514\n",
      "Epoch 21, Batch 768, Loss: 0.3878343403339386\n",
      "Epoch 21, Batch 769, Loss: 0.4099937379360199\n",
      "Epoch 21, Batch 770, Loss: 0.3502443730831146\n",
      "Epoch 21, Batch 771, Loss: 0.49099045991897583\n",
      "Epoch 21, Batch 772, Loss: 0.3480568528175354\n",
      "Epoch 21, Batch 773, Loss: 0.6040010452270508\n",
      "Epoch 21, Batch 774, Loss: 0.2524384558200836\n",
      "Epoch 21, Batch 775, Loss: 0.39441820979118347\n",
      "Epoch 21, Batch 776, Loss: 0.49673783779144287\n",
      "Epoch 21, Batch 777, Loss: 0.3768289089202881\n",
      "Epoch 21, Batch 778, Loss: 0.5716391801834106\n",
      "Epoch 21, Batch 779, Loss: 0.5436028838157654\n",
      "Epoch 21, Batch 780, Loss: 0.47995898127555847\n",
      "Epoch 21, Batch 781, Loss: 0.3394038677215576\n",
      "Epoch 21, Batch 782, Loss: 0.4839841425418854\n",
      "Epoch 21, Batch 783, Loss: 0.3166288137435913\n",
      "Epoch 21, Batch 784, Loss: 0.3220537304878235\n",
      "Epoch 21, Batch 785, Loss: 0.30092549324035645\n",
      "Epoch 21, Batch 786, Loss: 0.36765697598457336\n",
      "Epoch 21, Batch 787, Loss: 0.20879457890987396\n",
      "Epoch 21, Batch 788, Loss: 0.2926385700702667\n",
      "Epoch 21, Batch 789, Loss: 0.601189374923706\n",
      "Epoch 21, Batch 790, Loss: 0.5763018727302551\n",
      "Epoch 21, Batch 791, Loss: 0.4390745162963867\n",
      "Epoch 21, Batch 792, Loss: 0.3555936813354492\n",
      "Epoch 21, Batch 793, Loss: 0.4112016558647156\n",
      "Epoch 21, Batch 794, Loss: 0.2438894510269165\n",
      "Epoch 21, Batch 795, Loss: 0.3138996958732605\n",
      "Epoch 21, Batch 796, Loss: 0.39958441257476807\n",
      "Epoch 21, Batch 797, Loss: 0.38302093744277954\n",
      "Epoch 21, Batch 798, Loss: 0.3313952088356018\n",
      "Epoch 21, Batch 799, Loss: 0.2828102111816406\n",
      "Epoch 21, Batch 800, Loss: 0.32886651158332825\n",
      "Epoch 21, Batch 801, Loss: 0.40121492743492126\n",
      "Epoch 21, Batch 802, Loss: 0.6015458703041077\n",
      "Epoch 21, Batch 803, Loss: 0.3727761209011078\n",
      "Epoch 21, Batch 804, Loss: 0.14788131415843964\n",
      "Epoch 21, Batch 805, Loss: 0.3652573823928833\n",
      "Epoch 21, Batch 806, Loss: 0.46606874465942383\n",
      "Epoch 21, Batch 807, Loss: 0.5208824872970581\n",
      "Epoch 21, Batch 808, Loss: 0.5214519500732422\n",
      "Epoch 21, Batch 809, Loss: 0.39369937777519226\n",
      "Epoch 21, Batch 810, Loss: 0.5570890307426453\n",
      "Epoch 21, Batch 811, Loss: 0.47395429015159607\n",
      "Epoch 21, Batch 812, Loss: 0.4225578010082245\n",
      "Epoch 21, Batch 813, Loss: 0.4266119599342346\n",
      "Epoch 21, Batch 814, Loss: 0.5508394241333008\n",
      "Epoch 21, Batch 815, Loss: 0.3811016082763672\n",
      "Epoch 21, Batch 816, Loss: 0.46511590480804443\n",
      "Epoch 21, Batch 817, Loss: 0.23285403847694397\n",
      "Epoch 21, Batch 818, Loss: 0.35410887002944946\n",
      "Epoch 21, Batch 819, Loss: 0.45283451676368713\n",
      "Epoch 21, Batch 820, Loss: 0.35728850960731506\n",
      "Epoch 21, Batch 821, Loss: 0.49767744541168213\n",
      "Epoch 21, Batch 822, Loss: 0.3128317594528198\n",
      "Epoch 21, Batch 823, Loss: 0.3092290759086609\n",
      "Epoch 21, Batch 824, Loss: 0.2982475161552429\n",
      "Epoch 21, Batch 825, Loss: 0.3686041235923767\n",
      "Epoch 21, Batch 826, Loss: 0.3861045837402344\n",
      "Epoch 21, Batch 827, Loss: 0.2960061728954315\n",
      "Epoch 21, Batch 828, Loss: 0.3669624924659729\n",
      "Epoch 21, Batch 829, Loss: 0.33585119247436523\n",
      "Epoch 21, Batch 830, Loss: 0.5842891931533813\n",
      "Epoch 21, Batch 831, Loss: 0.6795046925544739\n",
      "Epoch 21, Batch 832, Loss: 0.3583831191062927\n",
      "Epoch 21, Batch 833, Loss: 0.35901546478271484\n",
      "Epoch 21, Batch 834, Loss: 0.29367995262145996\n",
      "Epoch 21, Batch 835, Loss: 0.43354344367980957\n",
      "Epoch 21, Batch 836, Loss: 0.5112704634666443\n",
      "Epoch 21, Batch 837, Loss: 0.4758913815021515\n",
      "Epoch 21, Batch 838, Loss: 0.4716591536998749\n",
      "Epoch 21, Batch 839, Loss: 0.43375152349472046\n",
      "Epoch 21, Batch 840, Loss: 0.34438514709472656\n",
      "Epoch 21, Batch 841, Loss: 0.3833371102809906\n",
      "Epoch 21, Batch 842, Loss: 0.3656899631023407\n",
      "Epoch 21, Batch 843, Loss: 0.3893004059791565\n",
      "Epoch 21, Batch 844, Loss: 0.47219350934028625\n",
      "Epoch 21, Batch 845, Loss: 0.2669968903064728\n",
      "Epoch 21, Batch 846, Loss: 0.19429272413253784\n",
      "Epoch 21, Batch 847, Loss: 0.3247617781162262\n",
      "Epoch 21, Batch 848, Loss: 0.5096665620803833\n",
      "Epoch 21, Batch 849, Loss: 0.548826277256012\n",
      "Epoch 21, Batch 850, Loss: 0.45110130310058594\n",
      "Epoch 21, Batch 851, Loss: 0.40032243728637695\n",
      "Epoch 21, Batch 852, Loss: 0.3149681091308594\n",
      "Epoch 21, Batch 853, Loss: 0.3262370228767395\n",
      "Epoch 21, Batch 854, Loss: 0.34986475110054016\n",
      "Epoch 21, Batch 855, Loss: 0.3367171585559845\n",
      "Epoch 21, Batch 856, Loss: 0.4778057634830475\n",
      "Epoch 21, Batch 857, Loss: 0.4861021637916565\n",
      "Epoch 21, Batch 858, Loss: 0.2955256998538971\n",
      "Epoch 21, Batch 859, Loss: 0.36245688796043396\n",
      "Epoch 21, Batch 860, Loss: 0.3862297236919403\n",
      "Epoch 21, Batch 861, Loss: 0.6165740489959717\n",
      "Epoch 21, Batch 862, Loss: 0.34960344433784485\n",
      "Epoch 21, Batch 863, Loss: 0.40300360321998596\n",
      "Epoch 21, Batch 864, Loss: 0.5323050022125244\n",
      "Epoch 21, Batch 865, Loss: 0.48319530487060547\n",
      "Epoch 21, Batch 866, Loss: 0.31287357211112976\n",
      "Epoch 21, Batch 867, Loss: 0.3912116587162018\n",
      "Epoch 21, Batch 868, Loss: 0.34682559967041016\n",
      "Epoch 21, Batch 869, Loss: 0.3741571605205536\n",
      "Epoch 21, Batch 870, Loss: 0.40765833854675293\n",
      "Epoch 21, Batch 871, Loss: 0.5267144441604614\n",
      "Epoch 21, Batch 872, Loss: 0.4102168679237366\n",
      "Epoch 21, Batch 873, Loss: 0.32721397280693054\n",
      "Epoch 21, Batch 874, Loss: 0.3851977586746216\n",
      "Epoch 21, Batch 875, Loss: 0.536916971206665\n",
      "Epoch 21, Batch 876, Loss: 0.38048669695854187\n",
      "Epoch 21, Batch 877, Loss: 0.21404655277729034\n",
      "Epoch 21, Batch 878, Loss: 0.4068135917186737\n",
      "Epoch 21, Batch 879, Loss: 0.5999909043312073\n",
      "Epoch 21, Batch 880, Loss: 0.4518854320049286\n",
      "Epoch 21, Batch 881, Loss: 0.41643187403678894\n",
      "Epoch 21, Batch 882, Loss: 0.34568992257118225\n",
      "Epoch 21, Batch 883, Loss: 0.2838280498981476\n",
      "Epoch 21, Batch 884, Loss: 0.30539342761039734\n",
      "Epoch 21, Batch 885, Loss: 0.3440748453140259\n",
      "Epoch 21, Batch 886, Loss: 0.3240615725517273\n",
      "Epoch 21, Batch 887, Loss: 0.445746511220932\n",
      "Epoch 21, Batch 888, Loss: 0.40941494703292847\n",
      "Epoch 21, Batch 889, Loss: 0.5127728581428528\n",
      "Epoch 21, Batch 890, Loss: 0.4876396059989929\n",
      "Epoch 21, Batch 891, Loss: 0.45452842116355896\n",
      "Epoch 21, Batch 892, Loss: 0.4157097041606903\n",
      "Epoch 21, Batch 893, Loss: 0.4367828369140625\n",
      "Epoch 21, Batch 894, Loss: 0.38641801476478577\n",
      "Epoch 21, Batch 895, Loss: 0.5293864607810974\n",
      "Epoch 21, Batch 896, Loss: 0.5192704200744629\n",
      "Epoch 21, Batch 897, Loss: 0.5158569812774658\n",
      "Epoch 21, Batch 898, Loss: 0.4680699408054352\n",
      "Epoch 21, Batch 899, Loss: 0.6037105321884155\n",
      "Epoch 21, Batch 900, Loss: 0.4239252805709839\n",
      "Epoch 21, Batch 901, Loss: 0.5394912362098694\n",
      "Epoch 21, Batch 902, Loss: 0.2988796830177307\n",
      "Epoch 21, Batch 903, Loss: 0.4510771930217743\n",
      "Epoch 21, Batch 904, Loss: 0.37445786595344543\n",
      "Epoch 21, Batch 905, Loss: 0.29881852865219116\n",
      "Epoch 21, Batch 906, Loss: 0.3245198726654053\n",
      "Epoch 21, Batch 907, Loss: 0.34590819478034973\n",
      "Epoch 21, Batch 908, Loss: 0.6235828399658203\n",
      "Epoch 21, Batch 909, Loss: 0.4541037082672119\n",
      "Epoch 21, Batch 910, Loss: 0.6168202757835388\n",
      "Epoch 21, Batch 911, Loss: 0.4449424147605896\n",
      "Epoch 21, Batch 912, Loss: 0.4905555546283722\n",
      "Epoch 21, Batch 913, Loss: 0.4195629358291626\n",
      "Epoch 21, Batch 914, Loss: 0.6507891416549683\n",
      "Epoch 21, Batch 915, Loss: 0.5297675132751465\n",
      "Epoch 21, Batch 916, Loss: 0.406935453414917\n",
      "Epoch 21, Batch 917, Loss: 0.3106784522533417\n",
      "Epoch 21, Batch 918, Loss: 0.28963369131088257\n",
      "Epoch 21, Batch 919, Loss: 0.38086220622062683\n",
      "Epoch 21, Batch 920, Loss: 0.4365467131137848\n",
      "Epoch 21, Batch 921, Loss: 0.44179287552833557\n",
      "Epoch 21, Batch 922, Loss: 0.5724542140960693\n",
      "Epoch 21, Batch 923, Loss: 0.3702840805053711\n",
      "Epoch 21, Batch 924, Loss: 0.5674074292182922\n",
      "Epoch 21, Batch 925, Loss: 0.47258397936820984\n",
      "Epoch 21, Batch 926, Loss: 0.44451403617858887\n",
      "Epoch 21, Batch 927, Loss: 0.3622397780418396\n",
      "Epoch 21, Batch 928, Loss: 0.6042694449424744\n",
      "Epoch 21, Batch 929, Loss: 0.3909769654273987\n",
      "Epoch 21, Batch 930, Loss: 0.31829601526260376\n",
      "Epoch 21, Batch 931, Loss: 0.30792102217674255\n",
      "Epoch 21, Batch 932, Loss: 0.4098528325557709\n",
      "Epoch 21, Batch 933, Loss: 0.6136864423751831\n",
      "Epoch 21, Batch 934, Loss: 0.27649280428886414\n",
      "Epoch 21, Batch 935, Loss: 0.3892965316772461\n",
      "Epoch 21, Batch 936, Loss: 0.3538496196269989\n",
      "Epoch 21, Batch 937, Loss: 0.15198080241680145\n",
      "Epoch 21, Batch 938, Loss: 0.2588289976119995\n",
      "Accuracy of train set: 0.8561833333333333\n",
      "Epoch 21, Batch 1, Test Loss: 0.3463643491268158\n",
      "Epoch 21, Batch 2, Test Loss: 0.5523181557655334\n",
      "Epoch 21, Batch 3, Test Loss: 0.42159366607666016\n",
      "Epoch 21, Batch 4, Test Loss: 0.4796018600463867\n",
      "Epoch 21, Batch 5, Test Loss: 0.4008800983428955\n",
      "Epoch 21, Batch 6, Test Loss: 0.4808408319950104\n",
      "Epoch 21, Batch 7, Test Loss: 0.42733117938041687\n",
      "Epoch 21, Batch 8, Test Loss: 0.37742334604263306\n",
      "Epoch 21, Batch 9, Test Loss: 0.35372352600097656\n",
      "Epoch 21, Batch 10, Test Loss: 0.4129883646965027\n",
      "Epoch 21, Batch 11, Test Loss: 0.2735147178173065\n",
      "Epoch 21, Batch 12, Test Loss: 0.3564474582672119\n",
      "Epoch 21, Batch 13, Test Loss: 0.49010926485061646\n",
      "Epoch 21, Batch 14, Test Loss: 0.3973248600959778\n",
      "Epoch 21, Batch 15, Test Loss: 0.22614482045173645\n",
      "Epoch 21, Batch 16, Test Loss: 0.4070161283016205\n",
      "Epoch 21, Batch 17, Test Loss: 0.4348665475845337\n",
      "Epoch 21, Batch 18, Test Loss: 0.413501113653183\n",
      "Epoch 21, Batch 19, Test Loss: 0.4311371445655823\n",
      "Epoch 21, Batch 20, Test Loss: 0.41564664244651794\n",
      "Epoch 21, Batch 21, Test Loss: 0.33028990030288696\n",
      "Epoch 21, Batch 22, Test Loss: 0.410837322473526\n",
      "Epoch 21, Batch 23, Test Loss: 0.4732692539691925\n",
      "Epoch 21, Batch 24, Test Loss: 0.3967689573764801\n",
      "Epoch 21, Batch 25, Test Loss: 0.4605182111263275\n",
      "Epoch 21, Batch 26, Test Loss: 0.3477441966533661\n",
      "Epoch 21, Batch 27, Test Loss: 0.44199037551879883\n",
      "Epoch 21, Batch 28, Test Loss: 0.3542846441268921\n",
      "Epoch 21, Batch 29, Test Loss: 0.48043498396873474\n",
      "Epoch 21, Batch 30, Test Loss: 0.3096981942653656\n",
      "Epoch 21, Batch 31, Test Loss: 0.46816039085388184\n",
      "Epoch 21, Batch 32, Test Loss: 0.24470975995063782\n",
      "Epoch 21, Batch 33, Test Loss: 0.34954631328582764\n",
      "Epoch 21, Batch 34, Test Loss: 0.3287154734134674\n",
      "Epoch 21, Batch 35, Test Loss: 0.3874087929725647\n",
      "Epoch 21, Batch 36, Test Loss: 0.3809421956539154\n",
      "Epoch 21, Batch 37, Test Loss: 0.3820410370826721\n",
      "Epoch 21, Batch 38, Test Loss: 0.4361579716205597\n",
      "Epoch 21, Batch 39, Test Loss: 0.3057589828968048\n",
      "Epoch 21, Batch 40, Test Loss: 0.3347853720188141\n",
      "Epoch 21, Batch 41, Test Loss: 0.3397720158100128\n",
      "Epoch 21, Batch 42, Test Loss: 0.6210720539093018\n",
      "Epoch 21, Batch 43, Test Loss: 0.32752737402915955\n",
      "Epoch 21, Batch 44, Test Loss: 0.5263881683349609\n",
      "Epoch 21, Batch 45, Test Loss: 0.4528154730796814\n",
      "Epoch 21, Batch 46, Test Loss: 0.28795963525772095\n",
      "Epoch 21, Batch 47, Test Loss: 0.30537867546081543\n",
      "Epoch 21, Batch 48, Test Loss: 0.2975023090839386\n",
      "Epoch 21, Batch 49, Test Loss: 0.272411048412323\n",
      "Epoch 21, Batch 50, Test Loss: 0.356229692697525\n",
      "Epoch 21, Batch 51, Test Loss: 0.3857927918434143\n",
      "Epoch 21, Batch 52, Test Loss: 0.3483853042125702\n",
      "Epoch 21, Batch 53, Test Loss: 0.5501440167427063\n",
      "Epoch 21, Batch 54, Test Loss: 0.5404099225997925\n",
      "Epoch 21, Batch 55, Test Loss: 0.3089577555656433\n",
      "Epoch 21, Batch 56, Test Loss: 0.40581637620925903\n",
      "Epoch 21, Batch 57, Test Loss: 0.45562076568603516\n",
      "Epoch 21, Batch 58, Test Loss: 0.3367428183555603\n",
      "Epoch 21, Batch 59, Test Loss: 0.4701462984085083\n",
      "Epoch 21, Batch 60, Test Loss: 0.4941115379333496\n",
      "Epoch 21, Batch 61, Test Loss: 0.5878041982650757\n",
      "Epoch 21, Batch 62, Test Loss: 0.4698001444339752\n",
      "Epoch 21, Batch 63, Test Loss: 0.3311498761177063\n",
      "Epoch 21, Batch 64, Test Loss: 0.354492723941803\n",
      "Epoch 21, Batch 65, Test Loss: 0.3335095942020416\n",
      "Epoch 21, Batch 66, Test Loss: 0.4585171639919281\n",
      "Epoch 21, Batch 67, Test Loss: 0.4972062110900879\n",
      "Epoch 21, Batch 68, Test Loss: 0.35148143768310547\n",
      "Epoch 21, Batch 69, Test Loss: 0.31895703077316284\n",
      "Epoch 21, Batch 70, Test Loss: 0.6528781652450562\n",
      "Epoch 21, Batch 71, Test Loss: 0.3188059628009796\n",
      "Epoch 21, Batch 72, Test Loss: 0.5003907680511475\n",
      "Epoch 21, Batch 73, Test Loss: 0.3594793677330017\n",
      "Epoch 21, Batch 74, Test Loss: 0.29380545020103455\n",
      "Epoch 21, Batch 75, Test Loss: 0.3003076910972595\n",
      "Epoch 21, Batch 76, Test Loss: 0.6085667610168457\n",
      "Epoch 21, Batch 77, Test Loss: 0.3430412709712982\n",
      "Epoch 21, Batch 78, Test Loss: 0.4322332441806793\n",
      "Epoch 21, Batch 79, Test Loss: 0.2699212431907654\n",
      "Epoch 21, Batch 80, Test Loss: 0.28946369886398315\n",
      "Epoch 21, Batch 81, Test Loss: 0.2711483836174011\n",
      "Epoch 21, Batch 82, Test Loss: 0.3353803753852844\n",
      "Epoch 21, Batch 83, Test Loss: 0.3649703860282898\n",
      "Epoch 21, Batch 84, Test Loss: 0.35712844133377075\n",
      "Epoch 21, Batch 85, Test Loss: 0.3293094336986542\n",
      "Epoch 21, Batch 86, Test Loss: 0.3930448293685913\n",
      "Epoch 21, Batch 87, Test Loss: 0.6288415789604187\n",
      "Epoch 21, Batch 88, Test Loss: 0.356564998626709\n",
      "Epoch 21, Batch 89, Test Loss: 0.4754130244255066\n",
      "Epoch 21, Batch 90, Test Loss: 0.27694934606552124\n",
      "Epoch 21, Batch 91, Test Loss: 0.4779204726219177\n",
      "Epoch 21, Batch 92, Test Loss: 0.7928979992866516\n",
      "Epoch 21, Batch 93, Test Loss: 0.3256853222846985\n",
      "Epoch 21, Batch 94, Test Loss: 0.45683765411376953\n",
      "Epoch 21, Batch 95, Test Loss: 0.20520570874214172\n",
      "Epoch 21, Batch 96, Test Loss: 0.2742191553115845\n",
      "Epoch 21, Batch 97, Test Loss: 0.4491252303123474\n",
      "Epoch 21, Batch 98, Test Loss: 0.40139901638031006\n",
      "Epoch 21, Batch 99, Test Loss: 0.37430980801582336\n",
      "Epoch 21, Batch 100, Test Loss: 0.33303582668304443\n",
      "Epoch 21, Batch 101, Test Loss: 0.4504416286945343\n",
      "Epoch 21, Batch 102, Test Loss: 0.5311059355735779\n",
      "Epoch 21, Batch 103, Test Loss: 0.27454036474227905\n",
      "Epoch 21, Batch 104, Test Loss: 0.48030680418014526\n",
      "Epoch 21, Batch 105, Test Loss: 0.503794252872467\n",
      "Epoch 21, Batch 106, Test Loss: 0.3995078504085541\n",
      "Epoch 21, Batch 107, Test Loss: 0.6069185733795166\n",
      "Epoch 21, Batch 108, Test Loss: 0.3369925320148468\n",
      "Epoch 21, Batch 109, Test Loss: 0.4798082709312439\n",
      "Epoch 21, Batch 110, Test Loss: 0.25391659140586853\n",
      "Epoch 21, Batch 111, Test Loss: 0.366410493850708\n",
      "Epoch 21, Batch 112, Test Loss: 0.35948795080184937\n",
      "Epoch 21, Batch 113, Test Loss: 0.3076951503753662\n",
      "Epoch 21, Batch 114, Test Loss: 0.2146371752023697\n",
      "Epoch 21, Batch 115, Test Loss: 0.2142399698495865\n",
      "Epoch 21, Batch 116, Test Loss: 0.6620789170265198\n",
      "Epoch 21, Batch 117, Test Loss: 0.4747992157936096\n",
      "Epoch 21, Batch 118, Test Loss: 0.5935245752334595\n",
      "Epoch 21, Batch 119, Test Loss: 0.4102160632610321\n",
      "Epoch 21, Batch 120, Test Loss: 0.5223181843757629\n",
      "Epoch 21, Batch 121, Test Loss: 0.6250286102294922\n",
      "Epoch 21, Batch 122, Test Loss: 0.35827726125717163\n",
      "Epoch 21, Batch 123, Test Loss: 0.29121845960617065\n",
      "Epoch 21, Batch 124, Test Loss: 0.42227721214294434\n",
      "Epoch 21, Batch 125, Test Loss: 0.4866032600402832\n",
      "Epoch 21, Batch 126, Test Loss: 0.3528580367565155\n",
      "Epoch 21, Batch 127, Test Loss: 0.4359939992427826\n",
      "Epoch 21, Batch 128, Test Loss: 0.34293484687805176\n",
      "Epoch 21, Batch 129, Test Loss: 0.37400293350219727\n",
      "Epoch 21, Batch 130, Test Loss: 0.4232116937637329\n",
      "Epoch 21, Batch 131, Test Loss: 0.26493778824806213\n",
      "Epoch 21, Batch 132, Test Loss: 0.46574312448501587\n",
      "Epoch 21, Batch 133, Test Loss: 0.4269787669181824\n",
      "Epoch 21, Batch 134, Test Loss: 0.36682426929473877\n",
      "Epoch 21, Batch 135, Test Loss: 0.4988672435283661\n",
      "Epoch 21, Batch 136, Test Loss: 0.46251872181892395\n",
      "Epoch 21, Batch 137, Test Loss: 0.3425976634025574\n",
      "Epoch 21, Batch 138, Test Loss: 0.30099985003471375\n",
      "Epoch 21, Batch 139, Test Loss: 0.4431670010089874\n",
      "Epoch 21, Batch 140, Test Loss: 0.5154305696487427\n",
      "Epoch 21, Batch 141, Test Loss: 0.5044689178466797\n",
      "Epoch 21, Batch 142, Test Loss: 0.3784736096858978\n",
      "Epoch 21, Batch 143, Test Loss: 0.6395992636680603\n",
      "Epoch 21, Batch 144, Test Loss: 0.3669256567955017\n",
      "Epoch 21, Batch 145, Test Loss: 0.33565640449523926\n",
      "Epoch 21, Batch 146, Test Loss: 0.3660871088504791\n",
      "Epoch 21, Batch 147, Test Loss: 0.31502923369407654\n",
      "Epoch 21, Batch 148, Test Loss: 0.37002459168434143\n",
      "Epoch 21, Batch 149, Test Loss: 0.5225452780723572\n",
      "Epoch 21, Batch 150, Test Loss: 0.5224171876907349\n",
      "Epoch 21, Batch 151, Test Loss: 0.47397151589393616\n",
      "Epoch 21, Batch 152, Test Loss: 0.34788331389427185\n",
      "Epoch 21, Batch 153, Test Loss: 0.5338836312294006\n",
      "Epoch 21, Batch 154, Test Loss: 0.5165713429450989\n",
      "Epoch 21, Batch 155, Test Loss: 0.2904691696166992\n",
      "Epoch 21, Batch 156, Test Loss: 0.3570222854614258\n",
      "Epoch 21, Batch 157, Test Loss: 0.4243428409099579\n",
      "Epoch 21, Batch 158, Test Loss: 0.5303966403007507\n",
      "Epoch 21, Batch 159, Test Loss: 0.5905013084411621\n",
      "Epoch 21, Batch 160, Test Loss: 0.3499208092689514\n",
      "Epoch 21, Batch 161, Test Loss: 0.32662031054496765\n",
      "Epoch 21, Batch 162, Test Loss: 0.3742266595363617\n",
      "Epoch 21, Batch 163, Test Loss: 0.44785866141319275\n",
      "Epoch 21, Batch 164, Test Loss: 0.2775117754936218\n",
      "Epoch 21, Batch 165, Test Loss: 0.23531493544578552\n",
      "Epoch 21, Batch 166, Test Loss: 0.30404943227767944\n",
      "Epoch 21, Batch 167, Test Loss: 0.4335859417915344\n",
      "Epoch 21, Batch 168, Test Loss: 0.3413407802581787\n",
      "Epoch 21, Batch 169, Test Loss: 0.5771531462669373\n",
      "Epoch 21, Batch 170, Test Loss: 0.4114350974559784\n",
      "Epoch 21, Batch 171, Test Loss: 0.5383391380310059\n",
      "Epoch 21, Batch 172, Test Loss: 0.3093031346797943\n",
      "Epoch 21, Batch 173, Test Loss: 0.28745371103286743\n",
      "Epoch 21, Batch 174, Test Loss: 0.6141791939735413\n",
      "Epoch 21, Batch 175, Test Loss: 0.3481470048427582\n",
      "Epoch 21, Batch 176, Test Loss: 0.4050423204898834\n",
      "Epoch 21, Batch 177, Test Loss: 0.38265886902809143\n",
      "Epoch 21, Batch 178, Test Loss: 0.3863658308982849\n",
      "Epoch 21, Batch 179, Test Loss: 0.5581098794937134\n",
      "Epoch 21, Batch 180, Test Loss: 0.42093268036842346\n",
      "Epoch 21, Batch 181, Test Loss: 0.3671959340572357\n",
      "Epoch 21, Batch 182, Test Loss: 0.4073514938354492\n",
      "Epoch 21, Batch 183, Test Loss: 0.3235775828361511\n",
      "Epoch 21, Batch 184, Test Loss: 0.6493908166885376\n",
      "Epoch 21, Batch 185, Test Loss: 0.4350338578224182\n",
      "Epoch 21, Batch 186, Test Loss: 0.34617868065834045\n",
      "Epoch 21, Batch 187, Test Loss: 0.4802464246749878\n",
      "Epoch 21, Batch 188, Test Loss: 0.27674174308776855\n",
      "Epoch 21, Batch 189, Test Loss: 0.36148613691329956\n",
      "Epoch 21, Batch 190, Test Loss: 0.48372694849967957\n",
      "Epoch 21, Batch 191, Test Loss: 0.2871347665786743\n",
      "Epoch 21, Batch 192, Test Loss: 0.4387012720108032\n",
      "Epoch 21, Batch 193, Test Loss: 0.26701784133911133\n",
      "Epoch 21, Batch 194, Test Loss: 0.28597894310951233\n",
      "Epoch 21, Batch 195, Test Loss: 0.5044676065444946\n",
      "Epoch 21, Batch 196, Test Loss: 0.49284037947654724\n",
      "Epoch 21, Batch 197, Test Loss: 0.3584997057914734\n",
      "Epoch 21, Batch 198, Test Loss: 0.41434919834136963\n",
      "Epoch 21, Batch 199, Test Loss: 0.3206125497817993\n",
      "Epoch 21, Batch 200, Test Loss: 0.3016589879989624\n",
      "Epoch 21, Batch 201, Test Loss: 0.23236814141273499\n",
      "Epoch 21, Batch 202, Test Loss: 0.3034704029560089\n",
      "Epoch 21, Batch 203, Test Loss: 0.218094602227211\n",
      "Epoch 21, Batch 204, Test Loss: 0.5972804427146912\n",
      "Epoch 21, Batch 205, Test Loss: 0.5208489298820496\n",
      "Epoch 21, Batch 206, Test Loss: 0.3104071319103241\n",
      "Epoch 21, Batch 207, Test Loss: 0.4247455298900604\n",
      "Epoch 21, Batch 208, Test Loss: 0.21077659726142883\n",
      "Epoch 21, Batch 209, Test Loss: 0.4368346631526947\n",
      "Epoch 21, Batch 210, Test Loss: 0.35809850692749023\n",
      "Epoch 21, Batch 211, Test Loss: 0.3748932480812073\n",
      "Epoch 21, Batch 212, Test Loss: 0.46728020906448364\n",
      "Epoch 21, Batch 213, Test Loss: 0.41801029443740845\n",
      "Epoch 21, Batch 214, Test Loss: 0.21761536598205566\n",
      "Epoch 21, Batch 215, Test Loss: 0.2889900803565979\n",
      "Epoch 21, Batch 216, Test Loss: 0.3906576931476593\n",
      "Epoch 21, Batch 217, Test Loss: 0.39026331901550293\n",
      "Epoch 21, Batch 218, Test Loss: 0.343074232339859\n",
      "Epoch 21, Batch 219, Test Loss: 0.5370593070983887\n",
      "Epoch 21, Batch 220, Test Loss: 0.26489952206611633\n",
      "Epoch 21, Batch 221, Test Loss: 0.23751617968082428\n",
      "Epoch 21, Batch 222, Test Loss: 0.24524274468421936\n",
      "Epoch 21, Batch 223, Test Loss: 0.47108709812164307\n",
      "Epoch 21, Batch 224, Test Loss: 0.480071097612381\n",
      "Epoch 21, Batch 225, Test Loss: 0.45218735933303833\n",
      "Epoch 21, Batch 226, Test Loss: 0.5173372626304626\n",
      "Epoch 21, Batch 227, Test Loss: 0.5216532945632935\n",
      "Epoch 21, Batch 228, Test Loss: 0.2888385057449341\n",
      "Epoch 21, Batch 229, Test Loss: 0.4182291626930237\n",
      "Epoch 21, Batch 230, Test Loss: 0.3232986629009247\n",
      "Epoch 21, Batch 231, Test Loss: 0.4812753200531006\n",
      "Epoch 21, Batch 232, Test Loss: 0.23981459438800812\n",
      "Epoch 21, Batch 233, Test Loss: 0.4196922183036804\n",
      "Epoch 21, Batch 234, Test Loss: 0.40566176176071167\n",
      "Epoch 21, Batch 235, Test Loss: 0.48037201166152954\n",
      "Epoch 21, Batch 236, Test Loss: 0.26690489053726196\n",
      "Epoch 21, Batch 237, Test Loss: 0.46561065316200256\n",
      "Epoch 21, Batch 238, Test Loss: 0.41265666484832764\n",
      "Epoch 21, Batch 239, Test Loss: 0.45781365036964417\n",
      "Epoch 21, Batch 240, Test Loss: 0.27855297923088074\n",
      "Epoch 21, Batch 241, Test Loss: 0.28088659048080444\n",
      "Epoch 21, Batch 242, Test Loss: 0.4423483610153198\n",
      "Epoch 21, Batch 243, Test Loss: 0.3238803744316101\n",
      "Epoch 21, Batch 244, Test Loss: 0.4359498918056488\n",
      "Epoch 21, Batch 245, Test Loss: 0.22642602026462555\n",
      "Epoch 21, Batch 246, Test Loss: 0.25554704666137695\n",
      "Epoch 21, Batch 247, Test Loss: 0.25383296608924866\n",
      "Epoch 21, Batch 248, Test Loss: 0.37471726536750793\n",
      "Epoch 21, Batch 249, Test Loss: 0.2682201564311981\n",
      "Epoch 21, Batch 250, Test Loss: 0.3627789616584778\n",
      "Epoch 21, Batch 251, Test Loss: 0.33271142840385437\n",
      "Epoch 21, Batch 252, Test Loss: 0.3816918134689331\n",
      "Epoch 21, Batch 253, Test Loss: 0.2277396023273468\n",
      "Epoch 21, Batch 254, Test Loss: 0.40243029594421387\n",
      "Epoch 21, Batch 255, Test Loss: 0.4408920407295227\n",
      "Epoch 21, Batch 256, Test Loss: 0.35171952843666077\n",
      "Epoch 21, Batch 257, Test Loss: 0.3927614390850067\n",
      "Epoch 21, Batch 258, Test Loss: 0.3963565528392792\n",
      "Epoch 21, Batch 259, Test Loss: 0.37867042422294617\n",
      "Epoch 21, Batch 260, Test Loss: 0.338783860206604\n",
      "Epoch 21, Batch 261, Test Loss: 0.48252782225608826\n",
      "Epoch 21, Batch 262, Test Loss: 0.432742178440094\n",
      "Epoch 21, Batch 263, Test Loss: 0.27378591895103455\n",
      "Epoch 21, Batch 264, Test Loss: 0.31383270025253296\n",
      "Epoch 21, Batch 265, Test Loss: 0.39350542426109314\n",
      "Epoch 21, Batch 266, Test Loss: 0.3907794952392578\n",
      "Epoch 21, Batch 267, Test Loss: 0.3996155261993408\n",
      "Epoch 21, Batch 268, Test Loss: 0.30842915177345276\n",
      "Epoch 21, Batch 269, Test Loss: 0.35804861783981323\n",
      "Epoch 21, Batch 270, Test Loss: 0.46550649404525757\n",
      "Epoch 21, Batch 271, Test Loss: 0.37843507528305054\n",
      "Epoch 21, Batch 272, Test Loss: 0.2721893787384033\n",
      "Epoch 21, Batch 273, Test Loss: 0.3416353464126587\n",
      "Epoch 21, Batch 274, Test Loss: 0.38332438468933105\n",
      "Epoch 21, Batch 275, Test Loss: 0.34568434953689575\n",
      "Epoch 21, Batch 276, Test Loss: 0.25427407026290894\n",
      "Epoch 21, Batch 277, Test Loss: 0.2484016716480255\n",
      "Epoch 21, Batch 278, Test Loss: 0.3098104000091553\n",
      "Epoch 21, Batch 279, Test Loss: 0.5364968776702881\n",
      "Epoch 21, Batch 280, Test Loss: 0.3742108941078186\n",
      "Epoch 21, Batch 281, Test Loss: 0.5190770626068115\n",
      "Epoch 21, Batch 282, Test Loss: 0.3722911775112152\n",
      "Epoch 21, Batch 283, Test Loss: 0.5138504505157471\n",
      "Epoch 21, Batch 284, Test Loss: 0.31646865606307983\n",
      "Epoch 21, Batch 285, Test Loss: 0.2469554841518402\n",
      "Epoch 21, Batch 286, Test Loss: 0.4138655960559845\n",
      "Epoch 21, Batch 287, Test Loss: 0.27699601650238037\n",
      "Epoch 21, Batch 288, Test Loss: 0.28217360377311707\n",
      "Epoch 21, Batch 289, Test Loss: 0.4297606348991394\n",
      "Epoch 21, Batch 290, Test Loss: 0.3582659363746643\n",
      "Epoch 21, Batch 291, Test Loss: 0.261689156293869\n",
      "Epoch 21, Batch 292, Test Loss: 0.39573588967323303\n",
      "Epoch 21, Batch 293, Test Loss: 0.5092917680740356\n",
      "Epoch 21, Batch 294, Test Loss: 0.3998021185398102\n",
      "Epoch 21, Batch 295, Test Loss: 0.47060370445251465\n",
      "Epoch 21, Batch 296, Test Loss: 0.6107903122901917\n",
      "Epoch 21, Batch 297, Test Loss: 0.3946562111377716\n",
      "Epoch 21, Batch 298, Test Loss: 0.41105929017066956\n",
      "Epoch 21, Batch 299, Test Loss: 0.3320193290710449\n",
      "Epoch 21, Batch 300, Test Loss: 0.3625802993774414\n",
      "Epoch 21, Batch 301, Test Loss: 0.32336342334747314\n",
      "Epoch 21, Batch 302, Test Loss: 0.21737173199653625\n",
      "Epoch 21, Batch 303, Test Loss: 0.32286566495895386\n",
      "Epoch 21, Batch 304, Test Loss: 0.49428844451904297\n",
      "Epoch 21, Batch 305, Test Loss: 0.38625526428222656\n",
      "Epoch 21, Batch 306, Test Loss: 0.3129359483718872\n",
      "Epoch 21, Batch 307, Test Loss: 0.4520891606807709\n",
      "Epoch 21, Batch 308, Test Loss: 0.42753610014915466\n",
      "Epoch 21, Batch 309, Test Loss: 0.5234888195991516\n",
      "Epoch 21, Batch 310, Test Loss: 0.5732181072235107\n",
      "Epoch 21, Batch 311, Test Loss: 0.5304679870605469\n",
      "Epoch 21, Batch 312, Test Loss: 0.37132763862609863\n",
      "Epoch 21, Batch 313, Test Loss: 0.43463778495788574\n",
      "Epoch 21, Batch 314, Test Loss: 0.45145779848098755\n",
      "Epoch 21, Batch 315, Test Loss: 0.4211674928665161\n",
      "Epoch 21, Batch 316, Test Loss: 0.31924694776535034\n",
      "Epoch 21, Batch 317, Test Loss: 0.3410181999206543\n",
      "Epoch 21, Batch 318, Test Loss: 0.3144402801990509\n",
      "Epoch 21, Batch 319, Test Loss: 0.3151772916316986\n",
      "Epoch 21, Batch 320, Test Loss: 0.20340940356254578\n",
      "Epoch 21, Batch 321, Test Loss: 0.4849516749382019\n",
      "Epoch 21, Batch 322, Test Loss: 0.2452436238527298\n",
      "Epoch 21, Batch 323, Test Loss: 0.37879514694213867\n",
      "Epoch 21, Batch 324, Test Loss: 0.3332461714744568\n",
      "Epoch 21, Batch 325, Test Loss: 0.44713127613067627\n",
      "Epoch 21, Batch 326, Test Loss: 0.6049919128417969\n",
      "Epoch 21, Batch 327, Test Loss: 0.39445316791534424\n",
      "Epoch 21, Batch 328, Test Loss: 0.39467331767082214\n",
      "Epoch 21, Batch 329, Test Loss: 0.3744805157184601\n",
      "Epoch 21, Batch 330, Test Loss: 0.41426777839660645\n",
      "Epoch 21, Batch 331, Test Loss: 0.16398997604846954\n",
      "Epoch 21, Batch 332, Test Loss: 0.43873652815818787\n",
      "Epoch 21, Batch 333, Test Loss: 0.4548145830631256\n",
      "Epoch 21, Batch 334, Test Loss: 0.5142857432365417\n",
      "Epoch 21, Batch 335, Test Loss: 0.23341646790504456\n",
      "Epoch 21, Batch 336, Test Loss: 0.46479013562202454\n",
      "Epoch 21, Batch 337, Test Loss: 0.32674744725227356\n",
      "Epoch 21, Batch 338, Test Loss: 0.3156563937664032\n",
      "Epoch 21, Batch 339, Test Loss: 0.39434945583343506\n",
      "Epoch 21, Batch 340, Test Loss: 0.3905032277107239\n",
      "Epoch 21, Batch 341, Test Loss: 0.4412582218647003\n",
      "Epoch 21, Batch 342, Test Loss: 0.3791395425796509\n",
      "Epoch 21, Batch 343, Test Loss: 0.4869375228881836\n",
      "Epoch 21, Batch 344, Test Loss: 0.5519066452980042\n",
      "Epoch 21, Batch 345, Test Loss: 0.2898670434951782\n",
      "Epoch 21, Batch 346, Test Loss: 0.4336548447608948\n",
      "Epoch 21, Batch 347, Test Loss: 0.32609233260154724\n",
      "Epoch 21, Batch 348, Test Loss: 0.3122071623802185\n",
      "Epoch 21, Batch 349, Test Loss: 0.4649915397167206\n",
      "Epoch 21, Batch 350, Test Loss: 0.43258392810821533\n",
      "Epoch 21, Batch 351, Test Loss: 0.23594196140766144\n",
      "Epoch 21, Batch 352, Test Loss: 0.3477468192577362\n",
      "Epoch 21, Batch 353, Test Loss: 0.3973996341228485\n",
      "Epoch 21, Batch 354, Test Loss: 0.17672425508499146\n",
      "Epoch 21, Batch 355, Test Loss: 0.5004828572273254\n",
      "Epoch 21, Batch 356, Test Loss: 0.28002190589904785\n",
      "Epoch 21, Batch 357, Test Loss: 0.39526212215423584\n",
      "Epoch 21, Batch 358, Test Loss: 0.6343063116073608\n",
      "Epoch 21, Batch 359, Test Loss: 0.34544041752815247\n",
      "Epoch 21, Batch 360, Test Loss: 0.4982229471206665\n",
      "Epoch 21, Batch 361, Test Loss: 0.412384569644928\n",
      "Epoch 21, Batch 362, Test Loss: 0.20919203758239746\n",
      "Epoch 21, Batch 363, Test Loss: 0.4720292389392853\n",
      "Epoch 21, Batch 364, Test Loss: 0.46313273906707764\n",
      "Epoch 21, Batch 365, Test Loss: 0.3089078962802887\n",
      "Epoch 21, Batch 366, Test Loss: 0.36336278915405273\n",
      "Epoch 21, Batch 367, Test Loss: 0.318203866481781\n",
      "Epoch 21, Batch 368, Test Loss: 0.4277665615081787\n",
      "Epoch 21, Batch 369, Test Loss: 0.40055719017982483\n",
      "Epoch 21, Batch 370, Test Loss: 0.5598945617675781\n",
      "Epoch 21, Batch 371, Test Loss: 0.5079782605171204\n",
      "Epoch 21, Batch 372, Test Loss: 0.27810409665107727\n",
      "Epoch 21, Batch 373, Test Loss: 0.4882733225822449\n",
      "Epoch 21, Batch 374, Test Loss: 0.4185328483581543\n",
      "Epoch 21, Batch 375, Test Loss: 0.44763609766960144\n",
      "Epoch 21, Batch 376, Test Loss: 0.3564019799232483\n",
      "Epoch 21, Batch 377, Test Loss: 0.2766423225402832\n",
      "Epoch 21, Batch 378, Test Loss: 0.35706308484077454\n",
      "Epoch 21, Batch 379, Test Loss: 0.3431430459022522\n",
      "Epoch 21, Batch 380, Test Loss: 0.29036349058151245\n",
      "Epoch 21, Batch 381, Test Loss: 0.40542927384376526\n",
      "Epoch 21, Batch 382, Test Loss: 0.45113641023635864\n",
      "Epoch 21, Batch 383, Test Loss: 0.5338478088378906\n",
      "Epoch 21, Batch 384, Test Loss: 0.3992556631565094\n",
      "Epoch 21, Batch 385, Test Loss: 0.57939612865448\n",
      "Epoch 21, Batch 386, Test Loss: 0.34777170419692993\n",
      "Epoch 21, Batch 387, Test Loss: 0.5068879723548889\n",
      "Epoch 21, Batch 388, Test Loss: 0.3812531530857086\n",
      "Epoch 21, Batch 389, Test Loss: 0.281936913728714\n",
      "Epoch 21, Batch 390, Test Loss: 0.3079839050769806\n",
      "Epoch 21, Batch 391, Test Loss: 0.4447228014469147\n",
      "Epoch 21, Batch 392, Test Loss: 0.4292491674423218\n",
      "Epoch 21, Batch 393, Test Loss: 0.33679667115211487\n",
      "Epoch 21, Batch 394, Test Loss: 0.31000444293022156\n",
      "Epoch 21, Batch 395, Test Loss: 0.4418899416923523\n",
      "Epoch 21, Batch 396, Test Loss: 0.35047462582588196\n",
      "Epoch 21, Batch 397, Test Loss: 0.3605007231235504\n",
      "Epoch 21, Batch 398, Test Loss: 0.3163500130176544\n",
      "Epoch 21, Batch 399, Test Loss: 0.3759016990661621\n",
      "Epoch 21, Batch 400, Test Loss: 0.4679296016693115\n",
      "Epoch 21, Batch 401, Test Loss: 0.46353986859321594\n",
      "Epoch 21, Batch 402, Test Loss: 0.5084041357040405\n",
      "Epoch 21, Batch 403, Test Loss: 0.25413382053375244\n",
      "Epoch 21, Batch 404, Test Loss: 0.36686813831329346\n",
      "Epoch 21, Batch 405, Test Loss: 0.5120441317558289\n",
      "Epoch 21, Batch 406, Test Loss: 0.2749709486961365\n",
      "Epoch 21, Batch 407, Test Loss: 0.3023734390735626\n",
      "Epoch 21, Batch 408, Test Loss: 0.33670201897621155\n",
      "Epoch 21, Batch 409, Test Loss: 0.4477013945579529\n",
      "Epoch 21, Batch 410, Test Loss: 0.40778133273124695\n",
      "Epoch 21, Batch 411, Test Loss: 0.26313063502311707\n",
      "Epoch 21, Batch 412, Test Loss: 0.27757373452186584\n",
      "Epoch 21, Batch 413, Test Loss: 0.35654139518737793\n",
      "Epoch 21, Batch 414, Test Loss: 0.6806991696357727\n",
      "Epoch 21, Batch 415, Test Loss: 0.31082460284233093\n",
      "Epoch 21, Batch 416, Test Loss: 0.4063994288444519\n",
      "Epoch 21, Batch 417, Test Loss: 0.4519229233264923\n",
      "Epoch 21, Batch 418, Test Loss: 0.3348126709461212\n",
      "Epoch 21, Batch 419, Test Loss: 0.29347193241119385\n",
      "Epoch 21, Batch 420, Test Loss: 0.2899649739265442\n",
      "Epoch 21, Batch 421, Test Loss: 0.33469706773757935\n",
      "Epoch 21, Batch 422, Test Loss: 0.33244773745536804\n",
      "Epoch 21, Batch 423, Test Loss: 0.39542508125305176\n",
      "Epoch 21, Batch 424, Test Loss: 0.3827253580093384\n",
      "Epoch 21, Batch 425, Test Loss: 0.4579452574253082\n",
      "Epoch 21, Batch 426, Test Loss: 0.45997560024261475\n",
      "Epoch 21, Batch 427, Test Loss: 0.428343266248703\n",
      "Epoch 21, Batch 428, Test Loss: 0.3868190050125122\n",
      "Epoch 21, Batch 429, Test Loss: 0.2769678831100464\n",
      "Epoch 21, Batch 430, Test Loss: 0.49483633041381836\n",
      "Epoch 21, Batch 431, Test Loss: 0.3814655542373657\n",
      "Epoch 21, Batch 432, Test Loss: 0.2775688171386719\n",
      "Epoch 21, Batch 433, Test Loss: 0.45542389154434204\n",
      "Epoch 21, Batch 434, Test Loss: 0.2979859709739685\n",
      "Epoch 21, Batch 435, Test Loss: 0.23509596288204193\n",
      "Epoch 21, Batch 436, Test Loss: 0.5911655426025391\n",
      "Epoch 21, Batch 437, Test Loss: 0.3825966417789459\n",
      "Epoch 21, Batch 438, Test Loss: 0.48724228143692017\n",
      "Epoch 21, Batch 439, Test Loss: 0.4655084013938904\n",
      "Epoch 21, Batch 440, Test Loss: 0.3806355893611908\n",
      "Epoch 21, Batch 441, Test Loss: 0.36200326681137085\n",
      "Epoch 21, Batch 442, Test Loss: 0.26328369975090027\n",
      "Epoch 21, Batch 443, Test Loss: 0.35587096214294434\n",
      "Epoch 21, Batch 444, Test Loss: 0.27121701836586\n",
      "Epoch 21, Batch 445, Test Loss: 0.4889508783817291\n",
      "Epoch 21, Batch 446, Test Loss: 0.2944849729537964\n",
      "Epoch 21, Batch 447, Test Loss: 0.28668978810310364\n",
      "Epoch 21, Batch 448, Test Loss: 0.48613426089286804\n",
      "Epoch 21, Batch 449, Test Loss: 0.6039292216300964\n",
      "Epoch 21, Batch 450, Test Loss: 0.41821837425231934\n",
      "Epoch 21, Batch 451, Test Loss: 0.4197025001049042\n",
      "Epoch 21, Batch 452, Test Loss: 0.3953128457069397\n",
      "Epoch 21, Batch 453, Test Loss: 0.3407534956932068\n",
      "Epoch 21, Batch 454, Test Loss: 0.5433618426322937\n",
      "Epoch 21, Batch 455, Test Loss: 0.37244367599487305\n",
      "Epoch 21, Batch 456, Test Loss: 0.2957195043563843\n",
      "Epoch 21, Batch 457, Test Loss: 0.41712045669555664\n",
      "Epoch 21, Batch 458, Test Loss: 0.40515679121017456\n",
      "Epoch 21, Batch 459, Test Loss: 0.34021756052970886\n",
      "Epoch 21, Batch 460, Test Loss: 0.36008232831954956\n",
      "Epoch 21, Batch 461, Test Loss: 0.43504852056503296\n",
      "Epoch 21, Batch 462, Test Loss: 0.5326204299926758\n",
      "Epoch 21, Batch 463, Test Loss: 0.4189004898071289\n",
      "Epoch 21, Batch 464, Test Loss: 0.36699098348617554\n",
      "Epoch 21, Batch 465, Test Loss: 0.45245644450187683\n",
      "Epoch 21, Batch 466, Test Loss: 0.4122847318649292\n",
      "Epoch 21, Batch 467, Test Loss: 0.4493347406387329\n",
      "Epoch 21, Batch 468, Test Loss: 0.4608638882637024\n",
      "Epoch 21, Batch 469, Test Loss: 0.646699070930481\n",
      "Epoch 21, Batch 470, Test Loss: 0.3321889638900757\n",
      "Epoch 21, Batch 471, Test Loss: 0.539786696434021\n",
      "Epoch 21, Batch 472, Test Loss: 0.38492268323898315\n",
      "Epoch 21, Batch 473, Test Loss: 0.499629408121109\n",
      "Epoch 21, Batch 474, Test Loss: 0.2573419213294983\n",
      "Epoch 21, Batch 475, Test Loss: 0.2927995026111603\n",
      "Epoch 21, Batch 476, Test Loss: 0.4685772657394409\n",
      "Epoch 21, Batch 477, Test Loss: 0.4635217785835266\n",
      "Epoch 21, Batch 478, Test Loss: 0.38399842381477356\n",
      "Epoch 21, Batch 479, Test Loss: 0.36714592576026917\n",
      "Epoch 21, Batch 480, Test Loss: 0.23957784473896027\n",
      "Epoch 21, Batch 481, Test Loss: 0.3998798727989197\n",
      "Epoch 21, Batch 482, Test Loss: 0.3638381063938141\n",
      "Epoch 21, Batch 483, Test Loss: 0.31092995405197144\n",
      "Epoch 21, Batch 484, Test Loss: 0.2691892087459564\n",
      "Epoch 21, Batch 485, Test Loss: 0.4945245385169983\n",
      "Epoch 21, Batch 486, Test Loss: 0.4414958357810974\n",
      "Epoch 21, Batch 487, Test Loss: 0.3110945224761963\n",
      "Epoch 21, Batch 488, Test Loss: 0.4050203263759613\n",
      "Epoch 21, Batch 489, Test Loss: 0.56044602394104\n",
      "Epoch 21, Batch 490, Test Loss: 0.25514745712280273\n",
      "Epoch 21, Batch 491, Test Loss: 0.32142043113708496\n",
      "Epoch 21, Batch 492, Test Loss: 0.41607406735420227\n",
      "Epoch 21, Batch 493, Test Loss: 0.5462476015090942\n",
      "Epoch 21, Batch 494, Test Loss: 0.2639229893684387\n",
      "Epoch 21, Batch 495, Test Loss: 0.4259102940559387\n",
      "Epoch 21, Batch 496, Test Loss: 0.2554856240749359\n",
      "Epoch 21, Batch 497, Test Loss: 0.42003875970840454\n",
      "Epoch 21, Batch 498, Test Loss: 0.2207484245300293\n",
      "Epoch 21, Batch 499, Test Loss: 0.46722835302352905\n",
      "Epoch 21, Batch 500, Test Loss: 0.39584508538246155\n",
      "Epoch 21, Batch 501, Test Loss: 0.23383976519107819\n",
      "Epoch 21, Batch 502, Test Loss: 0.41757580637931824\n",
      "Epoch 21, Batch 503, Test Loss: 0.413237988948822\n",
      "Epoch 21, Batch 504, Test Loss: 0.39928674697875977\n",
      "Epoch 21, Batch 505, Test Loss: 0.4630644917488098\n",
      "Epoch 21, Batch 506, Test Loss: 0.36770734190940857\n",
      "Epoch 21, Batch 507, Test Loss: 0.2755376100540161\n",
      "Epoch 21, Batch 508, Test Loss: 0.3217129409313202\n",
      "Epoch 21, Batch 509, Test Loss: 0.255367636680603\n",
      "Epoch 21, Batch 510, Test Loss: 0.5261578559875488\n",
      "Epoch 21, Batch 511, Test Loss: 0.2877713143825531\n",
      "Epoch 21, Batch 512, Test Loss: 0.3581123650074005\n",
      "Epoch 21, Batch 513, Test Loss: 0.31835660338401794\n",
      "Epoch 21, Batch 514, Test Loss: 0.5595178008079529\n",
      "Epoch 21, Batch 515, Test Loss: 0.29103216528892517\n",
      "Epoch 21, Batch 516, Test Loss: 0.3748176693916321\n",
      "Epoch 21, Batch 517, Test Loss: 0.5136746764183044\n",
      "Epoch 21, Batch 518, Test Loss: 0.2435181587934494\n",
      "Epoch 21, Batch 519, Test Loss: 0.7522137761116028\n",
      "Epoch 21, Batch 520, Test Loss: 0.31349077820777893\n",
      "Epoch 21, Batch 521, Test Loss: 0.3985075056552887\n",
      "Epoch 21, Batch 522, Test Loss: 0.27612733840942383\n",
      "Epoch 21, Batch 523, Test Loss: 0.4620402753353119\n",
      "Epoch 21, Batch 524, Test Loss: 0.5502274632453918\n",
      "Epoch 21, Batch 525, Test Loss: 0.3280136287212372\n",
      "Epoch 21, Batch 526, Test Loss: 0.3485162556171417\n",
      "Epoch 21, Batch 527, Test Loss: 0.4600042700767517\n",
      "Epoch 21, Batch 528, Test Loss: 0.37134894728660583\n",
      "Epoch 21, Batch 529, Test Loss: 0.2361396998167038\n",
      "Epoch 21, Batch 530, Test Loss: 0.3597621023654938\n",
      "Epoch 21, Batch 531, Test Loss: 0.33869853615760803\n",
      "Epoch 21, Batch 532, Test Loss: 0.45757821202278137\n",
      "Epoch 21, Batch 533, Test Loss: 0.374537855386734\n",
      "Epoch 21, Batch 534, Test Loss: 0.4049278497695923\n",
      "Epoch 21, Batch 535, Test Loss: 0.4400448203086853\n",
      "Epoch 21, Batch 536, Test Loss: 0.47535836696624756\n",
      "Epoch 21, Batch 537, Test Loss: 0.5190635323524475\n",
      "Epoch 21, Batch 538, Test Loss: 0.27259695529937744\n",
      "Epoch 21, Batch 539, Test Loss: 0.3591881990432739\n",
      "Epoch 21, Batch 540, Test Loss: 0.3221391439437866\n",
      "Epoch 21, Batch 541, Test Loss: 0.38759756088256836\n",
      "Epoch 21, Batch 542, Test Loss: 0.43093276023864746\n",
      "Epoch 21, Batch 543, Test Loss: 0.45764288306236267\n",
      "Epoch 21, Batch 544, Test Loss: 0.3787933588027954\n",
      "Epoch 21, Batch 545, Test Loss: 0.30241066217422485\n",
      "Epoch 21, Batch 546, Test Loss: 0.41366714239120483\n",
      "Epoch 21, Batch 547, Test Loss: 0.32815808057785034\n",
      "Epoch 21, Batch 548, Test Loss: 0.3260228633880615\n",
      "Epoch 21, Batch 549, Test Loss: 0.5215248465538025\n",
      "Epoch 21, Batch 550, Test Loss: 0.33919182419776917\n",
      "Epoch 21, Batch 551, Test Loss: 0.25589698553085327\n",
      "Epoch 21, Batch 552, Test Loss: 0.5163773894309998\n",
      "Epoch 21, Batch 553, Test Loss: 0.30257129669189453\n",
      "Epoch 21, Batch 554, Test Loss: 0.4180966019630432\n",
      "Epoch 21, Batch 555, Test Loss: 0.28914082050323486\n",
      "Epoch 21, Batch 556, Test Loss: 0.4315803647041321\n",
      "Epoch 21, Batch 557, Test Loss: 0.43900373578071594\n",
      "Epoch 21, Batch 558, Test Loss: 0.333381325006485\n",
      "Epoch 21, Batch 559, Test Loss: 0.4163654148578644\n",
      "Epoch 21, Batch 560, Test Loss: 0.4087655544281006\n",
      "Epoch 21, Batch 561, Test Loss: 0.5054274797439575\n",
      "Epoch 21, Batch 562, Test Loss: 0.4681151509284973\n",
      "Epoch 21, Batch 563, Test Loss: 0.2964724600315094\n",
      "Epoch 21, Batch 564, Test Loss: 0.35434120893478394\n",
      "Epoch 21, Batch 565, Test Loss: 0.28670889139175415\n",
      "Epoch 21, Batch 566, Test Loss: 0.34671393036842346\n",
      "Epoch 21, Batch 567, Test Loss: 0.5060607194900513\n",
      "Epoch 21, Batch 568, Test Loss: 0.58917236328125\n",
      "Epoch 21, Batch 569, Test Loss: 0.3217732906341553\n",
      "Epoch 21, Batch 570, Test Loss: 0.5391483902931213\n",
      "Epoch 21, Batch 571, Test Loss: 0.6705875992774963\n",
      "Epoch 21, Batch 572, Test Loss: 0.34317585825920105\n",
      "Epoch 21, Batch 573, Test Loss: 0.47824937105178833\n",
      "Epoch 21, Batch 574, Test Loss: 0.41269397735595703\n",
      "Epoch 21, Batch 575, Test Loss: 0.3718346357345581\n",
      "Epoch 21, Batch 576, Test Loss: 0.4357874393463135\n",
      "Epoch 21, Batch 577, Test Loss: 0.2601918876171112\n",
      "Epoch 21, Batch 578, Test Loss: 0.4674384891986847\n",
      "Epoch 21, Batch 579, Test Loss: 0.4198896288871765\n",
      "Epoch 21, Batch 580, Test Loss: 0.2571168541908264\n",
      "Epoch 21, Batch 581, Test Loss: 0.31859222054481506\n",
      "Epoch 21, Batch 582, Test Loss: 0.5038138031959534\n",
      "Epoch 21, Batch 583, Test Loss: 0.4147202968597412\n",
      "Epoch 21, Batch 584, Test Loss: 0.3858127295970917\n",
      "Epoch 21, Batch 585, Test Loss: 0.35606589913368225\n",
      "Epoch 21, Batch 586, Test Loss: 0.4636513292789459\n",
      "Epoch 21, Batch 587, Test Loss: 0.4684780538082123\n",
      "Epoch 21, Batch 588, Test Loss: 0.386629194021225\n",
      "Epoch 21, Batch 589, Test Loss: 0.22932574152946472\n",
      "Epoch 21, Batch 590, Test Loss: 0.3663064241409302\n",
      "Epoch 21, Batch 591, Test Loss: 0.3290785253047943\n",
      "Epoch 21, Batch 592, Test Loss: 0.41726601123809814\n",
      "Epoch 21, Batch 593, Test Loss: 0.3928528428077698\n",
      "Epoch 21, Batch 594, Test Loss: 0.4290175139904022\n",
      "Epoch 21, Batch 595, Test Loss: 0.36221569776535034\n",
      "Epoch 21, Batch 596, Test Loss: 0.44348013401031494\n",
      "Epoch 21, Batch 597, Test Loss: 0.43047207593917847\n",
      "Epoch 21, Batch 598, Test Loss: 0.5512657761573792\n",
      "Epoch 21, Batch 599, Test Loss: 0.27298247814178467\n",
      "Epoch 21, Batch 600, Test Loss: 0.36798322200775146\n",
      "Epoch 21, Batch 601, Test Loss: 0.3300754427909851\n",
      "Epoch 21, Batch 602, Test Loss: 0.4418092668056488\n",
      "Epoch 21, Batch 603, Test Loss: 0.4424790143966675\n",
      "Epoch 21, Batch 604, Test Loss: 0.31404629349708557\n",
      "Epoch 21, Batch 605, Test Loss: 0.47447481751441956\n",
      "Epoch 21, Batch 606, Test Loss: 0.3476204574108124\n",
      "Epoch 21, Batch 607, Test Loss: 0.2611547112464905\n",
      "Epoch 21, Batch 608, Test Loss: 0.5066834092140198\n",
      "Epoch 21, Batch 609, Test Loss: 0.6771432161331177\n",
      "Epoch 21, Batch 610, Test Loss: 0.41381704807281494\n",
      "Epoch 21, Batch 611, Test Loss: 0.3329627215862274\n",
      "Epoch 21, Batch 612, Test Loss: 0.31893450021743774\n",
      "Epoch 21, Batch 613, Test Loss: 0.2837153673171997\n",
      "Epoch 21, Batch 614, Test Loss: 0.3568947911262512\n",
      "Epoch 21, Batch 615, Test Loss: 0.5049006342887878\n",
      "Epoch 21, Batch 616, Test Loss: 0.29154857993125916\n",
      "Epoch 21, Batch 617, Test Loss: 0.3764715790748596\n",
      "Epoch 21, Batch 618, Test Loss: 0.24721311032772064\n",
      "Epoch 21, Batch 619, Test Loss: 0.4155452251434326\n",
      "Epoch 21, Batch 620, Test Loss: 0.3226841688156128\n",
      "Epoch 21, Batch 621, Test Loss: 0.38120904564857483\n",
      "Epoch 21, Batch 622, Test Loss: 0.35859057307243347\n",
      "Epoch 21, Batch 623, Test Loss: 0.2403184324502945\n",
      "Epoch 21, Batch 624, Test Loss: 0.38569706678390503\n",
      "Epoch 21, Batch 625, Test Loss: 0.3236578404903412\n",
      "Epoch 21, Batch 626, Test Loss: 0.44319847226142883\n",
      "Epoch 21, Batch 627, Test Loss: 0.44613105058670044\n",
      "Epoch 21, Batch 628, Test Loss: 0.2640557885169983\n",
      "Epoch 21, Batch 629, Test Loss: 0.42913782596588135\n",
      "Epoch 21, Batch 630, Test Loss: 0.47950825095176697\n",
      "Epoch 21, Batch 631, Test Loss: 0.22172048687934875\n",
      "Epoch 21, Batch 632, Test Loss: 0.6129105687141418\n",
      "Epoch 21, Batch 633, Test Loss: 0.3081493377685547\n",
      "Epoch 21, Batch 634, Test Loss: 0.42382174730300903\n",
      "Epoch 21, Batch 635, Test Loss: 0.5656241774559021\n",
      "Epoch 21, Batch 636, Test Loss: 0.4824134111404419\n",
      "Epoch 21, Batch 637, Test Loss: 0.4175259470939636\n",
      "Epoch 21, Batch 638, Test Loss: 0.417720228433609\n",
      "Epoch 21, Batch 639, Test Loss: 0.4590620994567871\n",
      "Epoch 21, Batch 640, Test Loss: 0.3679647743701935\n",
      "Epoch 21, Batch 641, Test Loss: 0.24797576665878296\n",
      "Epoch 21, Batch 642, Test Loss: 0.47575023770332336\n",
      "Epoch 21, Batch 643, Test Loss: 0.4742835462093353\n",
      "Epoch 21, Batch 644, Test Loss: 0.5293174982070923\n",
      "Epoch 21, Batch 645, Test Loss: 0.48751261830329895\n",
      "Epoch 21, Batch 646, Test Loss: 0.44919469952583313\n",
      "Epoch 21, Batch 647, Test Loss: 0.5304881930351257\n",
      "Epoch 21, Batch 648, Test Loss: 0.4075595438480377\n",
      "Epoch 21, Batch 649, Test Loss: 0.3737838864326477\n",
      "Epoch 21, Batch 650, Test Loss: 0.2456362545490265\n",
      "Epoch 21, Batch 651, Test Loss: 0.3061272203922272\n",
      "Epoch 21, Batch 652, Test Loss: 0.4617484509944916\n",
      "Epoch 21, Batch 653, Test Loss: 0.5016132593154907\n",
      "Epoch 21, Batch 654, Test Loss: 0.4575599431991577\n",
      "Epoch 21, Batch 655, Test Loss: 0.3872872591018677\n",
      "Epoch 21, Batch 656, Test Loss: 0.22567322850227356\n",
      "Epoch 21, Batch 657, Test Loss: 0.3344479203224182\n",
      "Epoch 21, Batch 658, Test Loss: 0.4509641230106354\n",
      "Epoch 21, Batch 659, Test Loss: 0.41078442335128784\n",
      "Epoch 21, Batch 660, Test Loss: 0.44545596837997437\n",
      "Epoch 21, Batch 661, Test Loss: 0.30121058225631714\n",
      "Epoch 21, Batch 662, Test Loss: 0.611358106136322\n",
      "Epoch 21, Batch 663, Test Loss: 0.30426082015037537\n",
      "Epoch 21, Batch 664, Test Loss: 0.37143152952194214\n",
      "Epoch 21, Batch 665, Test Loss: 0.31749391555786133\n",
      "Epoch 21, Batch 666, Test Loss: 0.3271358609199524\n",
      "Epoch 21, Batch 667, Test Loss: 0.2992159128189087\n",
      "Epoch 21, Batch 668, Test Loss: 0.3836224675178528\n",
      "Epoch 21, Batch 669, Test Loss: 0.3917539715766907\n",
      "Epoch 21, Batch 670, Test Loss: 0.46751195192337036\n",
      "Epoch 21, Batch 671, Test Loss: 0.43931689858436584\n",
      "Epoch 21, Batch 672, Test Loss: 0.5587601065635681\n",
      "Epoch 21, Batch 673, Test Loss: 0.3738214075565338\n",
      "Epoch 21, Batch 674, Test Loss: 0.40281185507774353\n",
      "Epoch 21, Batch 675, Test Loss: 0.35031747817993164\n",
      "Epoch 21, Batch 676, Test Loss: 0.3658366799354553\n",
      "Epoch 21, Batch 677, Test Loss: 0.2947486937046051\n",
      "Epoch 21, Batch 678, Test Loss: 0.4058859050273895\n",
      "Epoch 21, Batch 679, Test Loss: 0.3243042826652527\n",
      "Epoch 21, Batch 680, Test Loss: 0.478076308965683\n",
      "Epoch 21, Batch 681, Test Loss: 0.3268112242221832\n",
      "Epoch 21, Batch 682, Test Loss: 0.36966392397880554\n",
      "Epoch 21, Batch 683, Test Loss: 0.548056960105896\n",
      "Epoch 21, Batch 684, Test Loss: 0.33286571502685547\n",
      "Epoch 21, Batch 685, Test Loss: 0.27726054191589355\n",
      "Epoch 21, Batch 686, Test Loss: 0.33231276273727417\n",
      "Epoch 21, Batch 687, Test Loss: 0.37879443168640137\n",
      "Epoch 21, Batch 688, Test Loss: 0.29547056555747986\n",
      "Epoch 21, Batch 689, Test Loss: 0.26736176013946533\n",
      "Epoch 21, Batch 690, Test Loss: 0.5500929355621338\n",
      "Epoch 21, Batch 691, Test Loss: 0.30695146322250366\n",
      "Epoch 21, Batch 692, Test Loss: 0.39893683791160583\n",
      "Epoch 21, Batch 693, Test Loss: 0.4504063129425049\n",
      "Epoch 21, Batch 694, Test Loss: 0.15686389803886414\n",
      "Epoch 21, Batch 695, Test Loss: 0.4422360360622406\n",
      "Epoch 21, Batch 696, Test Loss: 0.4869499206542969\n",
      "Epoch 21, Batch 697, Test Loss: 0.3515351414680481\n",
      "Epoch 21, Batch 698, Test Loss: 0.36231276392936707\n",
      "Epoch 21, Batch 699, Test Loss: 0.31757086515426636\n",
      "Epoch 21, Batch 700, Test Loss: 0.4569884240627289\n",
      "Epoch 21, Batch 701, Test Loss: 0.4253157079219818\n",
      "Epoch 21, Batch 702, Test Loss: 0.48103177547454834\n",
      "Epoch 21, Batch 703, Test Loss: 0.27817052602767944\n",
      "Epoch 21, Batch 704, Test Loss: 0.32021257281303406\n",
      "Epoch 21, Batch 705, Test Loss: 0.38966235518455505\n",
      "Epoch 21, Batch 706, Test Loss: 0.6043243408203125\n",
      "Epoch 21, Batch 707, Test Loss: 0.44024139642715454\n",
      "Epoch 21, Batch 708, Test Loss: 0.38573822379112244\n",
      "Epoch 21, Batch 709, Test Loss: 0.5001345872879028\n",
      "Epoch 21, Batch 710, Test Loss: 0.27685585618019104\n",
      "Epoch 21, Batch 711, Test Loss: 0.5870313048362732\n",
      "Epoch 21, Batch 712, Test Loss: 0.6008127927780151\n",
      "Epoch 21, Batch 713, Test Loss: 0.5490813851356506\n",
      "Epoch 21, Batch 714, Test Loss: 0.4069581925868988\n",
      "Epoch 21, Batch 715, Test Loss: 0.3309222459793091\n",
      "Epoch 21, Batch 716, Test Loss: 0.35084256529808044\n",
      "Epoch 21, Batch 717, Test Loss: 0.2360704392194748\n",
      "Epoch 21, Batch 718, Test Loss: 0.3998124897480011\n",
      "Epoch 21, Batch 719, Test Loss: 0.4785918593406677\n",
      "Epoch 21, Batch 720, Test Loss: 0.37329375743865967\n",
      "Epoch 21, Batch 721, Test Loss: 0.2913801074028015\n",
      "Epoch 21, Batch 722, Test Loss: 0.43111562728881836\n",
      "Epoch 21, Batch 723, Test Loss: 0.4779775142669678\n",
      "Epoch 21, Batch 724, Test Loss: 0.4533948302268982\n",
      "Epoch 21, Batch 725, Test Loss: 0.5150111317634583\n",
      "Epoch 21, Batch 726, Test Loss: 0.41380882263183594\n",
      "Epoch 21, Batch 727, Test Loss: 0.42945775389671326\n",
      "Epoch 21, Batch 728, Test Loss: 0.31603702902793884\n",
      "Epoch 21, Batch 729, Test Loss: 0.31072965264320374\n",
      "Epoch 21, Batch 730, Test Loss: 0.478657603263855\n",
      "Epoch 21, Batch 731, Test Loss: 0.3040749728679657\n",
      "Epoch 21, Batch 732, Test Loss: 0.29174020886421204\n",
      "Epoch 21, Batch 733, Test Loss: 0.5008154511451721\n",
      "Epoch 21, Batch 734, Test Loss: 0.6143134832382202\n",
      "Epoch 21, Batch 735, Test Loss: 0.304812490940094\n",
      "Epoch 21, Batch 736, Test Loss: 0.45647069811820984\n",
      "Epoch 21, Batch 737, Test Loss: 0.41093116998672485\n",
      "Epoch 21, Batch 738, Test Loss: 0.4805087149143219\n",
      "Epoch 21, Batch 739, Test Loss: 0.2872678339481354\n",
      "Epoch 21, Batch 740, Test Loss: 0.4698227345943451\n",
      "Epoch 21, Batch 741, Test Loss: 0.5197171568870544\n",
      "Epoch 21, Batch 742, Test Loss: 0.3998062014579773\n",
      "Epoch 21, Batch 743, Test Loss: 0.2935645878314972\n",
      "Epoch 21, Batch 744, Test Loss: 0.2748798727989197\n",
      "Epoch 21, Batch 745, Test Loss: 0.32317230105400085\n",
      "Epoch 21, Batch 746, Test Loss: 0.5526172518730164\n",
      "Epoch 21, Batch 747, Test Loss: 0.30687806010246277\n",
      "Epoch 21, Batch 748, Test Loss: 0.2677158713340759\n",
      "Epoch 21, Batch 749, Test Loss: 0.3314776122570038\n",
      "Epoch 21, Batch 750, Test Loss: 0.47524821758270264\n",
      "Epoch 21, Batch 751, Test Loss: 0.34995415806770325\n",
      "Epoch 21, Batch 752, Test Loss: 0.39034467935562134\n",
      "Epoch 21, Batch 753, Test Loss: 0.23536571860313416\n",
      "Epoch 21, Batch 754, Test Loss: 0.3672547936439514\n",
      "Epoch 21, Batch 755, Test Loss: 0.3163861036300659\n",
      "Epoch 21, Batch 756, Test Loss: 0.6038485765457153\n",
      "Epoch 21, Batch 757, Test Loss: 0.23752273619174957\n",
      "Epoch 21, Batch 758, Test Loss: 0.3848493993282318\n",
      "Epoch 21, Batch 759, Test Loss: 0.5074580907821655\n",
      "Epoch 21, Batch 760, Test Loss: 0.49824708700180054\n",
      "Epoch 21, Batch 761, Test Loss: 0.4502054452896118\n",
      "Epoch 21, Batch 762, Test Loss: 0.40834924578666687\n",
      "Epoch 21, Batch 763, Test Loss: 0.47898250818252563\n",
      "Epoch 21, Batch 764, Test Loss: 0.44995230436325073\n",
      "Epoch 21, Batch 765, Test Loss: 0.486695259809494\n",
      "Epoch 21, Batch 766, Test Loss: 0.29448646306991577\n",
      "Epoch 21, Batch 767, Test Loss: 0.5463813543319702\n",
      "Epoch 21, Batch 768, Test Loss: 0.4525721073150635\n",
      "Epoch 21, Batch 769, Test Loss: 0.23951104283332825\n",
      "Epoch 21, Batch 770, Test Loss: 0.33918076753616333\n",
      "Epoch 21, Batch 771, Test Loss: 0.3230060935020447\n",
      "Epoch 21, Batch 772, Test Loss: 0.45708394050598145\n",
      "Epoch 21, Batch 773, Test Loss: 0.5498201251029968\n",
      "Epoch 21, Batch 774, Test Loss: 0.28442245721817017\n",
      "Epoch 21, Batch 775, Test Loss: 0.4571145474910736\n",
      "Epoch 21, Batch 776, Test Loss: 0.5185853242874146\n",
      "Epoch 21, Batch 777, Test Loss: 0.3278607726097107\n",
      "Epoch 21, Batch 778, Test Loss: 0.5448223948478699\n",
      "Epoch 21, Batch 779, Test Loss: 0.47293078899383545\n",
      "Epoch 21, Batch 780, Test Loss: 0.4928421378135681\n",
      "Epoch 21, Batch 781, Test Loss: 0.3696778118610382\n",
      "Epoch 21, Batch 782, Test Loss: 0.30856144428253174\n",
      "Epoch 21, Batch 783, Test Loss: 0.2598966956138611\n",
      "Epoch 21, Batch 784, Test Loss: 0.3987410068511963\n",
      "Epoch 21, Batch 785, Test Loss: 0.4207756221294403\n",
      "Epoch 21, Batch 786, Test Loss: 0.3428773283958435\n",
      "Epoch 21, Batch 787, Test Loss: 0.2744253873825073\n",
      "Epoch 21, Batch 788, Test Loss: 0.28869131207466125\n",
      "Epoch 21, Batch 789, Test Loss: 0.4549005329608917\n",
      "Epoch 21, Batch 790, Test Loss: 0.37522393465042114\n",
      "Epoch 21, Batch 791, Test Loss: 0.32411813735961914\n",
      "Epoch 21, Batch 792, Test Loss: 0.45278093218803406\n",
      "Epoch 21, Batch 793, Test Loss: 0.4337323009967804\n",
      "Epoch 21, Batch 794, Test Loss: 0.3391399085521698\n",
      "Epoch 21, Batch 795, Test Loss: 0.31485486030578613\n",
      "Epoch 21, Batch 796, Test Loss: 0.5365450978279114\n",
      "Epoch 21, Batch 797, Test Loss: 0.33374351263046265\n",
      "Epoch 21, Batch 798, Test Loss: 0.3547106385231018\n",
      "Epoch 21, Batch 799, Test Loss: 0.4376498758792877\n",
      "Epoch 21, Batch 800, Test Loss: 0.43891802430152893\n",
      "Epoch 21, Batch 801, Test Loss: 0.3437407910823822\n",
      "Epoch 21, Batch 802, Test Loss: 0.5105997323989868\n",
      "Epoch 21, Batch 803, Test Loss: 0.25182950496673584\n",
      "Epoch 21, Batch 804, Test Loss: 0.4223388135433197\n",
      "Epoch 21, Batch 805, Test Loss: 0.35037925839424133\n",
      "Epoch 21, Batch 806, Test Loss: 0.21703091263771057\n",
      "Epoch 21, Batch 807, Test Loss: 0.27516478300094604\n",
      "Epoch 21, Batch 808, Test Loss: 0.2759940028190613\n",
      "Epoch 21, Batch 809, Test Loss: 0.5488362908363342\n",
      "Epoch 21, Batch 810, Test Loss: 0.3907290995121002\n",
      "Epoch 21, Batch 811, Test Loss: 0.4994350075721741\n",
      "Epoch 21, Batch 812, Test Loss: 0.37295448780059814\n",
      "Epoch 21, Batch 813, Test Loss: 0.3409420847892761\n",
      "Epoch 21, Batch 814, Test Loss: 0.3179430365562439\n",
      "Epoch 21, Batch 815, Test Loss: 0.2279823273420334\n",
      "Epoch 21, Batch 816, Test Loss: 0.4244149625301361\n",
      "Epoch 21, Batch 817, Test Loss: 0.6084145903587341\n",
      "Epoch 21, Batch 818, Test Loss: 0.4212849736213684\n",
      "Epoch 21, Batch 819, Test Loss: 0.6787444949150085\n",
      "Epoch 21, Batch 820, Test Loss: 0.5056217312812805\n",
      "Epoch 21, Batch 821, Test Loss: 0.46285778284072876\n",
      "Epoch 21, Batch 822, Test Loss: 0.2947489619255066\n",
      "Epoch 21, Batch 823, Test Loss: 0.4445556402206421\n",
      "Epoch 21, Batch 824, Test Loss: 0.3434990644454956\n",
      "Epoch 21, Batch 825, Test Loss: 0.29092806577682495\n",
      "Epoch 21, Batch 826, Test Loss: 0.3010543882846832\n",
      "Epoch 21, Batch 827, Test Loss: 0.3572034239768982\n",
      "Epoch 21, Batch 828, Test Loss: 0.2297603040933609\n",
      "Epoch 21, Batch 829, Test Loss: 0.3548882305622101\n",
      "Epoch 21, Batch 830, Test Loss: 0.37836652994155884\n",
      "Epoch 21, Batch 831, Test Loss: 0.6019841432571411\n",
      "Epoch 21, Batch 832, Test Loss: 0.3725983500480652\n",
      "Epoch 21, Batch 833, Test Loss: 0.3589200973510742\n",
      "Epoch 21, Batch 834, Test Loss: 0.274606853723526\n",
      "Epoch 21, Batch 835, Test Loss: 0.3266870975494385\n",
      "Epoch 21, Batch 836, Test Loss: 0.26648643612861633\n",
      "Epoch 21, Batch 837, Test Loss: 0.3360150158405304\n",
      "Epoch 21, Batch 838, Test Loss: 0.472618043422699\n",
      "Epoch 21, Batch 839, Test Loss: 0.28848570585250854\n",
      "Epoch 21, Batch 840, Test Loss: 0.4703993797302246\n",
      "Epoch 21, Batch 841, Test Loss: 0.41106894612312317\n",
      "Epoch 21, Batch 842, Test Loss: 0.33711370825767517\n",
      "Epoch 21, Batch 843, Test Loss: 0.4465445280075073\n",
      "Epoch 21, Batch 844, Test Loss: 0.4157789349555969\n",
      "Epoch 21, Batch 845, Test Loss: 0.778151273727417\n",
      "Epoch 21, Batch 846, Test Loss: 0.40409478545188904\n",
      "Epoch 21, Batch 847, Test Loss: 0.31840386986732483\n",
      "Epoch 21, Batch 848, Test Loss: 0.5235803127288818\n",
      "Epoch 21, Batch 849, Test Loss: 0.5133212804794312\n",
      "Epoch 21, Batch 850, Test Loss: 0.3538321852684021\n",
      "Epoch 21, Batch 851, Test Loss: 0.3062903881072998\n",
      "Epoch 21, Batch 852, Test Loss: 0.2500366270542145\n",
      "Epoch 21, Batch 853, Test Loss: 0.4453238546848297\n",
      "Epoch 21, Batch 854, Test Loss: 0.358151376247406\n",
      "Epoch 21, Batch 855, Test Loss: 0.33862021565437317\n",
      "Epoch 21, Batch 856, Test Loss: 0.41508135199546814\n",
      "Epoch 21, Batch 857, Test Loss: 0.2800159752368927\n",
      "Epoch 21, Batch 858, Test Loss: 0.3826930522918701\n",
      "Epoch 21, Batch 859, Test Loss: 0.4488554000854492\n",
      "Epoch 21, Batch 860, Test Loss: 0.3115962743759155\n",
      "Epoch 21, Batch 861, Test Loss: 0.4178397059440613\n",
      "Epoch 21, Batch 862, Test Loss: 0.5426115989685059\n",
      "Epoch 21, Batch 863, Test Loss: 0.29976722598075867\n",
      "Epoch 21, Batch 864, Test Loss: 0.29374444484710693\n",
      "Epoch 21, Batch 865, Test Loss: 0.560919463634491\n",
      "Epoch 21, Batch 866, Test Loss: 0.24525614082813263\n",
      "Epoch 21, Batch 867, Test Loss: 0.5068079829216003\n",
      "Epoch 21, Batch 868, Test Loss: 0.42724868655204773\n",
      "Epoch 21, Batch 869, Test Loss: 0.36561495065689087\n",
      "Epoch 21, Batch 870, Test Loss: 0.402321994304657\n",
      "Epoch 21, Batch 871, Test Loss: 0.29639580845832825\n",
      "Epoch 21, Batch 872, Test Loss: 0.38373082876205444\n",
      "Epoch 21, Batch 873, Test Loss: 0.34662795066833496\n",
      "Epoch 21, Batch 874, Test Loss: 0.3997589945793152\n",
      "Epoch 21, Batch 875, Test Loss: 0.3468424379825592\n",
      "Epoch 21, Batch 876, Test Loss: 0.5025148987770081\n",
      "Epoch 21, Batch 877, Test Loss: 0.3961770236492157\n",
      "Epoch 21, Batch 878, Test Loss: 0.2991888225078583\n",
      "Epoch 21, Batch 879, Test Loss: 0.44566401839256287\n",
      "Epoch 21, Batch 880, Test Loss: 0.2868751585483551\n",
      "Epoch 21, Batch 881, Test Loss: 0.3408680260181427\n",
      "Epoch 21, Batch 882, Test Loss: 0.36082810163497925\n",
      "Epoch 21, Batch 883, Test Loss: 0.4075580835342407\n",
      "Epoch 21, Batch 884, Test Loss: 0.44006237387657166\n",
      "Epoch 21, Batch 885, Test Loss: 0.31397145986557007\n",
      "Epoch 21, Batch 886, Test Loss: 0.500624418258667\n",
      "Epoch 21, Batch 887, Test Loss: 0.3529367744922638\n",
      "Epoch 21, Batch 888, Test Loss: 0.6980315446853638\n",
      "Epoch 21, Batch 889, Test Loss: 0.45664095878601074\n",
      "Epoch 21, Batch 890, Test Loss: 0.5730336308479309\n",
      "Epoch 21, Batch 891, Test Loss: 0.3056041896343231\n",
      "Epoch 21, Batch 892, Test Loss: 0.6181569695472717\n",
      "Epoch 21, Batch 893, Test Loss: 0.2655010521411896\n",
      "Epoch 21, Batch 894, Test Loss: 0.32632678747177124\n",
      "Epoch 21, Batch 895, Test Loss: 0.43681660294532776\n",
      "Epoch 21, Batch 896, Test Loss: 0.2668946385383606\n",
      "Epoch 21, Batch 897, Test Loss: 0.3436538577079773\n",
      "Epoch 21, Batch 898, Test Loss: 0.368918776512146\n",
      "Epoch 21, Batch 899, Test Loss: 0.46471425890922546\n",
      "Epoch 21, Batch 900, Test Loss: 0.4341448247432709\n",
      "Epoch 21, Batch 901, Test Loss: 0.27282601594924927\n",
      "Epoch 21, Batch 902, Test Loss: 0.46345821022987366\n",
      "Epoch 21, Batch 903, Test Loss: 0.3953491449356079\n",
      "Epoch 21, Batch 904, Test Loss: 0.45282846689224243\n",
      "Epoch 21, Batch 905, Test Loss: 0.23466743528842926\n",
      "Epoch 21, Batch 906, Test Loss: 0.3233381509780884\n",
      "Epoch 21, Batch 907, Test Loss: 0.4783657491207123\n",
      "Epoch 21, Batch 908, Test Loss: 0.23389960825443268\n",
      "Epoch 21, Batch 909, Test Loss: 0.3749670386314392\n",
      "Epoch 21, Batch 910, Test Loss: 0.5520216226577759\n",
      "Epoch 21, Batch 911, Test Loss: 0.36390209197998047\n",
      "Epoch 21, Batch 912, Test Loss: 0.3723316788673401\n",
      "Epoch 21, Batch 913, Test Loss: 0.33122503757476807\n",
      "Epoch 21, Batch 914, Test Loss: 0.338981032371521\n",
      "Epoch 21, Batch 915, Test Loss: 0.3938436210155487\n",
      "Epoch 21, Batch 916, Test Loss: 0.41918355226516724\n",
      "Epoch 21, Batch 917, Test Loss: 0.5270006060600281\n",
      "Epoch 21, Batch 918, Test Loss: 0.4408613443374634\n",
      "Epoch 21, Batch 919, Test Loss: 0.4697543978691101\n",
      "Epoch 21, Batch 920, Test Loss: 0.2858593761920929\n",
      "Epoch 21, Batch 921, Test Loss: 0.22149509191513062\n",
      "Epoch 21, Batch 922, Test Loss: 0.351991206407547\n",
      "Epoch 21, Batch 923, Test Loss: 0.3300914764404297\n",
      "Epoch 21, Batch 924, Test Loss: 0.47637176513671875\n",
      "Epoch 21, Batch 925, Test Loss: 0.3723740875720978\n",
      "Epoch 21, Batch 926, Test Loss: 0.43041977286338806\n",
      "Epoch 21, Batch 927, Test Loss: 0.30324283242225647\n",
      "Epoch 21, Batch 928, Test Loss: 0.30007272958755493\n",
      "Epoch 21, Batch 929, Test Loss: 0.2777109146118164\n",
      "Epoch 21, Batch 930, Test Loss: 0.4178118109703064\n",
      "Epoch 21, Batch 931, Test Loss: 0.31648364663124084\n",
      "Epoch 21, Batch 932, Test Loss: 0.4327126741409302\n",
      "Epoch 21, Batch 933, Test Loss: 0.25943660736083984\n",
      "Epoch 21, Batch 934, Test Loss: 0.44127970933914185\n",
      "Epoch 21, Batch 935, Test Loss: 0.3435816168785095\n",
      "Epoch 21, Batch 936, Test Loss: 0.30084362626075745\n",
      "Epoch 21, Batch 937, Test Loss: 0.5533137321472168\n",
      "Epoch 21, Batch 938, Test Loss: 0.3063759207725525\n",
      "Accuracy of Test set: 0.8604333333333334\n",
      "Epoch 22, Batch 1, Loss: 0.3407266438007355\n",
      "Epoch 22, Batch 2, Loss: 0.544696569442749\n",
      "Epoch 22, Batch 3, Loss: 0.32729122042655945\n",
      "Epoch 22, Batch 4, Loss: 0.4301898777484894\n",
      "Epoch 22, Batch 5, Loss: 0.4043828845024109\n",
      "Epoch 22, Batch 6, Loss: 0.44607630372047424\n",
      "Epoch 22, Batch 7, Loss: 0.5258644223213196\n",
      "Epoch 22, Batch 8, Loss: 0.3604568839073181\n",
      "Epoch 22, Batch 9, Loss: 0.6382163763046265\n",
      "Epoch 22, Batch 10, Loss: 0.4812326431274414\n",
      "Epoch 22, Batch 11, Loss: 0.31492748856544495\n",
      "Epoch 22, Batch 12, Loss: 0.2779330015182495\n",
      "Epoch 22, Batch 13, Loss: 0.5079222917556763\n",
      "Epoch 22, Batch 14, Loss: 0.5679459571838379\n",
      "Epoch 22, Batch 15, Loss: 0.355575293302536\n",
      "Epoch 22, Batch 16, Loss: 0.37158727645874023\n",
      "Epoch 22, Batch 17, Loss: 0.34710296988487244\n",
      "Epoch 22, Batch 18, Loss: 0.6956026554107666\n",
      "Epoch 22, Batch 19, Loss: 0.4859788417816162\n",
      "Epoch 22, Batch 20, Loss: 0.494414746761322\n",
      "Epoch 22, Batch 21, Loss: 0.3189522624015808\n",
      "Epoch 22, Batch 22, Loss: 0.30847224593162537\n",
      "Epoch 22, Batch 23, Loss: 0.3792463541030884\n",
      "Epoch 22, Batch 24, Loss: 0.3802393078804016\n",
      "Epoch 22, Batch 25, Loss: 0.4849196672439575\n",
      "Epoch 22, Batch 26, Loss: 0.46719425916671753\n",
      "Epoch 22, Batch 27, Loss: 0.4123684763908386\n",
      "Epoch 22, Batch 28, Loss: 0.5612145662307739\n",
      "Epoch 22, Batch 29, Loss: 0.4735274016857147\n",
      "Epoch 22, Batch 30, Loss: 0.30528753995895386\n",
      "Epoch 22, Batch 31, Loss: 0.34483057260513306\n",
      "Epoch 22, Batch 32, Loss: 0.3169559836387634\n",
      "Epoch 22, Batch 33, Loss: 0.504895031452179\n",
      "Epoch 22, Batch 34, Loss: 0.2878572940826416\n",
      "Epoch 22, Batch 35, Loss: 0.3267258405685425\n",
      "Epoch 22, Batch 36, Loss: 0.5229108333587646\n",
      "Epoch 22, Batch 37, Loss: 0.4097302556037903\n",
      "Epoch 22, Batch 38, Loss: 0.4296678602695465\n",
      "Epoch 22, Batch 39, Loss: 0.31180277466773987\n",
      "Epoch 22, Batch 40, Loss: 0.26746776700019836\n",
      "Epoch 22, Batch 41, Loss: 0.3546578884124756\n",
      "Epoch 22, Batch 42, Loss: 0.3035794794559479\n",
      "Epoch 22, Batch 43, Loss: 0.44089165329933167\n",
      "Epoch 22, Batch 44, Loss: 0.2928878962993622\n",
      "Epoch 22, Batch 45, Loss: 0.2935789227485657\n",
      "Epoch 22, Batch 46, Loss: 0.2823893427848816\n",
      "Epoch 22, Batch 47, Loss: 0.2826950252056122\n",
      "Epoch 22, Batch 48, Loss: 0.4108838140964508\n",
      "Epoch 22, Batch 49, Loss: 0.4077088236808777\n",
      "Epoch 22, Batch 50, Loss: 0.398148775100708\n",
      "Epoch 22, Batch 51, Loss: 0.271248459815979\n",
      "Epoch 22, Batch 52, Loss: 0.2969420552253723\n",
      "Epoch 22, Batch 53, Loss: 0.3963817059993744\n",
      "Epoch 22, Batch 54, Loss: 0.4810901880264282\n",
      "Epoch 22, Batch 55, Loss: 0.5054830312728882\n",
      "Epoch 22, Batch 56, Loss: 0.395023912191391\n",
      "Epoch 22, Batch 57, Loss: 0.2729690968990326\n",
      "Epoch 22, Batch 58, Loss: 0.345114141702652\n",
      "Epoch 22, Batch 59, Loss: 0.39435166120529175\n",
      "Epoch 22, Batch 60, Loss: 0.7390796542167664\n",
      "Epoch 22, Batch 61, Loss: 0.3430737853050232\n",
      "Epoch 22, Batch 62, Loss: 0.37327975034713745\n",
      "Epoch 22, Batch 63, Loss: 0.31899046897888184\n",
      "Epoch 22, Batch 64, Loss: 0.41192880272865295\n",
      "Epoch 22, Batch 65, Loss: 0.26337581872940063\n",
      "Epoch 22, Batch 66, Loss: 0.45892542600631714\n",
      "Epoch 22, Batch 67, Loss: 0.37929314374923706\n",
      "Epoch 22, Batch 68, Loss: 0.32800403237342834\n",
      "Epoch 22, Batch 69, Loss: 0.3364443778991699\n",
      "Epoch 22, Batch 70, Loss: 0.6257354021072388\n",
      "Epoch 22, Batch 71, Loss: 0.39449915289878845\n",
      "Epoch 22, Batch 72, Loss: 0.42615875601768494\n",
      "Epoch 22, Batch 73, Loss: 0.41327396035194397\n",
      "Epoch 22, Batch 74, Loss: 0.4052221477031708\n",
      "Epoch 22, Batch 75, Loss: 0.5153155326843262\n",
      "Epoch 22, Batch 76, Loss: 0.4369005560874939\n",
      "Epoch 22, Batch 77, Loss: 0.5085523724555969\n",
      "Epoch 22, Batch 78, Loss: 0.4325357675552368\n",
      "Epoch 22, Batch 79, Loss: 0.34625276923179626\n",
      "Epoch 22, Batch 80, Loss: 0.23827773332595825\n",
      "Epoch 22, Batch 81, Loss: 0.5430009961128235\n",
      "Epoch 22, Batch 82, Loss: 0.2780977487564087\n",
      "Epoch 22, Batch 83, Loss: 0.5690118074417114\n",
      "Epoch 22, Batch 84, Loss: 0.3046633303165436\n",
      "Epoch 22, Batch 85, Loss: 0.5811370015144348\n",
      "Epoch 22, Batch 86, Loss: 0.5231695771217346\n",
      "Epoch 22, Batch 87, Loss: 0.39421382546424866\n",
      "Epoch 22, Batch 88, Loss: 0.3648605942726135\n",
      "Epoch 22, Batch 89, Loss: 0.35362282395362854\n",
      "Epoch 22, Batch 90, Loss: 0.43927332758903503\n",
      "Epoch 22, Batch 91, Loss: 0.4790232479572296\n",
      "Epoch 22, Batch 92, Loss: 0.47432518005371094\n",
      "Epoch 22, Batch 93, Loss: 0.29135870933532715\n",
      "Epoch 22, Batch 94, Loss: 0.38191476464271545\n",
      "Epoch 22, Batch 95, Loss: 0.47689926624298096\n",
      "Epoch 22, Batch 96, Loss: 0.2908816933631897\n",
      "Epoch 22, Batch 97, Loss: 0.4343457520008087\n",
      "Epoch 22, Batch 98, Loss: 0.49508047103881836\n",
      "Epoch 22, Batch 99, Loss: 0.28946176171302795\n",
      "Epoch 22, Batch 100, Loss: 0.43458157777786255\n",
      "Epoch 22, Batch 101, Loss: 0.5520645976066589\n",
      "Epoch 22, Batch 102, Loss: 0.35745444893836975\n",
      "Epoch 22, Batch 103, Loss: 0.25416240096092224\n",
      "Epoch 22, Batch 104, Loss: 0.43760034441947937\n",
      "Epoch 22, Batch 105, Loss: 0.33217114210128784\n",
      "Epoch 22, Batch 106, Loss: 0.46816161274909973\n",
      "Epoch 22, Batch 107, Loss: 0.3162585496902466\n",
      "Epoch 22, Batch 108, Loss: 0.46283531188964844\n",
      "Epoch 22, Batch 109, Loss: 0.27932265400886536\n",
      "Epoch 22, Batch 110, Loss: 0.414855420589447\n",
      "Epoch 22, Batch 111, Loss: 0.26279616355895996\n",
      "Epoch 22, Batch 112, Loss: 0.4928998351097107\n",
      "Epoch 22, Batch 113, Loss: 0.3595491051673889\n",
      "Epoch 22, Batch 114, Loss: 0.3438204526901245\n",
      "Epoch 22, Batch 115, Loss: 0.5260858535766602\n",
      "Epoch 22, Batch 116, Loss: 0.3974680006504059\n",
      "Epoch 22, Batch 117, Loss: 0.41549596190452576\n",
      "Epoch 22, Batch 118, Loss: 0.47258299589157104\n",
      "Epoch 22, Batch 119, Loss: 0.3657940924167633\n",
      "Epoch 22, Batch 120, Loss: 0.371013879776001\n",
      "Epoch 22, Batch 121, Loss: 0.6395597457885742\n",
      "Epoch 22, Batch 122, Loss: 0.4953992962837219\n",
      "Epoch 22, Batch 123, Loss: 0.4571962356567383\n",
      "Epoch 22, Batch 124, Loss: 0.37405651807785034\n",
      "Epoch 22, Batch 125, Loss: 0.4745407700538635\n",
      "Epoch 22, Batch 126, Loss: 0.4678129553794861\n",
      "Epoch 22, Batch 127, Loss: 0.37829750776290894\n",
      "Epoch 22, Batch 128, Loss: 0.25062820315361023\n",
      "Epoch 22, Batch 129, Loss: 0.31796005368232727\n",
      "Epoch 22, Batch 130, Loss: 0.5540434122085571\n",
      "Epoch 22, Batch 131, Loss: 0.3415472209453583\n",
      "Epoch 22, Batch 132, Loss: 0.5132468938827515\n",
      "Epoch 22, Batch 133, Loss: 0.4885530471801758\n",
      "Epoch 22, Batch 134, Loss: 0.5403265953063965\n",
      "Epoch 22, Batch 135, Loss: 0.428509920835495\n",
      "Epoch 22, Batch 136, Loss: 0.3445011079311371\n",
      "Epoch 22, Batch 137, Loss: 0.33161216974258423\n",
      "Epoch 22, Batch 138, Loss: 0.23095035552978516\n",
      "Epoch 22, Batch 139, Loss: 0.22476160526275635\n",
      "Epoch 22, Batch 140, Loss: 0.2697945833206177\n",
      "Epoch 22, Batch 141, Loss: 0.36883166432380676\n",
      "Epoch 22, Batch 142, Loss: 0.32036077976226807\n",
      "Epoch 22, Batch 143, Loss: 0.47984474897384644\n",
      "Epoch 22, Batch 144, Loss: 0.5626198053359985\n",
      "Epoch 22, Batch 145, Loss: 0.465733140707016\n",
      "Epoch 22, Batch 146, Loss: 0.3609963357448578\n",
      "Epoch 22, Batch 147, Loss: 0.3810211420059204\n",
      "Epoch 22, Batch 148, Loss: 0.30569538474082947\n",
      "Epoch 22, Batch 149, Loss: 0.4058142602443695\n",
      "Epoch 22, Batch 150, Loss: 0.32759225368499756\n",
      "Epoch 22, Batch 151, Loss: 0.2152726650238037\n",
      "Epoch 22, Batch 152, Loss: 0.24482780694961548\n",
      "Epoch 22, Batch 153, Loss: 0.3594934344291687\n",
      "Epoch 22, Batch 154, Loss: 0.47533753514289856\n",
      "Epoch 22, Batch 155, Loss: 0.3363037705421448\n",
      "Epoch 22, Batch 156, Loss: 0.7164231538772583\n",
      "Epoch 22, Batch 157, Loss: 0.4344632923603058\n",
      "Epoch 22, Batch 158, Loss: 0.322295606136322\n",
      "Epoch 22, Batch 159, Loss: 0.27207762002944946\n",
      "Epoch 22, Batch 160, Loss: 0.2678509056568146\n",
      "Epoch 22, Batch 161, Loss: 0.3782835304737091\n",
      "Epoch 22, Batch 162, Loss: 0.4661535918712616\n",
      "Epoch 22, Batch 163, Loss: 0.5936429500579834\n",
      "Epoch 22, Batch 164, Loss: 0.3981202244758606\n",
      "Epoch 22, Batch 165, Loss: 0.4274263381958008\n",
      "Epoch 22, Batch 166, Loss: 0.24949517846107483\n",
      "Epoch 22, Batch 167, Loss: 0.4505012631416321\n",
      "Epoch 22, Batch 168, Loss: 0.5159045457839966\n",
      "Epoch 22, Batch 169, Loss: 0.283458411693573\n",
      "Epoch 22, Batch 170, Loss: 0.46199530363082886\n",
      "Epoch 22, Batch 171, Loss: 0.39555421471595764\n",
      "Epoch 22, Batch 172, Loss: 0.30105724930763245\n",
      "Epoch 22, Batch 173, Loss: 0.23191089928150177\n",
      "Epoch 22, Batch 174, Loss: 0.3354940116405487\n",
      "Epoch 22, Batch 175, Loss: 0.3364121615886688\n",
      "Epoch 22, Batch 176, Loss: 0.44483450055122375\n",
      "Epoch 22, Batch 177, Loss: 0.3761266767978668\n",
      "Epoch 22, Batch 178, Loss: 0.5998820662498474\n",
      "Epoch 22, Batch 179, Loss: 0.17304328083992004\n",
      "Epoch 22, Batch 180, Loss: 0.19574648141860962\n",
      "Epoch 22, Batch 181, Loss: 0.39425188302993774\n",
      "Epoch 22, Batch 182, Loss: 0.49633100628852844\n",
      "Epoch 22, Batch 183, Loss: 0.40925055742263794\n",
      "Epoch 22, Batch 184, Loss: 0.633638858795166\n",
      "Epoch 22, Batch 185, Loss: 0.3246990442276001\n",
      "Epoch 22, Batch 186, Loss: 0.628901481628418\n",
      "Epoch 22, Batch 187, Loss: 0.4845825135707855\n",
      "Epoch 22, Batch 188, Loss: 0.5868217945098877\n",
      "Epoch 22, Batch 189, Loss: 0.3368174731731415\n",
      "Epoch 22, Batch 190, Loss: 0.4223824143409729\n",
      "Epoch 22, Batch 191, Loss: 0.3026171922683716\n",
      "Epoch 22, Batch 192, Loss: 0.3758555054664612\n",
      "Epoch 22, Batch 193, Loss: 0.33260923624038696\n",
      "Epoch 22, Batch 194, Loss: 0.4375859797000885\n",
      "Epoch 22, Batch 195, Loss: 0.42716771364212036\n",
      "Epoch 22, Batch 196, Loss: 0.2874830365180969\n",
      "Epoch 22, Batch 197, Loss: 0.47164273262023926\n",
      "Epoch 22, Batch 198, Loss: 0.3574325740337372\n",
      "Epoch 22, Batch 199, Loss: 0.36437639594078064\n",
      "Epoch 22, Batch 200, Loss: 0.38956961035728455\n",
      "Epoch 22, Batch 201, Loss: 0.4795750677585602\n",
      "Epoch 22, Batch 202, Loss: 0.5466766953468323\n",
      "Epoch 22, Batch 203, Loss: 0.2352108657360077\n",
      "Epoch 22, Batch 204, Loss: 0.4593646824359894\n",
      "Epoch 22, Batch 205, Loss: 0.5030221939086914\n",
      "Epoch 22, Batch 206, Loss: 0.48896312713623047\n",
      "Epoch 22, Batch 207, Loss: 0.37529414892196655\n",
      "Epoch 22, Batch 208, Loss: 0.5198626518249512\n",
      "Epoch 22, Batch 209, Loss: 0.4687287211418152\n",
      "Epoch 22, Batch 210, Loss: 0.46099939942359924\n",
      "Epoch 22, Batch 211, Loss: 0.4588356912136078\n",
      "Epoch 22, Batch 212, Loss: 0.372555673122406\n",
      "Epoch 22, Batch 213, Loss: 0.4436498284339905\n",
      "Epoch 22, Batch 214, Loss: 0.3758288323879242\n",
      "Epoch 22, Batch 215, Loss: 0.47354933619499207\n",
      "Epoch 22, Batch 216, Loss: 0.5798068046569824\n",
      "Epoch 22, Batch 217, Loss: 0.4967600405216217\n",
      "Epoch 22, Batch 218, Loss: 0.42416828870773315\n",
      "Epoch 22, Batch 219, Loss: 0.5633077621459961\n",
      "Epoch 22, Batch 220, Loss: 0.3930988311767578\n",
      "Epoch 22, Batch 221, Loss: 0.4338483214378357\n",
      "Epoch 22, Batch 222, Loss: 0.25412431359291077\n",
      "Epoch 22, Batch 223, Loss: 0.4873157739639282\n",
      "Epoch 22, Batch 224, Loss: 0.3743530213832855\n",
      "Epoch 22, Batch 225, Loss: 0.39290347695350647\n",
      "Epoch 22, Batch 226, Loss: 0.30506083369255066\n",
      "Epoch 22, Batch 227, Loss: 0.6528365612030029\n",
      "Epoch 22, Batch 228, Loss: 0.6726551651954651\n",
      "Epoch 22, Batch 229, Loss: 0.39235061407089233\n",
      "Epoch 22, Batch 230, Loss: 0.44940295815467834\n",
      "Epoch 22, Batch 231, Loss: 0.4865046739578247\n",
      "Epoch 22, Batch 232, Loss: 0.6497366428375244\n",
      "Epoch 22, Batch 233, Loss: 0.21722042560577393\n",
      "Epoch 22, Batch 234, Loss: 0.41010814905166626\n",
      "Epoch 22, Batch 235, Loss: 0.3400944471359253\n",
      "Epoch 22, Batch 236, Loss: 0.34666159749031067\n",
      "Epoch 22, Batch 237, Loss: 0.4059901833534241\n",
      "Epoch 22, Batch 238, Loss: 0.3210242688655853\n",
      "Epoch 22, Batch 239, Loss: 0.4263300895690918\n",
      "Epoch 22, Batch 240, Loss: 0.4719184637069702\n",
      "Epoch 22, Batch 241, Loss: 0.5976399779319763\n",
      "Epoch 22, Batch 242, Loss: 0.3520316183567047\n",
      "Epoch 22, Batch 243, Loss: 0.2567220628261566\n",
      "Epoch 22, Batch 244, Loss: 0.4911075532436371\n",
      "Epoch 22, Batch 245, Loss: 0.34785890579223633\n",
      "Epoch 22, Batch 246, Loss: 0.461530476808548\n",
      "Epoch 22, Batch 247, Loss: 0.3412655293941498\n",
      "Epoch 22, Batch 248, Loss: 0.4769916832447052\n",
      "Epoch 22, Batch 249, Loss: 0.45700883865356445\n",
      "Epoch 22, Batch 250, Loss: 0.2980123460292816\n",
      "Epoch 22, Batch 251, Loss: 0.28523656725883484\n",
      "Epoch 22, Batch 252, Loss: 0.5507203936576843\n",
      "Epoch 22, Batch 253, Loss: 0.4969567060470581\n",
      "Epoch 22, Batch 254, Loss: 0.22640463709831238\n",
      "Epoch 22, Batch 255, Loss: 0.39802899956703186\n",
      "Epoch 22, Batch 256, Loss: 0.5309535264968872\n",
      "Epoch 22, Batch 257, Loss: 0.32221195101737976\n",
      "Epoch 22, Batch 258, Loss: 0.4903064966201782\n",
      "Epoch 22, Batch 259, Loss: 0.41305074095726013\n",
      "Epoch 22, Batch 260, Loss: 0.45182275772094727\n",
      "Epoch 22, Batch 261, Loss: 0.3603740930557251\n",
      "Epoch 22, Batch 262, Loss: 0.3248221278190613\n",
      "Epoch 22, Batch 263, Loss: 0.42072442173957825\n",
      "Epoch 22, Batch 264, Loss: 0.3482728898525238\n",
      "Epoch 22, Batch 265, Loss: 0.379283607006073\n",
      "Epoch 22, Batch 266, Loss: 0.5838173627853394\n",
      "Epoch 22, Batch 267, Loss: 0.40767237544059753\n",
      "Epoch 22, Batch 268, Loss: 0.25964829325675964\n",
      "Epoch 22, Batch 269, Loss: 0.4097297489643097\n",
      "Epoch 22, Batch 270, Loss: 0.2620704174041748\n",
      "Epoch 22, Batch 271, Loss: 0.3606230616569519\n",
      "Epoch 22, Batch 272, Loss: 0.511414647102356\n",
      "Epoch 22, Batch 273, Loss: 0.49008601903915405\n",
      "Epoch 22, Batch 274, Loss: 0.40853315591812134\n",
      "Epoch 22, Batch 275, Loss: 0.5639892816543579\n",
      "Epoch 22, Batch 276, Loss: 0.35136252641677856\n",
      "Epoch 22, Batch 277, Loss: 0.3117601275444031\n",
      "Epoch 22, Batch 278, Loss: 0.3215469717979431\n",
      "Epoch 22, Batch 279, Loss: 0.4220525622367859\n",
      "Epoch 22, Batch 280, Loss: 0.3578602969646454\n",
      "Epoch 22, Batch 281, Loss: 0.442256361246109\n",
      "Epoch 22, Batch 282, Loss: 0.39268332719802856\n",
      "Epoch 22, Batch 283, Loss: 0.40472254157066345\n",
      "Epoch 22, Batch 284, Loss: 0.2706703245639801\n",
      "Epoch 22, Batch 285, Loss: 0.32357439398765564\n",
      "Epoch 22, Batch 286, Loss: 0.3552611172199249\n",
      "Epoch 22, Batch 287, Loss: 0.4153554439544678\n",
      "Epoch 22, Batch 288, Loss: 0.37029480934143066\n",
      "Epoch 22, Batch 289, Loss: 0.38650885224342346\n",
      "Epoch 22, Batch 290, Loss: 0.3049895763397217\n",
      "Epoch 22, Batch 291, Loss: 0.18683598935604095\n",
      "Epoch 22, Batch 292, Loss: 0.30255505442619324\n",
      "Epoch 22, Batch 293, Loss: 0.43433451652526855\n",
      "Epoch 22, Batch 294, Loss: 0.43152332305908203\n",
      "Epoch 22, Batch 295, Loss: 0.22741135954856873\n",
      "Epoch 22, Batch 296, Loss: 0.4282921850681305\n",
      "Epoch 22, Batch 297, Loss: 0.28564679622650146\n",
      "Epoch 22, Batch 298, Loss: 0.23759229481220245\n",
      "Epoch 22, Batch 299, Loss: 0.35320067405700684\n",
      "Epoch 22, Batch 300, Loss: 0.27642783522605896\n",
      "Epoch 22, Batch 301, Loss: 0.484112948179245\n",
      "Epoch 22, Batch 302, Loss: 0.34349244832992554\n",
      "Epoch 22, Batch 303, Loss: 0.32082730531692505\n",
      "Epoch 22, Batch 304, Loss: 0.28099724650382996\n",
      "Epoch 22, Batch 305, Loss: 0.580747127532959\n",
      "Epoch 22, Batch 306, Loss: 0.44380247592926025\n",
      "Epoch 22, Batch 307, Loss: 0.41321510076522827\n",
      "Epoch 22, Batch 308, Loss: 0.372054785490036\n",
      "Epoch 22, Batch 309, Loss: 0.30635321140289307\n",
      "Epoch 22, Batch 310, Loss: 0.4866044521331787\n",
      "Epoch 22, Batch 311, Loss: 0.3701067268848419\n",
      "Epoch 22, Batch 312, Loss: 0.4584265947341919\n",
      "Epoch 22, Batch 313, Loss: 0.48175299167633057\n",
      "Epoch 22, Batch 314, Loss: 0.40973103046417236\n",
      "Epoch 22, Batch 315, Loss: 0.26139405369758606\n",
      "Epoch 22, Batch 316, Loss: 0.3830530345439911\n",
      "Epoch 22, Batch 317, Loss: 0.8112559914588928\n",
      "Epoch 22, Batch 318, Loss: 0.48474764823913574\n",
      "Epoch 22, Batch 319, Loss: 0.3371904790401459\n",
      "Epoch 22, Batch 320, Loss: 0.22622831165790558\n",
      "Epoch 22, Batch 321, Loss: 0.3405521810054779\n",
      "Epoch 22, Batch 322, Loss: 0.5784855484962463\n",
      "Epoch 22, Batch 323, Loss: 0.5440115928649902\n",
      "Epoch 22, Batch 324, Loss: 0.30663952231407166\n",
      "Epoch 22, Batch 325, Loss: 0.3587471842765808\n",
      "Epoch 22, Batch 326, Loss: 0.3437960147857666\n",
      "Epoch 22, Batch 327, Loss: 0.37790563702583313\n",
      "Epoch 22, Batch 328, Loss: 0.31448671221733093\n",
      "Epoch 22, Batch 329, Loss: 0.5038251876831055\n",
      "Epoch 22, Batch 330, Loss: 0.5358299016952515\n",
      "Epoch 22, Batch 331, Loss: 0.4420291483402252\n",
      "Epoch 22, Batch 332, Loss: 0.5111003518104553\n",
      "Epoch 22, Batch 333, Loss: 0.1851186454296112\n",
      "Epoch 22, Batch 334, Loss: 0.32626205682754517\n",
      "Epoch 22, Batch 335, Loss: 0.3618631362915039\n",
      "Epoch 22, Batch 336, Loss: 0.41388869285583496\n",
      "Epoch 22, Batch 337, Loss: 0.3691423535346985\n",
      "Epoch 22, Batch 338, Loss: 0.4016338586807251\n",
      "Epoch 22, Batch 339, Loss: 0.2720259726047516\n",
      "Epoch 22, Batch 340, Loss: 0.28870558738708496\n",
      "Epoch 22, Batch 341, Loss: 0.35067057609558105\n",
      "Epoch 22, Batch 342, Loss: 0.30150020122528076\n",
      "Epoch 22, Batch 343, Loss: 0.33028551936149597\n",
      "Epoch 22, Batch 344, Loss: 0.2544482350349426\n",
      "Epoch 22, Batch 345, Loss: 0.25607219338417053\n",
      "Epoch 22, Batch 346, Loss: 0.34344717860221863\n",
      "Epoch 22, Batch 347, Loss: 0.48401787877082825\n",
      "Epoch 22, Batch 348, Loss: 0.2852095067501068\n",
      "Epoch 22, Batch 349, Loss: 0.304004430770874\n",
      "Epoch 22, Batch 350, Loss: 0.3439149558544159\n",
      "Epoch 22, Batch 351, Loss: 0.2903686463832855\n",
      "Epoch 22, Batch 352, Loss: 0.4558754563331604\n",
      "Epoch 22, Batch 353, Loss: 0.4305906593799591\n",
      "Epoch 22, Batch 354, Loss: 0.4135940670967102\n",
      "Epoch 22, Batch 355, Loss: 0.29632872343063354\n",
      "Epoch 22, Batch 356, Loss: 0.41416868567466736\n",
      "Epoch 22, Batch 357, Loss: 0.3891238868236542\n",
      "Epoch 22, Batch 358, Loss: 0.36130303144454956\n",
      "Epoch 22, Batch 359, Loss: 0.6157836318016052\n",
      "Epoch 22, Batch 360, Loss: 0.3955109119415283\n",
      "Epoch 22, Batch 361, Loss: 0.3816007375717163\n",
      "Epoch 22, Batch 362, Loss: 0.32184842228889465\n",
      "Epoch 22, Batch 363, Loss: 0.39525914192199707\n",
      "Epoch 22, Batch 364, Loss: 0.24375151097774506\n",
      "Epoch 22, Batch 365, Loss: 0.2893028259277344\n",
      "Epoch 22, Batch 366, Loss: 0.14362022280693054\n",
      "Epoch 22, Batch 367, Loss: 0.504014253616333\n",
      "Epoch 22, Batch 368, Loss: 0.5196899771690369\n",
      "Epoch 22, Batch 369, Loss: 0.3315252661705017\n",
      "Epoch 22, Batch 370, Loss: 0.48723024129867554\n",
      "Epoch 22, Batch 371, Loss: 0.4394403100013733\n",
      "Epoch 22, Batch 372, Loss: 0.41325515508651733\n",
      "Epoch 22, Batch 373, Loss: 0.4139266908168793\n",
      "Epoch 22, Batch 374, Loss: 0.4975992739200592\n",
      "Epoch 22, Batch 375, Loss: 0.3440392315387726\n",
      "Epoch 22, Batch 376, Loss: 0.3557749092578888\n",
      "Epoch 22, Batch 377, Loss: 0.5307642221450806\n",
      "Epoch 22, Batch 378, Loss: 0.5420592427253723\n",
      "Epoch 22, Batch 379, Loss: 0.5289266109466553\n",
      "Epoch 22, Batch 380, Loss: 0.3192006051540375\n",
      "Epoch 22, Batch 381, Loss: 0.30638837814331055\n",
      "Epoch 22, Batch 382, Loss: 0.44455045461654663\n",
      "Epoch 22, Batch 383, Loss: 0.1890283077955246\n",
      "Epoch 22, Batch 384, Loss: 0.5414290428161621\n",
      "Epoch 22, Batch 385, Loss: 0.24870450794696808\n",
      "Epoch 22, Batch 386, Loss: 0.3455882966518402\n",
      "Epoch 22, Batch 387, Loss: 0.3187252879142761\n",
      "Epoch 22, Batch 388, Loss: 0.49498260021209717\n",
      "Epoch 22, Batch 389, Loss: 0.24075552821159363\n",
      "Epoch 22, Batch 390, Loss: 0.39799755811691284\n",
      "Epoch 22, Batch 391, Loss: 0.5862549543380737\n",
      "Epoch 22, Batch 392, Loss: 0.46411365270614624\n",
      "Epoch 22, Batch 393, Loss: 0.3318786025047302\n",
      "Epoch 22, Batch 394, Loss: 0.26461222767829895\n",
      "Epoch 22, Batch 395, Loss: 0.2580144703388214\n",
      "Epoch 22, Batch 396, Loss: 0.3339121341705322\n",
      "Epoch 22, Batch 397, Loss: 0.337872713804245\n",
      "Epoch 22, Batch 398, Loss: 0.45504510402679443\n",
      "Epoch 22, Batch 399, Loss: 0.3850659430027008\n",
      "Epoch 22, Batch 400, Loss: 0.4360659122467041\n",
      "Epoch 22, Batch 401, Loss: 0.3420290946960449\n",
      "Epoch 22, Batch 402, Loss: 0.3109248876571655\n",
      "Epoch 22, Batch 403, Loss: 0.2916668653488159\n",
      "Epoch 22, Batch 404, Loss: 0.4078807234764099\n",
      "Epoch 22, Batch 405, Loss: 0.47426408529281616\n",
      "Epoch 22, Batch 406, Loss: 0.2409413456916809\n",
      "Epoch 22, Batch 407, Loss: 0.42267581820487976\n",
      "Epoch 22, Batch 408, Loss: 0.5773112177848816\n",
      "Epoch 22, Batch 409, Loss: 0.3712650239467621\n",
      "Epoch 22, Batch 410, Loss: 0.44808846712112427\n",
      "Epoch 22, Batch 411, Loss: 0.33067500591278076\n",
      "Epoch 22, Batch 412, Loss: 0.35919997096061707\n",
      "Epoch 22, Batch 413, Loss: 0.23230567574501038\n",
      "Epoch 22, Batch 414, Loss: 0.35694587230682373\n",
      "Epoch 22, Batch 415, Loss: 0.2666641175746918\n",
      "Epoch 22, Batch 416, Loss: 0.3270926773548126\n",
      "Epoch 22, Batch 417, Loss: 0.3363800048828125\n",
      "Epoch 22, Batch 418, Loss: 0.42385008931159973\n",
      "Epoch 22, Batch 419, Loss: 0.3530236482620239\n",
      "Epoch 22, Batch 420, Loss: 0.4104014039039612\n",
      "Epoch 22, Batch 421, Loss: 0.5008130073547363\n",
      "Epoch 22, Batch 422, Loss: 0.5080438256263733\n",
      "Epoch 22, Batch 423, Loss: 0.559005081653595\n",
      "Epoch 22, Batch 424, Loss: 0.25454947352409363\n",
      "Epoch 22, Batch 425, Loss: 0.3028041124343872\n",
      "Epoch 22, Batch 426, Loss: 0.4393777847290039\n",
      "Epoch 22, Batch 427, Loss: 0.3560302257537842\n",
      "Epoch 22, Batch 428, Loss: 0.4342646598815918\n",
      "Epoch 22, Batch 429, Loss: 0.4288250207901001\n",
      "Epoch 22, Batch 430, Loss: 0.5256902575492859\n",
      "Epoch 22, Batch 431, Loss: 0.38485467433929443\n",
      "Epoch 22, Batch 432, Loss: 0.48906368017196655\n",
      "Epoch 22, Batch 433, Loss: 0.3301994204521179\n",
      "Epoch 22, Batch 434, Loss: 0.5122711658477783\n",
      "Epoch 22, Batch 435, Loss: 0.34963563084602356\n",
      "Epoch 22, Batch 436, Loss: 0.39064133167266846\n",
      "Epoch 22, Batch 437, Loss: 0.3620372712612152\n",
      "Epoch 22, Batch 438, Loss: 0.4106912612915039\n",
      "Epoch 22, Batch 439, Loss: 0.5045484304428101\n",
      "Epoch 22, Batch 440, Loss: 0.32830896973609924\n",
      "Epoch 22, Batch 441, Loss: 0.2995934784412384\n",
      "Epoch 22, Batch 442, Loss: 0.3350415825843811\n",
      "Epoch 22, Batch 443, Loss: 0.33963543176651\n",
      "Epoch 22, Batch 444, Loss: 0.3749410808086395\n",
      "Epoch 22, Batch 445, Loss: 0.34740588068962097\n",
      "Epoch 22, Batch 446, Loss: 0.36430010199546814\n",
      "Epoch 22, Batch 447, Loss: 0.4343625009059906\n",
      "Epoch 22, Batch 448, Loss: 0.39672917127609253\n",
      "Epoch 22, Batch 449, Loss: 0.28805285692214966\n",
      "Epoch 22, Batch 450, Loss: 0.488639235496521\n",
      "Epoch 22, Batch 451, Loss: 0.4702550172805786\n",
      "Epoch 22, Batch 452, Loss: 0.24075907468795776\n",
      "Epoch 22, Batch 453, Loss: 0.4038219153881073\n",
      "Epoch 22, Batch 454, Loss: 0.2531866133213043\n",
      "Epoch 22, Batch 455, Loss: 0.40243738889694214\n",
      "Epoch 22, Batch 456, Loss: 0.48572999238967896\n",
      "Epoch 22, Batch 457, Loss: 0.3148577809333801\n",
      "Epoch 22, Batch 458, Loss: 0.30679696798324585\n",
      "Epoch 22, Batch 459, Loss: 0.1983802169561386\n",
      "Epoch 22, Batch 460, Loss: 0.24154382944107056\n",
      "Epoch 22, Batch 461, Loss: 0.22719378769397736\n",
      "Epoch 22, Batch 462, Loss: 0.39662301540374756\n",
      "Epoch 22, Batch 463, Loss: 0.3987686038017273\n",
      "Epoch 22, Batch 464, Loss: 0.42985036969184875\n",
      "Epoch 22, Batch 465, Loss: 0.3967989981174469\n",
      "Epoch 22, Batch 466, Loss: 0.20396026968955994\n",
      "Epoch 22, Batch 467, Loss: 0.4182834029197693\n",
      "Epoch 22, Batch 468, Loss: 0.3990286588668823\n",
      "Epoch 22, Batch 469, Loss: 0.3184312880039215\n",
      "Epoch 22, Batch 470, Loss: 0.51683109998703\n",
      "Epoch 22, Batch 471, Loss: 0.5882145762443542\n",
      "Epoch 22, Batch 472, Loss: 0.3743548095226288\n",
      "Epoch 22, Batch 473, Loss: 0.4893319308757782\n",
      "Epoch 22, Batch 474, Loss: 0.20701199769973755\n",
      "Epoch 22, Batch 475, Loss: 0.4226016700267792\n",
      "Epoch 22, Batch 476, Loss: 0.3097507953643799\n",
      "Epoch 22, Batch 477, Loss: 0.49602848291397095\n",
      "Epoch 22, Batch 478, Loss: 0.2830202579498291\n",
      "Epoch 22, Batch 479, Loss: 0.4844326376914978\n",
      "Epoch 22, Batch 480, Loss: 0.43485888838768005\n",
      "Epoch 22, Batch 481, Loss: 0.7240797877311707\n",
      "Epoch 22, Batch 482, Loss: 0.18383662402629852\n",
      "Epoch 22, Batch 483, Loss: 0.3595869243144989\n",
      "Epoch 22, Batch 484, Loss: 0.3974098265171051\n",
      "Epoch 22, Batch 485, Loss: 0.26997172832489014\n",
      "Epoch 22, Batch 486, Loss: 0.44833138585090637\n",
      "Epoch 22, Batch 487, Loss: 0.34336036443710327\n",
      "Epoch 22, Batch 488, Loss: 0.3461970388889313\n",
      "Epoch 22, Batch 489, Loss: 0.4309585690498352\n",
      "Epoch 22, Batch 490, Loss: 0.28302180767059326\n",
      "Epoch 22, Batch 491, Loss: 0.3331094980239868\n",
      "Epoch 22, Batch 492, Loss: 0.4161038100719452\n",
      "Epoch 22, Batch 493, Loss: 0.6011492013931274\n",
      "Epoch 22, Batch 494, Loss: 0.6162976622581482\n",
      "Epoch 22, Batch 495, Loss: 0.4137745499610901\n",
      "Epoch 22, Batch 496, Loss: 0.3384239673614502\n",
      "Epoch 22, Batch 497, Loss: 0.2908308506011963\n",
      "Epoch 22, Batch 498, Loss: 0.37223947048187256\n",
      "Epoch 22, Batch 499, Loss: 0.35192927718162537\n",
      "Epoch 22, Batch 500, Loss: 0.3467544615268707\n",
      "Epoch 22, Batch 501, Loss: 0.4097617268562317\n",
      "Epoch 22, Batch 502, Loss: 0.33745309710502625\n",
      "Epoch 22, Batch 503, Loss: 0.37656116485595703\n",
      "Epoch 22, Batch 504, Loss: 0.40773648023605347\n",
      "Epoch 22, Batch 505, Loss: 0.41043543815612793\n",
      "Epoch 22, Batch 506, Loss: 0.348782479763031\n",
      "Epoch 22, Batch 507, Loss: 0.436998575925827\n",
      "Epoch 22, Batch 508, Loss: 0.4230392575263977\n",
      "Epoch 22, Batch 509, Loss: 0.46905121207237244\n",
      "Epoch 22, Batch 510, Loss: 0.5282183289527893\n",
      "Epoch 22, Batch 511, Loss: 0.5233893394470215\n",
      "Epoch 22, Batch 512, Loss: 0.3846578896045685\n",
      "Epoch 22, Batch 513, Loss: 0.40614423155784607\n",
      "Epoch 22, Batch 514, Loss: 0.49288931488990784\n",
      "Epoch 22, Batch 515, Loss: 0.31548357009887695\n",
      "Epoch 22, Batch 516, Loss: 0.35216641426086426\n",
      "Epoch 22, Batch 517, Loss: 0.37444204092025757\n",
      "Epoch 22, Batch 518, Loss: 0.36271438002586365\n",
      "Epoch 22, Batch 519, Loss: 0.4137897193431854\n",
      "Epoch 22, Batch 520, Loss: 0.41833311319351196\n",
      "Epoch 22, Batch 521, Loss: 0.2622224986553192\n",
      "Epoch 22, Batch 522, Loss: 0.46165505051612854\n",
      "Epoch 22, Batch 523, Loss: 0.34638041257858276\n",
      "Epoch 22, Batch 524, Loss: 0.4070269465446472\n",
      "Epoch 22, Batch 525, Loss: 0.37215548753738403\n",
      "Epoch 22, Batch 526, Loss: 0.4056530296802521\n",
      "Epoch 22, Batch 527, Loss: 0.36297330260276794\n",
      "Epoch 22, Batch 528, Loss: 0.44263285398483276\n",
      "Epoch 22, Batch 529, Loss: 0.2141416221857071\n",
      "Epoch 22, Batch 530, Loss: 0.4482535719871521\n",
      "Epoch 22, Batch 531, Loss: 0.4950329065322876\n",
      "Epoch 22, Batch 532, Loss: 0.3609999418258667\n",
      "Epoch 22, Batch 533, Loss: 0.5043987035751343\n",
      "Epoch 22, Batch 534, Loss: 0.24636605381965637\n",
      "Epoch 22, Batch 535, Loss: 0.20476114749908447\n",
      "Epoch 22, Batch 536, Loss: 0.3222273588180542\n",
      "Epoch 22, Batch 537, Loss: 0.24772560596466064\n",
      "Epoch 22, Batch 538, Loss: 0.3416222631931305\n",
      "Epoch 22, Batch 539, Loss: 0.42880162596702576\n",
      "Epoch 22, Batch 540, Loss: 0.36543792486190796\n",
      "Epoch 22, Batch 541, Loss: 0.27887022495269775\n",
      "Epoch 22, Batch 542, Loss: 0.40030503273010254\n",
      "Epoch 22, Batch 543, Loss: 0.8171245455741882\n",
      "Epoch 22, Batch 544, Loss: 0.28138989210128784\n",
      "Epoch 22, Batch 545, Loss: 0.3876175880432129\n",
      "Epoch 22, Batch 546, Loss: 0.3114396333694458\n",
      "Epoch 22, Batch 547, Loss: 0.48695147037506104\n",
      "Epoch 22, Batch 548, Loss: 0.438720166683197\n",
      "Epoch 22, Batch 549, Loss: 0.42324134707450867\n",
      "Epoch 22, Batch 550, Loss: 0.38472187519073486\n",
      "Epoch 22, Batch 551, Loss: 0.48402148485183716\n",
      "Epoch 22, Batch 552, Loss: 0.4185374975204468\n",
      "Epoch 22, Batch 553, Loss: 0.47043877840042114\n",
      "Epoch 22, Batch 554, Loss: 0.37709999084472656\n",
      "Epoch 22, Batch 555, Loss: 0.29397717118263245\n",
      "Epoch 22, Batch 556, Loss: 0.4403308629989624\n",
      "Epoch 22, Batch 557, Loss: 0.37125706672668457\n",
      "Epoch 22, Batch 558, Loss: 0.7705200910568237\n",
      "Epoch 22, Batch 559, Loss: 0.4163978695869446\n",
      "Epoch 22, Batch 560, Loss: 0.3217957615852356\n",
      "Epoch 22, Batch 561, Loss: 0.7453932762145996\n",
      "Epoch 22, Batch 562, Loss: 0.27769577503204346\n",
      "Epoch 22, Batch 563, Loss: 0.34158456325531006\n",
      "Epoch 22, Batch 564, Loss: 0.32664117217063904\n",
      "Epoch 22, Batch 565, Loss: 0.32587170600891113\n",
      "Epoch 22, Batch 566, Loss: 0.36996808648109436\n",
      "Epoch 22, Batch 567, Loss: 0.48908713459968567\n",
      "Epoch 22, Batch 568, Loss: 0.34702229499816895\n",
      "Epoch 22, Batch 569, Loss: 0.353061705827713\n",
      "Epoch 22, Batch 570, Loss: 0.3341313302516937\n",
      "Epoch 22, Batch 571, Loss: 0.5018905401229858\n",
      "Epoch 22, Batch 572, Loss: 0.44118180871009827\n",
      "Epoch 22, Batch 573, Loss: 0.27773353457450867\n",
      "Epoch 22, Batch 574, Loss: 0.3967021107673645\n",
      "Epoch 22, Batch 575, Loss: 0.39705216884613037\n",
      "Epoch 22, Batch 576, Loss: 0.4148111343383789\n",
      "Epoch 22, Batch 577, Loss: 0.2870444655418396\n",
      "Epoch 22, Batch 578, Loss: 0.2639169991016388\n",
      "Epoch 22, Batch 579, Loss: 0.3027157783508301\n",
      "Epoch 22, Batch 580, Loss: 0.542998731136322\n",
      "Epoch 22, Batch 581, Loss: 0.33463653922080994\n",
      "Epoch 22, Batch 582, Loss: 0.36556246876716614\n",
      "Epoch 22, Batch 583, Loss: 0.4844030439853668\n",
      "Epoch 22, Batch 584, Loss: 0.3839503824710846\n",
      "Epoch 22, Batch 585, Loss: 0.3027893602848053\n",
      "Epoch 22, Batch 586, Loss: 0.3959408700466156\n",
      "Epoch 22, Batch 587, Loss: 0.4727441668510437\n",
      "Epoch 22, Batch 588, Loss: 0.5229186415672302\n",
      "Epoch 22, Batch 589, Loss: 0.4855515658855438\n",
      "Epoch 22, Batch 590, Loss: 0.31390368938446045\n",
      "Epoch 22, Batch 591, Loss: 0.42273035645484924\n",
      "Epoch 22, Batch 592, Loss: 0.35071441531181335\n",
      "Epoch 22, Batch 593, Loss: 0.33136606216430664\n",
      "Epoch 22, Batch 594, Loss: 0.3984569311141968\n",
      "Epoch 22, Batch 595, Loss: 0.6409991383552551\n",
      "Epoch 22, Batch 596, Loss: 0.6207938194274902\n",
      "Epoch 22, Batch 597, Loss: 0.3467862904071808\n",
      "Epoch 22, Batch 598, Loss: 0.4861568808555603\n",
      "Epoch 22, Batch 599, Loss: 0.52264004945755\n",
      "Epoch 22, Batch 600, Loss: 0.3758261203765869\n",
      "Epoch 22, Batch 601, Loss: 0.4560696482658386\n",
      "Epoch 22, Batch 602, Loss: 0.34103983640670776\n",
      "Epoch 22, Batch 603, Loss: 0.280597448348999\n",
      "Epoch 22, Batch 604, Loss: 0.28539204597473145\n",
      "Epoch 22, Batch 605, Loss: 0.33519068360328674\n",
      "Epoch 22, Batch 606, Loss: 0.3342123031616211\n",
      "Epoch 22, Batch 607, Loss: 0.2839357554912567\n",
      "Epoch 22, Batch 608, Loss: 0.39600038528442383\n",
      "Epoch 22, Batch 609, Loss: 0.3572149872779846\n",
      "Epoch 22, Batch 610, Loss: 0.4779053330421448\n",
      "Epoch 22, Batch 611, Loss: 0.5326731204986572\n",
      "Epoch 22, Batch 612, Loss: 0.42800599336624146\n",
      "Epoch 22, Batch 613, Loss: 0.49221906065940857\n",
      "Epoch 22, Batch 614, Loss: 0.27250969409942627\n",
      "Epoch 22, Batch 615, Loss: 0.42063406109809875\n",
      "Epoch 22, Batch 616, Loss: 0.27454620599746704\n",
      "Epoch 22, Batch 617, Loss: 0.38283172249794006\n",
      "Epoch 22, Batch 618, Loss: 0.3520883321762085\n",
      "Epoch 22, Batch 619, Loss: 0.5331117510795593\n",
      "Epoch 22, Batch 620, Loss: 0.5155415534973145\n",
      "Epoch 22, Batch 621, Loss: 0.46532776951789856\n",
      "Epoch 22, Batch 622, Loss: 0.3363930284976959\n",
      "Epoch 22, Batch 623, Loss: 0.34772980213165283\n",
      "Epoch 22, Batch 624, Loss: 0.5551435351371765\n",
      "Epoch 22, Batch 625, Loss: 0.5901345610618591\n",
      "Epoch 22, Batch 626, Loss: 0.3497917950153351\n",
      "Epoch 22, Batch 627, Loss: 0.20579765737056732\n",
      "Epoch 22, Batch 628, Loss: 0.4082404375076294\n",
      "Epoch 22, Batch 629, Loss: 0.26057302951812744\n",
      "Epoch 22, Batch 630, Loss: 0.5572914481163025\n",
      "Epoch 22, Batch 631, Loss: 0.20819933712482452\n",
      "Epoch 22, Batch 632, Loss: 0.34787532687187195\n",
      "Epoch 22, Batch 633, Loss: 0.2845091223716736\n",
      "Epoch 22, Batch 634, Loss: 0.4590285122394562\n",
      "Epoch 22, Batch 635, Loss: 0.3229651153087616\n",
      "Epoch 22, Batch 636, Loss: 0.4224849343299866\n",
      "Epoch 22, Batch 637, Loss: 0.44695544242858887\n",
      "Epoch 22, Batch 638, Loss: 0.39269784092903137\n",
      "Epoch 22, Batch 639, Loss: 0.29402729868888855\n",
      "Epoch 22, Batch 640, Loss: 0.3962656855583191\n",
      "Epoch 22, Batch 641, Loss: 0.2346922904253006\n",
      "Epoch 22, Batch 642, Loss: 0.2662498652935028\n",
      "Epoch 22, Batch 643, Loss: 0.5758776664733887\n",
      "Epoch 22, Batch 644, Loss: 0.45076385140419006\n",
      "Epoch 22, Batch 645, Loss: 0.41049033403396606\n",
      "Epoch 22, Batch 646, Loss: 0.3952751159667969\n",
      "Epoch 22, Batch 647, Loss: 0.23337948322296143\n",
      "Epoch 22, Batch 648, Loss: 0.3231161832809448\n",
      "Epoch 22, Batch 649, Loss: 0.4178318381309509\n",
      "Epoch 22, Batch 650, Loss: 0.34893932938575745\n",
      "Epoch 22, Batch 651, Loss: 0.2959344685077667\n",
      "Epoch 22, Batch 652, Loss: 0.364775151014328\n",
      "Epoch 22, Batch 653, Loss: 0.5357451438903809\n",
      "Epoch 22, Batch 654, Loss: 0.5618694424629211\n",
      "Epoch 22, Batch 655, Loss: 0.360882431268692\n",
      "Epoch 22, Batch 656, Loss: 0.35288751125335693\n",
      "Epoch 22, Batch 657, Loss: 0.3627798557281494\n",
      "Epoch 22, Batch 658, Loss: 0.42636051774024963\n",
      "Epoch 22, Batch 659, Loss: 0.48637834191322327\n",
      "Epoch 22, Batch 660, Loss: 0.5176874399185181\n",
      "Epoch 22, Batch 661, Loss: 0.3918169438838959\n",
      "Epoch 22, Batch 662, Loss: 0.30898112058639526\n",
      "Epoch 22, Batch 663, Loss: 0.4595518410205841\n",
      "Epoch 22, Batch 664, Loss: 0.42679548263549805\n",
      "Epoch 22, Batch 665, Loss: 0.5731196403503418\n",
      "Epoch 22, Batch 666, Loss: 0.4295772910118103\n",
      "Epoch 22, Batch 667, Loss: 0.2637580633163452\n",
      "Epoch 22, Batch 668, Loss: 0.3100336492061615\n",
      "Epoch 22, Batch 669, Loss: 0.34714680910110474\n",
      "Epoch 22, Batch 670, Loss: 0.379676878452301\n",
      "Epoch 22, Batch 671, Loss: 0.36723533272743225\n",
      "Epoch 22, Batch 672, Loss: 0.42248082160949707\n",
      "Epoch 22, Batch 673, Loss: 0.4107740521430969\n",
      "Epoch 22, Batch 674, Loss: 0.44149914383888245\n",
      "Epoch 22, Batch 675, Loss: 0.5509025454521179\n",
      "Epoch 22, Batch 676, Loss: 0.2509300410747528\n",
      "Epoch 22, Batch 677, Loss: 0.32679909467697144\n",
      "Epoch 22, Batch 678, Loss: 0.4193131625652313\n",
      "Epoch 22, Batch 679, Loss: 0.42962977290153503\n",
      "Epoch 22, Batch 680, Loss: 0.6357789635658264\n",
      "Epoch 22, Batch 681, Loss: 0.22679921984672546\n",
      "Epoch 22, Batch 682, Loss: 0.35132405161857605\n",
      "Epoch 22, Batch 683, Loss: 0.583206057548523\n",
      "Epoch 22, Batch 684, Loss: 0.4789682924747467\n",
      "Epoch 22, Batch 685, Loss: 0.4527871012687683\n",
      "Epoch 22, Batch 686, Loss: 0.5740518569946289\n",
      "Epoch 22, Batch 687, Loss: 0.4239087998867035\n",
      "Epoch 22, Batch 688, Loss: 0.3211126923561096\n",
      "Epoch 22, Batch 689, Loss: 0.281807005405426\n",
      "Epoch 22, Batch 690, Loss: 0.42204344272613525\n",
      "Epoch 22, Batch 691, Loss: 0.39061182737350464\n",
      "Epoch 22, Batch 692, Loss: 0.5697067975997925\n",
      "Epoch 22, Batch 693, Loss: 0.3355904221534729\n",
      "Epoch 22, Batch 694, Loss: 0.31284457445144653\n",
      "Epoch 22, Batch 695, Loss: 0.425548255443573\n",
      "Epoch 22, Batch 696, Loss: 0.4026317894458771\n",
      "Epoch 22, Batch 697, Loss: 0.4210355877876282\n",
      "Epoch 22, Batch 698, Loss: 0.3742969036102295\n",
      "Epoch 22, Batch 699, Loss: 0.30095595121383667\n",
      "Epoch 22, Batch 700, Loss: 0.3366338610649109\n",
      "Epoch 22, Batch 701, Loss: 0.303082674741745\n",
      "Epoch 22, Batch 702, Loss: 0.34169936180114746\n",
      "Epoch 22, Batch 703, Loss: 0.29113712906837463\n",
      "Epoch 22, Batch 704, Loss: 0.43761128187179565\n",
      "Epoch 22, Batch 705, Loss: 0.4203134775161743\n",
      "Epoch 22, Batch 706, Loss: 0.23084086179733276\n",
      "Epoch 22, Batch 707, Loss: 0.366417795419693\n",
      "Epoch 22, Batch 708, Loss: 0.4860366880893707\n",
      "Epoch 22, Batch 709, Loss: 0.38818004727363586\n",
      "Epoch 22, Batch 710, Loss: 0.44464853405952454\n",
      "Epoch 22, Batch 711, Loss: 0.4287315309047699\n",
      "Epoch 22, Batch 712, Loss: 0.41077330708503723\n",
      "Epoch 22, Batch 713, Loss: 0.3571717143058777\n",
      "Epoch 22, Batch 714, Loss: 0.32365739345550537\n",
      "Epoch 22, Batch 715, Loss: 0.31848451495170593\n",
      "Epoch 22, Batch 716, Loss: 0.6236567497253418\n",
      "Epoch 22, Batch 717, Loss: 0.38813596963882446\n",
      "Epoch 22, Batch 718, Loss: 0.34538984298706055\n",
      "Epoch 22, Batch 719, Loss: 0.5132274031639099\n",
      "Epoch 22, Batch 720, Loss: 0.34330856800079346\n",
      "Epoch 22, Batch 721, Loss: 0.3774985074996948\n",
      "Epoch 22, Batch 722, Loss: 0.27182072401046753\n",
      "Epoch 22, Batch 723, Loss: 0.29724031686782837\n",
      "Epoch 22, Batch 724, Loss: 0.45052480697631836\n",
      "Epoch 22, Batch 725, Loss: 0.43390408158302307\n",
      "Epoch 22, Batch 726, Loss: 0.474843293428421\n",
      "Epoch 22, Batch 727, Loss: 0.29953354597091675\n",
      "Epoch 22, Batch 728, Loss: 0.3428666889667511\n",
      "Epoch 22, Batch 729, Loss: 0.29843783378601074\n",
      "Epoch 22, Batch 730, Loss: 0.48243340849876404\n",
      "Epoch 22, Batch 731, Loss: 0.4582291543483734\n",
      "Epoch 22, Batch 732, Loss: 0.42323142290115356\n",
      "Epoch 22, Batch 733, Loss: 0.31686702370643616\n",
      "Epoch 22, Batch 734, Loss: 0.45699286460876465\n",
      "Epoch 22, Batch 735, Loss: 0.4064638018608093\n",
      "Epoch 22, Batch 736, Loss: 0.3581489622592926\n",
      "Epoch 22, Batch 737, Loss: 0.3284822106361389\n",
      "Epoch 22, Batch 738, Loss: 0.32742515206336975\n",
      "Epoch 22, Batch 739, Loss: 0.3363173007965088\n",
      "Epoch 22, Batch 740, Loss: 0.37399527430534363\n",
      "Epoch 22, Batch 741, Loss: 0.4445089101791382\n",
      "Epoch 22, Batch 742, Loss: 0.29374849796295166\n",
      "Epoch 22, Batch 743, Loss: 0.6519326567649841\n",
      "Epoch 22, Batch 744, Loss: 0.5432040691375732\n",
      "Epoch 22, Batch 745, Loss: 0.47745558619499207\n",
      "Epoch 22, Batch 746, Loss: 0.3995322287082672\n",
      "Epoch 22, Batch 747, Loss: 0.5183621048927307\n",
      "Epoch 22, Batch 748, Loss: 0.41183584928512573\n",
      "Epoch 22, Batch 749, Loss: 0.40720778703689575\n",
      "Epoch 22, Batch 750, Loss: 0.38761574029922485\n",
      "Epoch 22, Batch 751, Loss: 0.27207857370376587\n",
      "Epoch 22, Batch 752, Loss: 0.3366513252258301\n",
      "Epoch 22, Batch 753, Loss: 0.3234417140483856\n",
      "Epoch 22, Batch 754, Loss: 0.2206636667251587\n",
      "Epoch 22, Batch 755, Loss: 0.2722933888435364\n",
      "Epoch 22, Batch 756, Loss: 0.3855462670326233\n",
      "Epoch 22, Batch 757, Loss: 0.2940097749233246\n",
      "Epoch 22, Batch 758, Loss: 0.32331401109695435\n",
      "Epoch 22, Batch 759, Loss: 0.39162108302116394\n",
      "Epoch 22, Batch 760, Loss: 0.3684201240539551\n",
      "Epoch 22, Batch 761, Loss: 0.3821338713169098\n",
      "Epoch 22, Batch 762, Loss: 0.459970623254776\n",
      "Epoch 22, Batch 763, Loss: 0.4804241955280304\n",
      "Epoch 22, Batch 764, Loss: 0.5123817920684814\n",
      "Epoch 22, Batch 765, Loss: 0.42025554180145264\n",
      "Epoch 22, Batch 766, Loss: 0.284094899892807\n",
      "Epoch 22, Batch 767, Loss: 0.5052381753921509\n",
      "Epoch 22, Batch 768, Loss: 0.6032512187957764\n",
      "Epoch 22, Batch 769, Loss: 0.36146724224090576\n",
      "Epoch 22, Batch 770, Loss: 0.40418541431427\n",
      "Epoch 22, Batch 771, Loss: 0.45493414998054504\n",
      "Epoch 22, Batch 772, Loss: 0.4243801236152649\n",
      "Epoch 22, Batch 773, Loss: 0.275424987077713\n",
      "Epoch 22, Batch 774, Loss: 0.33533599972724915\n",
      "Epoch 22, Batch 775, Loss: 0.4964583218097687\n",
      "Epoch 22, Batch 776, Loss: 0.4432310461997986\n",
      "Epoch 22, Batch 777, Loss: 0.40632590651512146\n",
      "Epoch 22, Batch 778, Loss: 0.5367909669876099\n",
      "Epoch 22, Batch 779, Loss: 0.3176492750644684\n",
      "Epoch 22, Batch 780, Loss: 0.4339144825935364\n",
      "Epoch 22, Batch 781, Loss: 0.24789386987686157\n",
      "Epoch 22, Batch 782, Loss: 0.2895424962043762\n",
      "Epoch 22, Batch 783, Loss: 0.6035358905792236\n",
      "Epoch 22, Batch 784, Loss: 0.3419073820114136\n",
      "Epoch 22, Batch 785, Loss: 0.38677167892456055\n",
      "Epoch 22, Batch 786, Loss: 0.37714552879333496\n",
      "Epoch 22, Batch 787, Loss: 0.4159051477909088\n",
      "Epoch 22, Batch 788, Loss: 0.4112083613872528\n",
      "Epoch 22, Batch 789, Loss: 0.19349054992198944\n",
      "Epoch 22, Batch 790, Loss: 0.3487168550491333\n",
      "Epoch 22, Batch 791, Loss: 0.23847244679927826\n",
      "Epoch 22, Batch 792, Loss: 0.46005508303642273\n",
      "Epoch 22, Batch 793, Loss: 0.3534524738788605\n",
      "Epoch 22, Batch 794, Loss: 0.6097548604011536\n",
      "Epoch 22, Batch 795, Loss: 0.2955394387245178\n",
      "Epoch 22, Batch 796, Loss: 0.48489028215408325\n",
      "Epoch 22, Batch 797, Loss: 0.4902358651161194\n",
      "Epoch 22, Batch 798, Loss: 0.29834824800491333\n",
      "Epoch 22, Batch 799, Loss: 0.33142945170402527\n",
      "Epoch 22, Batch 800, Loss: 0.26464366912841797\n",
      "Epoch 22, Batch 801, Loss: 0.33202338218688965\n",
      "Epoch 22, Batch 802, Loss: 0.4641684591770172\n",
      "Epoch 22, Batch 803, Loss: 0.29783397912979126\n",
      "Epoch 22, Batch 804, Loss: 0.4904686212539673\n",
      "Epoch 22, Batch 805, Loss: 0.30772894620895386\n",
      "Epoch 22, Batch 806, Loss: 0.33543482422828674\n",
      "Epoch 22, Batch 807, Loss: 0.32823118567466736\n",
      "Epoch 22, Batch 808, Loss: 0.6194072961807251\n",
      "Epoch 22, Batch 809, Loss: 0.36483079195022583\n",
      "Epoch 22, Batch 810, Loss: 0.32523873448371887\n",
      "Epoch 22, Batch 811, Loss: 0.31811851263046265\n",
      "Epoch 22, Batch 812, Loss: 0.36305832862854004\n",
      "Epoch 22, Batch 813, Loss: 0.4619734287261963\n",
      "Epoch 22, Batch 814, Loss: 0.4358355700969696\n",
      "Epoch 22, Batch 815, Loss: 0.3129376173019409\n",
      "Epoch 22, Batch 816, Loss: 0.39403554797172546\n",
      "Epoch 22, Batch 817, Loss: 0.4160376787185669\n",
      "Epoch 22, Batch 818, Loss: 0.3734518587589264\n",
      "Epoch 22, Batch 819, Loss: 0.3237013518810272\n",
      "Epoch 22, Batch 820, Loss: 0.41701677441596985\n",
      "Epoch 22, Batch 821, Loss: 0.35652896761894226\n",
      "Epoch 22, Batch 822, Loss: 0.3997833728790283\n",
      "Epoch 22, Batch 823, Loss: 0.4302296042442322\n",
      "Epoch 22, Batch 824, Loss: 0.41728752851486206\n",
      "Epoch 22, Batch 825, Loss: 0.19334262609481812\n",
      "Epoch 22, Batch 826, Loss: 0.49927622079849243\n",
      "Epoch 22, Batch 827, Loss: 0.4368167817592621\n",
      "Epoch 22, Batch 828, Loss: 0.37299710512161255\n",
      "Epoch 22, Batch 829, Loss: 0.37480342388153076\n",
      "Epoch 22, Batch 830, Loss: 0.37347087264060974\n",
      "Epoch 22, Batch 831, Loss: 0.2595371603965759\n",
      "Epoch 22, Batch 832, Loss: 0.3752442002296448\n",
      "Epoch 22, Batch 833, Loss: 0.30640000104904175\n",
      "Epoch 22, Batch 834, Loss: 0.33790838718414307\n",
      "Epoch 22, Batch 835, Loss: 0.35704562067985535\n",
      "Epoch 22, Batch 836, Loss: 0.37029775977134705\n",
      "Epoch 22, Batch 837, Loss: 0.44051000475883484\n",
      "Epoch 22, Batch 838, Loss: 0.5201718211174011\n",
      "Epoch 22, Batch 839, Loss: 0.24754054844379425\n",
      "Epoch 22, Batch 840, Loss: 0.2461831122636795\n",
      "Epoch 22, Batch 841, Loss: 0.48506486415863037\n",
      "Epoch 22, Batch 842, Loss: 0.3187951445579529\n",
      "Epoch 22, Batch 843, Loss: 0.3442685902118683\n",
      "Epoch 22, Batch 844, Loss: 0.5520598292350769\n",
      "Epoch 22, Batch 845, Loss: 0.34828469157218933\n",
      "Epoch 22, Batch 846, Loss: 0.2800065577030182\n",
      "Epoch 22, Batch 847, Loss: 0.2285134494304657\n",
      "Epoch 22, Batch 848, Loss: 0.6535470485687256\n",
      "Epoch 22, Batch 849, Loss: 0.4595976173877716\n",
      "Epoch 22, Batch 850, Loss: 0.3769243359565735\n",
      "Epoch 22, Batch 851, Loss: 0.29899707436561584\n",
      "Epoch 22, Batch 852, Loss: 0.5382218360900879\n",
      "Epoch 22, Batch 853, Loss: 0.4182533919811249\n",
      "Epoch 22, Batch 854, Loss: 0.47443336248397827\n",
      "Epoch 22, Batch 855, Loss: 0.31882914900779724\n",
      "Epoch 22, Batch 856, Loss: 0.315769225358963\n",
      "Epoch 22, Batch 857, Loss: 0.5669090151786804\n",
      "Epoch 22, Batch 858, Loss: 0.3676317632198334\n",
      "Epoch 22, Batch 859, Loss: 0.4048762619495392\n",
      "Epoch 22, Batch 860, Loss: 0.35011348128318787\n",
      "Epoch 22, Batch 861, Loss: 0.26290377974510193\n",
      "Epoch 22, Batch 862, Loss: 0.3322451710700989\n",
      "Epoch 22, Batch 863, Loss: 0.24449296295642853\n",
      "Epoch 22, Batch 864, Loss: 0.4717958867549896\n",
      "Epoch 22, Batch 865, Loss: 0.6014657616615295\n",
      "Epoch 22, Batch 866, Loss: 0.4195939004421234\n",
      "Epoch 22, Batch 867, Loss: 0.22589457035064697\n",
      "Epoch 22, Batch 868, Loss: 0.3784102499485016\n",
      "Epoch 22, Batch 869, Loss: 0.4183827042579651\n",
      "Epoch 22, Batch 870, Loss: 0.44502195715904236\n",
      "Epoch 22, Batch 871, Loss: 0.34864428639411926\n",
      "Epoch 22, Batch 872, Loss: 0.43924686312675476\n",
      "Epoch 22, Batch 873, Loss: 0.3981449007987976\n",
      "Epoch 22, Batch 874, Loss: 0.20734566450119019\n",
      "Epoch 22, Batch 875, Loss: 0.38628557324409485\n",
      "Epoch 22, Batch 876, Loss: 0.3264729380607605\n",
      "Epoch 22, Batch 877, Loss: 0.4155130684375763\n",
      "Epoch 22, Batch 878, Loss: 0.40371575951576233\n",
      "Epoch 22, Batch 879, Loss: 0.46864238381385803\n",
      "Epoch 22, Batch 880, Loss: 0.2111225724220276\n",
      "Epoch 22, Batch 881, Loss: 0.21547651290893555\n",
      "Epoch 22, Batch 882, Loss: 0.3253363072872162\n",
      "Epoch 22, Batch 883, Loss: 0.324076384305954\n",
      "Epoch 22, Batch 884, Loss: 0.27947860956192017\n",
      "Epoch 22, Batch 885, Loss: 0.5132783651351929\n",
      "Epoch 22, Batch 886, Loss: 0.42970120906829834\n",
      "Epoch 22, Batch 887, Loss: 0.29551199078559875\n",
      "Epoch 22, Batch 888, Loss: 0.3864027261734009\n",
      "Epoch 22, Batch 889, Loss: 0.31480374932289124\n",
      "Epoch 22, Batch 890, Loss: 0.2866116762161255\n",
      "Epoch 22, Batch 891, Loss: 0.24567627906799316\n",
      "Epoch 22, Batch 892, Loss: 0.46638184785842896\n",
      "Epoch 22, Batch 893, Loss: 0.35207289457321167\n",
      "Epoch 22, Batch 894, Loss: 0.5175376534461975\n",
      "Epoch 22, Batch 895, Loss: 0.3085245192050934\n",
      "Epoch 22, Batch 896, Loss: 0.2714524567127228\n",
      "Epoch 22, Batch 897, Loss: 0.45020076632499695\n",
      "Epoch 22, Batch 898, Loss: 0.652123749256134\n",
      "Epoch 22, Batch 899, Loss: 0.5277916193008423\n",
      "Epoch 22, Batch 900, Loss: 0.5111507773399353\n",
      "Epoch 22, Batch 901, Loss: 0.35524868965148926\n",
      "Epoch 22, Batch 902, Loss: 0.3444835841655731\n",
      "Epoch 22, Batch 903, Loss: 0.2864111065864563\n",
      "Epoch 22, Batch 904, Loss: 0.3351997137069702\n",
      "Epoch 22, Batch 905, Loss: 0.3615749478340149\n",
      "Epoch 22, Batch 906, Loss: 0.30594804883003235\n",
      "Epoch 22, Batch 907, Loss: 0.333028644323349\n",
      "Epoch 22, Batch 908, Loss: 0.29032087326049805\n",
      "Epoch 22, Batch 909, Loss: 0.3305993974208832\n",
      "Epoch 22, Batch 910, Loss: 0.34807124733924866\n",
      "Epoch 22, Batch 911, Loss: 0.4125998914241791\n",
      "Epoch 22, Batch 912, Loss: 0.303756445646286\n",
      "Epoch 22, Batch 913, Loss: 0.2540085017681122\n",
      "Epoch 22, Batch 914, Loss: 0.6123042106628418\n",
      "Epoch 22, Batch 915, Loss: 0.4247197210788727\n",
      "Epoch 22, Batch 916, Loss: 0.4695666432380676\n",
      "Epoch 22, Batch 917, Loss: 0.47048407793045044\n",
      "Epoch 22, Batch 918, Loss: 0.42525535821914673\n",
      "Epoch 22, Batch 919, Loss: 0.26528018712997437\n",
      "Epoch 22, Batch 920, Loss: 0.37285029888153076\n",
      "Epoch 22, Batch 921, Loss: 0.7323906421661377\n",
      "Epoch 22, Batch 922, Loss: 0.2986734211444855\n",
      "Epoch 22, Batch 923, Loss: 0.6220008730888367\n",
      "Epoch 22, Batch 924, Loss: 0.6103218793869019\n",
      "Epoch 22, Batch 925, Loss: 0.32302865386009216\n",
      "Epoch 22, Batch 926, Loss: 0.4289283752441406\n",
      "Epoch 22, Batch 927, Loss: 0.29319554567337036\n",
      "Epoch 22, Batch 928, Loss: 0.47237294912338257\n",
      "Epoch 22, Batch 929, Loss: 0.3430291712284088\n",
      "Epoch 22, Batch 930, Loss: 0.4324924945831299\n",
      "Epoch 22, Batch 931, Loss: 0.5239391326904297\n",
      "Epoch 22, Batch 932, Loss: 0.5955256819725037\n",
      "Epoch 22, Batch 933, Loss: 0.5482897758483887\n",
      "Epoch 22, Batch 934, Loss: 0.38657012581825256\n",
      "Epoch 22, Batch 935, Loss: 0.32947587966918945\n",
      "Epoch 22, Batch 936, Loss: 0.3577617406845093\n",
      "Epoch 22, Batch 937, Loss: 0.39121025800704956\n",
      "Epoch 22, Batch 938, Loss: 0.6310251355171204\n",
      "Accuracy of train set: 0.8598833333333333\n",
      "Epoch 22, Batch 1, Test Loss: 0.5634274482727051\n",
      "Epoch 22, Batch 2, Test Loss: 0.38031142950057983\n",
      "Epoch 22, Batch 3, Test Loss: 0.3295784890651703\n",
      "Epoch 22, Batch 4, Test Loss: 0.5102038383483887\n",
      "Epoch 22, Batch 5, Test Loss: 0.5226318836212158\n",
      "Epoch 22, Batch 6, Test Loss: 0.5546852350234985\n",
      "Epoch 22, Batch 7, Test Loss: 0.6345762014389038\n",
      "Epoch 22, Batch 8, Test Loss: 0.34537142515182495\n",
      "Epoch 22, Batch 9, Test Loss: 0.4910200834274292\n",
      "Epoch 22, Batch 10, Test Loss: 0.5553279519081116\n",
      "Epoch 22, Batch 11, Test Loss: 0.3254229426383972\n",
      "Epoch 22, Batch 12, Test Loss: 0.3922613859176636\n",
      "Epoch 22, Batch 13, Test Loss: 0.37835466861724854\n",
      "Epoch 22, Batch 14, Test Loss: 0.517173171043396\n",
      "Epoch 22, Batch 15, Test Loss: 0.4239577054977417\n",
      "Epoch 22, Batch 16, Test Loss: 0.3662143349647522\n",
      "Epoch 22, Batch 17, Test Loss: 0.37124183773994446\n",
      "Epoch 22, Batch 18, Test Loss: 0.35845649242401123\n",
      "Epoch 22, Batch 19, Test Loss: 0.5604720711708069\n",
      "Epoch 22, Batch 20, Test Loss: 0.26522207260131836\n",
      "Epoch 22, Batch 21, Test Loss: 0.4433651566505432\n",
      "Epoch 22, Batch 22, Test Loss: 0.3642873764038086\n",
      "Epoch 22, Batch 23, Test Loss: 0.5174210071563721\n",
      "Epoch 22, Batch 24, Test Loss: 0.4153538942337036\n",
      "Epoch 22, Batch 25, Test Loss: 0.34352821111679077\n",
      "Epoch 22, Batch 26, Test Loss: 0.44133877754211426\n",
      "Epoch 22, Batch 27, Test Loss: 0.5722157955169678\n",
      "Epoch 22, Batch 28, Test Loss: 0.524044930934906\n",
      "Epoch 22, Batch 29, Test Loss: 0.36204320192337036\n",
      "Epoch 22, Batch 30, Test Loss: 0.41606405377388\n",
      "Epoch 22, Batch 31, Test Loss: 0.5853424668312073\n",
      "Epoch 22, Batch 32, Test Loss: 0.4366511106491089\n",
      "Epoch 22, Batch 33, Test Loss: 0.40133896470069885\n",
      "Epoch 22, Batch 34, Test Loss: 0.3341940641403198\n",
      "Epoch 22, Batch 35, Test Loss: 0.4711490869522095\n",
      "Epoch 22, Batch 36, Test Loss: 0.27536705136299133\n",
      "Epoch 22, Batch 37, Test Loss: 0.3692503869533539\n",
      "Epoch 22, Batch 38, Test Loss: 0.313166081905365\n",
      "Epoch 22, Batch 39, Test Loss: 0.4940279424190521\n",
      "Epoch 22, Batch 40, Test Loss: 0.41186806559562683\n",
      "Epoch 22, Batch 41, Test Loss: 0.36264050006866455\n",
      "Epoch 22, Batch 42, Test Loss: 0.3043404817581177\n",
      "Epoch 22, Batch 43, Test Loss: 0.3861708641052246\n",
      "Epoch 22, Batch 44, Test Loss: 0.35068556666374207\n",
      "Epoch 22, Batch 45, Test Loss: 0.3427528440952301\n",
      "Epoch 22, Batch 46, Test Loss: 0.493537038564682\n",
      "Epoch 22, Batch 47, Test Loss: 0.4690649211406708\n",
      "Epoch 22, Batch 48, Test Loss: 0.27025529742240906\n",
      "Epoch 22, Batch 49, Test Loss: 0.3146766424179077\n",
      "Epoch 22, Batch 50, Test Loss: 0.3549879193305969\n",
      "Epoch 22, Batch 51, Test Loss: 0.29250818490982056\n",
      "Epoch 22, Batch 52, Test Loss: 0.3928254246711731\n",
      "Epoch 22, Batch 53, Test Loss: 0.5950603485107422\n",
      "Epoch 22, Batch 54, Test Loss: 0.3197404146194458\n",
      "Epoch 22, Batch 55, Test Loss: 0.39274388551712036\n",
      "Epoch 22, Batch 56, Test Loss: 0.2864733636379242\n",
      "Epoch 22, Batch 57, Test Loss: 0.3713851869106293\n",
      "Epoch 22, Batch 58, Test Loss: 0.47171562910079956\n",
      "Epoch 22, Batch 59, Test Loss: 0.4273121654987335\n",
      "Epoch 22, Batch 60, Test Loss: 0.42510688304901123\n",
      "Epoch 22, Batch 61, Test Loss: 0.2580360472202301\n",
      "Epoch 22, Batch 62, Test Loss: 0.4297470152378082\n",
      "Epoch 22, Batch 63, Test Loss: 0.4245368540287018\n",
      "Epoch 22, Batch 64, Test Loss: 0.3222801685333252\n",
      "Epoch 22, Batch 65, Test Loss: 0.336911678314209\n",
      "Epoch 22, Batch 66, Test Loss: 0.5485156774520874\n",
      "Epoch 22, Batch 67, Test Loss: 0.5083578824996948\n",
      "Epoch 22, Batch 68, Test Loss: 0.34570765495300293\n",
      "Epoch 22, Batch 69, Test Loss: 0.341740220785141\n",
      "Epoch 22, Batch 70, Test Loss: 0.4960668683052063\n",
      "Epoch 22, Batch 71, Test Loss: 0.3334478437900543\n",
      "Epoch 22, Batch 72, Test Loss: 0.29270830750465393\n",
      "Epoch 22, Batch 73, Test Loss: 0.5700260400772095\n",
      "Epoch 22, Batch 74, Test Loss: 0.309272438287735\n",
      "Epoch 22, Batch 75, Test Loss: 0.30267542600631714\n",
      "Epoch 22, Batch 76, Test Loss: 0.5493442416191101\n",
      "Epoch 22, Batch 77, Test Loss: 0.2380828559398651\n",
      "Epoch 22, Batch 78, Test Loss: 0.4460494816303253\n",
      "Epoch 22, Batch 79, Test Loss: 0.27247804403305054\n",
      "Epoch 22, Batch 80, Test Loss: 0.37070006132125854\n",
      "Epoch 22, Batch 81, Test Loss: 0.2894858121871948\n",
      "Epoch 22, Batch 82, Test Loss: 0.30638688802719116\n",
      "Epoch 22, Batch 83, Test Loss: 0.28096267580986023\n",
      "Epoch 22, Batch 84, Test Loss: 0.3741532564163208\n",
      "Epoch 22, Batch 85, Test Loss: 0.3618498146533966\n",
      "Epoch 22, Batch 86, Test Loss: 0.5325514674186707\n",
      "Epoch 22, Batch 87, Test Loss: 0.42000678181648254\n",
      "Epoch 22, Batch 88, Test Loss: 0.45439016819000244\n",
      "Epoch 22, Batch 89, Test Loss: 0.29381951689720154\n",
      "Epoch 22, Batch 90, Test Loss: 0.45404699444770813\n",
      "Epoch 22, Batch 91, Test Loss: 0.5225952863693237\n",
      "Epoch 22, Batch 92, Test Loss: 0.4120497703552246\n",
      "Epoch 22, Batch 93, Test Loss: 0.4434027671813965\n",
      "Epoch 22, Batch 94, Test Loss: 0.23666426539421082\n",
      "Epoch 22, Batch 95, Test Loss: 0.5045740008354187\n",
      "Epoch 22, Batch 96, Test Loss: 0.5224659442901611\n",
      "Epoch 22, Batch 97, Test Loss: 0.4588964581489563\n",
      "Epoch 22, Batch 98, Test Loss: 0.3992921710014343\n",
      "Epoch 22, Batch 99, Test Loss: 0.34409043192863464\n",
      "Epoch 22, Batch 100, Test Loss: 0.3988138437271118\n",
      "Epoch 22, Batch 101, Test Loss: 0.5054126977920532\n",
      "Epoch 22, Batch 102, Test Loss: 0.3396133780479431\n",
      "Epoch 22, Batch 103, Test Loss: 0.3057093918323517\n",
      "Epoch 22, Batch 104, Test Loss: 0.2667681574821472\n",
      "Epoch 22, Batch 105, Test Loss: 0.5956905484199524\n",
      "Epoch 22, Batch 106, Test Loss: 0.3624635934829712\n",
      "Epoch 22, Batch 107, Test Loss: 0.5423473119735718\n",
      "Epoch 22, Batch 108, Test Loss: 0.48243165016174316\n",
      "Epoch 22, Batch 109, Test Loss: 0.41567957401275635\n",
      "Epoch 22, Batch 110, Test Loss: 0.35269448161125183\n",
      "Epoch 22, Batch 111, Test Loss: 0.23374570906162262\n",
      "Epoch 22, Batch 112, Test Loss: 0.35392412543296814\n",
      "Epoch 22, Batch 113, Test Loss: 0.42862245440483093\n",
      "Epoch 22, Batch 114, Test Loss: 0.3941265642642975\n",
      "Epoch 22, Batch 115, Test Loss: 0.45557254552841187\n",
      "Epoch 22, Batch 116, Test Loss: 0.35610687732696533\n",
      "Epoch 22, Batch 117, Test Loss: 0.2688777446746826\n",
      "Epoch 22, Batch 118, Test Loss: 0.5733011960983276\n",
      "Epoch 22, Batch 119, Test Loss: 0.24212761223316193\n",
      "Epoch 22, Batch 120, Test Loss: 0.4302264451980591\n",
      "Epoch 22, Batch 121, Test Loss: 0.40671518445014954\n",
      "Epoch 22, Batch 122, Test Loss: 0.36016300320625305\n",
      "Epoch 22, Batch 123, Test Loss: 0.3013773560523987\n",
      "Epoch 22, Batch 124, Test Loss: 0.2423824965953827\n",
      "Epoch 22, Batch 125, Test Loss: 0.29843682050704956\n",
      "Epoch 22, Batch 126, Test Loss: 0.40769463777542114\n",
      "Epoch 22, Batch 127, Test Loss: 0.45127949118614197\n",
      "Epoch 22, Batch 128, Test Loss: 0.3654130697250366\n",
      "Epoch 22, Batch 129, Test Loss: 0.9325637817382812\n",
      "Epoch 22, Batch 130, Test Loss: 0.415252149105072\n",
      "Epoch 22, Batch 131, Test Loss: 0.41962191462516785\n",
      "Epoch 22, Batch 132, Test Loss: 0.4112822413444519\n",
      "Epoch 22, Batch 133, Test Loss: 0.38011690974235535\n",
      "Epoch 22, Batch 134, Test Loss: 0.2909453809261322\n",
      "Epoch 22, Batch 135, Test Loss: 0.3398938775062561\n",
      "Epoch 22, Batch 136, Test Loss: 0.28734663128852844\n",
      "Epoch 22, Batch 137, Test Loss: 0.47279444336891174\n",
      "Epoch 22, Batch 138, Test Loss: 0.5360950231552124\n",
      "Epoch 22, Batch 139, Test Loss: 0.5446013808250427\n",
      "Epoch 22, Batch 140, Test Loss: 0.4284668564796448\n",
      "Epoch 22, Batch 141, Test Loss: 0.2975524365901947\n",
      "Epoch 22, Batch 142, Test Loss: 0.5037307739257812\n",
      "Epoch 22, Batch 143, Test Loss: 0.42616337537765503\n",
      "Epoch 22, Batch 144, Test Loss: 0.3203969895839691\n",
      "Epoch 22, Batch 145, Test Loss: 0.47862330079078674\n",
      "Epoch 22, Batch 146, Test Loss: 0.6786641478538513\n",
      "Epoch 22, Batch 147, Test Loss: 0.38244500756263733\n",
      "Epoch 22, Batch 148, Test Loss: 0.5284516215324402\n",
      "Epoch 22, Batch 149, Test Loss: 0.2865908741950989\n",
      "Epoch 22, Batch 150, Test Loss: 0.22905245423316956\n",
      "Epoch 22, Batch 151, Test Loss: 0.31880828738212585\n",
      "Epoch 22, Batch 152, Test Loss: 0.29287242889404297\n",
      "Epoch 22, Batch 153, Test Loss: 0.2989285886287689\n",
      "Epoch 22, Batch 154, Test Loss: 0.26952818036079407\n",
      "Epoch 22, Batch 155, Test Loss: 0.38548529148101807\n",
      "Epoch 22, Batch 156, Test Loss: 0.2457869052886963\n",
      "Epoch 22, Batch 157, Test Loss: 0.41452232003211975\n",
      "Epoch 22, Batch 158, Test Loss: 0.4153035879135132\n",
      "Epoch 22, Batch 159, Test Loss: 0.4486311972141266\n",
      "Epoch 22, Batch 160, Test Loss: 0.46770793199539185\n",
      "Epoch 22, Batch 161, Test Loss: 0.28913164138793945\n",
      "Epoch 22, Batch 162, Test Loss: 0.30681726336479187\n",
      "Epoch 22, Batch 163, Test Loss: 0.43374666571617126\n",
      "Epoch 22, Batch 164, Test Loss: 0.18963728845119476\n",
      "Epoch 22, Batch 165, Test Loss: 0.5474273562431335\n",
      "Epoch 22, Batch 166, Test Loss: 0.5170996785163879\n",
      "Epoch 22, Batch 167, Test Loss: 0.5610374212265015\n",
      "Epoch 22, Batch 168, Test Loss: 0.2782399356365204\n",
      "Epoch 22, Batch 169, Test Loss: 0.6113101243972778\n",
      "Epoch 22, Batch 170, Test Loss: 0.329256147146225\n",
      "Epoch 22, Batch 171, Test Loss: 0.3482549786567688\n",
      "Epoch 22, Batch 172, Test Loss: 0.38920438289642334\n",
      "Epoch 22, Batch 173, Test Loss: 0.41665929555892944\n",
      "Epoch 22, Batch 174, Test Loss: 0.3506842255592346\n",
      "Epoch 22, Batch 175, Test Loss: 0.42219850420951843\n",
      "Epoch 22, Batch 176, Test Loss: 0.29869475960731506\n",
      "Epoch 22, Batch 177, Test Loss: 0.2738575041294098\n",
      "Epoch 22, Batch 178, Test Loss: 0.20563408732414246\n",
      "Epoch 22, Batch 179, Test Loss: 0.36081573367118835\n",
      "Epoch 22, Batch 180, Test Loss: 0.4860355257987976\n",
      "Epoch 22, Batch 181, Test Loss: 0.40680599212646484\n",
      "Epoch 22, Batch 182, Test Loss: 0.38037699460983276\n",
      "Epoch 22, Batch 183, Test Loss: 0.5407748818397522\n",
      "Epoch 22, Batch 184, Test Loss: 0.4038357734680176\n",
      "Epoch 22, Batch 185, Test Loss: 0.3799194395542145\n",
      "Epoch 22, Batch 186, Test Loss: 0.45270201563835144\n",
      "Epoch 22, Batch 187, Test Loss: 0.2771635055541992\n",
      "Epoch 22, Batch 188, Test Loss: 0.3779750168323517\n",
      "Epoch 22, Batch 189, Test Loss: 0.33948561549186707\n",
      "Epoch 22, Batch 190, Test Loss: 0.4579536020755768\n",
      "Epoch 22, Batch 191, Test Loss: 0.33641111850738525\n",
      "Epoch 22, Batch 192, Test Loss: 0.43603426218032837\n",
      "Epoch 22, Batch 193, Test Loss: 0.32425567507743835\n",
      "Epoch 22, Batch 194, Test Loss: 0.35656002163887024\n",
      "Epoch 22, Batch 195, Test Loss: 0.6433890461921692\n",
      "Epoch 22, Batch 196, Test Loss: 0.2794823944568634\n",
      "Epoch 22, Batch 197, Test Loss: 0.30070731043815613\n",
      "Epoch 22, Batch 198, Test Loss: 0.4661107361316681\n",
      "Epoch 22, Batch 199, Test Loss: 0.4538293480873108\n",
      "Epoch 22, Batch 200, Test Loss: 0.3288704454898834\n",
      "Epoch 22, Batch 201, Test Loss: 0.4862436354160309\n",
      "Epoch 22, Batch 202, Test Loss: 0.5618566274642944\n",
      "Epoch 22, Batch 203, Test Loss: 0.5020726919174194\n",
      "Epoch 22, Batch 204, Test Loss: 0.2734938859939575\n",
      "Epoch 22, Batch 205, Test Loss: 0.3712746202945709\n",
      "Epoch 22, Batch 206, Test Loss: 0.5524824857711792\n",
      "Epoch 22, Batch 207, Test Loss: 0.5567338466644287\n",
      "Epoch 22, Batch 208, Test Loss: 0.4590506851673126\n",
      "Epoch 22, Batch 209, Test Loss: 0.30318760871887207\n",
      "Epoch 22, Batch 210, Test Loss: 0.5277105569839478\n",
      "Epoch 22, Batch 211, Test Loss: 0.23821471631526947\n",
      "Epoch 22, Batch 212, Test Loss: 0.3530392646789551\n",
      "Epoch 22, Batch 213, Test Loss: 0.4700009822845459\n",
      "Epoch 22, Batch 214, Test Loss: 0.3409231901168823\n",
      "Epoch 22, Batch 215, Test Loss: 0.5148615837097168\n",
      "Epoch 22, Batch 216, Test Loss: 0.42143675684928894\n",
      "Epoch 22, Batch 217, Test Loss: 0.35191506147384644\n",
      "Epoch 22, Batch 218, Test Loss: 0.41480475664138794\n",
      "Epoch 22, Batch 219, Test Loss: 0.3139417767524719\n",
      "Epoch 22, Batch 220, Test Loss: 0.39900678396224976\n",
      "Epoch 22, Batch 221, Test Loss: 0.47759154438972473\n",
      "Epoch 22, Batch 222, Test Loss: 0.42565596103668213\n",
      "Epoch 22, Batch 223, Test Loss: 0.3514706492424011\n",
      "Epoch 22, Batch 224, Test Loss: 0.3625081181526184\n",
      "Epoch 22, Batch 225, Test Loss: 0.33106350898742676\n",
      "Epoch 22, Batch 226, Test Loss: 0.23570473492145538\n",
      "Epoch 22, Batch 227, Test Loss: 0.28695061802864075\n",
      "Epoch 22, Batch 228, Test Loss: 0.3316965699195862\n",
      "Epoch 22, Batch 229, Test Loss: 0.2736664414405823\n",
      "Epoch 22, Batch 230, Test Loss: 0.33801132440567017\n",
      "Epoch 22, Batch 231, Test Loss: 0.39910998940467834\n",
      "Epoch 22, Batch 232, Test Loss: 0.5432908535003662\n",
      "Epoch 22, Batch 233, Test Loss: 0.3635067939758301\n",
      "Epoch 22, Batch 234, Test Loss: 0.4079400599002838\n",
      "Epoch 22, Batch 235, Test Loss: 0.3490902781486511\n",
      "Epoch 22, Batch 236, Test Loss: 0.4024549126625061\n",
      "Epoch 22, Batch 237, Test Loss: 0.306230753660202\n",
      "Epoch 22, Batch 238, Test Loss: 0.4198352098464966\n",
      "Epoch 22, Batch 239, Test Loss: 0.45641663670539856\n",
      "Epoch 22, Batch 240, Test Loss: 0.48470765352249146\n",
      "Epoch 22, Batch 241, Test Loss: 0.3793592154979706\n",
      "Epoch 22, Batch 242, Test Loss: 0.415884792804718\n",
      "Epoch 22, Batch 243, Test Loss: 0.39891165494918823\n",
      "Epoch 22, Batch 244, Test Loss: 0.35574543476104736\n",
      "Epoch 22, Batch 245, Test Loss: 0.5389693379402161\n",
      "Epoch 22, Batch 246, Test Loss: 0.44817525148391724\n",
      "Epoch 22, Batch 247, Test Loss: 0.382671058177948\n",
      "Epoch 22, Batch 248, Test Loss: 0.4609960913658142\n",
      "Epoch 22, Batch 249, Test Loss: 0.2563636898994446\n",
      "Epoch 22, Batch 250, Test Loss: 0.31965240836143494\n",
      "Epoch 22, Batch 251, Test Loss: 0.4364960491657257\n",
      "Epoch 22, Batch 252, Test Loss: 0.5131646990776062\n",
      "Epoch 22, Batch 253, Test Loss: 0.4232383966445923\n",
      "Epoch 22, Batch 254, Test Loss: 0.35244688391685486\n",
      "Epoch 22, Batch 255, Test Loss: 0.4200040102005005\n",
      "Epoch 22, Batch 256, Test Loss: 0.4647981524467468\n",
      "Epoch 22, Batch 257, Test Loss: 0.3313988149166107\n",
      "Epoch 22, Batch 258, Test Loss: 0.4217568039894104\n",
      "Epoch 22, Batch 259, Test Loss: 0.34121161699295044\n",
      "Epoch 22, Batch 260, Test Loss: 0.29001039266586304\n",
      "Epoch 22, Batch 261, Test Loss: 0.3660697340965271\n",
      "Epoch 22, Batch 262, Test Loss: 0.31469568610191345\n",
      "Epoch 22, Batch 263, Test Loss: 0.2464647889137268\n",
      "Epoch 22, Batch 264, Test Loss: 0.5120744109153748\n",
      "Epoch 22, Batch 265, Test Loss: 0.4338741600513458\n",
      "Epoch 22, Batch 266, Test Loss: 0.5104424953460693\n",
      "Epoch 22, Batch 267, Test Loss: 0.30830004811286926\n",
      "Epoch 22, Batch 268, Test Loss: 0.30208951234817505\n",
      "Epoch 22, Batch 269, Test Loss: 0.3633628487586975\n",
      "Epoch 22, Batch 270, Test Loss: 0.29338961839675903\n",
      "Epoch 22, Batch 271, Test Loss: 0.4375443160533905\n",
      "Epoch 22, Batch 272, Test Loss: 0.5120120048522949\n",
      "Epoch 22, Batch 273, Test Loss: 0.23334401845932007\n",
      "Epoch 22, Batch 274, Test Loss: 0.40626397728919983\n",
      "Epoch 22, Batch 275, Test Loss: 0.3900100290775299\n",
      "Epoch 22, Batch 276, Test Loss: 0.5057092905044556\n",
      "Epoch 22, Batch 277, Test Loss: 0.34903568029403687\n",
      "Epoch 22, Batch 278, Test Loss: 0.2526956796646118\n",
      "Epoch 22, Batch 279, Test Loss: 0.4204432964324951\n",
      "Epoch 22, Batch 280, Test Loss: 0.597776472568512\n",
      "Epoch 22, Batch 281, Test Loss: 0.4165675640106201\n",
      "Epoch 22, Batch 282, Test Loss: 0.41031718254089355\n",
      "Epoch 22, Batch 283, Test Loss: 0.38895946741104126\n",
      "Epoch 22, Batch 284, Test Loss: 0.5064112544059753\n",
      "Epoch 22, Batch 285, Test Loss: 0.41263383626937866\n",
      "Epoch 22, Batch 286, Test Loss: 0.38097143173217773\n",
      "Epoch 22, Batch 287, Test Loss: 0.331021785736084\n",
      "Epoch 22, Batch 288, Test Loss: 0.4190126955509186\n",
      "Epoch 22, Batch 289, Test Loss: 0.47171550989151\n",
      "Epoch 22, Batch 290, Test Loss: 0.22038128972053528\n",
      "Epoch 22, Batch 291, Test Loss: 0.29077187180519104\n",
      "Epoch 22, Batch 292, Test Loss: 0.4012260138988495\n",
      "Epoch 22, Batch 293, Test Loss: 0.3061386048793793\n",
      "Epoch 22, Batch 294, Test Loss: 0.420833021402359\n",
      "Epoch 22, Batch 295, Test Loss: 0.5034631490707397\n",
      "Epoch 22, Batch 296, Test Loss: 0.5230233669281006\n",
      "Epoch 22, Batch 297, Test Loss: 0.273135781288147\n",
      "Epoch 22, Batch 298, Test Loss: 0.4934144616127014\n",
      "Epoch 22, Batch 299, Test Loss: 0.43335261940956116\n",
      "Epoch 22, Batch 300, Test Loss: 0.3626302480697632\n",
      "Epoch 22, Batch 301, Test Loss: 0.4051404297351837\n",
      "Epoch 22, Batch 302, Test Loss: 0.4239450693130493\n",
      "Epoch 22, Batch 303, Test Loss: 0.44315826892852783\n",
      "Epoch 22, Batch 304, Test Loss: 0.3239736557006836\n",
      "Epoch 22, Batch 305, Test Loss: 0.5650726556777954\n",
      "Epoch 22, Batch 306, Test Loss: 0.4119313359260559\n",
      "Epoch 22, Batch 307, Test Loss: 0.2821008265018463\n",
      "Epoch 22, Batch 308, Test Loss: 0.46564072370529175\n",
      "Epoch 22, Batch 309, Test Loss: 0.6975322961807251\n",
      "Epoch 22, Batch 310, Test Loss: 0.46284639835357666\n",
      "Epoch 22, Batch 311, Test Loss: 0.369220495223999\n",
      "Epoch 22, Batch 312, Test Loss: 0.3092404305934906\n",
      "Epoch 22, Batch 313, Test Loss: 0.282578706741333\n",
      "Epoch 22, Batch 314, Test Loss: 0.2473936527967453\n",
      "Epoch 22, Batch 315, Test Loss: 0.4838704466819763\n",
      "Epoch 22, Batch 316, Test Loss: 0.6382595896720886\n",
      "Epoch 22, Batch 317, Test Loss: 0.4618661105632782\n",
      "Epoch 22, Batch 318, Test Loss: 0.43355098366737366\n",
      "Epoch 22, Batch 319, Test Loss: 0.3222472667694092\n",
      "Epoch 22, Batch 320, Test Loss: 0.41220349073410034\n",
      "Epoch 22, Batch 321, Test Loss: 0.3941553831100464\n",
      "Epoch 22, Batch 322, Test Loss: 0.24860434234142303\n",
      "Epoch 22, Batch 323, Test Loss: 0.3570375442504883\n",
      "Epoch 22, Batch 324, Test Loss: 0.4312269687652588\n",
      "Epoch 22, Batch 325, Test Loss: 0.4692760705947876\n",
      "Epoch 22, Batch 326, Test Loss: 0.37318944931030273\n",
      "Epoch 22, Batch 327, Test Loss: 0.2774101793766022\n",
      "Epoch 22, Batch 328, Test Loss: 0.3789253234863281\n",
      "Epoch 22, Batch 329, Test Loss: 0.3436317443847656\n",
      "Epoch 22, Batch 330, Test Loss: 0.4224727749824524\n",
      "Epoch 22, Batch 331, Test Loss: 0.42098987102508545\n",
      "Epoch 22, Batch 332, Test Loss: 0.4772239923477173\n",
      "Epoch 22, Batch 333, Test Loss: 0.46746760606765747\n",
      "Epoch 22, Batch 334, Test Loss: 0.4556097686290741\n",
      "Epoch 22, Batch 335, Test Loss: 0.42431992292404175\n",
      "Epoch 22, Batch 336, Test Loss: 0.24466030299663544\n",
      "Epoch 22, Batch 337, Test Loss: 0.40366503596305847\n",
      "Epoch 22, Batch 338, Test Loss: 0.37813156843185425\n",
      "Epoch 22, Batch 339, Test Loss: 0.586016058921814\n",
      "Epoch 22, Batch 340, Test Loss: 0.30835676193237305\n",
      "Epoch 22, Batch 341, Test Loss: 0.28076279163360596\n",
      "Epoch 22, Batch 342, Test Loss: 0.3428344428539276\n",
      "Epoch 22, Batch 343, Test Loss: 0.4924459755420685\n",
      "Epoch 22, Batch 344, Test Loss: 0.22862859070301056\n",
      "Epoch 22, Batch 345, Test Loss: 0.23701663315296173\n",
      "Epoch 22, Batch 346, Test Loss: 0.3138984441757202\n",
      "Epoch 22, Batch 347, Test Loss: 0.40516382455825806\n",
      "Epoch 22, Batch 348, Test Loss: 0.4290362298488617\n",
      "Epoch 22, Batch 349, Test Loss: 0.2935474216938019\n",
      "Epoch 22, Batch 350, Test Loss: 0.4775124192237854\n",
      "Epoch 22, Batch 351, Test Loss: 0.26316002011299133\n",
      "Epoch 22, Batch 352, Test Loss: 0.3101470470428467\n",
      "Epoch 22, Batch 353, Test Loss: 0.4380877912044525\n",
      "Epoch 22, Batch 354, Test Loss: 0.3405974507331848\n",
      "Epoch 22, Batch 355, Test Loss: 0.3210868537425995\n",
      "Epoch 22, Batch 356, Test Loss: 0.2705915570259094\n",
      "Epoch 22, Batch 357, Test Loss: 0.5181742906570435\n",
      "Epoch 22, Batch 358, Test Loss: 0.49709799885749817\n",
      "Epoch 22, Batch 359, Test Loss: 0.25534069538116455\n",
      "Epoch 22, Batch 360, Test Loss: 0.4078110158443451\n",
      "Epoch 22, Batch 361, Test Loss: 0.27478837966918945\n",
      "Epoch 22, Batch 362, Test Loss: 0.3726568818092346\n",
      "Epoch 22, Batch 363, Test Loss: 0.551277756690979\n",
      "Epoch 22, Batch 364, Test Loss: 0.4492504298686981\n",
      "Epoch 22, Batch 365, Test Loss: 0.6109210252761841\n",
      "Epoch 22, Batch 366, Test Loss: 0.39994969964027405\n",
      "Epoch 22, Batch 367, Test Loss: 0.4170188009738922\n",
      "Epoch 22, Batch 368, Test Loss: 0.5707728862762451\n",
      "Epoch 22, Batch 369, Test Loss: 0.2665450870990753\n",
      "Epoch 22, Batch 370, Test Loss: 0.3738518953323364\n",
      "Epoch 22, Batch 371, Test Loss: 0.4757980406284332\n",
      "Epoch 22, Batch 372, Test Loss: 0.39696431159973145\n",
      "Epoch 22, Batch 373, Test Loss: 0.4635292887687683\n",
      "Epoch 22, Batch 374, Test Loss: 0.26222631335258484\n",
      "Epoch 22, Batch 375, Test Loss: 0.5014598369598389\n",
      "Epoch 22, Batch 376, Test Loss: 0.3222370743751526\n",
      "Epoch 22, Batch 377, Test Loss: 0.49843159317970276\n",
      "Epoch 22, Batch 378, Test Loss: 0.4881552457809448\n",
      "Epoch 22, Batch 379, Test Loss: 0.3769821226596832\n",
      "Epoch 22, Batch 380, Test Loss: 0.2709503173828125\n",
      "Epoch 22, Batch 381, Test Loss: 0.33964353799819946\n",
      "Epoch 22, Batch 382, Test Loss: 0.37834393978118896\n",
      "Epoch 22, Batch 383, Test Loss: 0.4030876159667969\n",
      "Epoch 22, Batch 384, Test Loss: 0.42931684851646423\n",
      "Epoch 22, Batch 385, Test Loss: 0.5835666656494141\n",
      "Epoch 22, Batch 386, Test Loss: 0.24269989132881165\n",
      "Epoch 22, Batch 387, Test Loss: 0.36951276659965515\n",
      "Epoch 22, Batch 388, Test Loss: 0.42638418078422546\n",
      "Epoch 22, Batch 389, Test Loss: 0.37604522705078125\n",
      "Epoch 22, Batch 390, Test Loss: 0.4528622329235077\n",
      "Epoch 22, Batch 391, Test Loss: 0.3913974463939667\n",
      "Epoch 22, Batch 392, Test Loss: 0.38138914108276367\n",
      "Epoch 22, Batch 393, Test Loss: 0.554802417755127\n",
      "Epoch 22, Batch 394, Test Loss: 0.2732040584087372\n",
      "Epoch 22, Batch 395, Test Loss: 0.30992311239242554\n",
      "Epoch 22, Batch 396, Test Loss: 0.18312668800354004\n",
      "Epoch 22, Batch 397, Test Loss: 0.5137256979942322\n",
      "Epoch 22, Batch 398, Test Loss: 0.4718266725540161\n",
      "Epoch 22, Batch 399, Test Loss: 0.3667045831680298\n",
      "Epoch 22, Batch 400, Test Loss: 0.4406336843967438\n",
      "Epoch 22, Batch 401, Test Loss: 0.39810672402381897\n",
      "Epoch 22, Batch 402, Test Loss: 0.41857752203941345\n",
      "Epoch 22, Batch 403, Test Loss: 0.38468971848487854\n",
      "Epoch 22, Batch 404, Test Loss: 0.4877704977989197\n",
      "Epoch 22, Batch 405, Test Loss: 0.36788251996040344\n",
      "Epoch 22, Batch 406, Test Loss: 0.4148564338684082\n",
      "Epoch 22, Batch 407, Test Loss: 0.603489875793457\n",
      "Epoch 22, Batch 408, Test Loss: 0.43835413455963135\n",
      "Epoch 22, Batch 409, Test Loss: 0.3793782889842987\n",
      "Epoch 22, Batch 410, Test Loss: 0.2945621609687805\n",
      "Epoch 22, Batch 411, Test Loss: 0.35506024956703186\n",
      "Epoch 22, Batch 412, Test Loss: 0.5705407857894897\n",
      "Epoch 22, Batch 413, Test Loss: 0.29505494236946106\n",
      "Epoch 22, Batch 414, Test Loss: 0.3427616357803345\n",
      "Epoch 22, Batch 415, Test Loss: 0.5607730746269226\n",
      "Epoch 22, Batch 416, Test Loss: 0.6415824890136719\n",
      "Epoch 22, Batch 417, Test Loss: 0.46561944484710693\n",
      "Epoch 22, Batch 418, Test Loss: 0.4243718683719635\n",
      "Epoch 22, Batch 419, Test Loss: 0.3529055118560791\n",
      "Epoch 22, Batch 420, Test Loss: 0.347828209400177\n",
      "Epoch 22, Batch 421, Test Loss: 0.5476694703102112\n",
      "Epoch 22, Batch 422, Test Loss: 0.6887350678443909\n",
      "Epoch 22, Batch 423, Test Loss: 0.4696979522705078\n",
      "Epoch 22, Batch 424, Test Loss: 0.31764769554138184\n",
      "Epoch 22, Batch 425, Test Loss: 0.4369582235813141\n",
      "Epoch 22, Batch 426, Test Loss: 0.4417218267917633\n",
      "Epoch 22, Batch 427, Test Loss: 0.45786070823669434\n",
      "Epoch 22, Batch 428, Test Loss: 0.32073384523391724\n",
      "Epoch 22, Batch 429, Test Loss: 0.4967055320739746\n",
      "Epoch 22, Batch 430, Test Loss: 0.5226790904998779\n",
      "Epoch 22, Batch 431, Test Loss: 0.36127981543540955\n",
      "Epoch 22, Batch 432, Test Loss: 0.3638564646244049\n",
      "Epoch 22, Batch 433, Test Loss: 0.3469701409339905\n",
      "Epoch 22, Batch 434, Test Loss: 0.25785142183303833\n",
      "Epoch 22, Batch 435, Test Loss: 0.5553175806999207\n",
      "Epoch 22, Batch 436, Test Loss: 0.44738107919692993\n",
      "Epoch 22, Batch 437, Test Loss: 0.3992489278316498\n",
      "Epoch 22, Batch 438, Test Loss: 0.42973071336746216\n",
      "Epoch 22, Batch 439, Test Loss: 0.5005506873130798\n",
      "Epoch 22, Batch 440, Test Loss: 0.5698338150978088\n",
      "Epoch 22, Batch 441, Test Loss: 0.3330102264881134\n",
      "Epoch 22, Batch 442, Test Loss: 0.40888524055480957\n",
      "Epoch 22, Batch 443, Test Loss: 0.45123234391212463\n",
      "Epoch 22, Batch 444, Test Loss: 0.5709391236305237\n",
      "Epoch 22, Batch 445, Test Loss: 0.2699223458766937\n",
      "Epoch 22, Batch 446, Test Loss: 0.40062087774276733\n",
      "Epoch 22, Batch 447, Test Loss: 0.41886478662490845\n",
      "Epoch 22, Batch 448, Test Loss: 0.28946641087532043\n",
      "Epoch 22, Batch 449, Test Loss: 0.193532794713974\n",
      "Epoch 22, Batch 450, Test Loss: 0.417402982711792\n",
      "Epoch 22, Batch 451, Test Loss: 0.3285400867462158\n",
      "Epoch 22, Batch 452, Test Loss: 0.5569865107536316\n",
      "Epoch 22, Batch 453, Test Loss: 0.4852351248264313\n",
      "Epoch 22, Batch 454, Test Loss: 0.3404088616371155\n",
      "Epoch 22, Batch 455, Test Loss: 0.32096484303474426\n",
      "Epoch 22, Batch 456, Test Loss: 0.44331032037734985\n",
      "Epoch 22, Batch 457, Test Loss: 0.33886438608169556\n",
      "Epoch 22, Batch 458, Test Loss: 0.3290901482105255\n",
      "Epoch 22, Batch 459, Test Loss: 0.32803791761398315\n",
      "Epoch 22, Batch 460, Test Loss: 0.36627161502838135\n",
      "Epoch 22, Batch 461, Test Loss: 0.3291228413581848\n",
      "Epoch 22, Batch 462, Test Loss: 0.327580064535141\n",
      "Epoch 22, Batch 463, Test Loss: 0.403315931558609\n",
      "Epoch 22, Batch 464, Test Loss: 0.5036028027534485\n",
      "Epoch 22, Batch 465, Test Loss: 0.3053065240383148\n",
      "Epoch 22, Batch 466, Test Loss: 0.3082890510559082\n",
      "Epoch 22, Batch 467, Test Loss: 0.6198360919952393\n",
      "Epoch 22, Batch 468, Test Loss: 0.38812255859375\n",
      "Epoch 22, Batch 469, Test Loss: 0.4226763844490051\n",
      "Epoch 22, Batch 470, Test Loss: 0.4087558090686798\n",
      "Epoch 22, Batch 471, Test Loss: 0.41390031576156616\n",
      "Epoch 22, Batch 472, Test Loss: 0.3943822681903839\n",
      "Epoch 22, Batch 473, Test Loss: 0.34938833117485046\n",
      "Epoch 22, Batch 474, Test Loss: 0.3449450433254242\n",
      "Epoch 22, Batch 475, Test Loss: 0.7353833317756653\n",
      "Epoch 22, Batch 476, Test Loss: 0.38558557629585266\n",
      "Epoch 22, Batch 477, Test Loss: 0.2928884029388428\n",
      "Epoch 22, Batch 478, Test Loss: 0.5805063247680664\n",
      "Epoch 22, Batch 479, Test Loss: 0.3451656699180603\n",
      "Epoch 22, Batch 480, Test Loss: 0.39894139766693115\n",
      "Epoch 22, Batch 481, Test Loss: 0.5153271555900574\n",
      "Epoch 22, Batch 482, Test Loss: 0.35885870456695557\n",
      "Epoch 22, Batch 483, Test Loss: 0.4567267894744873\n",
      "Epoch 22, Batch 484, Test Loss: 0.45815080404281616\n",
      "Epoch 22, Batch 485, Test Loss: 0.6098088026046753\n",
      "Epoch 22, Batch 486, Test Loss: 0.4107835590839386\n",
      "Epoch 22, Batch 487, Test Loss: 0.3481987416744232\n",
      "Epoch 22, Batch 488, Test Loss: 0.33975082635879517\n",
      "Epoch 22, Batch 489, Test Loss: 0.42499056458473206\n",
      "Epoch 22, Batch 490, Test Loss: 0.47745147347450256\n",
      "Epoch 22, Batch 491, Test Loss: 0.18397784233093262\n",
      "Epoch 22, Batch 492, Test Loss: 0.4790298342704773\n",
      "Epoch 22, Batch 493, Test Loss: 0.38581112027168274\n",
      "Epoch 22, Batch 494, Test Loss: 0.578617513179779\n",
      "Epoch 22, Batch 495, Test Loss: 0.26967740058898926\n",
      "Epoch 22, Batch 496, Test Loss: 0.31392931938171387\n",
      "Epoch 22, Batch 497, Test Loss: 0.49471211433410645\n",
      "Epoch 22, Batch 498, Test Loss: 0.2754193842411041\n",
      "Epoch 22, Batch 499, Test Loss: 0.5181148052215576\n",
      "Epoch 22, Batch 500, Test Loss: 0.6085970401763916\n",
      "Epoch 22, Batch 501, Test Loss: 0.3913949429988861\n",
      "Epoch 22, Batch 502, Test Loss: 0.4211178123950958\n",
      "Epoch 22, Batch 503, Test Loss: 0.42770636081695557\n",
      "Epoch 22, Batch 504, Test Loss: 0.4679517149925232\n",
      "Epoch 22, Batch 505, Test Loss: 0.3423856198787689\n",
      "Epoch 22, Batch 506, Test Loss: 0.4810224175453186\n",
      "Epoch 22, Batch 507, Test Loss: 0.3864562511444092\n",
      "Epoch 22, Batch 508, Test Loss: 0.38155651092529297\n",
      "Epoch 22, Batch 509, Test Loss: 0.38175931572914124\n",
      "Epoch 22, Batch 510, Test Loss: 0.3783605694770813\n",
      "Epoch 22, Batch 511, Test Loss: 0.30086714029312134\n",
      "Epoch 22, Batch 512, Test Loss: 0.473965048789978\n",
      "Epoch 22, Batch 513, Test Loss: 0.40615758299827576\n",
      "Epoch 22, Batch 514, Test Loss: 0.41542595624923706\n",
      "Epoch 22, Batch 515, Test Loss: 0.5162235498428345\n",
      "Epoch 22, Batch 516, Test Loss: 0.49620604515075684\n",
      "Epoch 22, Batch 517, Test Loss: 0.7915507555007935\n",
      "Epoch 22, Batch 518, Test Loss: 0.370872437953949\n",
      "Epoch 22, Batch 519, Test Loss: 0.3603168725967407\n",
      "Epoch 22, Batch 520, Test Loss: 0.43512940406799316\n",
      "Epoch 22, Batch 521, Test Loss: 0.32637500762939453\n",
      "Epoch 22, Batch 522, Test Loss: 0.4085579216480255\n",
      "Epoch 22, Batch 523, Test Loss: 0.43537604808807373\n",
      "Epoch 22, Batch 524, Test Loss: 0.3520200550556183\n",
      "Epoch 22, Batch 525, Test Loss: 0.43413877487182617\n",
      "Epoch 22, Batch 526, Test Loss: 0.35667547583580017\n",
      "Epoch 22, Batch 527, Test Loss: 0.32444581389427185\n",
      "Epoch 22, Batch 528, Test Loss: 0.4226934313774109\n",
      "Epoch 22, Batch 529, Test Loss: 0.3491780459880829\n",
      "Epoch 22, Batch 530, Test Loss: 0.5042977333068848\n",
      "Epoch 22, Batch 531, Test Loss: 0.3020055294036865\n",
      "Epoch 22, Batch 532, Test Loss: 0.4541107714176178\n",
      "Epoch 22, Batch 533, Test Loss: 0.488221675157547\n",
      "Epoch 22, Batch 534, Test Loss: 0.202656090259552\n",
      "Epoch 22, Batch 535, Test Loss: 0.3953312933444977\n",
      "Epoch 22, Batch 536, Test Loss: 0.4542100727558136\n",
      "Epoch 22, Batch 537, Test Loss: 0.7485858798027039\n",
      "Epoch 22, Batch 538, Test Loss: 0.3378967046737671\n",
      "Epoch 22, Batch 539, Test Loss: 0.32987651228904724\n",
      "Epoch 22, Batch 540, Test Loss: 0.32857269048690796\n",
      "Epoch 22, Batch 541, Test Loss: 0.46992114186286926\n",
      "Epoch 22, Batch 542, Test Loss: 0.2947831153869629\n",
      "Epoch 22, Batch 543, Test Loss: 0.6285500526428223\n",
      "Epoch 22, Batch 544, Test Loss: 0.3627616763114929\n",
      "Epoch 22, Batch 545, Test Loss: 0.511806309223175\n",
      "Epoch 22, Batch 546, Test Loss: 0.4121476709842682\n",
      "Epoch 22, Batch 547, Test Loss: 0.27590805292129517\n",
      "Epoch 22, Batch 548, Test Loss: 0.35046735405921936\n",
      "Epoch 22, Batch 549, Test Loss: 0.44772225618362427\n",
      "Epoch 22, Batch 550, Test Loss: 0.3915841579437256\n",
      "Epoch 22, Batch 551, Test Loss: 0.4100348949432373\n",
      "Epoch 22, Batch 552, Test Loss: 0.3630233108997345\n",
      "Epoch 22, Batch 553, Test Loss: 0.3957499563694\n",
      "Epoch 22, Batch 554, Test Loss: 0.5155326128005981\n",
      "Epoch 22, Batch 555, Test Loss: 0.29314497113227844\n",
      "Epoch 22, Batch 556, Test Loss: 0.40510275959968567\n",
      "Epoch 22, Batch 557, Test Loss: 0.3847501277923584\n",
      "Epoch 22, Batch 558, Test Loss: 0.34590643644332886\n",
      "Epoch 22, Batch 559, Test Loss: 0.463235080242157\n",
      "Epoch 22, Batch 560, Test Loss: 0.46003788709640503\n",
      "Epoch 22, Batch 561, Test Loss: 0.3634064197540283\n",
      "Epoch 22, Batch 562, Test Loss: 0.5651994347572327\n",
      "Epoch 22, Batch 563, Test Loss: 0.6013666391372681\n",
      "Epoch 22, Batch 564, Test Loss: 0.2670219838619232\n",
      "Epoch 22, Batch 565, Test Loss: 0.4270196557044983\n",
      "Epoch 22, Batch 566, Test Loss: 0.39802831411361694\n",
      "Epoch 22, Batch 567, Test Loss: 0.49829939007759094\n",
      "Epoch 22, Batch 568, Test Loss: 0.3869622051715851\n",
      "Epoch 22, Batch 569, Test Loss: 0.38037049770355225\n",
      "Epoch 22, Batch 570, Test Loss: 0.4087832272052765\n",
      "Epoch 22, Batch 571, Test Loss: 0.3422706723213196\n",
      "Epoch 22, Batch 572, Test Loss: 0.3249492645263672\n",
      "Epoch 22, Batch 573, Test Loss: 0.23541545867919922\n",
      "Epoch 22, Batch 574, Test Loss: 0.49555015563964844\n",
      "Epoch 22, Batch 575, Test Loss: 0.3023022711277008\n",
      "Epoch 22, Batch 576, Test Loss: 0.581089973449707\n",
      "Epoch 22, Batch 577, Test Loss: 0.47368937730789185\n",
      "Epoch 22, Batch 578, Test Loss: 0.42505618929862976\n",
      "Epoch 22, Batch 579, Test Loss: 0.6351429224014282\n",
      "Epoch 22, Batch 580, Test Loss: 0.41664814949035645\n",
      "Epoch 22, Batch 581, Test Loss: 0.25517964363098145\n",
      "Epoch 22, Batch 582, Test Loss: 0.3494843542575836\n",
      "Epoch 22, Batch 583, Test Loss: 0.4903993308544159\n",
      "Epoch 22, Batch 584, Test Loss: 0.3132202625274658\n",
      "Epoch 22, Batch 585, Test Loss: 0.3684956133365631\n",
      "Epoch 22, Batch 586, Test Loss: 0.2512648403644562\n",
      "Epoch 22, Batch 587, Test Loss: 0.5042945742607117\n",
      "Epoch 22, Batch 588, Test Loss: 0.440107524394989\n",
      "Epoch 22, Batch 589, Test Loss: 0.3121677041053772\n",
      "Epoch 22, Batch 590, Test Loss: 0.2680949568748474\n",
      "Epoch 22, Batch 591, Test Loss: 0.45154672861099243\n",
      "Epoch 22, Batch 592, Test Loss: 0.5570824146270752\n",
      "Epoch 22, Batch 593, Test Loss: 0.36986932158470154\n",
      "Epoch 22, Batch 594, Test Loss: 0.20832230150699615\n",
      "Epoch 22, Batch 595, Test Loss: 0.22276580333709717\n",
      "Epoch 22, Batch 596, Test Loss: 0.48791059851646423\n",
      "Epoch 22, Batch 597, Test Loss: 0.4386841952800751\n",
      "Epoch 22, Batch 598, Test Loss: 0.3904518187046051\n",
      "Epoch 22, Batch 599, Test Loss: 0.5107232928276062\n",
      "Epoch 22, Batch 600, Test Loss: 0.35693684220314026\n",
      "Epoch 22, Batch 601, Test Loss: 0.6123737096786499\n",
      "Epoch 22, Batch 602, Test Loss: 0.3837839663028717\n",
      "Epoch 22, Batch 603, Test Loss: 0.2868529260158539\n",
      "Epoch 22, Batch 604, Test Loss: 0.22881680727005005\n",
      "Epoch 22, Batch 605, Test Loss: 0.4016127586364746\n",
      "Epoch 22, Batch 606, Test Loss: 0.34216463565826416\n",
      "Epoch 22, Batch 607, Test Loss: 0.36399972438812256\n",
      "Epoch 22, Batch 608, Test Loss: 0.2425745576620102\n",
      "Epoch 22, Batch 609, Test Loss: 0.24149583280086517\n",
      "Epoch 22, Batch 610, Test Loss: 0.32205700874328613\n",
      "Epoch 22, Batch 611, Test Loss: 0.3475704491138458\n",
      "Epoch 22, Batch 612, Test Loss: 0.4189033806324005\n",
      "Epoch 22, Batch 613, Test Loss: 0.4534628391265869\n",
      "Epoch 22, Batch 614, Test Loss: 0.20120076835155487\n",
      "Epoch 22, Batch 615, Test Loss: 0.24056103825569153\n",
      "Epoch 22, Batch 616, Test Loss: 0.5585125088691711\n",
      "Epoch 22, Batch 617, Test Loss: 0.4547797441482544\n",
      "Epoch 22, Batch 618, Test Loss: 0.32793545722961426\n",
      "Epoch 22, Batch 619, Test Loss: 0.424331933259964\n",
      "Epoch 22, Batch 620, Test Loss: 0.47027406096458435\n",
      "Epoch 22, Batch 621, Test Loss: 0.39871758222579956\n",
      "Epoch 22, Batch 622, Test Loss: 0.3008643388748169\n",
      "Epoch 22, Batch 623, Test Loss: 0.4008062481880188\n",
      "Epoch 22, Batch 624, Test Loss: 0.5420070886611938\n",
      "Epoch 22, Batch 625, Test Loss: 0.17974567413330078\n",
      "Epoch 22, Batch 626, Test Loss: 0.3666689395904541\n",
      "Epoch 22, Batch 627, Test Loss: 0.4188510775566101\n",
      "Epoch 22, Batch 628, Test Loss: 0.3461368680000305\n",
      "Epoch 22, Batch 629, Test Loss: 0.6327594518661499\n",
      "Epoch 22, Batch 630, Test Loss: 0.5558935403823853\n",
      "Epoch 22, Batch 631, Test Loss: 0.4712452292442322\n",
      "Epoch 22, Batch 632, Test Loss: 0.2752418518066406\n",
      "Epoch 22, Batch 633, Test Loss: 0.5375189781188965\n",
      "Epoch 22, Batch 634, Test Loss: 0.3862713873386383\n",
      "Epoch 22, Batch 635, Test Loss: 0.36762261390686035\n",
      "Epoch 22, Batch 636, Test Loss: 0.5186784863471985\n",
      "Epoch 22, Batch 637, Test Loss: 0.34745076298713684\n",
      "Epoch 22, Batch 638, Test Loss: 0.34554117918014526\n",
      "Epoch 22, Batch 639, Test Loss: 0.4324774444103241\n",
      "Epoch 22, Batch 640, Test Loss: 0.3840416967868805\n",
      "Epoch 22, Batch 641, Test Loss: 0.4755081236362457\n",
      "Epoch 22, Batch 642, Test Loss: 0.3126392960548401\n",
      "Epoch 22, Batch 643, Test Loss: 0.3473466634750366\n",
      "Epoch 22, Batch 644, Test Loss: 0.4616384506225586\n",
      "Epoch 22, Batch 645, Test Loss: 0.2735751271247864\n",
      "Epoch 22, Batch 646, Test Loss: 0.43547338247299194\n",
      "Epoch 22, Batch 647, Test Loss: 0.3265419006347656\n",
      "Epoch 22, Batch 648, Test Loss: 0.1883646845817566\n",
      "Epoch 22, Batch 649, Test Loss: 0.38607683777809143\n",
      "Epoch 22, Batch 650, Test Loss: 0.37581968307495117\n",
      "Epoch 22, Batch 651, Test Loss: 0.5208948850631714\n",
      "Epoch 22, Batch 652, Test Loss: 0.37557968497276306\n",
      "Epoch 22, Batch 653, Test Loss: 0.30466195940971375\n",
      "Epoch 22, Batch 654, Test Loss: 0.29179322719573975\n",
      "Epoch 22, Batch 655, Test Loss: 0.5205034017562866\n",
      "Epoch 22, Batch 656, Test Loss: 0.20485933125019073\n",
      "Epoch 22, Batch 657, Test Loss: 0.44373923540115356\n",
      "Epoch 22, Batch 658, Test Loss: 0.43867048621177673\n",
      "Epoch 22, Batch 659, Test Loss: 0.24829113483428955\n",
      "Epoch 22, Batch 660, Test Loss: 0.3629341125488281\n",
      "Epoch 22, Batch 661, Test Loss: 0.445896714925766\n",
      "Epoch 22, Batch 662, Test Loss: 0.3978397250175476\n",
      "Epoch 22, Batch 663, Test Loss: 0.43793871998786926\n",
      "Epoch 22, Batch 664, Test Loss: 0.3318621814250946\n",
      "Epoch 22, Batch 665, Test Loss: 0.3069908618927002\n",
      "Epoch 22, Batch 666, Test Loss: 0.495056688785553\n",
      "Epoch 22, Batch 667, Test Loss: 0.35619521141052246\n",
      "Epoch 22, Batch 668, Test Loss: 0.3215346932411194\n",
      "Epoch 22, Batch 669, Test Loss: 0.6017684936523438\n",
      "Epoch 22, Batch 670, Test Loss: 0.1779753714799881\n",
      "Epoch 22, Batch 671, Test Loss: 0.3602183759212494\n",
      "Epoch 22, Batch 672, Test Loss: 0.4605982303619385\n",
      "Epoch 22, Batch 673, Test Loss: 0.2802339494228363\n",
      "Epoch 22, Batch 674, Test Loss: 0.4006395637989044\n",
      "Epoch 22, Batch 675, Test Loss: 0.2367975413799286\n",
      "Epoch 22, Batch 676, Test Loss: 0.5163840651512146\n",
      "Epoch 22, Batch 677, Test Loss: 0.48700833320617676\n",
      "Epoch 22, Batch 678, Test Loss: 0.3520492613315582\n",
      "Epoch 22, Batch 679, Test Loss: 0.6746851205825806\n",
      "Epoch 22, Batch 680, Test Loss: 0.5236973762512207\n",
      "Epoch 22, Batch 681, Test Loss: 0.29735109210014343\n",
      "Epoch 22, Batch 682, Test Loss: 0.41691380739212036\n",
      "Epoch 22, Batch 683, Test Loss: 0.3834310472011566\n",
      "Epoch 22, Batch 684, Test Loss: 0.4887988269329071\n",
      "Epoch 22, Batch 685, Test Loss: 0.3267908990383148\n",
      "Epoch 22, Batch 686, Test Loss: 0.33946096897125244\n",
      "Epoch 22, Batch 687, Test Loss: 0.3041552007198334\n",
      "Epoch 22, Batch 688, Test Loss: 0.44614067673683167\n",
      "Epoch 22, Batch 689, Test Loss: 0.3230040371417999\n",
      "Epoch 22, Batch 690, Test Loss: 0.2929673492908478\n",
      "Epoch 22, Batch 691, Test Loss: 0.4490552842617035\n",
      "Epoch 22, Batch 692, Test Loss: 0.3554018437862396\n",
      "Epoch 22, Batch 693, Test Loss: 0.3714870810508728\n",
      "Epoch 22, Batch 694, Test Loss: 0.36545678973197937\n",
      "Epoch 22, Batch 695, Test Loss: 0.34649598598480225\n",
      "Epoch 22, Batch 696, Test Loss: 0.23016540706157684\n",
      "Epoch 22, Batch 697, Test Loss: 0.34648799896240234\n",
      "Epoch 22, Batch 698, Test Loss: 0.5761877298355103\n",
      "Epoch 22, Batch 699, Test Loss: 0.4330131411552429\n",
      "Epoch 22, Batch 700, Test Loss: 0.44764748215675354\n",
      "Epoch 22, Batch 701, Test Loss: 0.5078223347663879\n",
      "Epoch 22, Batch 702, Test Loss: 0.4397433400154114\n",
      "Epoch 22, Batch 703, Test Loss: 0.3853359818458557\n",
      "Epoch 22, Batch 704, Test Loss: 0.37372398376464844\n",
      "Epoch 22, Batch 705, Test Loss: 0.4099569618701935\n",
      "Epoch 22, Batch 706, Test Loss: 0.31702762842178345\n",
      "Epoch 22, Batch 707, Test Loss: 0.41757121682167053\n",
      "Epoch 22, Batch 708, Test Loss: 0.5693433284759521\n",
      "Epoch 22, Batch 709, Test Loss: 0.34185895323753357\n",
      "Epoch 22, Batch 710, Test Loss: 0.3715783953666687\n",
      "Epoch 22, Batch 711, Test Loss: 0.24162657558918\n",
      "Epoch 22, Batch 712, Test Loss: 0.5033673048019409\n",
      "Epoch 22, Batch 713, Test Loss: 0.4610779285430908\n",
      "Epoch 22, Batch 714, Test Loss: 0.23429439961910248\n",
      "Epoch 22, Batch 715, Test Loss: 0.32691365480422974\n",
      "Epoch 22, Batch 716, Test Loss: 0.35110941529273987\n",
      "Epoch 22, Batch 717, Test Loss: 0.2631370425224304\n",
      "Epoch 22, Batch 718, Test Loss: 0.5149644613265991\n",
      "Epoch 22, Batch 719, Test Loss: 0.3435959815979004\n",
      "Epoch 22, Batch 720, Test Loss: 0.5553838610649109\n",
      "Epoch 22, Batch 721, Test Loss: 0.3284405767917633\n",
      "Epoch 22, Batch 722, Test Loss: 0.2694369852542877\n",
      "Epoch 22, Batch 723, Test Loss: 0.3753946125507355\n",
      "Epoch 22, Batch 724, Test Loss: 0.3303016424179077\n",
      "Epoch 22, Batch 725, Test Loss: 0.49778255820274353\n",
      "Epoch 22, Batch 726, Test Loss: 0.4595089554786682\n",
      "Epoch 22, Batch 727, Test Loss: 0.4024721384048462\n",
      "Epoch 22, Batch 728, Test Loss: 0.37889713048934937\n",
      "Epoch 22, Batch 729, Test Loss: 0.3732646107673645\n",
      "Epoch 22, Batch 730, Test Loss: 0.3287923336029053\n",
      "Epoch 22, Batch 731, Test Loss: 0.3488549590110779\n",
      "Epoch 22, Batch 732, Test Loss: 0.34606561064720154\n",
      "Epoch 22, Batch 733, Test Loss: 0.3646344840526581\n",
      "Epoch 22, Batch 734, Test Loss: 0.5575610995292664\n",
      "Epoch 22, Batch 735, Test Loss: 0.21322664618492126\n",
      "Epoch 22, Batch 736, Test Loss: 0.4759833812713623\n",
      "Epoch 22, Batch 737, Test Loss: 0.3106120824813843\n",
      "Epoch 22, Batch 738, Test Loss: 0.36477917432785034\n",
      "Epoch 22, Batch 739, Test Loss: 0.5379538536071777\n",
      "Epoch 22, Batch 740, Test Loss: 0.39350998401641846\n",
      "Epoch 22, Batch 741, Test Loss: 0.4056614935398102\n",
      "Epoch 22, Batch 742, Test Loss: 0.45749109983444214\n",
      "Epoch 22, Batch 743, Test Loss: 0.41356223821640015\n",
      "Epoch 22, Batch 744, Test Loss: 0.24597647786140442\n",
      "Epoch 22, Batch 745, Test Loss: 0.35824570059776306\n",
      "Epoch 22, Batch 746, Test Loss: 0.3363494873046875\n",
      "Epoch 22, Batch 747, Test Loss: 0.18329569697380066\n",
      "Epoch 22, Batch 748, Test Loss: 0.34253737330436707\n",
      "Epoch 22, Batch 749, Test Loss: 0.5876781940460205\n",
      "Epoch 22, Batch 750, Test Loss: 0.6154285669326782\n",
      "Epoch 22, Batch 751, Test Loss: 0.3193320631980896\n",
      "Epoch 22, Batch 752, Test Loss: 0.24104231595993042\n",
      "Epoch 22, Batch 753, Test Loss: 0.3107401430606842\n",
      "Epoch 22, Batch 754, Test Loss: 0.31737640500068665\n",
      "Epoch 22, Batch 755, Test Loss: 0.43358463048934937\n",
      "Epoch 22, Batch 756, Test Loss: 0.4939405024051666\n",
      "Epoch 22, Batch 757, Test Loss: 0.3256489634513855\n",
      "Epoch 22, Batch 758, Test Loss: 0.3341495394706726\n",
      "Epoch 22, Batch 759, Test Loss: 0.37885403633117676\n",
      "Epoch 22, Batch 760, Test Loss: 0.44530943036079407\n",
      "Epoch 22, Batch 761, Test Loss: 0.3236187696456909\n",
      "Epoch 22, Batch 762, Test Loss: 0.5217444896697998\n",
      "Epoch 22, Batch 763, Test Loss: 0.48016026616096497\n",
      "Epoch 22, Batch 764, Test Loss: 0.4337230324745178\n",
      "Epoch 22, Batch 765, Test Loss: 0.545886218547821\n",
      "Epoch 22, Batch 766, Test Loss: 0.4034479260444641\n",
      "Epoch 22, Batch 767, Test Loss: 0.3266914486885071\n",
      "Epoch 22, Batch 768, Test Loss: 0.23027585446834564\n",
      "Epoch 22, Batch 769, Test Loss: 0.5248587727546692\n",
      "Epoch 22, Batch 770, Test Loss: 0.4032760560512543\n",
      "Epoch 22, Batch 771, Test Loss: 0.3522391617298126\n",
      "Epoch 22, Batch 772, Test Loss: 0.3007832467556\n",
      "Epoch 22, Batch 773, Test Loss: 0.43494462966918945\n",
      "Epoch 22, Batch 774, Test Loss: 0.41487330198287964\n",
      "Epoch 22, Batch 775, Test Loss: 0.28778600692749023\n",
      "Epoch 22, Batch 776, Test Loss: 0.48361197113990784\n",
      "Epoch 22, Batch 777, Test Loss: 0.4715834856033325\n",
      "Epoch 22, Batch 778, Test Loss: 0.43320563435554504\n",
      "Epoch 22, Batch 779, Test Loss: 0.4946398138999939\n",
      "Epoch 22, Batch 780, Test Loss: 0.28398358821868896\n",
      "Epoch 22, Batch 781, Test Loss: 0.5748090147972107\n",
      "Epoch 22, Batch 782, Test Loss: 0.31276991963386536\n",
      "Epoch 22, Batch 783, Test Loss: 0.35229042172431946\n",
      "Epoch 22, Batch 784, Test Loss: 0.47824907302856445\n",
      "Epoch 22, Batch 785, Test Loss: 0.32045048475265503\n",
      "Epoch 22, Batch 786, Test Loss: 0.400684118270874\n",
      "Epoch 22, Batch 787, Test Loss: 0.46110084652900696\n",
      "Epoch 22, Batch 788, Test Loss: 0.4138663411140442\n",
      "Epoch 22, Batch 789, Test Loss: 0.44769760966300964\n",
      "Epoch 22, Batch 790, Test Loss: 0.3355540335178375\n",
      "Epoch 22, Batch 791, Test Loss: 0.2609702944755554\n",
      "Epoch 22, Batch 792, Test Loss: 0.4758142828941345\n",
      "Epoch 22, Batch 793, Test Loss: 0.3593660593032837\n",
      "Epoch 22, Batch 794, Test Loss: 0.4807777404785156\n",
      "Epoch 22, Batch 795, Test Loss: 0.4517630934715271\n",
      "Epoch 22, Batch 796, Test Loss: 0.1598445624113083\n",
      "Epoch 22, Batch 797, Test Loss: 0.3627798557281494\n",
      "Epoch 22, Batch 798, Test Loss: 0.44174015522003174\n",
      "Epoch 22, Batch 799, Test Loss: 0.31043726205825806\n",
      "Epoch 22, Batch 800, Test Loss: 0.4229499399662018\n",
      "Epoch 22, Batch 801, Test Loss: 0.4262810945510864\n",
      "Epoch 22, Batch 802, Test Loss: 0.33173882961273193\n",
      "Epoch 22, Batch 803, Test Loss: 0.6167274713516235\n",
      "Epoch 22, Batch 804, Test Loss: 0.613810122013092\n",
      "Epoch 22, Batch 805, Test Loss: 0.39313963055610657\n",
      "Epoch 22, Batch 806, Test Loss: 0.38099968433380127\n",
      "Epoch 22, Batch 807, Test Loss: 0.42654842138290405\n",
      "Epoch 22, Batch 808, Test Loss: 0.24650368094444275\n",
      "Epoch 22, Batch 809, Test Loss: 0.5024035573005676\n",
      "Epoch 22, Batch 810, Test Loss: 0.2881561517715454\n",
      "Epoch 22, Batch 811, Test Loss: 0.48568400740623474\n",
      "Epoch 22, Batch 812, Test Loss: 0.43005290627479553\n",
      "Epoch 22, Batch 813, Test Loss: 0.4511706233024597\n",
      "Epoch 22, Batch 814, Test Loss: 0.4297458529472351\n",
      "Epoch 22, Batch 815, Test Loss: 0.47153446078300476\n",
      "Epoch 22, Batch 816, Test Loss: 0.4380766749382019\n",
      "Epoch 22, Batch 817, Test Loss: 0.49228769540786743\n",
      "Epoch 22, Batch 818, Test Loss: 0.3472385108470917\n",
      "Epoch 22, Batch 819, Test Loss: 0.5384153723716736\n",
      "Epoch 22, Batch 820, Test Loss: 0.4281443655490875\n",
      "Epoch 22, Batch 821, Test Loss: 0.2691018581390381\n",
      "Epoch 22, Batch 822, Test Loss: 0.7164716720581055\n",
      "Epoch 22, Batch 823, Test Loss: 0.3356332778930664\n",
      "Epoch 22, Batch 824, Test Loss: 0.3824608325958252\n",
      "Epoch 22, Batch 825, Test Loss: 0.335607647895813\n",
      "Epoch 22, Batch 826, Test Loss: 0.3213248550891876\n",
      "Epoch 22, Batch 827, Test Loss: 0.3673284947872162\n",
      "Epoch 22, Batch 828, Test Loss: 0.3886869251728058\n",
      "Epoch 22, Batch 829, Test Loss: 0.4843650758266449\n",
      "Epoch 22, Batch 830, Test Loss: 0.43527132272720337\n",
      "Epoch 22, Batch 831, Test Loss: 0.35506492853164673\n",
      "Epoch 22, Batch 832, Test Loss: 0.5828521251678467\n",
      "Epoch 22, Batch 833, Test Loss: 0.3298523426055908\n",
      "Epoch 22, Batch 834, Test Loss: 0.344828337430954\n",
      "Epoch 22, Batch 835, Test Loss: 0.3344915509223938\n",
      "Epoch 22, Batch 836, Test Loss: 0.24298186600208282\n",
      "Epoch 22, Batch 837, Test Loss: 0.4803253412246704\n",
      "Epoch 22, Batch 838, Test Loss: 0.36852365732192993\n",
      "Epoch 22, Batch 839, Test Loss: 0.4257611036300659\n",
      "Epoch 22, Batch 840, Test Loss: 0.6089719533920288\n",
      "Epoch 22, Batch 841, Test Loss: 0.3475603461265564\n",
      "Epoch 22, Batch 842, Test Loss: 0.40858402848243713\n",
      "Epoch 22, Batch 843, Test Loss: 0.4060697555541992\n",
      "Epoch 22, Batch 844, Test Loss: 0.3535082936286926\n",
      "Epoch 22, Batch 845, Test Loss: 0.4472784698009491\n",
      "Epoch 22, Batch 846, Test Loss: 0.5379517078399658\n",
      "Epoch 22, Batch 847, Test Loss: 0.32954341173171997\n",
      "Epoch 22, Batch 848, Test Loss: 0.39643827080726624\n",
      "Epoch 22, Batch 849, Test Loss: 0.25067099928855896\n",
      "Epoch 22, Batch 850, Test Loss: 0.23829114437103271\n",
      "Epoch 22, Batch 851, Test Loss: 0.2762203514575958\n",
      "Epoch 22, Batch 852, Test Loss: 0.40921974182128906\n",
      "Epoch 22, Batch 853, Test Loss: 0.44351646304130554\n",
      "Epoch 22, Batch 854, Test Loss: 0.3731149137020111\n",
      "Epoch 22, Batch 855, Test Loss: 0.3890327513217926\n",
      "Epoch 22, Batch 856, Test Loss: 0.33162349462509155\n",
      "Epoch 22, Batch 857, Test Loss: 0.48110783100128174\n",
      "Epoch 22, Batch 858, Test Loss: 0.3702404797077179\n",
      "Epoch 22, Batch 859, Test Loss: 0.5138756632804871\n",
      "Epoch 22, Batch 860, Test Loss: 0.4466286599636078\n",
      "Epoch 22, Batch 861, Test Loss: 0.37620455026626587\n",
      "Epoch 22, Batch 862, Test Loss: 0.4627456068992615\n",
      "Epoch 22, Batch 863, Test Loss: 0.22206418216228485\n",
      "Epoch 22, Batch 864, Test Loss: 0.24875640869140625\n",
      "Epoch 22, Batch 865, Test Loss: 0.43100830912590027\n",
      "Epoch 22, Batch 866, Test Loss: 0.4142211079597473\n",
      "Epoch 22, Batch 867, Test Loss: 0.45995354652404785\n",
      "Epoch 22, Batch 868, Test Loss: 0.3262687027454376\n",
      "Epoch 22, Batch 869, Test Loss: 0.39047467708587646\n",
      "Epoch 22, Batch 870, Test Loss: 0.182819202542305\n",
      "Epoch 22, Batch 871, Test Loss: 0.28122541308403015\n",
      "Epoch 22, Batch 872, Test Loss: 0.4604552984237671\n",
      "Epoch 22, Batch 873, Test Loss: 0.3883873522281647\n",
      "Epoch 22, Batch 874, Test Loss: 0.3490241765975952\n",
      "Epoch 22, Batch 875, Test Loss: 0.2586379051208496\n",
      "Epoch 22, Batch 876, Test Loss: 0.3904728293418884\n",
      "Epoch 22, Batch 877, Test Loss: 0.4178623855113983\n",
      "Epoch 22, Batch 878, Test Loss: 0.5318700671195984\n",
      "Epoch 22, Batch 879, Test Loss: 0.4754304587841034\n",
      "Epoch 22, Batch 880, Test Loss: 0.3979177474975586\n",
      "Epoch 22, Batch 881, Test Loss: 0.25654473900794983\n",
      "Epoch 22, Batch 882, Test Loss: 0.48321768641471863\n",
      "Epoch 22, Batch 883, Test Loss: 0.5536767244338989\n",
      "Epoch 22, Batch 884, Test Loss: 0.3663844168186188\n",
      "Epoch 22, Batch 885, Test Loss: 0.33008477091789246\n",
      "Epoch 22, Batch 886, Test Loss: 0.4212605655193329\n",
      "Epoch 22, Batch 887, Test Loss: 0.5454950332641602\n",
      "Epoch 22, Batch 888, Test Loss: 0.35776767134666443\n",
      "Epoch 22, Batch 889, Test Loss: 0.5712487697601318\n",
      "Epoch 22, Batch 890, Test Loss: 0.4449280798435211\n",
      "Epoch 22, Batch 891, Test Loss: 0.3388112783432007\n",
      "Epoch 22, Batch 892, Test Loss: 0.5105850696563721\n",
      "Epoch 22, Batch 893, Test Loss: 0.24393078684806824\n",
      "Epoch 22, Batch 894, Test Loss: 0.4015384018421173\n",
      "Epoch 22, Batch 895, Test Loss: 0.3757818937301636\n",
      "Epoch 22, Batch 896, Test Loss: 0.31638091802597046\n",
      "Epoch 22, Batch 897, Test Loss: 0.3035532236099243\n",
      "Epoch 22, Batch 898, Test Loss: 0.5029651522636414\n",
      "Epoch 22, Batch 899, Test Loss: 0.31095632910728455\n",
      "Epoch 22, Batch 900, Test Loss: 0.33287203311920166\n",
      "Epoch 22, Batch 901, Test Loss: 0.3638807535171509\n",
      "Epoch 22, Batch 902, Test Loss: 0.4819594621658325\n",
      "Epoch 22, Batch 903, Test Loss: 0.3245335817337036\n",
      "Epoch 22, Batch 904, Test Loss: 0.42882388830184937\n",
      "Epoch 22, Batch 905, Test Loss: 0.3407009541988373\n",
      "Epoch 22, Batch 906, Test Loss: 0.35452601313591003\n",
      "Epoch 22, Batch 907, Test Loss: 0.45337116718292236\n",
      "Epoch 22, Batch 908, Test Loss: 0.3228273093700409\n",
      "Epoch 22, Batch 909, Test Loss: 0.39987725019454956\n",
      "Epoch 22, Batch 910, Test Loss: 0.2555837631225586\n",
      "Epoch 22, Batch 911, Test Loss: 0.3506627380847931\n",
      "Epoch 22, Batch 912, Test Loss: 0.38595327734947205\n",
      "Epoch 22, Batch 913, Test Loss: 0.3470611870288849\n",
      "Epoch 22, Batch 914, Test Loss: 0.24526134133338928\n",
      "Epoch 22, Batch 915, Test Loss: 0.423197478055954\n",
      "Epoch 22, Batch 916, Test Loss: 0.42699217796325684\n",
      "Epoch 22, Batch 917, Test Loss: 0.43496501445770264\n",
      "Epoch 22, Batch 918, Test Loss: 0.40680989623069763\n",
      "Epoch 22, Batch 919, Test Loss: 0.4476679563522339\n",
      "Epoch 22, Batch 920, Test Loss: 0.3475894033908844\n",
      "Epoch 22, Batch 921, Test Loss: 0.22850966453552246\n",
      "Epoch 22, Batch 922, Test Loss: 0.5456583499908447\n",
      "Epoch 22, Batch 923, Test Loss: 0.30699679255485535\n",
      "Epoch 22, Batch 924, Test Loss: 0.447063148021698\n",
      "Epoch 22, Batch 925, Test Loss: 0.4658178985118866\n",
      "Epoch 22, Batch 926, Test Loss: 0.24469301104545593\n",
      "Epoch 22, Batch 927, Test Loss: 0.4195250868797302\n",
      "Epoch 22, Batch 928, Test Loss: 0.4205886721611023\n",
      "Epoch 22, Batch 929, Test Loss: 0.4700486958026886\n",
      "Epoch 22, Batch 930, Test Loss: 0.6676670908927917\n",
      "Epoch 22, Batch 931, Test Loss: 0.45008331537246704\n",
      "Epoch 22, Batch 932, Test Loss: 0.2560315728187561\n",
      "Epoch 22, Batch 933, Test Loss: 0.3966240882873535\n",
      "Epoch 22, Batch 934, Test Loss: 0.38667184114456177\n",
      "Epoch 22, Batch 935, Test Loss: 0.3252853453159332\n",
      "Epoch 22, Batch 936, Test Loss: 0.280568927526474\n",
      "Epoch 22, Batch 937, Test Loss: 0.4399074614048004\n",
      "Epoch 22, Batch 938, Test Loss: 0.1335987150669098\n",
      "Accuracy of Test set: 0.8554\n",
      "Epoch 23, Batch 1, Loss: 0.38262438774108887\n",
      "Epoch 23, Batch 2, Loss: 0.4609393775463104\n",
      "Epoch 23, Batch 3, Loss: 0.510861337184906\n",
      "Epoch 23, Batch 4, Loss: 0.5856940746307373\n",
      "Epoch 23, Batch 5, Loss: 0.3791797459125519\n",
      "Epoch 23, Batch 6, Loss: 0.480375736951828\n",
      "Epoch 23, Batch 7, Loss: 0.20936261117458344\n",
      "Epoch 23, Batch 8, Loss: 0.2946416437625885\n",
      "Epoch 23, Batch 9, Loss: 0.3209734559059143\n",
      "Epoch 23, Batch 10, Loss: 0.32751357555389404\n",
      "Epoch 23, Batch 11, Loss: 0.4372730553150177\n",
      "Epoch 23, Batch 12, Loss: 0.3696882128715515\n",
      "Epoch 23, Batch 13, Loss: 0.3040229380130768\n",
      "Epoch 23, Batch 14, Loss: 0.3855725824832916\n",
      "Epoch 23, Batch 15, Loss: 0.3768211007118225\n",
      "Epoch 23, Batch 16, Loss: 0.2522331476211548\n",
      "Epoch 23, Batch 17, Loss: 0.2929253578186035\n",
      "Epoch 23, Batch 18, Loss: 0.3567858934402466\n",
      "Epoch 23, Batch 19, Loss: 0.40665191411972046\n",
      "Epoch 23, Batch 20, Loss: 0.3584844768047333\n",
      "Epoch 23, Batch 21, Loss: 0.2903841733932495\n",
      "Epoch 23, Batch 22, Loss: 0.44829273223876953\n",
      "Epoch 23, Batch 23, Loss: 0.5827552080154419\n",
      "Epoch 23, Batch 24, Loss: 0.4372493028640747\n",
      "Epoch 23, Batch 25, Loss: 0.4773544371128082\n",
      "Epoch 23, Batch 26, Loss: 0.410575807094574\n",
      "Epoch 23, Batch 27, Loss: 0.5766554474830627\n",
      "Epoch 23, Batch 28, Loss: 0.4528343081474304\n",
      "Epoch 23, Batch 29, Loss: 0.44923973083496094\n",
      "Epoch 23, Batch 30, Loss: 0.22758834064006805\n",
      "Epoch 23, Batch 31, Loss: 0.4171493947505951\n",
      "Epoch 23, Batch 32, Loss: 0.5172334909439087\n",
      "Epoch 23, Batch 33, Loss: 0.462819904088974\n",
      "Epoch 23, Batch 34, Loss: 0.2940135598182678\n",
      "Epoch 23, Batch 35, Loss: 0.3860710859298706\n",
      "Epoch 23, Batch 36, Loss: 0.2416038066148758\n",
      "Epoch 23, Batch 37, Loss: 0.334560751914978\n",
      "Epoch 23, Batch 38, Loss: 0.3300406336784363\n",
      "Epoch 23, Batch 39, Loss: 0.5928471088409424\n",
      "Epoch 23, Batch 40, Loss: 0.31571412086486816\n",
      "Epoch 23, Batch 41, Loss: 0.4089087247848511\n",
      "Epoch 23, Batch 42, Loss: 0.4476047158241272\n",
      "Epoch 23, Batch 43, Loss: 0.3055422306060791\n",
      "Epoch 23, Batch 44, Loss: 0.43228772282600403\n",
      "Epoch 23, Batch 45, Loss: 0.2472347915172577\n",
      "Epoch 23, Batch 46, Loss: 0.2815152406692505\n",
      "Epoch 23, Batch 47, Loss: 0.41860121488571167\n",
      "Epoch 23, Batch 48, Loss: 0.3696601390838623\n",
      "Epoch 23, Batch 49, Loss: 0.34727296233177185\n",
      "Epoch 23, Batch 50, Loss: 0.29102012515068054\n",
      "Epoch 23, Batch 51, Loss: 0.3212083578109741\n",
      "Epoch 23, Batch 52, Loss: 0.36219844222068787\n",
      "Epoch 23, Batch 53, Loss: 0.36650756001472473\n",
      "Epoch 23, Batch 54, Loss: 0.5107850432395935\n",
      "Epoch 23, Batch 55, Loss: 0.5163435935974121\n",
      "Epoch 23, Batch 56, Loss: 0.4670480489730835\n",
      "Epoch 23, Batch 57, Loss: 0.32625067234039307\n",
      "Epoch 23, Batch 58, Loss: 0.2286812663078308\n",
      "Epoch 23, Batch 59, Loss: 0.3730897903442383\n",
      "Epoch 23, Batch 60, Loss: 0.45489561557769775\n",
      "Epoch 23, Batch 61, Loss: 0.4317663908004761\n",
      "Epoch 23, Batch 62, Loss: 0.4632427990436554\n",
      "Epoch 23, Batch 63, Loss: 0.31733497977256775\n",
      "Epoch 23, Batch 64, Loss: 0.24206146597862244\n",
      "Epoch 23, Batch 65, Loss: 0.3861885368824005\n",
      "Epoch 23, Batch 66, Loss: 0.385756254196167\n",
      "Epoch 23, Batch 67, Loss: 0.5665307641029358\n",
      "Epoch 23, Batch 68, Loss: 0.38202276825904846\n",
      "Epoch 23, Batch 69, Loss: 0.5209469199180603\n",
      "Epoch 23, Batch 70, Loss: 0.48285627365112305\n",
      "Epoch 23, Batch 71, Loss: 0.35682356357574463\n",
      "Epoch 23, Batch 72, Loss: 0.46399569511413574\n",
      "Epoch 23, Batch 73, Loss: 0.30801504850387573\n",
      "Epoch 23, Batch 74, Loss: 0.4693608582019806\n",
      "Epoch 23, Batch 75, Loss: 0.2832900285720825\n",
      "Epoch 23, Batch 76, Loss: 0.3870590329170227\n",
      "Epoch 23, Batch 77, Loss: 0.38589465618133545\n",
      "Epoch 23, Batch 78, Loss: 0.5326385498046875\n",
      "Epoch 23, Batch 79, Loss: 0.5586186647415161\n",
      "Epoch 23, Batch 80, Loss: 0.6375010013580322\n",
      "Epoch 23, Batch 81, Loss: 0.2660449147224426\n",
      "Epoch 23, Batch 82, Loss: 0.39557817578315735\n",
      "Epoch 23, Batch 83, Loss: 0.24574515223503113\n",
      "Epoch 23, Batch 84, Loss: 0.4563753008842468\n",
      "Epoch 23, Batch 85, Loss: 0.42944756150245667\n",
      "Epoch 23, Batch 86, Loss: 0.462121844291687\n",
      "Epoch 23, Batch 87, Loss: 0.4818936288356781\n",
      "Epoch 23, Batch 88, Loss: 0.3925309181213379\n",
      "Epoch 23, Batch 89, Loss: 0.3850734233856201\n",
      "Epoch 23, Batch 90, Loss: 0.29625964164733887\n",
      "Epoch 23, Batch 91, Loss: 0.32255735993385315\n",
      "Epoch 23, Batch 92, Loss: 0.31591442227363586\n",
      "Epoch 23, Batch 93, Loss: 0.41803866624832153\n",
      "Epoch 23, Batch 94, Loss: 0.3304640054702759\n",
      "Epoch 23, Batch 95, Loss: 0.5368558764457703\n",
      "Epoch 23, Batch 96, Loss: 0.2603476345539093\n",
      "Epoch 23, Batch 97, Loss: 0.3563321828842163\n",
      "Epoch 23, Batch 98, Loss: 0.37811657786369324\n",
      "Epoch 23, Batch 99, Loss: 0.5082862377166748\n",
      "Epoch 23, Batch 100, Loss: 0.7234201431274414\n",
      "Epoch 23, Batch 101, Loss: 0.5575846433639526\n",
      "Epoch 23, Batch 102, Loss: 0.3909634053707123\n",
      "Epoch 23, Batch 103, Loss: 0.38404422998428345\n",
      "Epoch 23, Batch 104, Loss: 0.5175154805183411\n",
      "Epoch 23, Batch 105, Loss: 0.3625897467136383\n",
      "Epoch 23, Batch 106, Loss: 0.3375856280326843\n",
      "Epoch 23, Batch 107, Loss: 0.3866846263408661\n",
      "Epoch 23, Batch 108, Loss: 0.3276910185813904\n",
      "Epoch 23, Batch 109, Loss: 0.2505221664905548\n",
      "Epoch 23, Batch 110, Loss: 0.3783858120441437\n",
      "Epoch 23, Batch 111, Loss: 0.560813307762146\n",
      "Epoch 23, Batch 112, Loss: 0.30565041303634644\n",
      "Epoch 23, Batch 113, Loss: 0.5443904399871826\n",
      "Epoch 23, Batch 114, Loss: 0.4806913137435913\n",
      "Epoch 23, Batch 115, Loss: 0.6189131736755371\n",
      "Epoch 23, Batch 116, Loss: 0.4582807719707489\n",
      "Epoch 23, Batch 117, Loss: 0.3192076086997986\n",
      "Epoch 23, Batch 118, Loss: 0.406429260969162\n",
      "Epoch 23, Batch 119, Loss: 0.7106772661209106\n",
      "Epoch 23, Batch 120, Loss: 0.28061580657958984\n",
      "Epoch 23, Batch 121, Loss: 0.4350113570690155\n",
      "Epoch 23, Batch 122, Loss: 0.6440731883049011\n",
      "Epoch 23, Batch 123, Loss: 0.357856810092926\n",
      "Epoch 23, Batch 124, Loss: 0.43135005235671997\n",
      "Epoch 23, Batch 125, Loss: 0.4870394468307495\n",
      "Epoch 23, Batch 126, Loss: 0.44605958461761475\n",
      "Epoch 23, Batch 127, Loss: 0.680976152420044\n",
      "Epoch 23, Batch 128, Loss: 0.5273277759552002\n",
      "Epoch 23, Batch 129, Loss: 0.5174981951713562\n",
      "Epoch 23, Batch 130, Loss: 0.5015764236450195\n",
      "Epoch 23, Batch 131, Loss: 0.30451688170433044\n",
      "Epoch 23, Batch 132, Loss: 0.35974445939064026\n",
      "Epoch 23, Batch 133, Loss: 0.3820720911026001\n",
      "Epoch 23, Batch 134, Loss: 0.3790784478187561\n",
      "Epoch 23, Batch 135, Loss: 0.38481923937797546\n",
      "Epoch 23, Batch 136, Loss: 0.3174322247505188\n",
      "Epoch 23, Batch 137, Loss: 0.8048157095909119\n",
      "Epoch 23, Batch 138, Loss: 0.49815016984939575\n",
      "Epoch 23, Batch 139, Loss: 0.5254982113838196\n",
      "Epoch 23, Batch 140, Loss: 0.32448625564575195\n",
      "Epoch 23, Batch 141, Loss: 0.35899636149406433\n",
      "Epoch 23, Batch 142, Loss: 0.40775829553604126\n",
      "Epoch 23, Batch 143, Loss: 0.3323543071746826\n",
      "Epoch 23, Batch 144, Loss: 0.5341988801956177\n",
      "Epoch 23, Batch 145, Loss: 0.4531860649585724\n",
      "Epoch 23, Batch 146, Loss: 0.40470629930496216\n",
      "Epoch 23, Batch 147, Loss: 0.4004577398300171\n",
      "Epoch 23, Batch 148, Loss: 0.3274494409561157\n",
      "Epoch 23, Batch 149, Loss: 0.391597718000412\n",
      "Epoch 23, Batch 150, Loss: 0.3449113965034485\n",
      "Epoch 23, Batch 151, Loss: 0.4565979838371277\n",
      "Epoch 23, Batch 152, Loss: 0.4442415237426758\n",
      "Epoch 23, Batch 153, Loss: 0.43761512637138367\n",
      "Epoch 23, Batch 154, Loss: 0.3619072437286377\n",
      "Epoch 23, Batch 155, Loss: 0.3931068181991577\n",
      "Epoch 23, Batch 156, Loss: 0.33935683965682983\n",
      "Epoch 23, Batch 157, Loss: 0.3912044167518616\n",
      "Epoch 23, Batch 158, Loss: 0.4632369577884674\n",
      "Epoch 23, Batch 159, Loss: 0.3411591649055481\n",
      "Epoch 23, Batch 160, Loss: 0.4601708948612213\n",
      "Epoch 23, Batch 161, Loss: 0.49501165747642517\n",
      "Epoch 23, Batch 162, Loss: 0.3645820915699005\n",
      "Epoch 23, Batch 163, Loss: 0.3665125072002411\n",
      "Epoch 23, Batch 164, Loss: 0.48879504203796387\n",
      "Epoch 23, Batch 165, Loss: 0.4393150806427002\n",
      "Epoch 23, Batch 166, Loss: 0.4926058351993561\n",
      "Epoch 23, Batch 167, Loss: 0.4504654109477997\n",
      "Epoch 23, Batch 168, Loss: 0.3945772349834442\n",
      "Epoch 23, Batch 169, Loss: 0.31337201595306396\n",
      "Epoch 23, Batch 170, Loss: 0.6943082809448242\n",
      "Epoch 23, Batch 171, Loss: 0.3365374505519867\n",
      "Epoch 23, Batch 172, Loss: 0.34640631079673767\n",
      "Epoch 23, Batch 173, Loss: 0.3129739761352539\n",
      "Epoch 23, Batch 174, Loss: 0.2857378423213959\n",
      "Epoch 23, Batch 175, Loss: 0.2844216227531433\n",
      "Epoch 23, Batch 176, Loss: 0.45358747243881226\n",
      "Epoch 23, Batch 177, Loss: 0.21745698153972626\n",
      "Epoch 23, Batch 178, Loss: 0.4191528558731079\n",
      "Epoch 23, Batch 179, Loss: 0.31818896532058716\n",
      "Epoch 23, Batch 180, Loss: 0.2716166377067566\n",
      "Epoch 23, Batch 181, Loss: 0.2828565239906311\n",
      "Epoch 23, Batch 182, Loss: 0.4761708974838257\n",
      "Epoch 23, Batch 183, Loss: 0.3747148811817169\n",
      "Epoch 23, Batch 184, Loss: 0.44383639097213745\n",
      "Epoch 23, Batch 185, Loss: 0.3570648729801178\n",
      "Epoch 23, Batch 186, Loss: 0.4411470592021942\n",
      "Epoch 23, Batch 187, Loss: 0.2729785740375519\n",
      "Epoch 23, Batch 188, Loss: 0.42048853635787964\n",
      "Epoch 23, Batch 189, Loss: 0.31628555059432983\n",
      "Epoch 23, Batch 190, Loss: 0.48033422231674194\n",
      "Epoch 23, Batch 191, Loss: 0.5425506234169006\n",
      "Epoch 23, Batch 192, Loss: 0.3200066089630127\n",
      "Epoch 23, Batch 193, Loss: 0.295431524515152\n",
      "Epoch 23, Batch 194, Loss: 0.45639657974243164\n",
      "Epoch 23, Batch 195, Loss: 0.4565137028694153\n",
      "Epoch 23, Batch 196, Loss: 0.4338185489177704\n",
      "Epoch 23, Batch 197, Loss: 0.3085063695907593\n",
      "Epoch 23, Batch 198, Loss: 0.42973804473876953\n",
      "Epoch 23, Batch 199, Loss: 0.2429349720478058\n",
      "Epoch 23, Batch 200, Loss: 0.46937286853790283\n",
      "Epoch 23, Batch 201, Loss: 0.36648374795913696\n",
      "Epoch 23, Batch 202, Loss: 0.5759220123291016\n",
      "Epoch 23, Batch 203, Loss: 0.27146872878074646\n",
      "Epoch 23, Batch 204, Loss: 0.5299693942070007\n",
      "Epoch 23, Batch 205, Loss: 0.2767578661441803\n",
      "Epoch 23, Batch 206, Loss: 0.29223325848579407\n",
      "Epoch 23, Batch 207, Loss: 0.34680676460266113\n",
      "Epoch 23, Batch 208, Loss: 0.3891436457633972\n",
      "Epoch 23, Batch 209, Loss: 0.4454396069049835\n",
      "Epoch 23, Batch 210, Loss: 0.41954678297042847\n",
      "Epoch 23, Batch 211, Loss: 0.3800768256187439\n",
      "Epoch 23, Batch 212, Loss: 0.7872540354728699\n",
      "Epoch 23, Batch 213, Loss: 0.33355724811553955\n",
      "Epoch 23, Batch 214, Loss: 0.41811418533325195\n",
      "Epoch 23, Batch 215, Loss: 0.35878434777259827\n",
      "Epoch 23, Batch 216, Loss: 0.48788803815841675\n",
      "Epoch 23, Batch 217, Loss: 0.46955159306526184\n",
      "Epoch 23, Batch 218, Loss: 0.1912500411272049\n",
      "Epoch 23, Batch 219, Loss: 0.31637656688690186\n",
      "Epoch 23, Batch 220, Loss: 0.37761205434799194\n",
      "Epoch 23, Batch 221, Loss: 0.3568854033946991\n",
      "Epoch 23, Batch 222, Loss: 0.42718541622161865\n",
      "Epoch 23, Batch 223, Loss: 0.39492326974868774\n",
      "Epoch 23, Batch 224, Loss: 0.35854023694992065\n",
      "Epoch 23, Batch 225, Loss: 0.293262779712677\n",
      "Epoch 23, Batch 226, Loss: 0.4356207251548767\n",
      "Epoch 23, Batch 227, Loss: 0.35800594091415405\n",
      "Epoch 23, Batch 228, Loss: 0.3162204623222351\n",
      "Epoch 23, Batch 229, Loss: 0.38864436745643616\n",
      "Epoch 23, Batch 230, Loss: 0.42480358481407166\n",
      "Epoch 23, Batch 231, Loss: 0.19951274991035461\n",
      "Epoch 23, Batch 232, Loss: 0.22881990671157837\n",
      "Epoch 23, Batch 233, Loss: 0.5073193907737732\n",
      "Epoch 23, Batch 234, Loss: 0.38811686635017395\n",
      "Epoch 23, Batch 235, Loss: 0.45354342460632324\n",
      "Epoch 23, Batch 236, Loss: 0.3818277418613434\n",
      "Epoch 23, Batch 237, Loss: 0.3725793659687042\n",
      "Epoch 23, Batch 238, Loss: 0.2709759473800659\n",
      "Epoch 23, Batch 239, Loss: 0.3004368245601654\n",
      "Epoch 23, Batch 240, Loss: 0.4297945201396942\n",
      "Epoch 23, Batch 241, Loss: 0.32674381136894226\n",
      "Epoch 23, Batch 242, Loss: 0.3996999263763428\n",
      "Epoch 23, Batch 243, Loss: 0.36040911078453064\n",
      "Epoch 23, Batch 244, Loss: 0.47761452198028564\n",
      "Epoch 23, Batch 245, Loss: 0.23023562133312225\n",
      "Epoch 23, Batch 246, Loss: 0.4101415276527405\n",
      "Epoch 23, Batch 247, Loss: 0.38794243335723877\n",
      "Epoch 23, Batch 248, Loss: 0.2672857940196991\n",
      "Epoch 23, Batch 249, Loss: 0.419116348028183\n",
      "Epoch 23, Batch 250, Loss: 0.3173118233680725\n",
      "Epoch 23, Batch 251, Loss: 0.4210747480392456\n",
      "Epoch 23, Batch 252, Loss: 0.246418297290802\n",
      "Epoch 23, Batch 253, Loss: 0.38561952114105225\n",
      "Epoch 23, Batch 254, Loss: 0.5499802827835083\n",
      "Epoch 23, Batch 255, Loss: 0.3397459089756012\n",
      "Epoch 23, Batch 256, Loss: 0.4450649619102478\n",
      "Epoch 23, Batch 257, Loss: 0.4786493182182312\n",
      "Epoch 23, Batch 258, Loss: 0.32645854353904724\n",
      "Epoch 23, Batch 259, Loss: 0.3346305191516876\n",
      "Epoch 23, Batch 260, Loss: 0.5146933197975159\n",
      "Epoch 23, Batch 261, Loss: 0.44877487421035767\n",
      "Epoch 23, Batch 262, Loss: 0.375362753868103\n",
      "Epoch 23, Batch 263, Loss: 0.30644798278808594\n",
      "Epoch 23, Batch 264, Loss: 0.29110297560691833\n",
      "Epoch 23, Batch 265, Loss: 0.4477216601371765\n",
      "Epoch 23, Batch 266, Loss: 0.3846820592880249\n",
      "Epoch 23, Batch 267, Loss: 0.30134961009025574\n",
      "Epoch 23, Batch 268, Loss: 0.3771691918373108\n",
      "Epoch 23, Batch 269, Loss: 0.350382924079895\n",
      "Epoch 23, Batch 270, Loss: 0.3459876477718353\n",
      "Epoch 23, Batch 271, Loss: 0.394012451171875\n",
      "Epoch 23, Batch 272, Loss: 0.3151993751525879\n",
      "Epoch 23, Batch 273, Loss: 0.4070218801498413\n",
      "Epoch 23, Batch 274, Loss: 0.28839901089668274\n",
      "Epoch 23, Batch 275, Loss: 0.34750211238861084\n",
      "Epoch 23, Batch 276, Loss: 0.33863797783851624\n",
      "Epoch 23, Batch 277, Loss: 0.2784348726272583\n",
      "Epoch 23, Batch 278, Loss: 0.4356166124343872\n",
      "Epoch 23, Batch 279, Loss: 0.46562910079956055\n",
      "Epoch 23, Batch 280, Loss: 0.29822149872779846\n",
      "Epoch 23, Batch 281, Loss: 0.44243746995925903\n",
      "Epoch 23, Batch 282, Loss: 0.434182345867157\n",
      "Epoch 23, Batch 283, Loss: 0.3005635738372803\n",
      "Epoch 23, Batch 284, Loss: 0.24498037993907928\n",
      "Epoch 23, Batch 285, Loss: 0.3373652994632721\n",
      "Epoch 23, Batch 286, Loss: 0.691353440284729\n",
      "Epoch 23, Batch 287, Loss: 0.4538203179836273\n",
      "Epoch 23, Batch 288, Loss: 0.3810040056705475\n",
      "Epoch 23, Batch 289, Loss: 0.321723073720932\n",
      "Epoch 23, Batch 290, Loss: 0.5166794061660767\n",
      "Epoch 23, Batch 291, Loss: 0.39088284969329834\n",
      "Epoch 23, Batch 292, Loss: 0.2849588394165039\n",
      "Epoch 23, Batch 293, Loss: 0.2806483209133148\n",
      "Epoch 23, Batch 294, Loss: 0.5523781180381775\n",
      "Epoch 23, Batch 295, Loss: 0.2899877727031708\n",
      "Epoch 23, Batch 296, Loss: 0.4093046188354492\n",
      "Epoch 23, Batch 297, Loss: 0.45700371265411377\n",
      "Epoch 23, Batch 298, Loss: 0.40745434165000916\n",
      "Epoch 23, Batch 299, Loss: 0.2554541230201721\n",
      "Epoch 23, Batch 300, Loss: 0.30354437232017517\n",
      "Epoch 23, Batch 301, Loss: 0.2728656530380249\n",
      "Epoch 23, Batch 302, Loss: 0.2950149178504944\n",
      "Epoch 23, Batch 303, Loss: 0.44099313020706177\n",
      "Epoch 23, Batch 304, Loss: 0.3058415949344635\n",
      "Epoch 23, Batch 305, Loss: 0.3084719181060791\n",
      "Epoch 23, Batch 306, Loss: 0.2791535556316376\n",
      "Epoch 23, Batch 307, Loss: 0.3302839398384094\n",
      "Epoch 23, Batch 308, Loss: 0.3140477240085602\n",
      "Epoch 23, Batch 309, Loss: 0.3774966299533844\n",
      "Epoch 23, Batch 310, Loss: 0.5690342783927917\n",
      "Epoch 23, Batch 311, Loss: 0.342907577753067\n",
      "Epoch 23, Batch 312, Loss: 0.2406637966632843\n",
      "Epoch 23, Batch 313, Loss: 0.39373284578323364\n",
      "Epoch 23, Batch 314, Loss: 0.3173580765724182\n",
      "Epoch 23, Batch 315, Loss: 0.43883782625198364\n",
      "Epoch 23, Batch 316, Loss: 0.2823346257209778\n",
      "Epoch 23, Batch 317, Loss: 0.3015761077404022\n",
      "Epoch 23, Batch 318, Loss: 0.2788996696472168\n",
      "Epoch 23, Batch 319, Loss: 0.41588452458381653\n",
      "Epoch 23, Batch 320, Loss: 0.32031965255737305\n",
      "Epoch 23, Batch 321, Loss: 0.23698049783706665\n",
      "Epoch 23, Batch 322, Loss: 0.216569185256958\n",
      "Epoch 23, Batch 323, Loss: 0.40342608094215393\n",
      "Epoch 23, Batch 324, Loss: 0.4609115719795227\n",
      "Epoch 23, Batch 325, Loss: 0.3733549118041992\n",
      "Epoch 23, Batch 326, Loss: 0.43153369426727295\n",
      "Epoch 23, Batch 327, Loss: 0.3165642321109772\n",
      "Epoch 23, Batch 328, Loss: 0.5500702857971191\n",
      "Epoch 23, Batch 329, Loss: 0.3329440951347351\n",
      "Epoch 23, Batch 330, Loss: 0.5841867923736572\n",
      "Epoch 23, Batch 331, Loss: 0.2501005530357361\n",
      "Epoch 23, Batch 332, Loss: 0.43350663781166077\n",
      "Epoch 23, Batch 333, Loss: 0.3460405468940735\n",
      "Epoch 23, Batch 334, Loss: 0.31866255402565\n",
      "Epoch 23, Batch 335, Loss: 0.2948913872241974\n",
      "Epoch 23, Batch 336, Loss: 0.2602579593658447\n",
      "Epoch 23, Batch 337, Loss: 0.35708343982696533\n",
      "Epoch 23, Batch 338, Loss: 0.31723856925964355\n",
      "Epoch 23, Batch 339, Loss: 0.35341978073120117\n",
      "Epoch 23, Batch 340, Loss: 0.37121865153312683\n",
      "Epoch 23, Batch 341, Loss: 0.39229828119277954\n",
      "Epoch 23, Batch 342, Loss: 0.40432509779930115\n",
      "Epoch 23, Batch 343, Loss: 0.528037428855896\n",
      "Epoch 23, Batch 344, Loss: 0.23987974226474762\n",
      "Epoch 23, Batch 345, Loss: 0.43275514245033264\n",
      "Epoch 23, Batch 346, Loss: 0.5529031157493591\n",
      "Epoch 23, Batch 347, Loss: 0.4918077290058136\n",
      "Epoch 23, Batch 348, Loss: 0.36569860577583313\n",
      "Epoch 23, Batch 349, Loss: 0.3345259726047516\n",
      "Epoch 23, Batch 350, Loss: 0.4859631061553955\n",
      "Epoch 23, Batch 351, Loss: 0.29645320773124695\n",
      "Epoch 23, Batch 352, Loss: 0.2872162461280823\n",
      "Epoch 23, Batch 353, Loss: 0.5241596698760986\n",
      "Epoch 23, Batch 354, Loss: 0.5386794805526733\n",
      "Epoch 23, Batch 355, Loss: 0.2601125240325928\n",
      "Epoch 23, Batch 356, Loss: 0.37698015570640564\n",
      "Epoch 23, Batch 357, Loss: 0.3177458941936493\n",
      "Epoch 23, Batch 358, Loss: 0.4067416191101074\n",
      "Epoch 23, Batch 359, Loss: 0.41590046882629395\n",
      "Epoch 23, Batch 360, Loss: 0.3361339271068573\n",
      "Epoch 23, Batch 361, Loss: 0.3496347963809967\n",
      "Epoch 23, Batch 362, Loss: 0.46574997901916504\n",
      "Epoch 23, Batch 363, Loss: 0.39658233523368835\n",
      "Epoch 23, Batch 364, Loss: 0.40900617837905884\n",
      "Epoch 23, Batch 365, Loss: 0.3152027130126953\n",
      "Epoch 23, Batch 366, Loss: 0.3226037621498108\n",
      "Epoch 23, Batch 367, Loss: 0.4729267358779907\n",
      "Epoch 23, Batch 368, Loss: 0.7543949484825134\n",
      "Epoch 23, Batch 369, Loss: 0.36219730973243713\n",
      "Epoch 23, Batch 370, Loss: 0.4639957845211029\n",
      "Epoch 23, Batch 371, Loss: 0.42587170004844666\n",
      "Epoch 23, Batch 372, Loss: 0.29924482107162476\n",
      "Epoch 23, Batch 373, Loss: 0.37093931436538696\n",
      "Epoch 23, Batch 374, Loss: 0.49643614888191223\n",
      "Epoch 23, Batch 375, Loss: 0.1995832473039627\n",
      "Epoch 23, Batch 376, Loss: 0.31259796023368835\n",
      "Epoch 23, Batch 377, Loss: 0.2850637435913086\n",
      "Epoch 23, Batch 378, Loss: 0.3235848546028137\n",
      "Epoch 23, Batch 379, Loss: 0.5270098447799683\n",
      "Epoch 23, Batch 380, Loss: 0.37865081429481506\n",
      "Epoch 23, Batch 381, Loss: 0.453728049993515\n",
      "Epoch 23, Batch 382, Loss: 0.392859548330307\n",
      "Epoch 23, Batch 383, Loss: 0.544110119342804\n",
      "Epoch 23, Batch 384, Loss: 0.7889251708984375\n",
      "Epoch 23, Batch 385, Loss: 0.44277411699295044\n",
      "Epoch 23, Batch 386, Loss: 0.35949012637138367\n",
      "Epoch 23, Batch 387, Loss: 0.613732635974884\n",
      "Epoch 23, Batch 388, Loss: 0.4467998147010803\n",
      "Epoch 23, Batch 389, Loss: 0.4056723117828369\n",
      "Epoch 23, Batch 390, Loss: 0.3148328959941864\n",
      "Epoch 23, Batch 391, Loss: 0.6012315154075623\n",
      "Epoch 23, Batch 392, Loss: 0.28328609466552734\n",
      "Epoch 23, Batch 393, Loss: 0.599097728729248\n",
      "Epoch 23, Batch 394, Loss: 0.3238261342048645\n",
      "Epoch 23, Batch 395, Loss: 0.42062023282051086\n",
      "Epoch 23, Batch 396, Loss: 0.37148717045783997\n",
      "Epoch 23, Batch 397, Loss: 0.1862781047821045\n",
      "Epoch 23, Batch 398, Loss: 0.31115832924842834\n",
      "Epoch 23, Batch 399, Loss: 0.2588963210582733\n",
      "Epoch 23, Batch 400, Loss: 0.49387118220329285\n",
      "Epoch 23, Batch 401, Loss: 0.2529323995113373\n",
      "Epoch 23, Batch 402, Loss: 0.5669136643409729\n",
      "Epoch 23, Batch 403, Loss: 0.45142245292663574\n",
      "Epoch 23, Batch 404, Loss: 0.4165281355381012\n",
      "Epoch 23, Batch 405, Loss: 0.4623098373413086\n",
      "Epoch 23, Batch 406, Loss: 0.34329918026924133\n",
      "Epoch 23, Batch 407, Loss: 0.43841737508773804\n",
      "Epoch 23, Batch 408, Loss: 0.3782033920288086\n",
      "Epoch 23, Batch 409, Loss: 0.3350064754486084\n",
      "Epoch 23, Batch 410, Loss: 0.3896064758300781\n",
      "Epoch 23, Batch 411, Loss: 0.37369510531425476\n",
      "Epoch 23, Batch 412, Loss: 0.4722251296043396\n",
      "Epoch 23, Batch 413, Loss: 0.39941298961639404\n",
      "Epoch 23, Batch 414, Loss: 0.22300149500370026\n",
      "Epoch 23, Batch 415, Loss: 0.5162798166275024\n",
      "Epoch 23, Batch 416, Loss: 0.3467809855937958\n",
      "Epoch 23, Batch 417, Loss: 0.35877227783203125\n",
      "Epoch 23, Batch 418, Loss: 0.3642040491104126\n",
      "Epoch 23, Batch 419, Loss: 0.565834641456604\n",
      "Epoch 23, Batch 420, Loss: 0.2104238122701645\n",
      "Epoch 23, Batch 421, Loss: 0.5192671418190002\n",
      "Epoch 23, Batch 422, Loss: 0.6052830815315247\n",
      "Epoch 23, Batch 423, Loss: 0.4836995303630829\n",
      "Epoch 23, Batch 424, Loss: 0.3916034996509552\n",
      "Epoch 23, Batch 425, Loss: 0.2465987354516983\n",
      "Epoch 23, Batch 426, Loss: 0.3860211670398712\n",
      "Epoch 23, Batch 427, Loss: 0.41439342498779297\n",
      "Epoch 23, Batch 428, Loss: 0.6193604469299316\n",
      "Epoch 23, Batch 429, Loss: 0.40078356862068176\n",
      "Epoch 23, Batch 430, Loss: 0.4873164892196655\n",
      "Epoch 23, Batch 431, Loss: 0.4330885708332062\n",
      "Epoch 23, Batch 432, Loss: 0.28024905920028687\n",
      "Epoch 23, Batch 433, Loss: 0.4622335731983185\n",
      "Epoch 23, Batch 434, Loss: 0.37168821692466736\n",
      "Epoch 23, Batch 435, Loss: 0.3771454691886902\n",
      "Epoch 23, Batch 436, Loss: 0.2899340093135834\n",
      "Epoch 23, Batch 437, Loss: 0.38806140422821045\n",
      "Epoch 23, Batch 438, Loss: 0.49929535388946533\n",
      "Epoch 23, Batch 439, Loss: 0.5044864416122437\n",
      "Epoch 23, Batch 440, Loss: 0.32000261545181274\n",
      "Epoch 23, Batch 441, Loss: 0.41540393233299255\n",
      "Epoch 23, Batch 442, Loss: 0.29178327322006226\n",
      "Epoch 23, Batch 443, Loss: 0.33747148513793945\n",
      "Epoch 23, Batch 444, Loss: 0.4441065788269043\n",
      "Epoch 23, Batch 445, Loss: 0.39670616388320923\n",
      "Epoch 23, Batch 446, Loss: 0.29787081480026245\n",
      "Epoch 23, Batch 447, Loss: 0.4586360454559326\n",
      "Epoch 23, Batch 448, Loss: 0.4509521722793579\n",
      "Epoch 23, Batch 449, Loss: 0.27173420786857605\n",
      "Epoch 23, Batch 450, Loss: 0.28997644782066345\n",
      "Epoch 23, Batch 451, Loss: 0.25373661518096924\n",
      "Epoch 23, Batch 452, Loss: 0.33960971236228943\n",
      "Epoch 23, Batch 453, Loss: 0.2902224659919739\n",
      "Epoch 23, Batch 454, Loss: 0.45989006757736206\n",
      "Epoch 23, Batch 455, Loss: 0.3229064643383026\n",
      "Epoch 23, Batch 456, Loss: 0.22370287775993347\n",
      "Epoch 23, Batch 457, Loss: 0.3076114058494568\n",
      "Epoch 23, Batch 458, Loss: 0.4265287220478058\n",
      "Epoch 23, Batch 459, Loss: 0.40360957384109497\n",
      "Epoch 23, Batch 460, Loss: 0.3044320344924927\n",
      "Epoch 23, Batch 461, Loss: 0.4878683388233185\n",
      "Epoch 23, Batch 462, Loss: 0.3985404968261719\n",
      "Epoch 23, Batch 463, Loss: 0.41586074233055115\n",
      "Epoch 23, Batch 464, Loss: 0.47985953092575073\n",
      "Epoch 23, Batch 465, Loss: 0.4007454216480255\n",
      "Epoch 23, Batch 466, Loss: 0.40184950828552246\n",
      "Epoch 23, Batch 467, Loss: 0.2843160331249237\n",
      "Epoch 23, Batch 468, Loss: 0.22778859734535217\n",
      "Epoch 23, Batch 469, Loss: 0.46310955286026\n",
      "Epoch 23, Batch 470, Loss: 0.5157028436660767\n",
      "Epoch 23, Batch 471, Loss: 0.24624192714691162\n",
      "Epoch 23, Batch 472, Loss: 0.44156208634376526\n",
      "Epoch 23, Batch 473, Loss: 0.45985859632492065\n",
      "Epoch 23, Batch 474, Loss: 0.28571444749832153\n",
      "Epoch 23, Batch 475, Loss: 0.26986193656921387\n",
      "Epoch 23, Batch 476, Loss: 0.4975862503051758\n",
      "Epoch 23, Batch 477, Loss: 0.5709947943687439\n",
      "Epoch 23, Batch 478, Loss: 0.3142993450164795\n",
      "Epoch 23, Batch 479, Loss: 0.33684104681015015\n",
      "Epoch 23, Batch 480, Loss: 0.43586382269859314\n",
      "Epoch 23, Batch 481, Loss: 0.3541175127029419\n",
      "Epoch 23, Batch 482, Loss: 0.3100178837776184\n",
      "Epoch 23, Batch 483, Loss: 0.37169477343559265\n",
      "Epoch 23, Batch 484, Loss: 0.44969800114631653\n",
      "Epoch 23, Batch 485, Loss: 0.43690550327301025\n",
      "Epoch 23, Batch 486, Loss: 0.4014601409435272\n",
      "Epoch 23, Batch 487, Loss: 0.5077043771743774\n",
      "Epoch 23, Batch 488, Loss: 0.18037699162960052\n",
      "Epoch 23, Batch 489, Loss: 0.24244408309459686\n",
      "Epoch 23, Batch 490, Loss: 0.24623113870620728\n",
      "Epoch 23, Batch 491, Loss: 0.4047029912471771\n",
      "Epoch 23, Batch 492, Loss: 0.4503032863140106\n",
      "Epoch 23, Batch 493, Loss: 0.45545870065689087\n",
      "Epoch 23, Batch 494, Loss: 0.255704790353775\n",
      "Epoch 23, Batch 495, Loss: 0.24183613061904907\n",
      "Epoch 23, Batch 496, Loss: 0.5301160216331482\n",
      "Epoch 23, Batch 497, Loss: 0.3067469000816345\n",
      "Epoch 23, Batch 498, Loss: 0.3654072880744934\n",
      "Epoch 23, Batch 499, Loss: 0.32350853085517883\n",
      "Epoch 23, Batch 500, Loss: 0.38240641355514526\n",
      "Epoch 23, Batch 501, Loss: 0.4105437099933624\n",
      "Epoch 23, Batch 502, Loss: 0.28929224610328674\n",
      "Epoch 23, Batch 503, Loss: 0.3859168291091919\n",
      "Epoch 23, Batch 504, Loss: 0.48713818192481995\n",
      "Epoch 23, Batch 505, Loss: 0.3905647099018097\n",
      "Epoch 23, Batch 506, Loss: 0.38425642251968384\n",
      "Epoch 23, Batch 507, Loss: 0.4651380777359009\n",
      "Epoch 23, Batch 508, Loss: 0.5656156539916992\n",
      "Epoch 23, Batch 509, Loss: 0.34612247347831726\n",
      "Epoch 23, Batch 510, Loss: 0.5366393327713013\n",
      "Epoch 23, Batch 511, Loss: 0.5914608836174011\n",
      "Epoch 23, Batch 512, Loss: 0.22832447290420532\n",
      "Epoch 23, Batch 513, Loss: 0.3127517104148865\n",
      "Epoch 23, Batch 514, Loss: 0.5405117869377136\n",
      "Epoch 23, Batch 515, Loss: 0.3823665976524353\n",
      "Epoch 23, Batch 516, Loss: 0.3870594799518585\n",
      "Epoch 23, Batch 517, Loss: 0.3722885847091675\n",
      "Epoch 23, Batch 518, Loss: 0.4145362675189972\n",
      "Epoch 23, Batch 519, Loss: 0.7401360273361206\n",
      "Epoch 23, Batch 520, Loss: 0.5882231593132019\n",
      "Epoch 23, Batch 521, Loss: 0.37413665652275085\n",
      "Epoch 23, Batch 522, Loss: 0.2516768276691437\n",
      "Epoch 23, Batch 523, Loss: 0.6023502945899963\n",
      "Epoch 23, Batch 524, Loss: 0.3432326316833496\n",
      "Epoch 23, Batch 525, Loss: 0.26622217893600464\n",
      "Epoch 23, Batch 526, Loss: 0.3685646951198578\n",
      "Epoch 23, Batch 527, Loss: 0.42258304357528687\n",
      "Epoch 23, Batch 528, Loss: 0.4120689928531647\n",
      "Epoch 23, Batch 529, Loss: 0.38059890270233154\n",
      "Epoch 23, Batch 530, Loss: 0.31224846839904785\n",
      "Epoch 23, Batch 531, Loss: 0.30473950505256653\n",
      "Epoch 23, Batch 532, Loss: 0.2926434874534607\n",
      "Epoch 23, Batch 533, Loss: 0.4082176089286804\n",
      "Epoch 23, Batch 534, Loss: 0.40366053581237793\n",
      "Epoch 23, Batch 535, Loss: 0.47555866837501526\n",
      "Epoch 23, Batch 536, Loss: 0.2747798562049866\n",
      "Epoch 23, Batch 537, Loss: 0.4312582314014435\n",
      "Epoch 23, Batch 538, Loss: 0.5009791851043701\n",
      "Epoch 23, Batch 539, Loss: 0.4137245714664459\n",
      "Epoch 23, Batch 540, Loss: 0.535683810710907\n",
      "Epoch 23, Batch 541, Loss: 0.355739951133728\n",
      "Epoch 23, Batch 542, Loss: 0.33232951164245605\n",
      "Epoch 23, Batch 543, Loss: 0.2474568784236908\n",
      "Epoch 23, Batch 544, Loss: 0.22988015413284302\n",
      "Epoch 23, Batch 545, Loss: 0.3895178437232971\n",
      "Epoch 23, Batch 546, Loss: 0.304353803396225\n",
      "Epoch 23, Batch 547, Loss: 0.5725916624069214\n",
      "Epoch 23, Batch 548, Loss: 0.3443174362182617\n",
      "Epoch 23, Batch 549, Loss: 0.4469265043735504\n",
      "Epoch 23, Batch 550, Loss: 0.371680349111557\n",
      "Epoch 23, Batch 551, Loss: 0.3345327079296112\n",
      "Epoch 23, Batch 552, Loss: 0.32228586077690125\n",
      "Epoch 23, Batch 553, Loss: 0.3061298131942749\n",
      "Epoch 23, Batch 554, Loss: 0.3437994718551636\n",
      "Epoch 23, Batch 555, Loss: 0.3069779872894287\n",
      "Epoch 23, Batch 556, Loss: 0.5373811721801758\n",
      "Epoch 23, Batch 557, Loss: 0.4157177805900574\n",
      "Epoch 23, Batch 558, Loss: 0.39464616775512695\n",
      "Epoch 23, Batch 559, Loss: 0.3830622136592865\n",
      "Epoch 23, Batch 560, Loss: 0.3446325957775116\n",
      "Epoch 23, Batch 561, Loss: 0.25935888290405273\n",
      "Epoch 23, Batch 562, Loss: 0.3596131503582001\n",
      "Epoch 23, Batch 563, Loss: 0.49731212854385376\n",
      "Epoch 23, Batch 564, Loss: 0.36586087942123413\n",
      "Epoch 23, Batch 565, Loss: 0.3700428605079651\n",
      "Epoch 23, Batch 566, Loss: 0.35406482219696045\n",
      "Epoch 23, Batch 567, Loss: 0.45668184757232666\n",
      "Epoch 23, Batch 568, Loss: 0.38783666491508484\n",
      "Epoch 23, Batch 569, Loss: 0.44393596053123474\n",
      "Epoch 23, Batch 570, Loss: 0.5661774277687073\n",
      "Epoch 23, Batch 571, Loss: 0.4454171061515808\n",
      "Epoch 23, Batch 572, Loss: 0.43569502234458923\n",
      "Epoch 23, Batch 573, Loss: 0.320531964302063\n",
      "Epoch 23, Batch 574, Loss: 0.4934312701225281\n",
      "Epoch 23, Batch 575, Loss: 0.3636423647403717\n",
      "Epoch 23, Batch 576, Loss: 0.3740192949771881\n",
      "Epoch 23, Batch 577, Loss: 0.2809757888317108\n",
      "Epoch 23, Batch 578, Loss: 0.32439470291137695\n",
      "Epoch 23, Batch 579, Loss: 0.26079049706459045\n",
      "Epoch 23, Batch 580, Loss: 0.3779272735118866\n",
      "Epoch 23, Batch 581, Loss: 0.4256298840045929\n",
      "Epoch 23, Batch 582, Loss: 0.25528037548065186\n",
      "Epoch 23, Batch 583, Loss: 0.5547962188720703\n",
      "Epoch 23, Batch 584, Loss: 0.5023321509361267\n",
      "Epoch 23, Batch 585, Loss: 0.40523621439933777\n",
      "Epoch 23, Batch 586, Loss: 0.6384120583534241\n",
      "Epoch 23, Batch 587, Loss: 0.5042232871055603\n",
      "Epoch 23, Batch 588, Loss: 0.4010530710220337\n",
      "Epoch 23, Batch 589, Loss: 0.3450070023536682\n",
      "Epoch 23, Batch 590, Loss: 0.3057776391506195\n",
      "Epoch 23, Batch 591, Loss: 0.3036242723464966\n",
      "Epoch 23, Batch 592, Loss: 0.3778899013996124\n",
      "Epoch 23, Batch 593, Loss: 0.35727524757385254\n",
      "Epoch 23, Batch 594, Loss: 0.4774029850959778\n",
      "Epoch 23, Batch 595, Loss: 0.2670113742351532\n",
      "Epoch 23, Batch 596, Loss: 0.2934126853942871\n",
      "Epoch 23, Batch 597, Loss: 0.2907918393611908\n",
      "Epoch 23, Batch 598, Loss: 0.5252692699432373\n",
      "Epoch 23, Batch 599, Loss: 0.3965865671634674\n",
      "Epoch 23, Batch 600, Loss: 0.3001363277435303\n",
      "Epoch 23, Batch 601, Loss: 0.25006282329559326\n",
      "Epoch 23, Batch 602, Loss: 0.510951042175293\n",
      "Epoch 23, Batch 603, Loss: 0.29586875438690186\n",
      "Epoch 23, Batch 604, Loss: 0.27093756198883057\n",
      "Epoch 23, Batch 605, Loss: 0.2970704436302185\n",
      "Epoch 23, Batch 606, Loss: 0.6057154536247253\n",
      "Epoch 23, Batch 607, Loss: 0.45455098152160645\n",
      "Epoch 23, Batch 608, Loss: 0.3437519371509552\n",
      "Epoch 23, Batch 609, Loss: 0.4148520231246948\n",
      "Epoch 23, Batch 610, Loss: 0.43672484159469604\n",
      "Epoch 23, Batch 611, Loss: 0.2447894960641861\n",
      "Epoch 23, Batch 612, Loss: 0.7649044394493103\n",
      "Epoch 23, Batch 613, Loss: 0.3013165593147278\n",
      "Epoch 23, Batch 614, Loss: 0.437715083360672\n",
      "Epoch 23, Batch 615, Loss: 0.4064675569534302\n",
      "Epoch 23, Batch 616, Loss: 0.4168946146965027\n",
      "Epoch 23, Batch 617, Loss: 0.4156690537929535\n",
      "Epoch 23, Batch 618, Loss: 0.4161463677883148\n",
      "Epoch 23, Batch 619, Loss: 0.24829745292663574\n",
      "Epoch 23, Batch 620, Loss: 0.28930148482322693\n",
      "Epoch 23, Batch 621, Loss: 0.403690904378891\n",
      "Epoch 23, Batch 622, Loss: 0.43093952536582947\n",
      "Epoch 23, Batch 623, Loss: 0.3825649917125702\n",
      "Epoch 23, Batch 624, Loss: 0.4094429314136505\n",
      "Epoch 23, Batch 625, Loss: 0.5045471787452698\n",
      "Epoch 23, Batch 626, Loss: 0.42776188254356384\n",
      "Epoch 23, Batch 627, Loss: 0.40715929865837097\n",
      "Epoch 23, Batch 628, Loss: 0.18762101233005524\n",
      "Epoch 23, Batch 629, Loss: 0.2709563970565796\n",
      "Epoch 23, Batch 630, Loss: 0.44529789686203003\n",
      "Epoch 23, Batch 631, Loss: 0.4392232596874237\n",
      "Epoch 23, Batch 632, Loss: 0.4742407202720642\n",
      "Epoch 23, Batch 633, Loss: 0.3864609897136688\n",
      "Epoch 23, Batch 634, Loss: 0.46153295040130615\n",
      "Epoch 23, Batch 635, Loss: 0.38217175006866455\n",
      "Epoch 23, Batch 636, Loss: 0.4705023765563965\n",
      "Epoch 23, Batch 637, Loss: 0.19152173399925232\n",
      "Epoch 23, Batch 638, Loss: 0.3705570101737976\n",
      "Epoch 23, Batch 639, Loss: 0.2856779396533966\n",
      "Epoch 23, Batch 640, Loss: 0.24709086120128632\n",
      "Epoch 23, Batch 641, Loss: 0.3317068815231323\n",
      "Epoch 23, Batch 642, Loss: 0.1686067283153534\n",
      "Epoch 23, Batch 643, Loss: 0.32471218705177307\n",
      "Epoch 23, Batch 644, Loss: 0.5445600748062134\n",
      "Epoch 23, Batch 645, Loss: 0.4623214602470398\n",
      "Epoch 23, Batch 646, Loss: 0.2791815400123596\n",
      "Epoch 23, Batch 647, Loss: 0.4822489619255066\n",
      "Epoch 23, Batch 648, Loss: 0.556336522102356\n",
      "Epoch 23, Batch 649, Loss: 0.3968769907951355\n",
      "Epoch 23, Batch 650, Loss: 0.5592278242111206\n",
      "Epoch 23, Batch 651, Loss: 0.4394880533218384\n",
      "Epoch 23, Batch 652, Loss: 0.4600085914134979\n",
      "Epoch 23, Batch 653, Loss: 0.39518478512763977\n",
      "Epoch 23, Batch 654, Loss: 0.4303256869316101\n",
      "Epoch 23, Batch 655, Loss: 0.2532484829425812\n",
      "Epoch 23, Batch 656, Loss: 0.39410966634750366\n",
      "Epoch 23, Batch 657, Loss: 0.4469297528266907\n",
      "Epoch 23, Batch 658, Loss: 0.3001231849193573\n",
      "Epoch 23, Batch 659, Loss: 0.27149006724357605\n",
      "Epoch 23, Batch 660, Loss: 0.38546910881996155\n",
      "Epoch 23, Batch 661, Loss: 0.512047290802002\n",
      "Epoch 23, Batch 662, Loss: 0.4565943777561188\n",
      "Epoch 23, Batch 663, Loss: 0.3405223786830902\n",
      "Epoch 23, Batch 664, Loss: 0.4786607325077057\n",
      "Epoch 23, Batch 665, Loss: 0.31618645787239075\n",
      "Epoch 23, Batch 666, Loss: 0.3264874517917633\n",
      "Epoch 23, Batch 667, Loss: 0.36158323287963867\n",
      "Epoch 23, Batch 668, Loss: 0.4214317500591278\n",
      "Epoch 23, Batch 669, Loss: 0.45389437675476074\n",
      "Epoch 23, Batch 670, Loss: 0.338449627161026\n",
      "Epoch 23, Batch 671, Loss: 0.4440774619579315\n",
      "Epoch 23, Batch 672, Loss: 0.6038342714309692\n",
      "Epoch 23, Batch 673, Loss: 0.23452937602996826\n",
      "Epoch 23, Batch 674, Loss: 0.4305298328399658\n",
      "Epoch 23, Batch 675, Loss: 0.44326964020729065\n",
      "Epoch 23, Batch 676, Loss: 0.2260289490222931\n",
      "Epoch 23, Batch 677, Loss: 0.3223937451839447\n",
      "Epoch 23, Batch 678, Loss: 0.2874217629432678\n",
      "Epoch 23, Batch 679, Loss: 0.4197697639465332\n",
      "Epoch 23, Batch 680, Loss: 0.3941292464733124\n",
      "Epoch 23, Batch 681, Loss: 0.46443483233451843\n",
      "Epoch 23, Batch 682, Loss: 0.40857329964637756\n",
      "Epoch 23, Batch 683, Loss: 0.30655714869499207\n",
      "Epoch 23, Batch 684, Loss: 0.4667087495326996\n",
      "Epoch 23, Batch 685, Loss: 0.4142448902130127\n",
      "Epoch 23, Batch 686, Loss: 0.5502246618270874\n",
      "Epoch 23, Batch 687, Loss: 0.2686265707015991\n",
      "Epoch 23, Batch 688, Loss: 0.26608553528785706\n",
      "Epoch 23, Batch 689, Loss: 0.40449053049087524\n",
      "Epoch 23, Batch 690, Loss: 0.28978583216667175\n",
      "Epoch 23, Batch 691, Loss: 0.2330121546983719\n",
      "Epoch 23, Batch 692, Loss: 0.23810666799545288\n",
      "Epoch 23, Batch 693, Loss: 0.48707664012908936\n",
      "Epoch 23, Batch 694, Loss: 0.3962555229663849\n",
      "Epoch 23, Batch 695, Loss: 0.5342409014701843\n",
      "Epoch 23, Batch 696, Loss: 0.3368918299674988\n",
      "Epoch 23, Batch 697, Loss: 0.39643311500549316\n",
      "Epoch 23, Batch 698, Loss: 0.48663416504859924\n",
      "Epoch 23, Batch 699, Loss: 0.33328568935394287\n",
      "Epoch 23, Batch 700, Loss: 0.4459388256072998\n",
      "Epoch 23, Batch 701, Loss: 0.2024613320827484\n",
      "Epoch 23, Batch 702, Loss: 0.3496852517127991\n",
      "Epoch 23, Batch 703, Loss: 0.3781231939792633\n",
      "Epoch 23, Batch 704, Loss: 0.2792205810546875\n",
      "Epoch 23, Batch 705, Loss: 0.41614842414855957\n",
      "Epoch 23, Batch 706, Loss: 0.3426288962364197\n",
      "Epoch 23, Batch 707, Loss: 0.2417786568403244\n",
      "Epoch 23, Batch 708, Loss: 0.2759736180305481\n",
      "Epoch 23, Batch 709, Loss: 0.35728582739830017\n",
      "Epoch 23, Batch 710, Loss: 0.3156399130821228\n",
      "Epoch 23, Batch 711, Loss: 0.45406368374824524\n",
      "Epoch 23, Batch 712, Loss: 0.3823170065879822\n",
      "Epoch 23, Batch 713, Loss: 0.520850419998169\n",
      "Epoch 23, Batch 714, Loss: 0.4409559965133667\n",
      "Epoch 23, Batch 715, Loss: 0.32112419605255127\n",
      "Epoch 23, Batch 716, Loss: 0.263200044631958\n",
      "Epoch 23, Batch 717, Loss: 0.3536057472229004\n",
      "Epoch 23, Batch 718, Loss: 0.33399859070777893\n",
      "Epoch 23, Batch 719, Loss: 0.41651079058647156\n",
      "Epoch 23, Batch 720, Loss: 0.19072602689266205\n",
      "Epoch 23, Batch 721, Loss: 0.3331097662448883\n",
      "Epoch 23, Batch 722, Loss: 0.37752842903137207\n",
      "Epoch 23, Batch 723, Loss: 0.46496009826660156\n",
      "Epoch 23, Batch 724, Loss: 0.5953487753868103\n",
      "Epoch 23, Batch 725, Loss: 0.35201841592788696\n",
      "Epoch 23, Batch 726, Loss: 0.6469696164131165\n",
      "Epoch 23, Batch 727, Loss: 0.5747442841529846\n",
      "Epoch 23, Batch 728, Loss: 0.43198248744010925\n",
      "Epoch 23, Batch 729, Loss: 0.38651490211486816\n",
      "Epoch 23, Batch 730, Loss: 0.3149004876613617\n",
      "Epoch 23, Batch 731, Loss: 0.3215481638908386\n",
      "Epoch 23, Batch 732, Loss: 0.34334608912467957\n",
      "Epoch 23, Batch 733, Loss: 0.5921210050582886\n",
      "Epoch 23, Batch 734, Loss: 0.3572120666503906\n",
      "Epoch 23, Batch 735, Loss: 0.3681684732437134\n",
      "Epoch 23, Batch 736, Loss: 0.33289527893066406\n",
      "Epoch 23, Batch 737, Loss: 0.20987224578857422\n",
      "Epoch 23, Batch 738, Loss: 0.34920868277549744\n",
      "Epoch 23, Batch 739, Loss: 0.23271781206130981\n",
      "Epoch 23, Batch 740, Loss: 0.3632132411003113\n",
      "Epoch 23, Batch 741, Loss: 0.27291131019592285\n",
      "Epoch 23, Batch 742, Loss: 0.23730789124965668\n",
      "Epoch 23, Batch 743, Loss: 0.5111668705940247\n",
      "Epoch 23, Batch 744, Loss: 0.4396284818649292\n",
      "Epoch 23, Batch 745, Loss: 0.42474767565727234\n",
      "Epoch 23, Batch 746, Loss: 0.4283270835876465\n",
      "Epoch 23, Batch 747, Loss: 0.3589334189891815\n",
      "Epoch 23, Batch 748, Loss: 0.35568028688430786\n",
      "Epoch 23, Batch 749, Loss: 0.32724136114120483\n",
      "Epoch 23, Batch 750, Loss: 0.466229647397995\n",
      "Epoch 23, Batch 751, Loss: 0.25386476516723633\n",
      "Epoch 23, Batch 752, Loss: 0.5745949149131775\n",
      "Epoch 23, Batch 753, Loss: 0.4343890845775604\n",
      "Epoch 23, Batch 754, Loss: 0.2221297174692154\n",
      "Epoch 23, Batch 755, Loss: 0.381191223859787\n",
      "Epoch 23, Batch 756, Loss: 0.38788488507270813\n",
      "Epoch 23, Batch 757, Loss: 0.33597904443740845\n",
      "Epoch 23, Batch 758, Loss: 0.35773733258247375\n",
      "Epoch 23, Batch 759, Loss: 0.19363853335380554\n",
      "Epoch 23, Batch 760, Loss: 0.44049543142318726\n",
      "Epoch 23, Batch 761, Loss: 0.4255821704864502\n",
      "Epoch 23, Batch 762, Loss: 0.3506801426410675\n",
      "Epoch 23, Batch 763, Loss: 0.5927766561508179\n",
      "Epoch 23, Batch 764, Loss: 0.6061732172966003\n",
      "Epoch 23, Batch 765, Loss: 0.32343557476997375\n",
      "Epoch 23, Batch 766, Loss: 0.4252479672431946\n",
      "Epoch 23, Batch 767, Loss: 0.38374289870262146\n",
      "Epoch 23, Batch 768, Loss: 0.42133015394210815\n",
      "Epoch 23, Batch 769, Loss: 0.2743157744407654\n",
      "Epoch 23, Batch 770, Loss: 0.40010085701942444\n",
      "Epoch 23, Batch 771, Loss: 0.46926048398017883\n",
      "Epoch 23, Batch 772, Loss: 0.19408875703811646\n",
      "Epoch 23, Batch 773, Loss: 0.41674473881721497\n",
      "Epoch 23, Batch 774, Loss: 0.5354122519493103\n",
      "Epoch 23, Batch 775, Loss: 0.5009599924087524\n",
      "Epoch 23, Batch 776, Loss: 0.30679428577423096\n",
      "Epoch 23, Batch 777, Loss: 0.31520405411720276\n",
      "Epoch 23, Batch 778, Loss: 0.3655707836151123\n",
      "Epoch 23, Batch 779, Loss: 0.24188284575939178\n",
      "Epoch 23, Batch 780, Loss: 0.4857773184776306\n",
      "Epoch 23, Batch 781, Loss: 0.2559090256690979\n",
      "Epoch 23, Batch 782, Loss: 0.3324451744556427\n",
      "Epoch 23, Batch 783, Loss: 0.39346763491630554\n",
      "Epoch 23, Batch 784, Loss: 0.3741503059864044\n",
      "Epoch 23, Batch 785, Loss: 0.4133322238922119\n",
      "Epoch 23, Batch 786, Loss: 0.3249041438102722\n",
      "Epoch 23, Batch 787, Loss: 0.36124593019485474\n",
      "Epoch 23, Batch 788, Loss: 0.4392310380935669\n",
      "Epoch 23, Batch 789, Loss: 0.34283342957496643\n",
      "Epoch 23, Batch 790, Loss: 0.37199679017066956\n",
      "Epoch 23, Batch 791, Loss: 0.3239934742450714\n",
      "Epoch 23, Batch 792, Loss: 0.4127681851387024\n",
      "Epoch 23, Batch 793, Loss: 0.23505344986915588\n",
      "Epoch 23, Batch 794, Loss: 0.49530285596847534\n",
      "Epoch 23, Batch 795, Loss: 0.3746355175971985\n",
      "Epoch 23, Batch 796, Loss: 0.3496303856372833\n",
      "Epoch 23, Batch 797, Loss: 0.40898188948631287\n",
      "Epoch 23, Batch 798, Loss: 0.4066641926765442\n",
      "Epoch 23, Batch 799, Loss: 0.432584673166275\n",
      "Epoch 23, Batch 800, Loss: 0.498401403427124\n",
      "Epoch 23, Batch 801, Loss: 0.2732447385787964\n",
      "Epoch 23, Batch 802, Loss: 0.5049603581428528\n",
      "Epoch 23, Batch 803, Loss: 0.29782959818840027\n",
      "Epoch 23, Batch 804, Loss: 0.38244858384132385\n",
      "Epoch 23, Batch 805, Loss: 0.29398849606513977\n",
      "Epoch 23, Batch 806, Loss: 0.5307226777076721\n",
      "Epoch 23, Batch 807, Loss: 0.4290125072002411\n",
      "Epoch 23, Batch 808, Loss: 0.2728966176509857\n",
      "Epoch 23, Batch 809, Loss: 0.35859760642051697\n",
      "Epoch 23, Batch 810, Loss: 0.34154897928237915\n",
      "Epoch 23, Batch 811, Loss: 0.8051036596298218\n",
      "Epoch 23, Batch 812, Loss: 0.24819208681583405\n",
      "Epoch 23, Batch 813, Loss: 0.638728141784668\n",
      "Epoch 23, Batch 814, Loss: 0.3729780316352844\n",
      "Epoch 23, Batch 815, Loss: 0.3681223690509796\n",
      "Epoch 23, Batch 816, Loss: 0.42325708270072937\n",
      "Epoch 23, Batch 817, Loss: 0.26189494132995605\n",
      "Epoch 23, Batch 818, Loss: 0.36159753799438477\n",
      "Epoch 23, Batch 819, Loss: 0.2595592141151428\n",
      "Epoch 23, Batch 820, Loss: 0.4777514338493347\n",
      "Epoch 23, Batch 821, Loss: 0.38194209337234497\n",
      "Epoch 23, Batch 822, Loss: 0.37178507447242737\n",
      "Epoch 23, Batch 823, Loss: 0.2624030113220215\n",
      "Epoch 23, Batch 824, Loss: 0.3617067337036133\n",
      "Epoch 23, Batch 825, Loss: 0.30628350377082825\n",
      "Epoch 23, Batch 826, Loss: 0.47399935126304626\n",
      "Epoch 23, Batch 827, Loss: 0.2721486985683441\n",
      "Epoch 23, Batch 828, Loss: 0.33904188871383667\n",
      "Epoch 23, Batch 829, Loss: 0.28214797377586365\n",
      "Epoch 23, Batch 830, Loss: 0.4085102081298828\n",
      "Epoch 23, Batch 831, Loss: 0.3293968439102173\n",
      "Epoch 23, Batch 832, Loss: 0.3427228629589081\n",
      "Epoch 23, Batch 833, Loss: 0.5156180262565613\n",
      "Epoch 23, Batch 834, Loss: 0.32521945238113403\n",
      "Epoch 23, Batch 835, Loss: 0.44732555747032166\n",
      "Epoch 23, Batch 836, Loss: 0.4583868086338043\n",
      "Epoch 23, Batch 837, Loss: 0.44094133377075195\n",
      "Epoch 23, Batch 838, Loss: 0.26942601799964905\n",
      "Epoch 23, Batch 839, Loss: 0.3939130902290344\n",
      "Epoch 23, Batch 840, Loss: 0.22940693795681\n",
      "Epoch 23, Batch 841, Loss: 0.30487608909606934\n",
      "Epoch 23, Batch 842, Loss: 0.34139007329940796\n",
      "Epoch 23, Batch 843, Loss: 0.38943666219711304\n",
      "Epoch 23, Batch 844, Loss: 0.3687300682067871\n",
      "Epoch 23, Batch 845, Loss: 0.23510584235191345\n",
      "Epoch 23, Batch 846, Loss: 0.3871370851993561\n",
      "Epoch 23, Batch 847, Loss: 0.4571280777454376\n",
      "Epoch 23, Batch 848, Loss: 0.2947843074798584\n",
      "Epoch 23, Batch 849, Loss: 0.26708507537841797\n",
      "Epoch 23, Batch 850, Loss: 0.2130964994430542\n",
      "Epoch 23, Batch 851, Loss: 0.6339370012283325\n",
      "Epoch 23, Batch 852, Loss: 0.3488656282424927\n",
      "Epoch 23, Batch 853, Loss: 0.4576353132724762\n",
      "Epoch 23, Batch 854, Loss: 0.3812127411365509\n",
      "Epoch 23, Batch 855, Loss: 0.2501431107521057\n",
      "Epoch 23, Batch 856, Loss: 0.44077277183532715\n",
      "Epoch 23, Batch 857, Loss: 0.29184603691101074\n",
      "Epoch 23, Batch 858, Loss: 0.2664169669151306\n",
      "Epoch 23, Batch 859, Loss: 0.4238761067390442\n",
      "Epoch 23, Batch 860, Loss: 0.5295861959457397\n",
      "Epoch 23, Batch 861, Loss: 0.5045498609542847\n",
      "Epoch 23, Batch 862, Loss: 0.3107415735721588\n",
      "Epoch 23, Batch 863, Loss: 0.4221262037754059\n",
      "Epoch 23, Batch 864, Loss: 0.34892475605010986\n",
      "Epoch 23, Batch 865, Loss: 0.37806087732315063\n",
      "Epoch 23, Batch 866, Loss: 0.4391455054283142\n",
      "Epoch 23, Batch 867, Loss: 0.3148430585861206\n",
      "Epoch 23, Batch 868, Loss: 0.3025968074798584\n",
      "Epoch 23, Batch 869, Loss: 0.32248497009277344\n",
      "Epoch 23, Batch 870, Loss: 0.3852274417877197\n",
      "Epoch 23, Batch 871, Loss: 0.37335649132728577\n",
      "Epoch 23, Batch 872, Loss: 0.3261897563934326\n",
      "Epoch 23, Batch 873, Loss: 0.44084662199020386\n",
      "Epoch 23, Batch 874, Loss: 0.4337086081504822\n",
      "Epoch 23, Batch 875, Loss: 0.33926910161972046\n",
      "Epoch 23, Batch 876, Loss: 0.35257506370544434\n",
      "Epoch 23, Batch 877, Loss: 0.3293033540248871\n",
      "Epoch 23, Batch 878, Loss: 0.3190321922302246\n",
      "Epoch 23, Batch 879, Loss: 0.3301537036895752\n",
      "Epoch 23, Batch 880, Loss: 0.22880461812019348\n",
      "Epoch 23, Batch 881, Loss: 0.29224157333374023\n",
      "Epoch 23, Batch 882, Loss: 0.45221543312072754\n",
      "Epoch 23, Batch 883, Loss: 0.36201006174087524\n",
      "Epoch 23, Batch 884, Loss: 0.5596275329589844\n",
      "Epoch 23, Batch 885, Loss: 0.4631355404853821\n",
      "Epoch 23, Batch 886, Loss: 0.2749413251876831\n",
      "Epoch 23, Batch 887, Loss: 0.3125641345977783\n",
      "Epoch 23, Batch 888, Loss: 0.39711034297943115\n",
      "Epoch 23, Batch 889, Loss: 0.43758296966552734\n",
      "Epoch 23, Batch 890, Loss: 0.3654657006263733\n",
      "Epoch 23, Batch 891, Loss: 0.48375457525253296\n",
      "Epoch 23, Batch 892, Loss: 0.37304261326789856\n",
      "Epoch 23, Batch 893, Loss: 0.4149474799633026\n",
      "Epoch 23, Batch 894, Loss: 0.5407695770263672\n",
      "Epoch 23, Batch 895, Loss: 0.33267658948898315\n",
      "Epoch 23, Batch 896, Loss: 0.42897695302963257\n",
      "Epoch 23, Batch 897, Loss: 0.3759644329547882\n",
      "Epoch 23, Batch 898, Loss: 0.6103032827377319\n",
      "Epoch 23, Batch 899, Loss: 0.39096158742904663\n",
      "Epoch 23, Batch 900, Loss: 0.22971263527870178\n",
      "Epoch 23, Batch 901, Loss: 0.26962584257125854\n",
      "Epoch 23, Batch 902, Loss: 0.40447932481765747\n",
      "Epoch 23, Batch 903, Loss: 0.36181822419166565\n",
      "Epoch 23, Batch 904, Loss: 0.3366401195526123\n",
      "Epoch 23, Batch 905, Loss: 0.37236273288726807\n",
      "Epoch 23, Batch 906, Loss: 0.1352577954530716\n",
      "Epoch 23, Batch 907, Loss: 0.22768068313598633\n",
      "Epoch 23, Batch 908, Loss: 0.4382815957069397\n",
      "Epoch 23, Batch 909, Loss: 0.37056633830070496\n",
      "Epoch 23, Batch 910, Loss: 0.35150009393692017\n",
      "Epoch 23, Batch 911, Loss: 0.6227089762687683\n",
      "Epoch 23, Batch 912, Loss: 0.25290340185165405\n",
      "Epoch 23, Batch 913, Loss: 0.3126010000705719\n",
      "Epoch 23, Batch 914, Loss: 0.3731865882873535\n",
      "Epoch 23, Batch 915, Loss: 0.39533597230911255\n",
      "Epoch 23, Batch 916, Loss: 0.3795277178287506\n",
      "Epoch 23, Batch 917, Loss: 0.33082371950149536\n",
      "Epoch 23, Batch 918, Loss: 0.38579970598220825\n",
      "Epoch 23, Batch 919, Loss: 0.34472858905792236\n",
      "Epoch 23, Batch 920, Loss: 0.32215166091918945\n",
      "Epoch 23, Batch 921, Loss: 0.5258885622024536\n",
      "Epoch 23, Batch 922, Loss: 0.4834635853767395\n",
      "Epoch 23, Batch 923, Loss: 0.40405604243278503\n",
      "Epoch 23, Batch 924, Loss: 0.35919901728630066\n",
      "Epoch 23, Batch 925, Loss: 0.6188533306121826\n",
      "Epoch 23, Batch 926, Loss: 0.2236086130142212\n",
      "Epoch 23, Batch 927, Loss: 0.2288922816514969\n",
      "Epoch 23, Batch 928, Loss: 0.4189221262931824\n",
      "Epoch 23, Batch 929, Loss: 0.23416617512702942\n",
      "Epoch 23, Batch 930, Loss: 0.36881178617477417\n",
      "Epoch 23, Batch 931, Loss: 0.45874297618865967\n",
      "Epoch 23, Batch 932, Loss: 0.27897143363952637\n",
      "Epoch 23, Batch 933, Loss: 0.38040870428085327\n",
      "Epoch 23, Batch 934, Loss: 0.2574656307697296\n",
      "Epoch 23, Batch 935, Loss: 0.5816418528556824\n",
      "Epoch 23, Batch 936, Loss: 0.44860389828681946\n",
      "Epoch 23, Batch 937, Loss: 0.2848021686077118\n",
      "Epoch 23, Batch 938, Loss: 0.5217157602310181\n",
      "Accuracy of train set: 0.8616\n",
      "Epoch 23, Batch 1, Test Loss: 0.5859734416007996\n",
      "Epoch 23, Batch 2, Test Loss: 0.4984298348426819\n",
      "Epoch 23, Batch 3, Test Loss: 0.3433080315589905\n",
      "Epoch 23, Batch 4, Test Loss: 0.3253316879272461\n",
      "Epoch 23, Batch 5, Test Loss: 0.3000083565711975\n",
      "Epoch 23, Batch 6, Test Loss: 0.33811983466148376\n",
      "Epoch 23, Batch 7, Test Loss: 0.524332582950592\n",
      "Epoch 23, Batch 8, Test Loss: 0.4486839175224304\n",
      "Epoch 23, Batch 9, Test Loss: 0.4372045397758484\n",
      "Epoch 23, Batch 10, Test Loss: 0.32223713397979736\n",
      "Epoch 23, Batch 11, Test Loss: 0.4173567295074463\n",
      "Epoch 23, Batch 12, Test Loss: 0.46600210666656494\n",
      "Epoch 23, Batch 13, Test Loss: 0.4226764738559723\n",
      "Epoch 23, Batch 14, Test Loss: 0.5409523248672485\n",
      "Epoch 23, Batch 15, Test Loss: 0.6719774007797241\n",
      "Epoch 23, Batch 16, Test Loss: 0.41180408000946045\n",
      "Epoch 23, Batch 17, Test Loss: 0.36702102422714233\n",
      "Epoch 23, Batch 18, Test Loss: 0.5394628643989563\n",
      "Epoch 23, Batch 19, Test Loss: 0.46269530057907104\n",
      "Epoch 23, Batch 20, Test Loss: 0.5516873598098755\n",
      "Epoch 23, Batch 21, Test Loss: 0.5982379913330078\n",
      "Epoch 23, Batch 22, Test Loss: 0.5297896265983582\n",
      "Epoch 23, Batch 23, Test Loss: 0.5144610404968262\n",
      "Epoch 23, Batch 24, Test Loss: 0.3819277286529541\n",
      "Epoch 23, Batch 25, Test Loss: 0.39781689643859863\n",
      "Epoch 23, Batch 26, Test Loss: 0.5610882639884949\n",
      "Epoch 23, Batch 27, Test Loss: 0.4201590120792389\n",
      "Epoch 23, Batch 28, Test Loss: 0.21999119222164154\n",
      "Epoch 23, Batch 29, Test Loss: 0.5441082119941711\n",
      "Epoch 23, Batch 30, Test Loss: 0.5181341767311096\n",
      "Epoch 23, Batch 31, Test Loss: 0.6578054428100586\n",
      "Epoch 23, Batch 32, Test Loss: 0.5405462980270386\n",
      "Epoch 23, Batch 33, Test Loss: 0.3531658947467804\n",
      "Epoch 23, Batch 34, Test Loss: 0.7484092712402344\n",
      "Epoch 23, Batch 35, Test Loss: 0.44230684638023376\n",
      "Epoch 23, Batch 36, Test Loss: 0.564509391784668\n",
      "Epoch 23, Batch 37, Test Loss: 0.6759991645812988\n",
      "Epoch 23, Batch 38, Test Loss: 0.5398387312889099\n",
      "Epoch 23, Batch 39, Test Loss: 0.26949241757392883\n",
      "Epoch 23, Batch 40, Test Loss: 0.44610595703125\n",
      "Epoch 23, Batch 41, Test Loss: 0.4539260268211365\n",
      "Epoch 23, Batch 42, Test Loss: 0.5785583257675171\n",
      "Epoch 23, Batch 43, Test Loss: 0.5622981190681458\n",
      "Epoch 23, Batch 44, Test Loss: 0.3706388473510742\n",
      "Epoch 23, Batch 45, Test Loss: 0.6128529906272888\n",
      "Epoch 23, Batch 46, Test Loss: 0.6746437549591064\n",
      "Epoch 23, Batch 47, Test Loss: 0.6000066995620728\n",
      "Epoch 23, Batch 48, Test Loss: 0.48676037788391113\n",
      "Epoch 23, Batch 49, Test Loss: 0.5087264776229858\n",
      "Epoch 23, Batch 50, Test Loss: 0.37393683195114136\n",
      "Epoch 23, Batch 51, Test Loss: 0.4193633794784546\n",
      "Epoch 23, Batch 52, Test Loss: 0.3631855249404907\n",
      "Epoch 23, Batch 53, Test Loss: 0.594527006149292\n",
      "Epoch 23, Batch 54, Test Loss: 0.5550119876861572\n",
      "Epoch 23, Batch 55, Test Loss: 0.3867156207561493\n",
      "Epoch 23, Batch 56, Test Loss: 0.34424716234207153\n",
      "Epoch 23, Batch 57, Test Loss: 0.6709975600242615\n",
      "Epoch 23, Batch 58, Test Loss: 0.4244091510772705\n",
      "Epoch 23, Batch 59, Test Loss: 0.6626036167144775\n",
      "Epoch 23, Batch 60, Test Loss: 0.40960147976875305\n",
      "Epoch 23, Batch 61, Test Loss: 0.3681291341781616\n",
      "Epoch 23, Batch 62, Test Loss: 0.415343314409256\n",
      "Epoch 23, Batch 63, Test Loss: 0.3701136112213135\n",
      "Epoch 23, Batch 64, Test Loss: 0.44043201208114624\n",
      "Epoch 23, Batch 65, Test Loss: 0.4247409403324127\n",
      "Epoch 23, Batch 66, Test Loss: 0.40781170129776\n",
      "Epoch 23, Batch 67, Test Loss: 0.46470940113067627\n",
      "Epoch 23, Batch 68, Test Loss: 0.6340453624725342\n",
      "Epoch 23, Batch 69, Test Loss: 0.5086036920547485\n",
      "Epoch 23, Batch 70, Test Loss: 0.4413042366504669\n",
      "Epoch 23, Batch 71, Test Loss: 0.3436051309108734\n",
      "Epoch 23, Batch 72, Test Loss: 0.2953341603279114\n",
      "Epoch 23, Batch 73, Test Loss: 0.3862723112106323\n",
      "Epoch 23, Batch 74, Test Loss: 0.4124678373336792\n",
      "Epoch 23, Batch 75, Test Loss: 0.385917067527771\n",
      "Epoch 23, Batch 76, Test Loss: 0.49059247970581055\n",
      "Epoch 23, Batch 77, Test Loss: 0.3521866798400879\n",
      "Epoch 23, Batch 78, Test Loss: 0.403638631105423\n",
      "Epoch 23, Batch 79, Test Loss: 0.3424130380153656\n",
      "Epoch 23, Batch 80, Test Loss: 0.4646157920360565\n",
      "Epoch 23, Batch 81, Test Loss: 0.6201456785202026\n",
      "Epoch 23, Batch 82, Test Loss: 0.4762517809867859\n",
      "Epoch 23, Batch 83, Test Loss: 0.4319799840450287\n",
      "Epoch 23, Batch 84, Test Loss: 0.26534122228622437\n",
      "Epoch 23, Batch 85, Test Loss: 0.3786062002182007\n",
      "Epoch 23, Batch 86, Test Loss: 0.5696827173233032\n",
      "Epoch 23, Batch 87, Test Loss: 0.5121235251426697\n",
      "Epoch 23, Batch 88, Test Loss: 0.3929072618484497\n",
      "Epoch 23, Batch 89, Test Loss: 0.3682780861854553\n",
      "Epoch 23, Batch 90, Test Loss: 0.32691943645477295\n",
      "Epoch 23, Batch 91, Test Loss: 0.35301944613456726\n",
      "Epoch 23, Batch 92, Test Loss: 0.49628567695617676\n",
      "Epoch 23, Batch 93, Test Loss: 0.5083814859390259\n",
      "Epoch 23, Batch 94, Test Loss: 0.2218734323978424\n",
      "Epoch 23, Batch 95, Test Loss: 0.31783783435821533\n",
      "Epoch 23, Batch 96, Test Loss: 0.23453183472156525\n",
      "Epoch 23, Batch 97, Test Loss: 0.35360196232795715\n",
      "Epoch 23, Batch 98, Test Loss: 0.597047746181488\n",
      "Epoch 23, Batch 99, Test Loss: 0.6741023659706116\n",
      "Epoch 23, Batch 100, Test Loss: 0.5680729150772095\n",
      "Epoch 23, Batch 101, Test Loss: 0.5475944876670837\n",
      "Epoch 23, Batch 102, Test Loss: 0.4756195545196533\n",
      "Epoch 23, Batch 103, Test Loss: 0.5923570394515991\n",
      "Epoch 23, Batch 104, Test Loss: 0.45448267459869385\n",
      "Epoch 23, Batch 105, Test Loss: 0.4321126341819763\n",
      "Epoch 23, Batch 106, Test Loss: 0.37415897846221924\n",
      "Epoch 23, Batch 107, Test Loss: 0.4558289051055908\n",
      "Epoch 23, Batch 108, Test Loss: 0.4894283413887024\n",
      "Epoch 23, Batch 109, Test Loss: 0.49713289737701416\n",
      "Epoch 23, Batch 110, Test Loss: 0.50658118724823\n",
      "Epoch 23, Batch 111, Test Loss: 0.4391036927700043\n",
      "Epoch 23, Batch 112, Test Loss: 0.5837786197662354\n",
      "Epoch 23, Batch 113, Test Loss: 0.3873450458049774\n",
      "Epoch 23, Batch 114, Test Loss: 0.5886106491088867\n",
      "Epoch 23, Batch 115, Test Loss: 0.37944042682647705\n",
      "Epoch 23, Batch 116, Test Loss: 0.3675181269645691\n",
      "Epoch 23, Batch 117, Test Loss: 0.44796988368034363\n",
      "Epoch 23, Batch 118, Test Loss: 0.36101555824279785\n",
      "Epoch 23, Batch 119, Test Loss: 0.5423877835273743\n",
      "Epoch 23, Batch 120, Test Loss: 0.34492823481559753\n",
      "Epoch 23, Batch 121, Test Loss: 0.36670228838920593\n",
      "Epoch 23, Batch 122, Test Loss: 0.3695978820323944\n",
      "Epoch 23, Batch 123, Test Loss: 0.4667872488498688\n",
      "Epoch 23, Batch 124, Test Loss: 0.3853060007095337\n",
      "Epoch 23, Batch 125, Test Loss: 0.3365486264228821\n",
      "Epoch 23, Batch 126, Test Loss: 0.49218589067459106\n",
      "Epoch 23, Batch 127, Test Loss: 0.4732339084148407\n",
      "Epoch 23, Batch 128, Test Loss: 0.5466660857200623\n",
      "Epoch 23, Batch 129, Test Loss: 0.5155342817306519\n",
      "Epoch 23, Batch 130, Test Loss: 0.4734953045845032\n",
      "Epoch 23, Batch 131, Test Loss: 0.48421576619148254\n",
      "Epoch 23, Batch 132, Test Loss: 0.38492169976234436\n",
      "Epoch 23, Batch 133, Test Loss: 0.6426098346710205\n",
      "Epoch 23, Batch 134, Test Loss: 0.34721997380256653\n",
      "Epoch 23, Batch 135, Test Loss: 0.35898610949516296\n",
      "Epoch 23, Batch 136, Test Loss: 0.47875866293907166\n",
      "Epoch 23, Batch 137, Test Loss: 0.37492242455482483\n",
      "Epoch 23, Batch 138, Test Loss: 0.7389058470726013\n",
      "Epoch 23, Batch 139, Test Loss: 0.43575337529182434\n",
      "Epoch 23, Batch 140, Test Loss: 0.463943213224411\n",
      "Epoch 23, Batch 141, Test Loss: 0.42926767468452454\n",
      "Epoch 23, Batch 142, Test Loss: 0.3826559782028198\n",
      "Epoch 23, Batch 143, Test Loss: 0.4944066107273102\n",
      "Epoch 23, Batch 144, Test Loss: 0.45773226022720337\n",
      "Epoch 23, Batch 145, Test Loss: 0.4515860080718994\n",
      "Epoch 23, Batch 146, Test Loss: 0.5555239319801331\n",
      "Epoch 23, Batch 147, Test Loss: 0.33123287558555603\n",
      "Epoch 23, Batch 148, Test Loss: 0.6192158460617065\n",
      "Epoch 23, Batch 149, Test Loss: 0.4982250928878784\n",
      "Epoch 23, Batch 150, Test Loss: 0.3911312222480774\n",
      "Epoch 23, Batch 151, Test Loss: 0.3520871698856354\n",
      "Epoch 23, Batch 152, Test Loss: 0.3951939046382904\n",
      "Epoch 23, Batch 153, Test Loss: 0.5157503485679626\n",
      "Epoch 23, Batch 154, Test Loss: 0.6302917003631592\n",
      "Epoch 23, Batch 155, Test Loss: 0.4034409821033478\n",
      "Epoch 23, Batch 156, Test Loss: 0.7543215155601501\n",
      "Epoch 23, Batch 157, Test Loss: 0.3963465690612793\n",
      "Epoch 23, Batch 158, Test Loss: 0.3687177002429962\n",
      "Epoch 23, Batch 159, Test Loss: 0.4631218910217285\n",
      "Epoch 23, Batch 160, Test Loss: 0.44160011410713196\n",
      "Epoch 23, Batch 161, Test Loss: 0.4502497613430023\n",
      "Epoch 23, Batch 162, Test Loss: 0.4905529320240021\n",
      "Epoch 23, Batch 163, Test Loss: 0.2712893486022949\n",
      "Epoch 23, Batch 164, Test Loss: 0.44515591859817505\n",
      "Epoch 23, Batch 165, Test Loss: 0.2930316627025604\n",
      "Epoch 23, Batch 166, Test Loss: 0.39908701181411743\n",
      "Epoch 23, Batch 167, Test Loss: 0.5179229378700256\n",
      "Epoch 23, Batch 168, Test Loss: 0.4174189567565918\n",
      "Epoch 23, Batch 169, Test Loss: 0.35760048031806946\n",
      "Epoch 23, Batch 170, Test Loss: 0.42043593525886536\n",
      "Epoch 23, Batch 171, Test Loss: 0.7394611239433289\n",
      "Epoch 23, Batch 172, Test Loss: 0.5461843609809875\n",
      "Epoch 23, Batch 173, Test Loss: 0.5652727484703064\n",
      "Epoch 23, Batch 174, Test Loss: 0.3384435474872589\n",
      "Epoch 23, Batch 175, Test Loss: 0.491619348526001\n",
      "Epoch 23, Batch 176, Test Loss: 0.40192997455596924\n",
      "Epoch 23, Batch 177, Test Loss: 0.24732330441474915\n",
      "Epoch 23, Batch 178, Test Loss: 0.5762282609939575\n",
      "Epoch 23, Batch 179, Test Loss: 0.5772897005081177\n",
      "Epoch 23, Batch 180, Test Loss: 0.34947749972343445\n",
      "Epoch 23, Batch 181, Test Loss: 0.27075791358947754\n",
      "Epoch 23, Batch 182, Test Loss: 0.31410476565361023\n",
      "Epoch 23, Batch 183, Test Loss: 0.36118024587631226\n",
      "Epoch 23, Batch 184, Test Loss: 0.3225895166397095\n",
      "Epoch 23, Batch 185, Test Loss: 0.490251749753952\n",
      "Epoch 23, Batch 186, Test Loss: 0.6603081822395325\n",
      "Epoch 23, Batch 187, Test Loss: 0.4471209645271301\n",
      "Epoch 23, Batch 188, Test Loss: 0.4386909604072571\n",
      "Epoch 23, Batch 189, Test Loss: 0.41305258870124817\n",
      "Epoch 23, Batch 190, Test Loss: 0.29031673073768616\n",
      "Epoch 23, Batch 191, Test Loss: 0.57121741771698\n",
      "Epoch 23, Batch 192, Test Loss: 0.45381903648376465\n",
      "Epoch 23, Batch 193, Test Loss: 0.4480755031108856\n",
      "Epoch 23, Batch 194, Test Loss: 0.5470175743103027\n",
      "Epoch 23, Batch 195, Test Loss: 0.5879198908805847\n",
      "Epoch 23, Batch 196, Test Loss: 0.4142991900444031\n",
      "Epoch 23, Batch 197, Test Loss: 0.5557352900505066\n",
      "Epoch 23, Batch 198, Test Loss: 0.49660593271255493\n",
      "Epoch 23, Batch 199, Test Loss: 0.4220168888568878\n",
      "Epoch 23, Batch 200, Test Loss: 0.34141790866851807\n",
      "Epoch 23, Batch 201, Test Loss: 0.4380718469619751\n",
      "Epoch 23, Batch 202, Test Loss: 0.49632588028907776\n",
      "Epoch 23, Batch 203, Test Loss: 0.38069701194763184\n",
      "Epoch 23, Batch 204, Test Loss: 0.753882646560669\n",
      "Epoch 23, Batch 205, Test Loss: 0.3528672456741333\n",
      "Epoch 23, Batch 206, Test Loss: 0.304200679063797\n",
      "Epoch 23, Batch 207, Test Loss: 0.4280146360397339\n",
      "Epoch 23, Batch 208, Test Loss: 0.35562998056411743\n",
      "Epoch 23, Batch 209, Test Loss: 0.41009432077407837\n",
      "Epoch 23, Batch 210, Test Loss: 0.372086763381958\n",
      "Epoch 23, Batch 211, Test Loss: 0.41807952523231506\n",
      "Epoch 23, Batch 212, Test Loss: 0.7328643202781677\n",
      "Epoch 23, Batch 213, Test Loss: 0.49551910161972046\n",
      "Epoch 23, Batch 214, Test Loss: 0.32739192247390747\n",
      "Epoch 23, Batch 215, Test Loss: 0.3586732745170593\n",
      "Epoch 23, Batch 216, Test Loss: 0.41382256150245667\n",
      "Epoch 23, Batch 217, Test Loss: 0.35166043043136597\n",
      "Epoch 23, Batch 218, Test Loss: 0.50545734167099\n",
      "Epoch 23, Batch 219, Test Loss: 0.4585069715976715\n",
      "Epoch 23, Batch 220, Test Loss: 0.6000553369522095\n",
      "Epoch 23, Batch 221, Test Loss: 0.4548729658126831\n",
      "Epoch 23, Batch 222, Test Loss: 0.4530876576900482\n",
      "Epoch 23, Batch 223, Test Loss: 0.4096081852912903\n",
      "Epoch 23, Batch 224, Test Loss: 0.45293599367141724\n",
      "Epoch 23, Batch 225, Test Loss: 0.5121128559112549\n",
      "Epoch 23, Batch 226, Test Loss: 0.45874717831611633\n",
      "Epoch 23, Batch 227, Test Loss: 0.5324835777282715\n",
      "Epoch 23, Batch 228, Test Loss: 0.5766987800598145\n",
      "Epoch 23, Batch 229, Test Loss: 0.38254714012145996\n",
      "Epoch 23, Batch 230, Test Loss: 0.4597877264022827\n",
      "Epoch 23, Batch 231, Test Loss: 0.29564642906188965\n",
      "Epoch 23, Batch 232, Test Loss: 0.49149343371391296\n",
      "Epoch 23, Batch 233, Test Loss: 0.5945289731025696\n",
      "Epoch 23, Batch 234, Test Loss: 0.4283559024333954\n",
      "Epoch 23, Batch 235, Test Loss: 0.43888840079307556\n",
      "Epoch 23, Batch 236, Test Loss: 0.3615725636482239\n",
      "Epoch 23, Batch 237, Test Loss: 0.6016707420349121\n",
      "Epoch 23, Batch 238, Test Loss: 0.5408205389976501\n",
      "Epoch 23, Batch 239, Test Loss: 0.3819452226161957\n",
      "Epoch 23, Batch 240, Test Loss: 0.3040136396884918\n",
      "Epoch 23, Batch 241, Test Loss: 0.4940043091773987\n",
      "Epoch 23, Batch 242, Test Loss: 0.7629542946815491\n",
      "Epoch 23, Batch 243, Test Loss: 0.4779517948627472\n",
      "Epoch 23, Batch 244, Test Loss: 0.3604106605052948\n",
      "Epoch 23, Batch 245, Test Loss: 0.3878917396068573\n",
      "Epoch 23, Batch 246, Test Loss: 0.5009898543357849\n",
      "Epoch 23, Batch 247, Test Loss: 0.3230847716331482\n",
      "Epoch 23, Batch 248, Test Loss: 0.5248149633407593\n",
      "Epoch 23, Batch 249, Test Loss: 0.4092137813568115\n",
      "Epoch 23, Batch 250, Test Loss: 0.3271709084510803\n",
      "Epoch 23, Batch 251, Test Loss: 0.23723572492599487\n",
      "Epoch 23, Batch 252, Test Loss: 0.546380341053009\n",
      "Epoch 23, Batch 253, Test Loss: 0.33113595843315125\n",
      "Epoch 23, Batch 254, Test Loss: 0.446439653635025\n",
      "Epoch 23, Batch 255, Test Loss: 0.6267040967941284\n",
      "Epoch 23, Batch 256, Test Loss: 0.45694711804389954\n",
      "Epoch 23, Batch 257, Test Loss: 0.4335125684738159\n",
      "Epoch 23, Batch 258, Test Loss: 0.312516450881958\n",
      "Epoch 23, Batch 259, Test Loss: 0.42320817708969116\n",
      "Epoch 23, Batch 260, Test Loss: 0.3533225953578949\n",
      "Epoch 23, Batch 261, Test Loss: 0.39284124970436096\n",
      "Epoch 23, Batch 262, Test Loss: 0.31759801506996155\n",
      "Epoch 23, Batch 263, Test Loss: 0.39351987838745117\n",
      "Epoch 23, Batch 264, Test Loss: 0.6457890868186951\n",
      "Epoch 23, Batch 265, Test Loss: 0.30453628301620483\n",
      "Epoch 23, Batch 266, Test Loss: 0.6019716858863831\n",
      "Epoch 23, Batch 267, Test Loss: 0.3849196434020996\n",
      "Epoch 23, Batch 268, Test Loss: 0.4852144420146942\n",
      "Epoch 23, Batch 269, Test Loss: 0.30994102358818054\n",
      "Epoch 23, Batch 270, Test Loss: 0.5222800374031067\n",
      "Epoch 23, Batch 271, Test Loss: 0.4732494056224823\n",
      "Epoch 23, Batch 272, Test Loss: 0.4956608712673187\n",
      "Epoch 23, Batch 273, Test Loss: 0.4782354235649109\n",
      "Epoch 23, Batch 274, Test Loss: 0.3933335840702057\n",
      "Epoch 23, Batch 275, Test Loss: 0.37761878967285156\n",
      "Epoch 23, Batch 276, Test Loss: 0.4237981140613556\n",
      "Epoch 23, Batch 277, Test Loss: 0.48707854747772217\n",
      "Epoch 23, Batch 278, Test Loss: 0.38585546612739563\n",
      "Epoch 23, Batch 279, Test Loss: 0.454877108335495\n",
      "Epoch 23, Batch 280, Test Loss: 0.3925236165523529\n",
      "Epoch 23, Batch 281, Test Loss: 0.5602940917015076\n",
      "Epoch 23, Batch 282, Test Loss: 0.4086057245731354\n",
      "Epoch 23, Batch 283, Test Loss: 0.4324690103530884\n",
      "Epoch 23, Batch 284, Test Loss: 0.31527674198150635\n",
      "Epoch 23, Batch 285, Test Loss: 0.5494486689567566\n",
      "Epoch 23, Batch 286, Test Loss: 0.6125378012657166\n",
      "Epoch 23, Batch 287, Test Loss: 0.370643675327301\n",
      "Epoch 23, Batch 288, Test Loss: 0.43769651651382446\n",
      "Epoch 23, Batch 289, Test Loss: 0.374146968126297\n",
      "Epoch 23, Batch 290, Test Loss: 0.6005071997642517\n",
      "Epoch 23, Batch 291, Test Loss: 0.2949313521385193\n",
      "Epoch 23, Batch 292, Test Loss: 0.5048454403877258\n",
      "Epoch 23, Batch 293, Test Loss: 0.5122831463813782\n",
      "Epoch 23, Batch 294, Test Loss: 0.3423953056335449\n",
      "Epoch 23, Batch 295, Test Loss: 0.3720720112323761\n",
      "Epoch 23, Batch 296, Test Loss: 0.5077683925628662\n",
      "Epoch 23, Batch 297, Test Loss: 0.35854607820510864\n",
      "Epoch 23, Batch 298, Test Loss: 0.4706302583217621\n",
      "Epoch 23, Batch 299, Test Loss: 0.3840723931789398\n",
      "Epoch 23, Batch 300, Test Loss: 0.45542362332344055\n",
      "Epoch 23, Batch 301, Test Loss: 0.4875476360321045\n",
      "Epoch 23, Batch 302, Test Loss: 0.37526142597198486\n",
      "Epoch 23, Batch 303, Test Loss: 0.4720880091190338\n",
      "Epoch 23, Batch 304, Test Loss: 0.3131049871444702\n",
      "Epoch 23, Batch 305, Test Loss: 0.47537481784820557\n",
      "Epoch 23, Batch 306, Test Loss: 0.4626348316669464\n",
      "Epoch 23, Batch 307, Test Loss: 0.4633426368236542\n",
      "Epoch 23, Batch 308, Test Loss: 0.4554409384727478\n",
      "Epoch 23, Batch 309, Test Loss: 0.3719435930252075\n",
      "Epoch 23, Batch 310, Test Loss: 0.3920745849609375\n",
      "Epoch 23, Batch 311, Test Loss: 0.39605551958084106\n",
      "Epoch 23, Batch 312, Test Loss: 0.4565037190914154\n",
      "Epoch 23, Batch 313, Test Loss: 0.43089333176612854\n",
      "Epoch 23, Batch 314, Test Loss: 0.41192346811294556\n",
      "Epoch 23, Batch 315, Test Loss: 0.5077096223831177\n",
      "Epoch 23, Batch 316, Test Loss: 0.3993706703186035\n",
      "Epoch 23, Batch 317, Test Loss: 0.36283132433891296\n",
      "Epoch 23, Batch 318, Test Loss: 0.4858289361000061\n",
      "Epoch 23, Batch 319, Test Loss: 0.34204238653182983\n",
      "Epoch 23, Batch 320, Test Loss: 0.40501925349235535\n",
      "Epoch 23, Batch 321, Test Loss: 0.5219440460205078\n",
      "Epoch 23, Batch 322, Test Loss: 0.6208614110946655\n",
      "Epoch 23, Batch 323, Test Loss: 0.463238924741745\n",
      "Epoch 23, Batch 324, Test Loss: 0.40406057238578796\n",
      "Epoch 23, Batch 325, Test Loss: 0.3035620152950287\n",
      "Epoch 23, Batch 326, Test Loss: 0.3728863596916199\n",
      "Epoch 23, Batch 327, Test Loss: 0.5431423187255859\n",
      "Epoch 23, Batch 328, Test Loss: 0.46110641956329346\n",
      "Epoch 23, Batch 329, Test Loss: 0.518186092376709\n",
      "Epoch 23, Batch 330, Test Loss: 0.44621315598487854\n",
      "Epoch 23, Batch 331, Test Loss: 0.3953953683376312\n",
      "Epoch 23, Batch 332, Test Loss: 0.3955153226852417\n",
      "Epoch 23, Batch 333, Test Loss: 0.3684592545032501\n",
      "Epoch 23, Batch 334, Test Loss: 0.458922415971756\n",
      "Epoch 23, Batch 335, Test Loss: 0.2873002290725708\n",
      "Epoch 23, Batch 336, Test Loss: 0.3386421799659729\n",
      "Epoch 23, Batch 337, Test Loss: 0.49382784962654114\n",
      "Epoch 23, Batch 338, Test Loss: 0.7758598923683167\n",
      "Epoch 23, Batch 339, Test Loss: 0.45807722210884094\n",
      "Epoch 23, Batch 340, Test Loss: 0.3214961886405945\n",
      "Epoch 23, Batch 341, Test Loss: 0.3355010747909546\n",
      "Epoch 23, Batch 342, Test Loss: 0.493403822183609\n",
      "Epoch 23, Batch 343, Test Loss: 0.4835846424102783\n",
      "Epoch 23, Batch 344, Test Loss: 0.45602428913116455\n",
      "Epoch 23, Batch 345, Test Loss: 0.32380399107933044\n",
      "Epoch 23, Batch 346, Test Loss: 0.4956061840057373\n",
      "Epoch 23, Batch 347, Test Loss: 0.39407920837402344\n",
      "Epoch 23, Batch 348, Test Loss: 0.4582926034927368\n",
      "Epoch 23, Batch 349, Test Loss: 0.31503286957740784\n",
      "Epoch 23, Batch 350, Test Loss: 0.504450798034668\n",
      "Epoch 23, Batch 351, Test Loss: 0.6993927955627441\n",
      "Epoch 23, Batch 352, Test Loss: 0.48385798931121826\n",
      "Epoch 23, Batch 353, Test Loss: 0.3049736022949219\n",
      "Epoch 23, Batch 354, Test Loss: 0.702978789806366\n",
      "Epoch 23, Batch 355, Test Loss: 0.3228238821029663\n",
      "Epoch 23, Batch 356, Test Loss: 0.34788352251052856\n",
      "Epoch 23, Batch 357, Test Loss: 0.30675187706947327\n",
      "Epoch 23, Batch 358, Test Loss: 0.36034804582595825\n",
      "Epoch 23, Batch 359, Test Loss: 0.31071123480796814\n",
      "Epoch 23, Batch 360, Test Loss: 0.9231905341148376\n",
      "Epoch 23, Batch 361, Test Loss: 0.34633827209472656\n",
      "Epoch 23, Batch 362, Test Loss: 0.3943589925765991\n",
      "Epoch 23, Batch 363, Test Loss: 0.26200440526008606\n",
      "Epoch 23, Batch 364, Test Loss: 0.39882344007492065\n",
      "Epoch 23, Batch 365, Test Loss: 0.5343766212463379\n",
      "Epoch 23, Batch 366, Test Loss: 0.6145368218421936\n",
      "Epoch 23, Batch 367, Test Loss: 0.5021212100982666\n",
      "Epoch 23, Batch 368, Test Loss: 0.4717794358730316\n",
      "Epoch 23, Batch 369, Test Loss: 0.43772652745246887\n",
      "Epoch 23, Batch 370, Test Loss: 0.40637072920799255\n",
      "Epoch 23, Batch 371, Test Loss: 0.3535870611667633\n",
      "Epoch 23, Batch 372, Test Loss: 0.5788324475288391\n",
      "Epoch 23, Batch 373, Test Loss: 0.6738901734352112\n",
      "Epoch 23, Batch 374, Test Loss: 0.6734046339988708\n",
      "Epoch 23, Batch 375, Test Loss: 0.5881373286247253\n",
      "Epoch 23, Batch 376, Test Loss: 0.45225560665130615\n",
      "Epoch 23, Batch 377, Test Loss: 0.49304190278053284\n",
      "Epoch 23, Batch 378, Test Loss: 0.5093085169792175\n",
      "Epoch 23, Batch 379, Test Loss: 0.540568470954895\n",
      "Epoch 23, Batch 380, Test Loss: 0.33211344480514526\n",
      "Epoch 23, Batch 381, Test Loss: 0.5764676928520203\n",
      "Epoch 23, Batch 382, Test Loss: 0.30765777826309204\n",
      "Epoch 23, Batch 383, Test Loss: 0.4193480908870697\n",
      "Epoch 23, Batch 384, Test Loss: 0.32777994871139526\n",
      "Epoch 23, Batch 385, Test Loss: 0.47245001792907715\n",
      "Epoch 23, Batch 386, Test Loss: 0.3759770393371582\n",
      "Epoch 23, Batch 387, Test Loss: 0.44928935170173645\n",
      "Epoch 23, Batch 388, Test Loss: 0.47992053627967834\n",
      "Epoch 23, Batch 389, Test Loss: 0.3525351583957672\n",
      "Epoch 23, Batch 390, Test Loss: 0.3112185597419739\n",
      "Epoch 23, Batch 391, Test Loss: 0.20286178588867188\n",
      "Epoch 23, Batch 392, Test Loss: 0.4284761846065521\n",
      "Epoch 23, Batch 393, Test Loss: 0.4267599582672119\n",
      "Epoch 23, Batch 394, Test Loss: 0.5201370716094971\n",
      "Epoch 23, Batch 395, Test Loss: 0.5821026563644409\n",
      "Epoch 23, Batch 396, Test Loss: 0.5837244987487793\n",
      "Epoch 23, Batch 397, Test Loss: 0.39030808210372925\n",
      "Epoch 23, Batch 398, Test Loss: 0.4519236385822296\n",
      "Epoch 23, Batch 399, Test Loss: 0.5621709227561951\n",
      "Epoch 23, Batch 400, Test Loss: 0.46769803762435913\n",
      "Epoch 23, Batch 401, Test Loss: 0.5400338768959045\n",
      "Epoch 23, Batch 402, Test Loss: 0.4897683560848236\n",
      "Epoch 23, Batch 403, Test Loss: 0.264617383480072\n",
      "Epoch 23, Batch 404, Test Loss: 0.468364417552948\n",
      "Epoch 23, Batch 405, Test Loss: 0.5166524648666382\n",
      "Epoch 23, Batch 406, Test Loss: 0.5634992122650146\n",
      "Epoch 23, Batch 407, Test Loss: 0.4629136323928833\n",
      "Epoch 23, Batch 408, Test Loss: 0.5282630920410156\n",
      "Epoch 23, Batch 409, Test Loss: 0.5309726595878601\n",
      "Epoch 23, Batch 410, Test Loss: 0.4807467758655548\n",
      "Epoch 23, Batch 411, Test Loss: 0.2925960421562195\n",
      "Epoch 23, Batch 412, Test Loss: 0.5697116851806641\n",
      "Epoch 23, Batch 413, Test Loss: 0.5086066126823425\n",
      "Epoch 23, Batch 414, Test Loss: 0.39713218808174133\n",
      "Epoch 23, Batch 415, Test Loss: 0.3580489158630371\n",
      "Epoch 23, Batch 416, Test Loss: 0.6162253022193909\n",
      "Epoch 23, Batch 417, Test Loss: 0.4729847013950348\n",
      "Epoch 23, Batch 418, Test Loss: 0.3596316874027252\n",
      "Epoch 23, Batch 419, Test Loss: 0.6910200715065002\n",
      "Epoch 23, Batch 420, Test Loss: 0.2797296941280365\n",
      "Epoch 23, Batch 421, Test Loss: 0.28007861971855164\n",
      "Epoch 23, Batch 422, Test Loss: 0.6541862487792969\n",
      "Epoch 23, Batch 423, Test Loss: 0.6434711813926697\n",
      "Epoch 23, Batch 424, Test Loss: 0.38700971007347107\n",
      "Epoch 23, Batch 425, Test Loss: 0.4208906888961792\n",
      "Epoch 23, Batch 426, Test Loss: 0.4178561866283417\n",
      "Epoch 23, Batch 427, Test Loss: 0.3318873345851898\n",
      "Epoch 23, Batch 428, Test Loss: 0.3629467487335205\n",
      "Epoch 23, Batch 429, Test Loss: 0.37218576669692993\n",
      "Epoch 23, Batch 430, Test Loss: 0.3525136709213257\n",
      "Epoch 23, Batch 431, Test Loss: 0.47116753458976746\n",
      "Epoch 23, Batch 432, Test Loss: 0.4424514174461365\n",
      "Epoch 23, Batch 433, Test Loss: 0.3794821500778198\n",
      "Epoch 23, Batch 434, Test Loss: 0.3406805396080017\n",
      "Epoch 23, Batch 435, Test Loss: 0.3324642777442932\n",
      "Epoch 23, Batch 436, Test Loss: 0.3433375954627991\n",
      "Epoch 23, Batch 437, Test Loss: 0.3868412375450134\n",
      "Epoch 23, Batch 438, Test Loss: 0.6398824453353882\n",
      "Epoch 23, Batch 439, Test Loss: 0.5147097706794739\n",
      "Epoch 23, Batch 440, Test Loss: 0.49752479791641235\n",
      "Epoch 23, Batch 441, Test Loss: 0.3570147156715393\n",
      "Epoch 23, Batch 442, Test Loss: 0.37075433135032654\n",
      "Epoch 23, Batch 443, Test Loss: 0.4364173710346222\n",
      "Epoch 23, Batch 444, Test Loss: 0.2353874295949936\n",
      "Epoch 23, Batch 445, Test Loss: 0.5004270076751709\n",
      "Epoch 23, Batch 446, Test Loss: 0.5607844591140747\n",
      "Epoch 23, Batch 447, Test Loss: 0.261562317609787\n",
      "Epoch 23, Batch 448, Test Loss: 0.38752028346061707\n",
      "Epoch 23, Batch 449, Test Loss: 0.4664357006549835\n",
      "Epoch 23, Batch 450, Test Loss: 0.5675889849662781\n",
      "Epoch 23, Batch 451, Test Loss: 0.4537104070186615\n",
      "Epoch 23, Batch 452, Test Loss: 0.5694371461868286\n",
      "Epoch 23, Batch 453, Test Loss: 0.5918315649032593\n",
      "Epoch 23, Batch 454, Test Loss: 0.33625122904777527\n",
      "Epoch 23, Batch 455, Test Loss: 0.5634862184524536\n",
      "Epoch 23, Batch 456, Test Loss: 0.3814159035682678\n",
      "Epoch 23, Batch 457, Test Loss: 0.32282134890556335\n",
      "Epoch 23, Batch 458, Test Loss: 0.3385149836540222\n",
      "Epoch 23, Batch 459, Test Loss: 0.6337377429008484\n",
      "Epoch 23, Batch 460, Test Loss: 0.7000823616981506\n",
      "Epoch 23, Batch 461, Test Loss: 0.4326607882976532\n",
      "Epoch 23, Batch 462, Test Loss: 0.47010543942451477\n",
      "Epoch 23, Batch 463, Test Loss: 0.4815372824668884\n",
      "Epoch 23, Batch 464, Test Loss: 0.6647107601165771\n",
      "Epoch 23, Batch 465, Test Loss: 0.4865715205669403\n",
      "Epoch 23, Batch 466, Test Loss: 0.42326125502586365\n",
      "Epoch 23, Batch 467, Test Loss: 0.4059290289878845\n",
      "Epoch 23, Batch 468, Test Loss: 0.4761967957019806\n",
      "Epoch 23, Batch 469, Test Loss: 0.42554715275764465\n",
      "Epoch 23, Batch 470, Test Loss: 0.4192964732646942\n",
      "Epoch 23, Batch 471, Test Loss: 0.38853153586387634\n",
      "Epoch 23, Batch 472, Test Loss: 0.31646794080734253\n",
      "Epoch 23, Batch 473, Test Loss: 0.49732857942581177\n",
      "Epoch 23, Batch 474, Test Loss: 0.40323615074157715\n",
      "Epoch 23, Batch 475, Test Loss: 0.28297629952430725\n",
      "Epoch 23, Batch 476, Test Loss: 0.24110323190689087\n",
      "Epoch 23, Batch 477, Test Loss: 0.44380366802215576\n",
      "Epoch 23, Batch 478, Test Loss: 0.408584862947464\n",
      "Epoch 23, Batch 479, Test Loss: 0.27648013830184937\n",
      "Epoch 23, Batch 480, Test Loss: 0.41111379861831665\n",
      "Epoch 23, Batch 481, Test Loss: 0.42104268074035645\n",
      "Epoch 23, Batch 482, Test Loss: 0.4244583249092102\n",
      "Epoch 23, Batch 483, Test Loss: 0.4426707327365875\n",
      "Epoch 23, Batch 484, Test Loss: 0.4474804401397705\n",
      "Epoch 23, Batch 485, Test Loss: 0.5288286209106445\n",
      "Epoch 23, Batch 486, Test Loss: 0.4961399435997009\n",
      "Epoch 23, Batch 487, Test Loss: 0.5675632953643799\n",
      "Epoch 23, Batch 488, Test Loss: 0.3137131333351135\n",
      "Epoch 23, Batch 489, Test Loss: 0.49465951323509216\n",
      "Epoch 23, Batch 490, Test Loss: 0.5955910682678223\n",
      "Epoch 23, Batch 491, Test Loss: 0.5201950669288635\n",
      "Epoch 23, Batch 492, Test Loss: 0.5330621600151062\n",
      "Epoch 23, Batch 493, Test Loss: 0.3850008547306061\n",
      "Epoch 23, Batch 494, Test Loss: 0.31926241517066956\n",
      "Epoch 23, Batch 495, Test Loss: 0.4087185263633728\n",
      "Epoch 23, Batch 496, Test Loss: 0.5086826086044312\n",
      "Epoch 23, Batch 497, Test Loss: 0.6392739415168762\n",
      "Epoch 23, Batch 498, Test Loss: 0.41532576084136963\n",
      "Epoch 23, Batch 499, Test Loss: 0.7430493831634521\n",
      "Epoch 23, Batch 500, Test Loss: 0.5469908714294434\n",
      "Epoch 23, Batch 501, Test Loss: 0.45522207021713257\n",
      "Epoch 23, Batch 502, Test Loss: 0.24791383743286133\n",
      "Epoch 23, Batch 503, Test Loss: 0.43829238414764404\n",
      "Epoch 23, Batch 504, Test Loss: 0.28156375885009766\n",
      "Epoch 23, Batch 505, Test Loss: 0.5924806594848633\n",
      "Epoch 23, Batch 506, Test Loss: 0.6119670271873474\n",
      "Epoch 23, Batch 507, Test Loss: 0.45090988278388977\n",
      "Epoch 23, Batch 508, Test Loss: 0.482699453830719\n",
      "Epoch 23, Batch 509, Test Loss: 0.1956297904253006\n",
      "Epoch 23, Batch 510, Test Loss: 0.4716958701610565\n",
      "Epoch 23, Batch 511, Test Loss: 0.3583287000656128\n",
      "Epoch 23, Batch 512, Test Loss: 0.6260446310043335\n",
      "Epoch 23, Batch 513, Test Loss: 0.4923820495605469\n",
      "Epoch 23, Batch 514, Test Loss: 0.30045831203460693\n",
      "Epoch 23, Batch 515, Test Loss: 0.3953748345375061\n",
      "Epoch 23, Batch 516, Test Loss: 0.493214875459671\n",
      "Epoch 23, Batch 517, Test Loss: 0.40329620242118835\n",
      "Epoch 23, Batch 518, Test Loss: 0.4192323088645935\n",
      "Epoch 23, Batch 519, Test Loss: 0.4196428656578064\n",
      "Epoch 23, Batch 520, Test Loss: 0.42134034633636475\n",
      "Epoch 23, Batch 521, Test Loss: 0.41788187623023987\n",
      "Epoch 23, Batch 522, Test Loss: 0.3846326172351837\n",
      "Epoch 23, Batch 523, Test Loss: 0.5627186298370361\n",
      "Epoch 23, Batch 524, Test Loss: 0.49901479482650757\n",
      "Epoch 23, Batch 525, Test Loss: 0.3602747321128845\n",
      "Epoch 23, Batch 526, Test Loss: 0.4692234992980957\n",
      "Epoch 23, Batch 527, Test Loss: 0.40943434834480286\n",
      "Epoch 23, Batch 528, Test Loss: 0.401456356048584\n",
      "Epoch 23, Batch 529, Test Loss: 0.5856960415840149\n",
      "Epoch 23, Batch 530, Test Loss: 0.47713354229927063\n",
      "Epoch 23, Batch 531, Test Loss: 0.4193946421146393\n",
      "Epoch 23, Batch 532, Test Loss: 0.4943312406539917\n",
      "Epoch 23, Batch 533, Test Loss: 0.7152237892150879\n",
      "Epoch 23, Batch 534, Test Loss: 0.5143643021583557\n",
      "Epoch 23, Batch 535, Test Loss: 0.34870871901512146\n",
      "Epoch 23, Batch 536, Test Loss: 0.4742589592933655\n",
      "Epoch 23, Batch 537, Test Loss: 0.4586368799209595\n",
      "Epoch 23, Batch 538, Test Loss: 0.19794362783432007\n",
      "Epoch 23, Batch 539, Test Loss: 0.5681313276290894\n",
      "Epoch 23, Batch 540, Test Loss: 0.3692460060119629\n",
      "Epoch 23, Batch 541, Test Loss: 0.7490878105163574\n",
      "Epoch 23, Batch 542, Test Loss: 0.36437568068504333\n",
      "Epoch 23, Batch 543, Test Loss: 0.3706727623939514\n",
      "Epoch 23, Batch 544, Test Loss: 0.34352752566337585\n",
      "Epoch 23, Batch 545, Test Loss: 0.4070470333099365\n",
      "Epoch 23, Batch 546, Test Loss: 0.39681124687194824\n",
      "Epoch 23, Batch 547, Test Loss: 0.5116263031959534\n",
      "Epoch 23, Batch 548, Test Loss: 0.3192860782146454\n",
      "Epoch 23, Batch 549, Test Loss: 0.41511622071266174\n",
      "Epoch 23, Batch 550, Test Loss: 0.4723680317401886\n",
      "Epoch 23, Batch 551, Test Loss: 0.6287168860435486\n",
      "Epoch 23, Batch 552, Test Loss: 0.2949601113796234\n",
      "Epoch 23, Batch 553, Test Loss: 0.3695599138736725\n",
      "Epoch 23, Batch 554, Test Loss: 0.3694452941417694\n",
      "Epoch 23, Batch 555, Test Loss: 0.43446511030197144\n",
      "Epoch 23, Batch 556, Test Loss: 0.5413532853126526\n",
      "Epoch 23, Batch 557, Test Loss: 0.3064490556716919\n",
      "Epoch 23, Batch 558, Test Loss: 0.5049489736557007\n",
      "Epoch 23, Batch 559, Test Loss: 0.4142310917377472\n",
      "Epoch 23, Batch 560, Test Loss: 0.4252811670303345\n",
      "Epoch 23, Batch 561, Test Loss: 0.4879056513309479\n",
      "Epoch 23, Batch 562, Test Loss: 0.39321374893188477\n",
      "Epoch 23, Batch 563, Test Loss: 0.36260658502578735\n",
      "Epoch 23, Batch 564, Test Loss: 0.5230423808097839\n",
      "Epoch 23, Batch 565, Test Loss: 0.4228331446647644\n",
      "Epoch 23, Batch 566, Test Loss: 0.49773553013801575\n",
      "Epoch 23, Batch 567, Test Loss: 0.565471887588501\n",
      "Epoch 23, Batch 568, Test Loss: 0.33735400438308716\n",
      "Epoch 23, Batch 569, Test Loss: 0.5949463248252869\n",
      "Epoch 23, Batch 570, Test Loss: 0.5193396806716919\n",
      "Epoch 23, Batch 571, Test Loss: 0.42464953660964966\n",
      "Epoch 23, Batch 572, Test Loss: 0.4306066334247589\n",
      "Epoch 23, Batch 573, Test Loss: 0.389988511800766\n",
      "Epoch 23, Batch 574, Test Loss: 0.4247552156448364\n",
      "Epoch 23, Batch 575, Test Loss: 0.35403934121131897\n",
      "Epoch 23, Batch 576, Test Loss: 0.37608596682548523\n",
      "Epoch 23, Batch 577, Test Loss: 0.29320529103279114\n",
      "Epoch 23, Batch 578, Test Loss: 0.3091908395290375\n",
      "Epoch 23, Batch 579, Test Loss: 0.4035930335521698\n",
      "Epoch 23, Batch 580, Test Loss: 0.4780412018299103\n",
      "Epoch 23, Batch 581, Test Loss: 0.5443055033683777\n",
      "Epoch 23, Batch 582, Test Loss: 0.5677801966667175\n",
      "Epoch 23, Batch 583, Test Loss: 0.5337280035018921\n",
      "Epoch 23, Batch 584, Test Loss: 0.7774844169616699\n",
      "Epoch 23, Batch 585, Test Loss: 0.34428808093070984\n",
      "Epoch 23, Batch 586, Test Loss: 0.4083843231201172\n",
      "Epoch 23, Batch 587, Test Loss: 0.3716889023780823\n",
      "Epoch 23, Batch 588, Test Loss: 0.5851481556892395\n",
      "Epoch 23, Batch 589, Test Loss: 0.36584946513175964\n",
      "Epoch 23, Batch 590, Test Loss: 0.4792941212654114\n",
      "Epoch 23, Batch 591, Test Loss: 0.4829418659210205\n",
      "Epoch 23, Batch 592, Test Loss: 0.39958828687667847\n",
      "Epoch 23, Batch 593, Test Loss: 0.4736558794975281\n",
      "Epoch 23, Batch 594, Test Loss: 0.5364056825637817\n",
      "Epoch 23, Batch 595, Test Loss: 0.44906872510910034\n",
      "Epoch 23, Batch 596, Test Loss: 0.36946284770965576\n",
      "Epoch 23, Batch 597, Test Loss: 0.4556584358215332\n",
      "Epoch 23, Batch 598, Test Loss: 0.5961415767669678\n",
      "Epoch 23, Batch 599, Test Loss: 0.31402984261512756\n",
      "Epoch 23, Batch 600, Test Loss: 0.42715370655059814\n",
      "Epoch 23, Batch 601, Test Loss: 0.4155840575695038\n",
      "Epoch 23, Batch 602, Test Loss: 0.4914141297340393\n",
      "Epoch 23, Batch 603, Test Loss: 0.24294055998325348\n",
      "Epoch 23, Batch 604, Test Loss: 0.624779999256134\n",
      "Epoch 23, Batch 605, Test Loss: 0.3112007975578308\n",
      "Epoch 23, Batch 606, Test Loss: 0.4575800597667694\n",
      "Epoch 23, Batch 607, Test Loss: 0.43017828464508057\n",
      "Epoch 23, Batch 608, Test Loss: 0.3602305054664612\n",
      "Epoch 23, Batch 609, Test Loss: 0.23126426339149475\n",
      "Epoch 23, Batch 610, Test Loss: 0.3532218337059021\n",
      "Epoch 23, Batch 611, Test Loss: 0.47977930307388306\n",
      "Epoch 23, Batch 612, Test Loss: 0.37871426343917847\n",
      "Epoch 23, Batch 613, Test Loss: 0.6433424353599548\n",
      "Epoch 23, Batch 614, Test Loss: 0.32683905959129333\n",
      "Epoch 23, Batch 615, Test Loss: 0.49769967794418335\n",
      "Epoch 23, Batch 616, Test Loss: 0.44806861877441406\n",
      "Epoch 23, Batch 617, Test Loss: 0.4585192799568176\n",
      "Epoch 23, Batch 618, Test Loss: 0.41968628764152527\n",
      "Epoch 23, Batch 619, Test Loss: 0.6451254487037659\n",
      "Epoch 23, Batch 620, Test Loss: 0.44168663024902344\n",
      "Epoch 23, Batch 621, Test Loss: 0.3662163019180298\n",
      "Epoch 23, Batch 622, Test Loss: 0.5156124830245972\n",
      "Epoch 23, Batch 623, Test Loss: 0.36929798126220703\n",
      "Epoch 23, Batch 624, Test Loss: 0.46693384647369385\n",
      "Epoch 23, Batch 625, Test Loss: 0.3557627201080322\n",
      "Epoch 23, Batch 626, Test Loss: 0.42966821789741516\n",
      "Epoch 23, Batch 627, Test Loss: 0.40037834644317627\n",
      "Epoch 23, Batch 628, Test Loss: 0.5558774471282959\n",
      "Epoch 23, Batch 629, Test Loss: 0.3945828080177307\n",
      "Epoch 23, Batch 630, Test Loss: 0.5699754357337952\n",
      "Epoch 23, Batch 631, Test Loss: 0.38130688667297363\n",
      "Epoch 23, Batch 632, Test Loss: 0.5896191596984863\n",
      "Epoch 23, Batch 633, Test Loss: 0.3811558485031128\n",
      "Epoch 23, Batch 634, Test Loss: 0.7075034379959106\n",
      "Epoch 23, Batch 635, Test Loss: 0.6964621543884277\n",
      "Epoch 23, Batch 636, Test Loss: 0.3464052379131317\n",
      "Epoch 23, Batch 637, Test Loss: 0.466368168592453\n",
      "Epoch 23, Batch 638, Test Loss: 0.6785597205162048\n",
      "Epoch 23, Batch 639, Test Loss: 0.49524709582328796\n",
      "Epoch 23, Batch 640, Test Loss: 0.5694503784179688\n",
      "Epoch 23, Batch 641, Test Loss: 0.46988314390182495\n",
      "Epoch 23, Batch 642, Test Loss: 0.3786256015300751\n",
      "Epoch 23, Batch 643, Test Loss: 0.3958803415298462\n",
      "Epoch 23, Batch 644, Test Loss: 0.5191421508789062\n",
      "Epoch 23, Batch 645, Test Loss: 0.3397185206413269\n",
      "Epoch 23, Batch 646, Test Loss: 0.67779141664505\n",
      "Epoch 23, Batch 647, Test Loss: 0.4149962365627289\n",
      "Epoch 23, Batch 648, Test Loss: 0.39312055706977844\n",
      "Epoch 23, Batch 649, Test Loss: 0.5103906989097595\n",
      "Epoch 23, Batch 650, Test Loss: 0.396455854177475\n",
      "Epoch 23, Batch 651, Test Loss: 0.42470115423202515\n",
      "Epoch 23, Batch 652, Test Loss: 0.49352777004241943\n",
      "Epoch 23, Batch 653, Test Loss: 0.48530852794647217\n",
      "Epoch 23, Batch 654, Test Loss: 0.412448525428772\n",
      "Epoch 23, Batch 655, Test Loss: 0.38232553005218506\n",
      "Epoch 23, Batch 656, Test Loss: 0.6453746557235718\n",
      "Epoch 23, Batch 657, Test Loss: 0.6176855564117432\n",
      "Epoch 23, Batch 658, Test Loss: 0.37733787298202515\n",
      "Epoch 23, Batch 659, Test Loss: 0.38405558466911316\n",
      "Epoch 23, Batch 660, Test Loss: 0.30156370997428894\n",
      "Epoch 23, Batch 661, Test Loss: 0.34667330980300903\n",
      "Epoch 23, Batch 662, Test Loss: 0.592540979385376\n",
      "Epoch 23, Batch 663, Test Loss: 0.3476235270500183\n",
      "Epoch 23, Batch 664, Test Loss: 0.5077654123306274\n",
      "Epoch 23, Batch 665, Test Loss: 0.3078887164592743\n",
      "Epoch 23, Batch 666, Test Loss: 0.3774326741695404\n",
      "Epoch 23, Batch 667, Test Loss: 0.42961955070495605\n",
      "Epoch 23, Batch 668, Test Loss: 0.45775431394577026\n",
      "Epoch 23, Batch 669, Test Loss: 0.4090236723423004\n",
      "Epoch 23, Batch 670, Test Loss: 0.6593465805053711\n",
      "Epoch 23, Batch 671, Test Loss: 0.3959255814552307\n",
      "Epoch 23, Batch 672, Test Loss: 0.6710915565490723\n",
      "Epoch 23, Batch 673, Test Loss: 0.4848698377609253\n",
      "Epoch 23, Batch 674, Test Loss: 0.5131858587265015\n",
      "Epoch 23, Batch 675, Test Loss: 0.4858878254890442\n",
      "Epoch 23, Batch 676, Test Loss: 0.40398621559143066\n",
      "Epoch 23, Batch 677, Test Loss: 0.8720012903213501\n",
      "Epoch 23, Batch 678, Test Loss: 0.4378429055213928\n",
      "Epoch 23, Batch 679, Test Loss: 0.36916640400886536\n",
      "Epoch 23, Batch 680, Test Loss: 0.4070485532283783\n",
      "Epoch 23, Batch 681, Test Loss: 0.28830331563949585\n",
      "Epoch 23, Batch 682, Test Loss: 0.5391340255737305\n",
      "Epoch 23, Batch 683, Test Loss: 0.41255614161491394\n",
      "Epoch 23, Batch 684, Test Loss: 0.6736516952514648\n",
      "Epoch 23, Batch 685, Test Loss: 0.3099666237831116\n",
      "Epoch 23, Batch 686, Test Loss: 0.7306971549987793\n",
      "Epoch 23, Batch 687, Test Loss: 0.300259530544281\n",
      "Epoch 23, Batch 688, Test Loss: 0.5759995579719543\n",
      "Epoch 23, Batch 689, Test Loss: 0.3455285429954529\n",
      "Epoch 23, Batch 690, Test Loss: 0.2717989385128021\n",
      "Epoch 23, Batch 691, Test Loss: 0.5583449006080627\n",
      "Epoch 23, Batch 692, Test Loss: 0.517839252948761\n",
      "Epoch 23, Batch 693, Test Loss: 0.44773542881011963\n",
      "Epoch 23, Batch 694, Test Loss: 0.4500998556613922\n",
      "Epoch 23, Batch 695, Test Loss: 0.4891548156738281\n",
      "Epoch 23, Batch 696, Test Loss: 0.4259359538555145\n",
      "Epoch 23, Batch 697, Test Loss: 0.5821082592010498\n",
      "Epoch 23, Batch 698, Test Loss: 0.37689006328582764\n",
      "Epoch 23, Batch 699, Test Loss: 0.5388377904891968\n",
      "Epoch 23, Batch 700, Test Loss: 0.5408774614334106\n",
      "Epoch 23, Batch 701, Test Loss: 0.43585455417633057\n",
      "Epoch 23, Batch 702, Test Loss: 0.3238160014152527\n",
      "Epoch 23, Batch 703, Test Loss: 0.5738928318023682\n",
      "Epoch 23, Batch 704, Test Loss: 0.506766676902771\n",
      "Epoch 23, Batch 705, Test Loss: 0.43800124526023865\n",
      "Epoch 23, Batch 706, Test Loss: 0.3308328092098236\n",
      "Epoch 23, Batch 707, Test Loss: 0.6570602655410767\n",
      "Epoch 23, Batch 708, Test Loss: 0.5797432661056519\n",
      "Epoch 23, Batch 709, Test Loss: 0.6970123052597046\n",
      "Epoch 23, Batch 710, Test Loss: 0.4707114100456238\n",
      "Epoch 23, Batch 711, Test Loss: 0.4038398563861847\n",
      "Epoch 23, Batch 712, Test Loss: 0.5704209208488464\n",
      "Epoch 23, Batch 713, Test Loss: 0.5169649124145508\n",
      "Epoch 23, Batch 714, Test Loss: 0.38266080617904663\n",
      "Epoch 23, Batch 715, Test Loss: 0.48875918984413147\n",
      "Epoch 23, Batch 716, Test Loss: 0.3964686095714569\n",
      "Epoch 23, Batch 717, Test Loss: 0.4536498188972473\n",
      "Epoch 23, Batch 718, Test Loss: 0.43115079402923584\n",
      "Epoch 23, Batch 719, Test Loss: 0.5208041071891785\n",
      "Epoch 23, Batch 720, Test Loss: 0.5429834127426147\n",
      "Epoch 23, Batch 721, Test Loss: 0.5252605080604553\n",
      "Epoch 23, Batch 722, Test Loss: 0.441689670085907\n",
      "Epoch 23, Batch 723, Test Loss: 0.4799872636795044\n",
      "Epoch 23, Batch 724, Test Loss: 0.2878091633319855\n",
      "Epoch 23, Batch 725, Test Loss: 0.5450795292854309\n",
      "Epoch 23, Batch 726, Test Loss: 0.5655777454376221\n",
      "Epoch 23, Batch 727, Test Loss: 0.5324753522872925\n",
      "Epoch 23, Batch 728, Test Loss: 0.340472549200058\n",
      "Epoch 23, Batch 729, Test Loss: 0.4390904903411865\n",
      "Epoch 23, Batch 730, Test Loss: 0.434713214635849\n",
      "Epoch 23, Batch 731, Test Loss: 0.5243634581565857\n",
      "Epoch 23, Batch 732, Test Loss: 0.24098438024520874\n",
      "Epoch 23, Batch 733, Test Loss: 0.3305480480194092\n",
      "Epoch 23, Batch 734, Test Loss: 0.4690256118774414\n",
      "Epoch 23, Batch 735, Test Loss: 0.4177975058555603\n",
      "Epoch 23, Batch 736, Test Loss: 0.39426615834236145\n",
      "Epoch 23, Batch 737, Test Loss: 0.298903226852417\n",
      "Epoch 23, Batch 738, Test Loss: 0.41622623801231384\n",
      "Epoch 23, Batch 739, Test Loss: 0.47621190547943115\n",
      "Epoch 23, Batch 740, Test Loss: 0.41628408432006836\n",
      "Epoch 23, Batch 741, Test Loss: 0.324422150850296\n",
      "Epoch 23, Batch 742, Test Loss: 0.52411288022995\n",
      "Epoch 23, Batch 743, Test Loss: 0.7046786546707153\n",
      "Epoch 23, Batch 744, Test Loss: 0.6248242855072021\n",
      "Epoch 23, Batch 745, Test Loss: 0.5230592489242554\n",
      "Epoch 23, Batch 746, Test Loss: 0.5052491426467896\n",
      "Epoch 23, Batch 747, Test Loss: 0.3361400365829468\n",
      "Epoch 23, Batch 748, Test Loss: 0.39348870515823364\n",
      "Epoch 23, Batch 749, Test Loss: 0.36832183599472046\n",
      "Epoch 23, Batch 750, Test Loss: 0.23410481214523315\n",
      "Epoch 23, Batch 751, Test Loss: 0.42148488759994507\n",
      "Epoch 23, Batch 752, Test Loss: 0.4741254150867462\n",
      "Epoch 23, Batch 753, Test Loss: 0.632377564907074\n",
      "Epoch 23, Batch 754, Test Loss: 0.44253385066986084\n",
      "Epoch 23, Batch 755, Test Loss: 0.4865081310272217\n",
      "Epoch 23, Batch 756, Test Loss: 0.3195527195930481\n",
      "Epoch 23, Batch 757, Test Loss: 0.4674830734729767\n",
      "Epoch 23, Batch 758, Test Loss: 0.5213620662689209\n",
      "Epoch 23, Batch 759, Test Loss: 0.35504835844039917\n",
      "Epoch 23, Batch 760, Test Loss: 0.45664170384407043\n",
      "Epoch 23, Batch 761, Test Loss: 0.5298436284065247\n",
      "Epoch 23, Batch 762, Test Loss: 0.46537408232688904\n",
      "Epoch 23, Batch 763, Test Loss: 0.2629891633987427\n",
      "Epoch 23, Batch 764, Test Loss: 0.44788727164268494\n",
      "Epoch 23, Batch 765, Test Loss: 0.5838242173194885\n",
      "Epoch 23, Batch 766, Test Loss: 0.47810882329940796\n",
      "Epoch 23, Batch 767, Test Loss: 0.37884587049484253\n",
      "Epoch 23, Batch 768, Test Loss: 0.40409141778945923\n",
      "Epoch 23, Batch 769, Test Loss: 0.39084964990615845\n",
      "Epoch 23, Batch 770, Test Loss: 0.47761645913124084\n",
      "Epoch 23, Batch 771, Test Loss: 0.41009119153022766\n",
      "Epoch 23, Batch 772, Test Loss: 0.5006594657897949\n",
      "Epoch 23, Batch 773, Test Loss: 0.6107606887817383\n",
      "Epoch 23, Batch 774, Test Loss: 0.4748460054397583\n",
      "Epoch 23, Batch 775, Test Loss: 0.33305028080940247\n",
      "Epoch 23, Batch 776, Test Loss: 0.3725966215133667\n",
      "Epoch 23, Batch 777, Test Loss: 0.4568369686603546\n",
      "Epoch 23, Batch 778, Test Loss: 0.520749568939209\n",
      "Epoch 23, Batch 779, Test Loss: 0.5226388573646545\n",
      "Epoch 23, Batch 780, Test Loss: 0.4779265820980072\n",
      "Epoch 23, Batch 781, Test Loss: 0.5436391234397888\n",
      "Epoch 23, Batch 782, Test Loss: 0.31433308124542236\n",
      "Epoch 23, Batch 783, Test Loss: 0.45280027389526367\n",
      "Epoch 23, Batch 784, Test Loss: 0.36614343523979187\n",
      "Epoch 23, Batch 785, Test Loss: 0.654229998588562\n",
      "Epoch 23, Batch 786, Test Loss: 0.3912857174873352\n",
      "Epoch 23, Batch 787, Test Loss: 0.44542574882507324\n",
      "Epoch 23, Batch 788, Test Loss: 0.5538780689239502\n",
      "Epoch 23, Batch 789, Test Loss: 0.3066427707672119\n",
      "Epoch 23, Batch 790, Test Loss: 0.4526040256023407\n",
      "Epoch 23, Batch 791, Test Loss: 0.6122221946716309\n",
      "Epoch 23, Batch 792, Test Loss: 0.4947357773780823\n",
      "Epoch 23, Batch 793, Test Loss: 0.31805524230003357\n",
      "Epoch 23, Batch 794, Test Loss: 0.20380264520645142\n",
      "Epoch 23, Batch 795, Test Loss: 0.38004952669143677\n",
      "Epoch 23, Batch 796, Test Loss: 0.45855283737182617\n",
      "Epoch 23, Batch 797, Test Loss: 0.45590686798095703\n",
      "Epoch 23, Batch 798, Test Loss: 0.40266290307044983\n",
      "Epoch 23, Batch 799, Test Loss: 0.347314715385437\n",
      "Epoch 23, Batch 800, Test Loss: 0.4138474464416504\n",
      "Epoch 23, Batch 801, Test Loss: 0.46726223826408386\n",
      "Epoch 23, Batch 802, Test Loss: 0.46217846870422363\n",
      "Epoch 23, Batch 803, Test Loss: 0.24844220280647278\n",
      "Epoch 23, Batch 804, Test Loss: 0.5967528820037842\n",
      "Epoch 23, Batch 805, Test Loss: 0.6493117213249207\n",
      "Epoch 23, Batch 806, Test Loss: 0.29129451513290405\n",
      "Epoch 23, Batch 807, Test Loss: 0.43539318442344666\n",
      "Epoch 23, Batch 808, Test Loss: 0.5206466317176819\n",
      "Epoch 23, Batch 809, Test Loss: 0.5332725048065186\n",
      "Epoch 23, Batch 810, Test Loss: 0.4918716251850128\n",
      "Epoch 23, Batch 811, Test Loss: 0.645622730255127\n",
      "Epoch 23, Batch 812, Test Loss: 0.3652420938014984\n",
      "Epoch 23, Batch 813, Test Loss: 0.48800840973854065\n",
      "Epoch 23, Batch 814, Test Loss: 0.4987524449825287\n",
      "Epoch 23, Batch 815, Test Loss: 0.48881760239601135\n",
      "Epoch 23, Batch 816, Test Loss: 0.5139797329902649\n",
      "Epoch 23, Batch 817, Test Loss: 0.36871442198753357\n",
      "Epoch 23, Batch 818, Test Loss: 0.5677714347839355\n",
      "Epoch 23, Batch 819, Test Loss: 0.4299886226654053\n",
      "Epoch 23, Batch 820, Test Loss: 0.44209951162338257\n",
      "Epoch 23, Batch 821, Test Loss: 0.5653642416000366\n",
      "Epoch 23, Batch 822, Test Loss: 0.6264782547950745\n",
      "Epoch 23, Batch 823, Test Loss: 0.4137483239173889\n",
      "Epoch 23, Batch 824, Test Loss: 0.5051906108856201\n",
      "Epoch 23, Batch 825, Test Loss: 0.43162164092063904\n",
      "Epoch 23, Batch 826, Test Loss: 0.5475400686264038\n",
      "Epoch 23, Batch 827, Test Loss: 0.515917181968689\n",
      "Epoch 23, Batch 828, Test Loss: 0.3762781023979187\n",
      "Epoch 23, Batch 829, Test Loss: 0.3277224004268646\n",
      "Epoch 23, Batch 830, Test Loss: 0.4442155063152313\n",
      "Epoch 23, Batch 831, Test Loss: 0.5030682682991028\n",
      "Epoch 23, Batch 832, Test Loss: 0.6506854891777039\n",
      "Epoch 23, Batch 833, Test Loss: 0.5126251578330994\n",
      "Epoch 23, Batch 834, Test Loss: 0.432378351688385\n",
      "Epoch 23, Batch 835, Test Loss: 0.24718177318572998\n",
      "Epoch 23, Batch 836, Test Loss: 0.5767123699188232\n",
      "Epoch 23, Batch 837, Test Loss: 0.2778431177139282\n",
      "Epoch 23, Batch 838, Test Loss: 0.4655848741531372\n",
      "Epoch 23, Batch 839, Test Loss: 0.5542072653770447\n",
      "Epoch 23, Batch 840, Test Loss: 0.30304914712905884\n",
      "Epoch 23, Batch 841, Test Loss: 0.355374813079834\n",
      "Epoch 23, Batch 842, Test Loss: 0.48234251141548157\n",
      "Epoch 23, Batch 843, Test Loss: 0.5959129929542542\n",
      "Epoch 23, Batch 844, Test Loss: 0.7999745607376099\n",
      "Epoch 23, Batch 845, Test Loss: 0.6594836115837097\n",
      "Epoch 23, Batch 846, Test Loss: 0.4062105119228363\n",
      "Epoch 23, Batch 847, Test Loss: 0.378886878490448\n",
      "Epoch 23, Batch 848, Test Loss: 0.3441392779350281\n",
      "Epoch 23, Batch 849, Test Loss: 0.5521844625473022\n",
      "Epoch 23, Batch 850, Test Loss: 0.2935310900211334\n",
      "Epoch 23, Batch 851, Test Loss: 0.32859915494918823\n",
      "Epoch 23, Batch 852, Test Loss: 0.2526432275772095\n",
      "Epoch 23, Batch 853, Test Loss: 0.4586286246776581\n",
      "Epoch 23, Batch 854, Test Loss: 0.43788090348243713\n",
      "Epoch 23, Batch 855, Test Loss: 0.4074716567993164\n",
      "Epoch 23, Batch 856, Test Loss: 0.43766528367996216\n",
      "Epoch 23, Batch 857, Test Loss: 0.508302628993988\n",
      "Epoch 23, Batch 858, Test Loss: 0.5918959379196167\n",
      "Epoch 23, Batch 859, Test Loss: 0.617307722568512\n",
      "Epoch 23, Batch 860, Test Loss: 0.4105554223060608\n",
      "Epoch 23, Batch 861, Test Loss: 0.5547828674316406\n",
      "Epoch 23, Batch 862, Test Loss: 0.38243383169174194\n",
      "Epoch 23, Batch 863, Test Loss: 0.2938574552536011\n",
      "Epoch 23, Batch 864, Test Loss: 0.32021307945251465\n",
      "Epoch 23, Batch 865, Test Loss: 0.6231745481491089\n",
      "Epoch 23, Batch 866, Test Loss: 0.5450587272644043\n",
      "Epoch 23, Batch 867, Test Loss: 0.2670277953147888\n",
      "Epoch 23, Batch 868, Test Loss: 0.4046945571899414\n",
      "Epoch 23, Batch 869, Test Loss: 0.32628539204597473\n",
      "Epoch 23, Batch 870, Test Loss: 0.353875070810318\n",
      "Epoch 23, Batch 871, Test Loss: 0.3346075415611267\n",
      "Epoch 23, Batch 872, Test Loss: 0.397004097700119\n",
      "Epoch 23, Batch 873, Test Loss: 0.49937310814857483\n",
      "Epoch 23, Batch 874, Test Loss: 0.3268442153930664\n",
      "Epoch 23, Batch 875, Test Loss: 0.44036951661109924\n",
      "Epoch 23, Batch 876, Test Loss: 0.5087875723838806\n",
      "Epoch 23, Batch 877, Test Loss: 0.4298180043697357\n",
      "Epoch 23, Batch 878, Test Loss: 0.5346908569335938\n",
      "Epoch 23, Batch 879, Test Loss: 0.5653060674667358\n",
      "Epoch 23, Batch 880, Test Loss: 0.31117555499076843\n",
      "Epoch 23, Batch 881, Test Loss: 0.5563051104545593\n",
      "Epoch 23, Batch 882, Test Loss: 0.5785278081893921\n",
      "Epoch 23, Batch 883, Test Loss: 0.35256221890449524\n",
      "Epoch 23, Batch 884, Test Loss: 0.5908896923065186\n",
      "Epoch 23, Batch 885, Test Loss: 0.40971440076828003\n",
      "Epoch 23, Batch 886, Test Loss: 0.23359830677509308\n",
      "Epoch 23, Batch 887, Test Loss: 0.341104120016098\n",
      "Epoch 23, Batch 888, Test Loss: 0.42591995000839233\n",
      "Epoch 23, Batch 889, Test Loss: 0.37029626965522766\n",
      "Epoch 23, Batch 890, Test Loss: 0.722187876701355\n",
      "Epoch 23, Batch 891, Test Loss: 0.4671616554260254\n",
      "Epoch 23, Batch 892, Test Loss: 0.38684672117233276\n",
      "Epoch 23, Batch 893, Test Loss: 0.5401952266693115\n",
      "Epoch 23, Batch 894, Test Loss: 0.3449273109436035\n",
      "Epoch 23, Batch 895, Test Loss: 0.5922099947929382\n",
      "Epoch 23, Batch 896, Test Loss: 0.47410446405410767\n",
      "Epoch 23, Batch 897, Test Loss: 0.2915050685405731\n",
      "Epoch 23, Batch 898, Test Loss: 0.4182162284851074\n",
      "Epoch 23, Batch 899, Test Loss: 0.5878970623016357\n",
      "Epoch 23, Batch 900, Test Loss: 0.35338425636291504\n",
      "Epoch 23, Batch 901, Test Loss: 0.554534912109375\n",
      "Epoch 23, Batch 902, Test Loss: 0.3497608006000519\n",
      "Epoch 23, Batch 903, Test Loss: 0.34639179706573486\n",
      "Epoch 23, Batch 904, Test Loss: 0.5811337828636169\n",
      "Epoch 23, Batch 905, Test Loss: 0.3425626754760742\n",
      "Epoch 23, Batch 906, Test Loss: 0.624031662940979\n",
      "Epoch 23, Batch 907, Test Loss: 0.3735240399837494\n",
      "Epoch 23, Batch 908, Test Loss: 0.6133902668952942\n",
      "Epoch 23, Batch 909, Test Loss: 0.48084646463394165\n",
      "Epoch 23, Batch 910, Test Loss: 0.2768571674823761\n",
      "Epoch 23, Batch 911, Test Loss: 0.35653144121170044\n",
      "Epoch 23, Batch 912, Test Loss: 0.42106929421424866\n",
      "Epoch 23, Batch 913, Test Loss: 0.3389451205730438\n",
      "Epoch 23, Batch 914, Test Loss: 0.5987299680709839\n",
      "Epoch 23, Batch 915, Test Loss: 0.4156351089477539\n",
      "Epoch 23, Batch 916, Test Loss: 0.28142744302749634\n",
      "Epoch 23, Batch 917, Test Loss: 0.5272648334503174\n",
      "Epoch 23, Batch 918, Test Loss: 0.5273630619049072\n",
      "Epoch 23, Batch 919, Test Loss: 0.5735833048820496\n",
      "Epoch 23, Batch 920, Test Loss: 0.3948947787284851\n",
      "Epoch 23, Batch 921, Test Loss: 0.6330908536911011\n",
      "Epoch 23, Batch 922, Test Loss: 0.3606850504875183\n",
      "Epoch 23, Batch 923, Test Loss: 0.35576748847961426\n",
      "Epoch 23, Batch 924, Test Loss: 0.7084012031555176\n",
      "Epoch 23, Batch 925, Test Loss: 0.6186333298683167\n",
      "Epoch 23, Batch 926, Test Loss: 0.34603458642959595\n",
      "Epoch 23, Batch 927, Test Loss: 0.41412249207496643\n",
      "Epoch 23, Batch 928, Test Loss: 0.44689759612083435\n",
      "Epoch 23, Batch 929, Test Loss: 0.3818225860595703\n",
      "Epoch 23, Batch 930, Test Loss: 0.43842554092407227\n",
      "Epoch 23, Batch 931, Test Loss: 0.29747700691223145\n",
      "Epoch 23, Batch 932, Test Loss: 0.4955846071243286\n",
      "Epoch 23, Batch 933, Test Loss: 0.5204740166664124\n",
      "Epoch 23, Batch 934, Test Loss: 0.4276462197303772\n",
      "Epoch 23, Batch 935, Test Loss: 0.5611015558242798\n",
      "Epoch 23, Batch 936, Test Loss: 0.4826321601867676\n",
      "Epoch 23, Batch 937, Test Loss: 0.5379555821418762\n",
      "Epoch 23, Batch 938, Test Loss: 0.3440001606941223\n",
      "Accuracy of Test set: 0.83165\n",
      "Epoch 24, Batch 1, Loss: 0.5516170263290405\n",
      "Epoch 24, Batch 2, Loss: 0.36355048418045044\n",
      "Epoch 24, Batch 3, Loss: 0.3415948748588562\n",
      "Epoch 24, Batch 4, Loss: 0.2869972884654999\n",
      "Epoch 24, Batch 5, Loss: 0.44623756408691406\n",
      "Epoch 24, Batch 6, Loss: 0.42123040556907654\n",
      "Epoch 24, Batch 7, Loss: 0.33840423822402954\n",
      "Epoch 24, Batch 8, Loss: 0.21457956731319427\n",
      "Epoch 24, Batch 9, Loss: 0.42732536792755127\n",
      "Epoch 24, Batch 10, Loss: 0.26370590925216675\n",
      "Epoch 24, Batch 11, Loss: 0.3806874454021454\n",
      "Epoch 24, Batch 12, Loss: 0.32326602935791016\n",
      "Epoch 24, Batch 13, Loss: 0.40316858887672424\n",
      "Epoch 24, Batch 14, Loss: 0.3684091866016388\n",
      "Epoch 24, Batch 15, Loss: 0.43520182371139526\n",
      "Epoch 24, Batch 16, Loss: 0.2979671359062195\n",
      "Epoch 24, Batch 17, Loss: 0.5054210424423218\n",
      "Epoch 24, Batch 18, Loss: 0.3711414933204651\n",
      "Epoch 24, Batch 19, Loss: 0.2877146899700165\n",
      "Epoch 24, Batch 20, Loss: 0.4127345383167267\n",
      "Epoch 24, Batch 21, Loss: 0.3604046106338501\n",
      "Epoch 24, Batch 22, Loss: 0.2983819246292114\n",
      "Epoch 24, Batch 23, Loss: 0.407492458820343\n",
      "Epoch 24, Batch 24, Loss: 0.39833274483680725\n",
      "Epoch 24, Batch 25, Loss: 0.3391930162906647\n",
      "Epoch 24, Batch 26, Loss: 0.4214933216571808\n",
      "Epoch 24, Batch 27, Loss: 0.4341553747653961\n",
      "Epoch 24, Batch 28, Loss: 0.3953145742416382\n",
      "Epoch 24, Batch 29, Loss: 0.26264917850494385\n",
      "Epoch 24, Batch 30, Loss: 0.3418087959289551\n",
      "Epoch 24, Batch 31, Loss: 0.36440369486808777\n",
      "Epoch 24, Batch 32, Loss: 0.21111781895160675\n",
      "Epoch 24, Batch 33, Loss: 0.5870487689971924\n",
      "Epoch 24, Batch 34, Loss: 0.34659889340400696\n",
      "Epoch 24, Batch 35, Loss: 0.21995343267917633\n",
      "Epoch 24, Batch 36, Loss: 0.33598944544792175\n",
      "Epoch 24, Batch 37, Loss: 0.37698403000831604\n",
      "Epoch 24, Batch 38, Loss: 0.2432902753353119\n",
      "Epoch 24, Batch 39, Loss: 0.3695802092552185\n",
      "Epoch 24, Batch 40, Loss: 0.542898416519165\n",
      "Epoch 24, Batch 41, Loss: 0.29008492827415466\n",
      "Epoch 24, Batch 42, Loss: 0.31724852323532104\n",
      "Epoch 24, Batch 43, Loss: 0.5157927870750427\n",
      "Epoch 24, Batch 44, Loss: 0.36491066217422485\n",
      "Epoch 24, Batch 45, Loss: 0.42861300706863403\n",
      "Epoch 24, Batch 46, Loss: 0.44150567054748535\n",
      "Epoch 24, Batch 47, Loss: 0.27762454748153687\n",
      "Epoch 24, Batch 48, Loss: 0.46358412504196167\n",
      "Epoch 24, Batch 49, Loss: 0.3907877504825592\n",
      "Epoch 24, Batch 50, Loss: 0.42409929633140564\n",
      "Epoch 24, Batch 51, Loss: 0.29718348383903503\n",
      "Epoch 24, Batch 52, Loss: 0.3522462844848633\n",
      "Epoch 24, Batch 53, Loss: 0.33021265268325806\n",
      "Epoch 24, Batch 54, Loss: 0.2569828927516937\n",
      "Epoch 24, Batch 55, Loss: 0.39067283272743225\n",
      "Epoch 24, Batch 56, Loss: 0.329850971698761\n",
      "Epoch 24, Batch 57, Loss: 0.4705618619918823\n",
      "Epoch 24, Batch 58, Loss: 0.5213063955307007\n",
      "Epoch 24, Batch 59, Loss: 0.31217584013938904\n",
      "Epoch 24, Batch 60, Loss: 0.3145217299461365\n",
      "Epoch 24, Batch 61, Loss: 0.4450177550315857\n",
      "Epoch 24, Batch 62, Loss: 0.32811814546585083\n",
      "Epoch 24, Batch 63, Loss: 0.3493664860725403\n",
      "Epoch 24, Batch 64, Loss: 0.3872973322868347\n",
      "Epoch 24, Batch 65, Loss: 0.586380660533905\n",
      "Epoch 24, Batch 66, Loss: 0.23936474323272705\n",
      "Epoch 24, Batch 67, Loss: 0.2805729806423187\n",
      "Epoch 24, Batch 68, Loss: 0.37359046936035156\n",
      "Epoch 24, Batch 69, Loss: 0.37529969215393066\n",
      "Epoch 24, Batch 70, Loss: 0.25581252574920654\n",
      "Epoch 24, Batch 71, Loss: 0.4760284721851349\n",
      "Epoch 24, Batch 72, Loss: 0.5360031723976135\n",
      "Epoch 24, Batch 73, Loss: 0.2885768711566925\n",
      "Epoch 24, Batch 74, Loss: 0.4813742935657501\n",
      "Epoch 24, Batch 75, Loss: 0.41722068190574646\n",
      "Epoch 24, Batch 76, Loss: 0.43816083669662476\n",
      "Epoch 24, Batch 77, Loss: 0.32665905356407166\n",
      "Epoch 24, Batch 78, Loss: 0.5331698060035706\n",
      "Epoch 24, Batch 79, Loss: 0.33122915029525757\n",
      "Epoch 24, Batch 80, Loss: 0.46154001355171204\n",
      "Epoch 24, Batch 81, Loss: 0.3207513689994812\n",
      "Epoch 24, Batch 82, Loss: 0.49805182218551636\n",
      "Epoch 24, Batch 83, Loss: 0.3059532940387726\n",
      "Epoch 24, Batch 84, Loss: 0.24425506591796875\n",
      "Epoch 24, Batch 85, Loss: 0.3092729449272156\n",
      "Epoch 24, Batch 86, Loss: 0.280238538980484\n",
      "Epoch 24, Batch 87, Loss: 0.41288143396377563\n",
      "Epoch 24, Batch 88, Loss: 0.4194793403148651\n",
      "Epoch 24, Batch 89, Loss: 0.44818222522735596\n",
      "Epoch 24, Batch 90, Loss: 0.5488847494125366\n",
      "Epoch 24, Batch 91, Loss: 0.3937784731388092\n",
      "Epoch 24, Batch 92, Loss: 0.290060430765152\n",
      "Epoch 24, Batch 93, Loss: 0.24211378395557404\n",
      "Epoch 24, Batch 94, Loss: 0.4478873908519745\n",
      "Epoch 24, Batch 95, Loss: 0.3668142259120941\n",
      "Epoch 24, Batch 96, Loss: 0.5901426672935486\n",
      "Epoch 24, Batch 97, Loss: 0.39906883239746094\n",
      "Epoch 24, Batch 98, Loss: 0.2822456359863281\n",
      "Epoch 24, Batch 99, Loss: 0.40894678235054016\n",
      "Epoch 24, Batch 100, Loss: 0.3539198040962219\n",
      "Epoch 24, Batch 101, Loss: 0.3466174900531769\n",
      "Epoch 24, Batch 102, Loss: 0.5775682330131531\n",
      "Epoch 24, Batch 103, Loss: 0.33884748816490173\n",
      "Epoch 24, Batch 104, Loss: 0.4111052453517914\n",
      "Epoch 24, Batch 105, Loss: 0.4288505017757416\n",
      "Epoch 24, Batch 106, Loss: 0.4206427037715912\n",
      "Epoch 24, Batch 107, Loss: 0.5565976500511169\n",
      "Epoch 24, Batch 108, Loss: 0.47694724798202515\n",
      "Epoch 24, Batch 109, Loss: 0.3383444547653198\n",
      "Epoch 24, Batch 110, Loss: 0.3300069272518158\n",
      "Epoch 24, Batch 111, Loss: 0.20404767990112305\n",
      "Epoch 24, Batch 112, Loss: 0.369626522064209\n",
      "Epoch 24, Batch 113, Loss: 0.5063353776931763\n",
      "Epoch 24, Batch 114, Loss: 0.3839234411716461\n",
      "Epoch 24, Batch 115, Loss: 0.42059218883514404\n",
      "Epoch 24, Batch 116, Loss: 0.4376182556152344\n",
      "Epoch 24, Batch 117, Loss: 0.4117048978805542\n",
      "Epoch 24, Batch 118, Loss: 0.4914265275001526\n",
      "Epoch 24, Batch 119, Loss: 0.24005642533302307\n",
      "Epoch 24, Batch 120, Loss: 0.4215987026691437\n",
      "Epoch 24, Batch 121, Loss: 0.33106499910354614\n",
      "Epoch 24, Batch 122, Loss: 0.3502664566040039\n",
      "Epoch 24, Batch 123, Loss: 0.46633654832839966\n",
      "Epoch 24, Batch 124, Loss: 0.4538368880748749\n",
      "Epoch 24, Batch 125, Loss: 0.3442630171775818\n",
      "Epoch 24, Batch 126, Loss: 0.26809990406036377\n",
      "Epoch 24, Batch 127, Loss: 0.4533095061779022\n",
      "Epoch 24, Batch 128, Loss: 0.3415308892726898\n",
      "Epoch 24, Batch 129, Loss: 0.4550696015357971\n",
      "Epoch 24, Batch 130, Loss: 0.3069898784160614\n",
      "Epoch 24, Batch 131, Loss: 0.22302575409412384\n",
      "Epoch 24, Batch 132, Loss: 0.22882474958896637\n",
      "Epoch 24, Batch 133, Loss: 0.535793662071228\n",
      "Epoch 24, Batch 134, Loss: 0.4671541750431061\n",
      "Epoch 24, Batch 135, Loss: 0.29383325576782227\n",
      "Epoch 24, Batch 136, Loss: 0.40315473079681396\n",
      "Epoch 24, Batch 137, Loss: 0.5387158989906311\n",
      "Epoch 24, Batch 138, Loss: 0.4729568064212799\n",
      "Epoch 24, Batch 139, Loss: 0.4053361117839813\n",
      "Epoch 24, Batch 140, Loss: 0.4113749563694\n",
      "Epoch 24, Batch 141, Loss: 0.5383167266845703\n",
      "Epoch 24, Batch 142, Loss: 0.4858035743236542\n",
      "Epoch 24, Batch 143, Loss: 0.4325040578842163\n",
      "Epoch 24, Batch 144, Loss: 0.2900024950504303\n",
      "Epoch 24, Batch 145, Loss: 0.43871790170669556\n",
      "Epoch 24, Batch 146, Loss: 0.3292526304721832\n",
      "Epoch 24, Batch 147, Loss: 0.3028027415275574\n",
      "Epoch 24, Batch 148, Loss: 0.5275641679763794\n",
      "Epoch 24, Batch 149, Loss: 0.4863114655017853\n",
      "Epoch 24, Batch 150, Loss: 0.4820272922515869\n",
      "Epoch 24, Batch 151, Loss: 0.3761996626853943\n",
      "Epoch 24, Batch 152, Loss: 0.36949291825294495\n",
      "Epoch 24, Batch 153, Loss: 0.3262627124786377\n",
      "Epoch 24, Batch 154, Loss: 0.279672235250473\n",
      "Epoch 24, Batch 155, Loss: 0.2549181580543518\n",
      "Epoch 24, Batch 156, Loss: 0.35145992040634155\n",
      "Epoch 24, Batch 157, Loss: 0.3527466058731079\n",
      "Epoch 24, Batch 158, Loss: 0.6780170798301697\n",
      "Epoch 24, Batch 159, Loss: 0.3365130126476288\n",
      "Epoch 24, Batch 160, Loss: 0.2891482710838318\n",
      "Epoch 24, Batch 161, Loss: 0.2503761351108551\n",
      "Epoch 24, Batch 162, Loss: 0.4574110507965088\n",
      "Epoch 24, Batch 163, Loss: 0.6380399465560913\n",
      "Epoch 24, Batch 164, Loss: 0.34951460361480713\n",
      "Epoch 24, Batch 165, Loss: 0.3646465539932251\n",
      "Epoch 24, Batch 166, Loss: 0.25054365396499634\n",
      "Epoch 24, Batch 167, Loss: 0.6142456531524658\n",
      "Epoch 24, Batch 168, Loss: 0.37364450097084045\n",
      "Epoch 24, Batch 169, Loss: 0.33510565757751465\n",
      "Epoch 24, Batch 170, Loss: 0.3927296996116638\n",
      "Epoch 24, Batch 171, Loss: 0.3220185339450836\n",
      "Epoch 24, Batch 172, Loss: 0.3788282871246338\n",
      "Epoch 24, Batch 173, Loss: 0.28542977571487427\n",
      "Epoch 24, Batch 174, Loss: 0.36384740471839905\n",
      "Epoch 24, Batch 175, Loss: 0.25527462363243103\n",
      "Epoch 24, Batch 176, Loss: 0.4177454710006714\n",
      "Epoch 24, Batch 177, Loss: 0.35799166560173035\n",
      "Epoch 24, Batch 178, Loss: 0.4564989507198334\n",
      "Epoch 24, Batch 179, Loss: 0.3715669512748718\n",
      "Epoch 24, Batch 180, Loss: 0.4346477687358856\n",
      "Epoch 24, Batch 181, Loss: 0.37322112917900085\n",
      "Epoch 24, Batch 182, Loss: 0.25979846715927124\n",
      "Epoch 24, Batch 183, Loss: 0.39224597811698914\n",
      "Epoch 24, Batch 184, Loss: 0.3451228141784668\n",
      "Epoch 24, Batch 185, Loss: 0.2039703130722046\n",
      "Epoch 24, Batch 186, Loss: 0.2543189227581024\n",
      "Epoch 24, Batch 187, Loss: 0.26148244738578796\n",
      "Epoch 24, Batch 188, Loss: 0.39329519867897034\n",
      "Epoch 24, Batch 189, Loss: 0.3400726318359375\n",
      "Epoch 24, Batch 190, Loss: 0.4066753685474396\n",
      "Epoch 24, Batch 191, Loss: 0.40339815616607666\n",
      "Epoch 24, Batch 192, Loss: 0.21470695734024048\n",
      "Epoch 24, Batch 193, Loss: 0.32142335176467896\n",
      "Epoch 24, Batch 194, Loss: 0.26712116599082947\n",
      "Epoch 24, Batch 195, Loss: 0.6216026544570923\n",
      "Epoch 24, Batch 196, Loss: 0.34601810574531555\n",
      "Epoch 24, Batch 197, Loss: 0.561697781085968\n",
      "Epoch 24, Batch 198, Loss: 0.35817664861679077\n",
      "Epoch 24, Batch 199, Loss: 0.6046848893165588\n",
      "Epoch 24, Batch 200, Loss: 0.3599167466163635\n",
      "Epoch 24, Batch 201, Loss: 0.4416848123073578\n",
      "Epoch 24, Batch 202, Loss: 0.33540740609169006\n",
      "Epoch 24, Batch 203, Loss: 0.45084214210510254\n",
      "Epoch 24, Batch 204, Loss: 0.30832505226135254\n",
      "Epoch 24, Batch 205, Loss: 0.44776031374931335\n",
      "Epoch 24, Batch 206, Loss: 0.48155832290649414\n",
      "Epoch 24, Batch 207, Loss: 0.28422749042510986\n",
      "Epoch 24, Batch 208, Loss: 0.2640305757522583\n",
      "Epoch 24, Batch 209, Loss: 0.6048738956451416\n",
      "Epoch 24, Batch 210, Loss: 0.5013604760169983\n",
      "Epoch 24, Batch 211, Loss: 0.20588532090187073\n",
      "Epoch 24, Batch 212, Loss: 0.3401455879211426\n",
      "Epoch 24, Batch 213, Loss: 0.22684279084205627\n",
      "Epoch 24, Batch 214, Loss: 0.27338817715644836\n",
      "Epoch 24, Batch 215, Loss: 0.36709436774253845\n",
      "Epoch 24, Batch 216, Loss: 0.38488659262657166\n",
      "Epoch 24, Batch 217, Loss: 0.35941416025161743\n",
      "Epoch 24, Batch 218, Loss: 0.3075379729270935\n",
      "Epoch 24, Batch 219, Loss: 0.44714340567588806\n",
      "Epoch 24, Batch 220, Loss: 0.40816840529441833\n",
      "Epoch 24, Batch 221, Loss: 0.5253966450691223\n",
      "Epoch 24, Batch 222, Loss: 0.4611920714378357\n",
      "Epoch 24, Batch 223, Loss: 0.37364262342453003\n",
      "Epoch 24, Batch 224, Loss: 0.31922855973243713\n",
      "Epoch 24, Batch 225, Loss: 0.2942628860473633\n",
      "Epoch 24, Batch 226, Loss: 0.35609573125839233\n",
      "Epoch 24, Batch 227, Loss: 0.5678449869155884\n",
      "Epoch 24, Batch 228, Loss: 0.3811992406845093\n",
      "Epoch 24, Batch 229, Loss: 0.5509833097457886\n",
      "Epoch 24, Batch 230, Loss: 0.40119099617004395\n",
      "Epoch 24, Batch 231, Loss: 0.37735262513160706\n",
      "Epoch 24, Batch 232, Loss: 0.39303818345069885\n",
      "Epoch 24, Batch 233, Loss: 0.3270024359226227\n",
      "Epoch 24, Batch 234, Loss: 0.18943160772323608\n",
      "Epoch 24, Batch 235, Loss: 0.3546186089515686\n",
      "Epoch 24, Batch 236, Loss: 0.3797212839126587\n",
      "Epoch 24, Batch 237, Loss: 0.5191310048103333\n",
      "Epoch 24, Batch 238, Loss: 0.2812877893447876\n",
      "Epoch 24, Batch 239, Loss: 0.45850008726119995\n",
      "Epoch 24, Batch 240, Loss: 0.2559281587600708\n",
      "Epoch 24, Batch 241, Loss: 0.31140613555908203\n",
      "Epoch 24, Batch 242, Loss: 0.3960093855857849\n",
      "Epoch 24, Batch 243, Loss: 0.3016263246536255\n",
      "Epoch 24, Batch 244, Loss: 0.47013604640960693\n",
      "Epoch 24, Batch 245, Loss: 0.4834864139556885\n",
      "Epoch 24, Batch 246, Loss: 0.3349634110927582\n",
      "Epoch 24, Batch 247, Loss: 0.35665395855903625\n",
      "Epoch 24, Batch 248, Loss: 0.33284488320350647\n",
      "Epoch 24, Batch 249, Loss: 0.3090660870075226\n",
      "Epoch 24, Batch 250, Loss: 0.3785559833049774\n",
      "Epoch 24, Batch 251, Loss: 0.2986176908016205\n",
      "Epoch 24, Batch 252, Loss: 0.25936999917030334\n",
      "Epoch 24, Batch 253, Loss: 0.3315301239490509\n",
      "Epoch 24, Batch 254, Loss: 0.465887188911438\n",
      "Epoch 24, Batch 255, Loss: 0.4737952649593353\n",
      "Epoch 24, Batch 256, Loss: 0.4148271083831787\n",
      "Epoch 24, Batch 257, Loss: 0.5268272757530212\n",
      "Epoch 24, Batch 258, Loss: 0.5885152220726013\n",
      "Epoch 24, Batch 259, Loss: 0.34395498037338257\n",
      "Epoch 24, Batch 260, Loss: 0.4577438235282898\n",
      "Epoch 24, Batch 261, Loss: 0.3152717351913452\n",
      "Epoch 24, Batch 262, Loss: 0.27659711241722107\n",
      "Epoch 24, Batch 263, Loss: 0.2347967028617859\n",
      "Epoch 24, Batch 264, Loss: 0.5117852091789246\n",
      "Epoch 24, Batch 265, Loss: 0.29196882247924805\n",
      "Epoch 24, Batch 266, Loss: 0.2342091202735901\n",
      "Epoch 24, Batch 267, Loss: 0.4327853322029114\n",
      "Epoch 24, Batch 268, Loss: 0.36808136105537415\n",
      "Epoch 24, Batch 269, Loss: 0.3604055345058441\n",
      "Epoch 24, Batch 270, Loss: 0.31545788049697876\n",
      "Epoch 24, Batch 271, Loss: 0.3334086537361145\n",
      "Epoch 24, Batch 272, Loss: 0.30450955033302307\n",
      "Epoch 24, Batch 273, Loss: 0.3781060576438904\n",
      "Epoch 24, Batch 274, Loss: 0.37498778104782104\n",
      "Epoch 24, Batch 275, Loss: 0.4466687738895416\n",
      "Epoch 24, Batch 276, Loss: 0.24137108027935028\n",
      "Epoch 24, Batch 277, Loss: 0.39578741788864136\n",
      "Epoch 24, Batch 278, Loss: 0.4580102264881134\n",
      "Epoch 24, Batch 279, Loss: 0.34248116612434387\n",
      "Epoch 24, Batch 280, Loss: 0.3697393536567688\n",
      "Epoch 24, Batch 281, Loss: 0.24108263850212097\n",
      "Epoch 24, Batch 282, Loss: 0.30106842517852783\n",
      "Epoch 24, Batch 283, Loss: 0.2992321252822876\n",
      "Epoch 24, Batch 284, Loss: 0.5184551477432251\n",
      "Epoch 24, Batch 285, Loss: 0.2792341709136963\n",
      "Epoch 24, Batch 286, Loss: 0.2667393684387207\n",
      "Epoch 24, Batch 287, Loss: 0.3321300148963928\n",
      "Epoch 24, Batch 288, Loss: 0.41487917304039\n",
      "Epoch 24, Batch 289, Loss: 0.45120978355407715\n",
      "Epoch 24, Batch 290, Loss: 0.4926720857620239\n",
      "Epoch 24, Batch 291, Loss: 0.23870110511779785\n",
      "Epoch 24, Batch 292, Loss: 0.42755210399627686\n",
      "Epoch 24, Batch 293, Loss: 0.3984876871109009\n",
      "Epoch 24, Batch 294, Loss: 0.4465588331222534\n",
      "Epoch 24, Batch 295, Loss: 0.4108620584011078\n",
      "Epoch 24, Batch 296, Loss: 0.41627323627471924\n",
      "Epoch 24, Batch 297, Loss: 0.3692600429058075\n",
      "Epoch 24, Batch 298, Loss: 0.2924644351005554\n",
      "Epoch 24, Batch 299, Loss: 0.21935111284255981\n",
      "Epoch 24, Batch 300, Loss: 0.31074953079223633\n",
      "Epoch 24, Batch 301, Loss: 0.3347235321998596\n",
      "Epoch 24, Batch 302, Loss: 0.269999623298645\n",
      "Epoch 24, Batch 303, Loss: 0.4370167851448059\n",
      "Epoch 24, Batch 304, Loss: 0.41144758462905884\n",
      "Epoch 24, Batch 305, Loss: 0.22586950659751892\n",
      "Epoch 24, Batch 306, Loss: 0.4621885418891907\n",
      "Epoch 24, Batch 307, Loss: 0.375039666891098\n",
      "Epoch 24, Batch 308, Loss: 0.5366049408912659\n",
      "Epoch 24, Batch 309, Loss: 0.4926173985004425\n",
      "Epoch 24, Batch 310, Loss: 0.3515988886356354\n",
      "Epoch 24, Batch 311, Loss: 0.5259722471237183\n",
      "Epoch 24, Batch 312, Loss: 0.3452480137348175\n",
      "Epoch 24, Batch 313, Loss: 0.4709460139274597\n",
      "Epoch 24, Batch 314, Loss: 0.5329870581626892\n",
      "Epoch 24, Batch 315, Loss: 0.4855485260486603\n",
      "Epoch 24, Batch 316, Loss: 0.2640237510204315\n",
      "Epoch 24, Batch 317, Loss: 0.3363175094127655\n",
      "Epoch 24, Batch 318, Loss: 0.3648265302181244\n",
      "Epoch 24, Batch 319, Loss: 0.21748727560043335\n",
      "Epoch 24, Batch 320, Loss: 0.519504964351654\n",
      "Epoch 24, Batch 321, Loss: 0.47694599628448486\n",
      "Epoch 24, Batch 322, Loss: 0.38029998540878296\n",
      "Epoch 24, Batch 323, Loss: 0.4945448338985443\n",
      "Epoch 24, Batch 324, Loss: 0.3746528625488281\n",
      "Epoch 24, Batch 325, Loss: 0.38648509979248047\n",
      "Epoch 24, Batch 326, Loss: 0.5413954257965088\n",
      "Epoch 24, Batch 327, Loss: 0.4410017728805542\n",
      "Epoch 24, Batch 328, Loss: 0.22634091973304749\n",
      "Epoch 24, Batch 329, Loss: 0.3070266842842102\n",
      "Epoch 24, Batch 330, Loss: 0.377873033285141\n",
      "Epoch 24, Batch 331, Loss: 0.2707842290401459\n",
      "Epoch 24, Batch 332, Loss: 0.42136505246162415\n",
      "Epoch 24, Batch 333, Loss: 0.23434767127037048\n",
      "Epoch 24, Batch 334, Loss: 0.19114135205745697\n",
      "Epoch 24, Batch 335, Loss: 0.3602145314216614\n",
      "Epoch 24, Batch 336, Loss: 0.4634719491004944\n",
      "Epoch 24, Batch 337, Loss: 0.354347288608551\n",
      "Epoch 24, Batch 338, Loss: 0.31378626823425293\n",
      "Epoch 24, Batch 339, Loss: 0.49012428522109985\n",
      "Epoch 24, Batch 340, Loss: 0.34701424837112427\n",
      "Epoch 24, Batch 341, Loss: 0.3849128484725952\n",
      "Epoch 24, Batch 342, Loss: 0.3986915946006775\n",
      "Epoch 24, Batch 343, Loss: 0.1570032835006714\n",
      "Epoch 24, Batch 344, Loss: 0.3472110629081726\n",
      "Epoch 24, Batch 345, Loss: 0.3091263175010681\n",
      "Epoch 24, Batch 346, Loss: 0.26390311121940613\n",
      "Epoch 24, Batch 347, Loss: 0.5293674468994141\n",
      "Epoch 24, Batch 348, Loss: 0.24139000475406647\n",
      "Epoch 24, Batch 349, Loss: 0.4558199942111969\n",
      "Epoch 24, Batch 350, Loss: 0.3201606273651123\n",
      "Epoch 24, Batch 351, Loss: 0.47868257761001587\n",
      "Epoch 24, Batch 352, Loss: 0.7065754532814026\n",
      "Epoch 24, Batch 353, Loss: 0.3745696246623993\n",
      "Epoch 24, Batch 354, Loss: 0.24665547907352448\n",
      "Epoch 24, Batch 355, Loss: 0.35630619525909424\n",
      "Epoch 24, Batch 356, Loss: 0.29539164900779724\n",
      "Epoch 24, Batch 357, Loss: 0.356969952583313\n",
      "Epoch 24, Batch 358, Loss: 0.46963387727737427\n",
      "Epoch 24, Batch 359, Loss: 0.42684221267700195\n",
      "Epoch 24, Batch 360, Loss: 0.24529460072517395\n",
      "Epoch 24, Batch 361, Loss: 0.5393563508987427\n",
      "Epoch 24, Batch 362, Loss: 0.30378684401512146\n",
      "Epoch 24, Batch 363, Loss: 0.5485363006591797\n",
      "Epoch 24, Batch 364, Loss: 0.3666040301322937\n",
      "Epoch 24, Batch 365, Loss: 0.5099158883094788\n",
      "Epoch 24, Batch 366, Loss: 0.42782869935035706\n",
      "Epoch 24, Batch 367, Loss: 0.2579044699668884\n",
      "Epoch 24, Batch 368, Loss: 0.40529733896255493\n",
      "Epoch 24, Batch 369, Loss: 0.5108999013900757\n",
      "Epoch 24, Batch 370, Loss: 0.31816524267196655\n",
      "Epoch 24, Batch 371, Loss: 0.29249244928359985\n",
      "Epoch 24, Batch 372, Loss: 0.5020778179168701\n",
      "Epoch 24, Batch 373, Loss: 0.3740018606185913\n",
      "Epoch 24, Batch 374, Loss: 0.3735687732696533\n",
      "Epoch 24, Batch 375, Loss: 0.44353723526000977\n",
      "Epoch 24, Batch 376, Loss: 0.20307697355747223\n",
      "Epoch 24, Batch 377, Loss: 0.39004045724868774\n",
      "Epoch 24, Batch 378, Loss: 0.6174558997154236\n",
      "Epoch 24, Batch 379, Loss: 0.4050373136997223\n",
      "Epoch 24, Batch 380, Loss: 0.3311329782009125\n",
      "Epoch 24, Batch 381, Loss: 0.2537618577480316\n",
      "Epoch 24, Batch 382, Loss: 0.3859795331954956\n",
      "Epoch 24, Batch 383, Loss: 0.3266719579696655\n",
      "Epoch 24, Batch 384, Loss: 0.255062997341156\n",
      "Epoch 24, Batch 385, Loss: 0.2876524031162262\n",
      "Epoch 24, Batch 386, Loss: 0.311639666557312\n",
      "Epoch 24, Batch 387, Loss: 0.45113030076026917\n",
      "Epoch 24, Batch 388, Loss: 0.4544505476951599\n",
      "Epoch 24, Batch 389, Loss: 0.37296393513679504\n",
      "Epoch 24, Batch 390, Loss: 0.36448463797569275\n",
      "Epoch 24, Batch 391, Loss: 0.436822772026062\n",
      "Epoch 24, Batch 392, Loss: 0.31371843814849854\n",
      "Epoch 24, Batch 393, Loss: 0.43419939279556274\n",
      "Epoch 24, Batch 394, Loss: 0.4105335474014282\n",
      "Epoch 24, Batch 395, Loss: 0.4929761290550232\n",
      "Epoch 24, Batch 396, Loss: 0.2534686028957367\n",
      "Epoch 24, Batch 397, Loss: 0.4752219319343567\n",
      "Epoch 24, Batch 398, Loss: 0.49427157640457153\n",
      "Epoch 24, Batch 399, Loss: 0.45711779594421387\n",
      "Epoch 24, Batch 400, Loss: 0.2577129006385803\n",
      "Epoch 24, Batch 401, Loss: 0.5769366025924683\n",
      "Epoch 24, Batch 402, Loss: 0.38546282052993774\n",
      "Epoch 24, Batch 403, Loss: 0.27383366227149963\n",
      "Epoch 24, Batch 404, Loss: 0.4427229166030884\n",
      "Epoch 24, Batch 405, Loss: 0.5597048997879028\n",
      "Epoch 24, Batch 406, Loss: 0.40638917684555054\n",
      "Epoch 24, Batch 407, Loss: 0.44329190254211426\n",
      "Epoch 24, Batch 408, Loss: 0.32254359126091003\n",
      "Epoch 24, Batch 409, Loss: 0.5398570895195007\n",
      "Epoch 24, Batch 410, Loss: 0.38980361819267273\n",
      "Epoch 24, Batch 411, Loss: 0.28969013690948486\n",
      "Epoch 24, Batch 412, Loss: 0.4791187047958374\n",
      "Epoch 24, Batch 413, Loss: 0.3664988875389099\n",
      "Epoch 24, Batch 414, Loss: 0.43224379420280457\n",
      "Epoch 24, Batch 415, Loss: 0.3293578028678894\n",
      "Epoch 24, Batch 416, Loss: 0.38480955362319946\n",
      "Epoch 24, Batch 417, Loss: 0.5067748427391052\n",
      "Epoch 24, Batch 418, Loss: 0.5854124426841736\n",
      "Epoch 24, Batch 419, Loss: 0.45022904872894287\n",
      "Epoch 24, Batch 420, Loss: 0.5235275030136108\n",
      "Epoch 24, Batch 421, Loss: 0.4170214533805847\n",
      "Epoch 24, Batch 422, Loss: 0.44023793935775757\n",
      "Epoch 24, Batch 423, Loss: 0.28587526082992554\n",
      "Epoch 24, Batch 424, Loss: 0.416706919670105\n",
      "Epoch 24, Batch 425, Loss: 0.3844277858734131\n",
      "Epoch 24, Batch 426, Loss: 0.29367557168006897\n",
      "Epoch 24, Batch 427, Loss: 0.32895392179489136\n",
      "Epoch 24, Batch 428, Loss: 0.38349252939224243\n",
      "Epoch 24, Batch 429, Loss: 0.4607597589492798\n",
      "Epoch 24, Batch 430, Loss: 0.47766822576522827\n",
      "Epoch 24, Batch 431, Loss: 0.41351833939552307\n",
      "Epoch 24, Batch 432, Loss: 0.4011146128177643\n",
      "Epoch 24, Batch 433, Loss: 0.3044268488883972\n",
      "Epoch 24, Batch 434, Loss: 0.46633055806159973\n",
      "Epoch 24, Batch 435, Loss: 0.38548344373703003\n",
      "Epoch 24, Batch 436, Loss: 0.2894268035888672\n",
      "Epoch 24, Batch 437, Loss: 0.4681612551212311\n",
      "Epoch 24, Batch 438, Loss: 0.4856986105442047\n",
      "Epoch 24, Batch 439, Loss: 0.5089958310127258\n",
      "Epoch 24, Batch 440, Loss: 0.4747837781906128\n",
      "Epoch 24, Batch 441, Loss: 0.3556652367115021\n",
      "Epoch 24, Batch 442, Loss: 0.41043975949287415\n",
      "Epoch 24, Batch 443, Loss: 0.2348286658525467\n",
      "Epoch 24, Batch 444, Loss: 0.4604780375957489\n",
      "Epoch 24, Batch 445, Loss: 0.3899022340774536\n",
      "Epoch 24, Batch 446, Loss: 0.5470359921455383\n",
      "Epoch 24, Batch 447, Loss: 0.26575979590415955\n",
      "Epoch 24, Batch 448, Loss: 0.24247115850448608\n",
      "Epoch 24, Batch 449, Loss: 0.2846720218658447\n",
      "Epoch 24, Batch 450, Loss: 0.44505101442337036\n",
      "Epoch 24, Batch 451, Loss: 0.34150153398513794\n",
      "Epoch 24, Batch 452, Loss: 0.38360223174095154\n",
      "Epoch 24, Batch 453, Loss: 0.4748431444168091\n",
      "Epoch 24, Batch 454, Loss: 0.33707067370414734\n",
      "Epoch 24, Batch 455, Loss: 0.3923477828502655\n",
      "Epoch 24, Batch 456, Loss: 0.3740091919898987\n",
      "Epoch 24, Batch 457, Loss: 0.1977253258228302\n",
      "Epoch 24, Batch 458, Loss: 0.35347187519073486\n",
      "Epoch 24, Batch 459, Loss: 0.5081682801246643\n",
      "Epoch 24, Batch 460, Loss: 0.23644356429576874\n",
      "Epoch 24, Batch 461, Loss: 0.3102998435497284\n",
      "Epoch 24, Batch 462, Loss: 0.4663478136062622\n",
      "Epoch 24, Batch 463, Loss: 0.2130035012960434\n",
      "Epoch 24, Batch 464, Loss: 0.5671775937080383\n",
      "Epoch 24, Batch 465, Loss: 0.3401791453361511\n",
      "Epoch 24, Batch 466, Loss: 0.3607781231403351\n",
      "Epoch 24, Batch 467, Loss: 0.42025700211524963\n",
      "Epoch 24, Batch 468, Loss: 0.30989760160446167\n",
      "Epoch 24, Batch 469, Loss: 0.4259624779224396\n",
      "Epoch 24, Batch 470, Loss: 0.48403018712997437\n",
      "Epoch 24, Batch 471, Loss: 0.4346391558647156\n",
      "Epoch 24, Batch 472, Loss: 0.4074876010417938\n",
      "Epoch 24, Batch 473, Loss: 0.23038926720619202\n",
      "Epoch 24, Batch 474, Loss: 0.41390693187713623\n",
      "Epoch 24, Batch 475, Loss: 0.36860382556915283\n",
      "Epoch 24, Batch 476, Loss: 0.37558838725090027\n",
      "Epoch 24, Batch 477, Loss: 0.2449830323457718\n",
      "Epoch 24, Batch 478, Loss: 0.4561053216457367\n",
      "Epoch 24, Batch 479, Loss: 0.30946144461631775\n",
      "Epoch 24, Batch 480, Loss: 0.29110610485076904\n",
      "Epoch 24, Batch 481, Loss: 0.3088788390159607\n",
      "Epoch 24, Batch 482, Loss: 0.2065514773130417\n",
      "Epoch 24, Batch 483, Loss: 0.24314381182193756\n",
      "Epoch 24, Batch 484, Loss: 0.23059259355068207\n",
      "Epoch 24, Batch 485, Loss: 0.34502989053726196\n",
      "Epoch 24, Batch 486, Loss: 0.26726266741752625\n",
      "Epoch 24, Batch 487, Loss: 0.43480637669563293\n",
      "Epoch 24, Batch 488, Loss: 0.27259552478790283\n",
      "Epoch 24, Batch 489, Loss: 0.22306570410728455\n",
      "Epoch 24, Batch 490, Loss: 0.37443798780441284\n",
      "Epoch 24, Batch 491, Loss: 0.2594679594039917\n",
      "Epoch 24, Batch 492, Loss: 0.5299962759017944\n",
      "Epoch 24, Batch 493, Loss: 0.35508671402931213\n",
      "Epoch 24, Batch 494, Loss: 0.4046438932418823\n",
      "Epoch 24, Batch 495, Loss: 0.4180777072906494\n",
      "Epoch 24, Batch 496, Loss: 0.362697958946228\n",
      "Epoch 24, Batch 497, Loss: 0.20865891873836517\n",
      "Epoch 24, Batch 498, Loss: 0.31509366631507874\n",
      "Epoch 24, Batch 499, Loss: 0.39266976714134216\n",
      "Epoch 24, Batch 500, Loss: 0.40003013610839844\n",
      "Epoch 24, Batch 501, Loss: 0.30615198612213135\n",
      "Epoch 24, Batch 502, Loss: 0.478894978761673\n",
      "Epoch 24, Batch 503, Loss: 0.3490792512893677\n",
      "Epoch 24, Batch 504, Loss: 0.46772366762161255\n",
      "Epoch 24, Batch 505, Loss: 0.19155263900756836\n",
      "Epoch 24, Batch 506, Loss: 0.24579764902591705\n",
      "Epoch 24, Batch 507, Loss: 0.40599170327186584\n",
      "Epoch 24, Batch 508, Loss: 0.4314604699611664\n",
      "Epoch 24, Batch 509, Loss: 0.19479268789291382\n",
      "Epoch 24, Batch 510, Loss: 0.29152631759643555\n",
      "Epoch 24, Batch 511, Loss: 0.32742035388946533\n",
      "Epoch 24, Batch 512, Loss: 0.30810531973838806\n",
      "Epoch 24, Batch 513, Loss: 0.19152002036571503\n",
      "Epoch 24, Batch 514, Loss: 0.6224753856658936\n",
      "Epoch 24, Batch 515, Loss: 0.5033573508262634\n",
      "Epoch 24, Batch 516, Loss: 0.31788602471351624\n",
      "Epoch 24, Batch 517, Loss: 0.4268627166748047\n",
      "Epoch 24, Batch 518, Loss: 0.27407512068748474\n",
      "Epoch 24, Batch 519, Loss: 0.40889090299606323\n",
      "Epoch 24, Batch 520, Loss: 0.33696192502975464\n",
      "Epoch 24, Batch 521, Loss: 0.4354923665523529\n",
      "Epoch 24, Batch 522, Loss: 0.32727161049842834\n",
      "Epoch 24, Batch 523, Loss: 0.39800792932510376\n",
      "Epoch 24, Batch 524, Loss: 0.21722888946533203\n",
      "Epoch 24, Batch 525, Loss: 0.2712762653827667\n",
      "Epoch 24, Batch 526, Loss: 0.33930307626724243\n",
      "Epoch 24, Batch 527, Loss: 0.27071255445480347\n",
      "Epoch 24, Batch 528, Loss: 0.37631452083587646\n",
      "Epoch 24, Batch 529, Loss: 0.36457517743110657\n",
      "Epoch 24, Batch 530, Loss: 0.25262919068336487\n",
      "Epoch 24, Batch 531, Loss: 0.33615079522132874\n",
      "Epoch 24, Batch 532, Loss: 0.3141362965106964\n",
      "Epoch 24, Batch 533, Loss: 0.48050498962402344\n",
      "Epoch 24, Batch 534, Loss: 0.3890685439109802\n",
      "Epoch 24, Batch 535, Loss: 0.5441941022872925\n",
      "Epoch 24, Batch 536, Loss: 0.3376881778240204\n",
      "Epoch 24, Batch 537, Loss: 0.5391163229942322\n",
      "Epoch 24, Batch 538, Loss: 0.3350486755371094\n",
      "Epoch 24, Batch 539, Loss: 0.24298855662345886\n",
      "Epoch 24, Batch 540, Loss: 0.5090683698654175\n",
      "Epoch 24, Batch 541, Loss: 0.4428846836090088\n",
      "Epoch 24, Batch 542, Loss: 0.35966116189956665\n",
      "Epoch 24, Batch 543, Loss: 0.39831486344337463\n",
      "Epoch 24, Batch 544, Loss: 0.3064916133880615\n",
      "Epoch 24, Batch 545, Loss: 0.2594931423664093\n",
      "Epoch 24, Batch 546, Loss: 0.4281724989414215\n",
      "Epoch 24, Batch 547, Loss: 0.4674776792526245\n",
      "Epoch 24, Batch 548, Loss: 0.4584660530090332\n",
      "Epoch 24, Batch 549, Loss: 0.2985706627368927\n",
      "Epoch 24, Batch 550, Loss: 0.33884552121162415\n",
      "Epoch 24, Batch 551, Loss: 0.4887734353542328\n",
      "Epoch 24, Batch 552, Loss: 0.3968152701854706\n",
      "Epoch 24, Batch 553, Loss: 0.4444350302219391\n",
      "Epoch 24, Batch 554, Loss: 0.5027651786804199\n",
      "Epoch 24, Batch 555, Loss: 0.26090121269226074\n",
      "Epoch 24, Batch 556, Loss: 0.5807763338088989\n",
      "Epoch 24, Batch 557, Loss: 0.46779900789260864\n",
      "Epoch 24, Batch 558, Loss: 0.5016772150993347\n",
      "Epoch 24, Batch 559, Loss: 0.5502753853797913\n",
      "Epoch 24, Batch 560, Loss: 0.38012993335723877\n",
      "Epoch 24, Batch 561, Loss: 0.3610800802707672\n",
      "Epoch 24, Batch 562, Loss: 0.48508989810943604\n",
      "Epoch 24, Batch 563, Loss: 0.4331416189670563\n",
      "Epoch 24, Batch 564, Loss: 0.27672603726387024\n",
      "Epoch 24, Batch 565, Loss: 0.29736649990081787\n",
      "Epoch 24, Batch 566, Loss: 0.28244447708129883\n",
      "Epoch 24, Batch 567, Loss: 0.5610226392745972\n",
      "Epoch 24, Batch 568, Loss: 0.42550066113471985\n",
      "Epoch 24, Batch 569, Loss: 0.4447970390319824\n",
      "Epoch 24, Batch 570, Loss: 0.4450663626194\n",
      "Epoch 24, Batch 571, Loss: 0.40903419256210327\n",
      "Epoch 24, Batch 572, Loss: 0.5115956664085388\n",
      "Epoch 24, Batch 573, Loss: 0.2922724485397339\n",
      "Epoch 24, Batch 574, Loss: 0.37803253531455994\n",
      "Epoch 24, Batch 575, Loss: 0.2622586488723755\n",
      "Epoch 24, Batch 576, Loss: 0.43275579810142517\n",
      "Epoch 24, Batch 577, Loss: 0.43559470772743225\n",
      "Epoch 24, Batch 578, Loss: 0.4206489324569702\n",
      "Epoch 24, Batch 579, Loss: 0.22995193302631378\n",
      "Epoch 24, Batch 580, Loss: 0.5123249292373657\n",
      "Epoch 24, Batch 581, Loss: 0.31182944774627686\n",
      "Epoch 24, Batch 582, Loss: 0.4807739853858948\n",
      "Epoch 24, Batch 583, Loss: 0.5021629333496094\n",
      "Epoch 24, Batch 584, Loss: 0.42836397886276245\n",
      "Epoch 24, Batch 585, Loss: 0.39507347345352173\n",
      "Epoch 24, Batch 586, Loss: 0.3139071762561798\n",
      "Epoch 24, Batch 587, Loss: 0.29501256346702576\n",
      "Epoch 24, Batch 588, Loss: 0.25137466192245483\n",
      "Epoch 24, Batch 589, Loss: 0.40608254075050354\n",
      "Epoch 24, Batch 590, Loss: 0.25092172622680664\n",
      "Epoch 24, Batch 591, Loss: 0.17644038796424866\n",
      "Epoch 24, Batch 592, Loss: 0.20527373254299164\n",
      "Epoch 24, Batch 593, Loss: 0.31846484541893005\n",
      "Epoch 24, Batch 594, Loss: 0.2528980076313019\n",
      "Epoch 24, Batch 595, Loss: 0.3891632556915283\n",
      "Epoch 24, Batch 596, Loss: 0.44787830114364624\n",
      "Epoch 24, Batch 597, Loss: 0.303340882062912\n",
      "Epoch 24, Batch 598, Loss: 0.3962234854698181\n",
      "Epoch 24, Batch 599, Loss: 0.5900177955627441\n",
      "Epoch 24, Batch 600, Loss: 0.31334632635116577\n",
      "Epoch 24, Batch 601, Loss: 0.43198052048683167\n",
      "Epoch 24, Batch 602, Loss: 0.21581192314624786\n",
      "Epoch 24, Batch 603, Loss: 0.2541479468345642\n",
      "Epoch 24, Batch 604, Loss: 0.31001371145248413\n",
      "Epoch 24, Batch 605, Loss: 0.4701617956161499\n",
      "Epoch 24, Batch 606, Loss: 0.3426518738269806\n",
      "Epoch 24, Batch 607, Loss: 0.36774110794067383\n",
      "Epoch 24, Batch 608, Loss: 0.4779230058193207\n",
      "Epoch 24, Batch 609, Loss: 0.41222769021987915\n",
      "Epoch 24, Batch 610, Loss: 0.5341728329658508\n",
      "Epoch 24, Batch 611, Loss: 0.3871545195579529\n",
      "Epoch 24, Batch 612, Loss: 0.4433708190917969\n",
      "Epoch 24, Batch 613, Loss: 0.4774540960788727\n",
      "Epoch 24, Batch 614, Loss: 0.49074819684028625\n",
      "Epoch 24, Batch 615, Loss: 0.4116067886352539\n",
      "Epoch 24, Batch 616, Loss: 0.3340405821800232\n",
      "Epoch 24, Batch 617, Loss: 0.2394619584083557\n",
      "Epoch 24, Batch 618, Loss: 0.4600948095321655\n",
      "Epoch 24, Batch 619, Loss: 0.2101232260465622\n",
      "Epoch 24, Batch 620, Loss: 0.35266542434692383\n",
      "Epoch 24, Batch 621, Loss: 0.24722014367580414\n",
      "Epoch 24, Batch 622, Loss: 0.3838367760181427\n",
      "Epoch 24, Batch 623, Loss: 0.24244965612888336\n",
      "Epoch 24, Batch 624, Loss: 0.3710399270057678\n",
      "Epoch 24, Batch 625, Loss: 0.4953155219554901\n",
      "Epoch 24, Batch 626, Loss: 0.46007364988327026\n",
      "Epoch 24, Batch 627, Loss: 0.4286077618598938\n",
      "Epoch 24, Batch 628, Loss: 0.42273086309432983\n",
      "Epoch 24, Batch 629, Loss: 0.46934643387794495\n",
      "Epoch 24, Batch 630, Loss: 0.41079822182655334\n",
      "Epoch 24, Batch 631, Loss: 0.4349222481250763\n",
      "Epoch 24, Batch 632, Loss: 0.44960281252861023\n",
      "Epoch 24, Batch 633, Loss: 0.2790733575820923\n",
      "Epoch 24, Batch 634, Loss: 0.32858240604400635\n",
      "Epoch 24, Batch 635, Loss: 0.4061978757381439\n",
      "Epoch 24, Batch 636, Loss: 0.27825647592544556\n",
      "Epoch 24, Batch 637, Loss: 0.3619450330734253\n",
      "Epoch 24, Batch 638, Loss: 0.2611890733242035\n",
      "Epoch 24, Batch 639, Loss: 0.4414900541305542\n",
      "Epoch 24, Batch 640, Loss: 0.377981036901474\n",
      "Epoch 24, Batch 641, Loss: 0.22976091504096985\n",
      "Epoch 24, Batch 642, Loss: 0.4558037519454956\n",
      "Epoch 24, Batch 643, Loss: 0.4870884120464325\n",
      "Epoch 24, Batch 644, Loss: 0.432016521692276\n",
      "Epoch 24, Batch 645, Loss: 0.4365416169166565\n",
      "Epoch 24, Batch 646, Loss: 0.3420896530151367\n",
      "Epoch 24, Batch 647, Loss: 0.2969294488430023\n",
      "Epoch 24, Batch 648, Loss: 0.3709760904312134\n",
      "Epoch 24, Batch 649, Loss: 0.49297600984573364\n",
      "Epoch 24, Batch 650, Loss: 0.390963077545166\n",
      "Epoch 24, Batch 651, Loss: 0.49009114503860474\n",
      "Epoch 24, Batch 652, Loss: 0.44219499826431274\n",
      "Epoch 24, Batch 653, Loss: 0.4022643268108368\n",
      "Epoch 24, Batch 654, Loss: 0.27895358204841614\n",
      "Epoch 24, Batch 655, Loss: 0.272957980632782\n",
      "Epoch 24, Batch 656, Loss: 0.48335808515548706\n",
      "Epoch 24, Batch 657, Loss: 0.38153648376464844\n",
      "Epoch 24, Batch 658, Loss: 0.3254602551460266\n",
      "Epoch 24, Batch 659, Loss: 0.5647969841957092\n",
      "Epoch 24, Batch 660, Loss: 0.5589911937713623\n",
      "Epoch 24, Batch 661, Loss: 0.2687341868877411\n",
      "Epoch 24, Batch 662, Loss: 0.4707642197608948\n",
      "Epoch 24, Batch 663, Loss: 0.3387831449508667\n",
      "Epoch 24, Batch 664, Loss: 0.3424373269081116\n",
      "Epoch 24, Batch 665, Loss: 0.4634469151496887\n",
      "Epoch 24, Batch 666, Loss: 0.4112500548362732\n",
      "Epoch 24, Batch 667, Loss: 0.3050461411476135\n",
      "Epoch 24, Batch 668, Loss: 0.48934808373451233\n",
      "Epoch 24, Batch 669, Loss: 0.36702901124954224\n",
      "Epoch 24, Batch 670, Loss: 0.4778326153755188\n",
      "Epoch 24, Batch 671, Loss: 0.5328713059425354\n",
      "Epoch 24, Batch 672, Loss: 0.4902985095977783\n",
      "Epoch 24, Batch 673, Loss: 0.3865695297718048\n",
      "Epoch 24, Batch 674, Loss: 0.3125416040420532\n",
      "Epoch 24, Batch 675, Loss: 0.4793623089790344\n",
      "Epoch 24, Batch 676, Loss: 0.35851776599884033\n",
      "Epoch 24, Batch 677, Loss: 0.6164396405220032\n",
      "Epoch 24, Batch 678, Loss: 0.5701213479042053\n",
      "Epoch 24, Batch 679, Loss: 0.30794528126716614\n",
      "Epoch 24, Batch 680, Loss: 0.7756652235984802\n",
      "Epoch 24, Batch 681, Loss: 0.4677909016609192\n",
      "Epoch 24, Batch 682, Loss: 0.4123205542564392\n",
      "Epoch 24, Batch 683, Loss: 0.4570317566394806\n",
      "Epoch 24, Batch 684, Loss: 0.43120813369750977\n",
      "Epoch 24, Batch 685, Loss: 0.4962746798992157\n",
      "Epoch 24, Batch 686, Loss: 0.396077036857605\n",
      "Epoch 24, Batch 687, Loss: 0.4038011133670807\n",
      "Epoch 24, Batch 688, Loss: 0.41654080152511597\n",
      "Epoch 24, Batch 689, Loss: 0.37006238102912903\n",
      "Epoch 24, Batch 690, Loss: 0.33473634719848633\n",
      "Epoch 24, Batch 691, Loss: 0.3387865722179413\n",
      "Epoch 24, Batch 692, Loss: 0.2987602651119232\n",
      "Epoch 24, Batch 693, Loss: 0.28703245520591736\n",
      "Epoch 24, Batch 694, Loss: 0.3382975459098816\n",
      "Epoch 24, Batch 695, Loss: 0.3240196108818054\n",
      "Epoch 24, Batch 696, Loss: 0.2823452949523926\n",
      "Epoch 24, Batch 697, Loss: 0.5890613794326782\n",
      "Epoch 24, Batch 698, Loss: 0.2919279932975769\n",
      "Epoch 24, Batch 699, Loss: 0.38743042945861816\n",
      "Epoch 24, Batch 700, Loss: 0.46060076355934143\n",
      "Epoch 24, Batch 701, Loss: 0.38433513045310974\n",
      "Epoch 24, Batch 702, Loss: 0.3161828815937042\n",
      "Epoch 24, Batch 703, Loss: 0.5285605192184448\n",
      "Epoch 24, Batch 704, Loss: 0.48518723249435425\n",
      "Epoch 24, Batch 705, Loss: 0.41788893938064575\n",
      "Epoch 24, Batch 706, Loss: 0.4852693974971771\n",
      "Epoch 24, Batch 707, Loss: 0.45938587188720703\n",
      "Epoch 24, Batch 708, Loss: 0.4390670955181122\n",
      "Epoch 24, Batch 709, Loss: 0.4297247529029846\n",
      "Epoch 24, Batch 710, Loss: 0.4897627830505371\n",
      "Epoch 24, Batch 711, Loss: 0.3407748341560364\n",
      "Epoch 24, Batch 712, Loss: 0.36103203892707825\n",
      "Epoch 24, Batch 713, Loss: 0.23064078390598297\n",
      "Epoch 24, Batch 714, Loss: 0.34875309467315674\n",
      "Epoch 24, Batch 715, Loss: 0.6268795728683472\n",
      "Epoch 24, Batch 716, Loss: 0.2540687322616577\n",
      "Epoch 24, Batch 717, Loss: 0.3220025300979614\n",
      "Epoch 24, Batch 718, Loss: 0.2799089252948761\n",
      "Epoch 24, Batch 719, Loss: 0.48860377073287964\n",
      "Epoch 24, Batch 720, Loss: 0.4312252402305603\n",
      "Epoch 24, Batch 721, Loss: 0.3778785467147827\n",
      "Epoch 24, Batch 722, Loss: 0.3256005644798279\n",
      "Epoch 24, Batch 723, Loss: 0.390571266412735\n",
      "Epoch 24, Batch 724, Loss: 0.3311005234718323\n",
      "Epoch 24, Batch 725, Loss: 0.30689501762390137\n",
      "Epoch 24, Batch 726, Loss: 0.49661192297935486\n",
      "Epoch 24, Batch 727, Loss: 0.4447336792945862\n",
      "Epoch 24, Batch 728, Loss: 0.330299437046051\n",
      "Epoch 24, Batch 729, Loss: 0.2574201226234436\n",
      "Epoch 24, Batch 730, Loss: 0.3799740672111511\n",
      "Epoch 24, Batch 731, Loss: 0.27750164270401\n",
      "Epoch 24, Batch 732, Loss: 0.38580790162086487\n",
      "Epoch 24, Batch 733, Loss: 0.49562594294548035\n",
      "Epoch 24, Batch 734, Loss: 0.6498124599456787\n",
      "Epoch 24, Batch 735, Loss: 0.3139427602291107\n",
      "Epoch 24, Batch 736, Loss: 0.26887208223342896\n",
      "Epoch 24, Batch 737, Loss: 0.6148278713226318\n",
      "Epoch 24, Batch 738, Loss: 0.44040989875793457\n",
      "Epoch 24, Batch 739, Loss: 0.5039398670196533\n",
      "Epoch 24, Batch 740, Loss: 0.41395658254623413\n",
      "Epoch 24, Batch 741, Loss: 0.6821476221084595\n",
      "Epoch 24, Batch 742, Loss: 0.3437802791595459\n",
      "Epoch 24, Batch 743, Loss: 0.31496867537498474\n",
      "Epoch 24, Batch 744, Loss: 0.4219089150428772\n",
      "Epoch 24, Batch 745, Loss: 0.44452887773513794\n",
      "Epoch 24, Batch 746, Loss: 0.487194687128067\n",
      "Epoch 24, Batch 747, Loss: 0.5619875192642212\n",
      "Epoch 24, Batch 748, Loss: 0.34390586614608765\n",
      "Epoch 24, Batch 749, Loss: 0.3324577510356903\n",
      "Epoch 24, Batch 750, Loss: 0.3486447036266327\n",
      "Epoch 24, Batch 751, Loss: 0.4975759983062744\n",
      "Epoch 24, Batch 752, Loss: 0.30330997705459595\n",
      "Epoch 24, Batch 753, Loss: 0.5166968703269958\n",
      "Epoch 24, Batch 754, Loss: 0.33165842294692993\n",
      "Epoch 24, Batch 755, Loss: 0.33924052119255066\n",
      "Epoch 24, Batch 756, Loss: 0.3685096800327301\n",
      "Epoch 24, Batch 757, Loss: 0.35699254274368286\n",
      "Epoch 24, Batch 758, Loss: 0.27871960401535034\n",
      "Epoch 24, Batch 759, Loss: 0.44496211409568787\n",
      "Epoch 24, Batch 760, Loss: 0.3906649947166443\n",
      "Epoch 24, Batch 761, Loss: 0.542858362197876\n",
      "Epoch 24, Batch 762, Loss: 0.28290265798568726\n",
      "Epoch 24, Batch 763, Loss: 0.3519107401371002\n",
      "Epoch 24, Batch 764, Loss: 0.321900337934494\n",
      "Epoch 24, Batch 765, Loss: 0.29131394624710083\n",
      "Epoch 24, Batch 766, Loss: 0.5258411765098572\n",
      "Epoch 24, Batch 767, Loss: 0.24329301714897156\n",
      "Epoch 24, Batch 768, Loss: 0.3991053104400635\n",
      "Epoch 24, Batch 769, Loss: 0.2346186339855194\n",
      "Epoch 24, Batch 770, Loss: 0.4699152112007141\n",
      "Epoch 24, Batch 771, Loss: 0.22634318470954895\n",
      "Epoch 24, Batch 772, Loss: 0.3597274422645569\n",
      "Epoch 24, Batch 773, Loss: 0.3584127128124237\n",
      "Epoch 24, Batch 774, Loss: 0.3596099019050598\n",
      "Epoch 24, Batch 775, Loss: 0.22826139628887177\n",
      "Epoch 24, Batch 776, Loss: 0.7085373401641846\n",
      "Epoch 24, Batch 777, Loss: 0.4475817084312439\n",
      "Epoch 24, Batch 778, Loss: 0.38132190704345703\n",
      "Epoch 24, Batch 779, Loss: 0.33971911668777466\n",
      "Epoch 24, Batch 780, Loss: 0.44015926122665405\n",
      "Epoch 24, Batch 781, Loss: 0.2801591157913208\n",
      "Epoch 24, Batch 782, Loss: 0.3922885060310364\n",
      "Epoch 24, Batch 783, Loss: 0.2634934186935425\n",
      "Epoch 24, Batch 784, Loss: 0.43056708574295044\n",
      "Epoch 24, Batch 785, Loss: 0.31423118710517883\n",
      "Epoch 24, Batch 786, Loss: 0.3513074815273285\n",
      "Epoch 24, Batch 787, Loss: 0.4278246760368347\n",
      "Epoch 24, Batch 788, Loss: 0.23634284734725952\n",
      "Epoch 24, Batch 789, Loss: 0.3334272503852844\n",
      "Epoch 24, Batch 790, Loss: 0.26432931423187256\n",
      "Epoch 24, Batch 791, Loss: 0.3567970097064972\n",
      "Epoch 24, Batch 792, Loss: 0.30691641569137573\n",
      "Epoch 24, Batch 793, Loss: 0.38511353731155396\n",
      "Epoch 24, Batch 794, Loss: 0.4434592127799988\n",
      "Epoch 24, Batch 795, Loss: 0.29639461636543274\n",
      "Epoch 24, Batch 796, Loss: 0.3245842754840851\n",
      "Epoch 24, Batch 797, Loss: 0.3005470931529999\n",
      "Epoch 24, Batch 798, Loss: 0.49652570486068726\n",
      "Epoch 24, Batch 799, Loss: 0.4819399118423462\n",
      "Epoch 24, Batch 800, Loss: 0.2859521210193634\n",
      "Epoch 24, Batch 801, Loss: 0.36337578296661377\n",
      "Epoch 24, Batch 802, Loss: 0.5358771085739136\n",
      "Epoch 24, Batch 803, Loss: 0.47629815340042114\n",
      "Epoch 24, Batch 804, Loss: 0.38914042711257935\n",
      "Epoch 24, Batch 805, Loss: 0.2959381341934204\n",
      "Epoch 24, Batch 806, Loss: 0.3185998499393463\n",
      "Epoch 24, Batch 807, Loss: 0.35622698068618774\n",
      "Epoch 24, Batch 808, Loss: 0.25176239013671875\n",
      "Epoch 24, Batch 809, Loss: 0.4512002766132355\n",
      "Epoch 24, Batch 810, Loss: 0.5592689514160156\n",
      "Epoch 24, Batch 811, Loss: 0.44050267338752747\n",
      "Epoch 24, Batch 812, Loss: 0.32494065165519714\n",
      "Epoch 24, Batch 813, Loss: 0.25344476103782654\n",
      "Epoch 24, Batch 814, Loss: 0.329847127199173\n",
      "Epoch 24, Batch 815, Loss: 0.6271647214889526\n",
      "Epoch 24, Batch 816, Loss: 0.4325716197490692\n",
      "Epoch 24, Batch 817, Loss: 0.4453870356082916\n",
      "Epoch 24, Batch 818, Loss: 0.45599156618118286\n",
      "Epoch 24, Batch 819, Loss: 0.415514200925827\n",
      "Epoch 24, Batch 820, Loss: 0.25044694542884827\n",
      "Epoch 24, Batch 821, Loss: 0.47242987155914307\n",
      "Epoch 24, Batch 822, Loss: 0.4224562346935272\n",
      "Epoch 24, Batch 823, Loss: 0.6593751311302185\n",
      "Epoch 24, Batch 824, Loss: 0.220400869846344\n",
      "Epoch 24, Batch 825, Loss: 0.2770833969116211\n",
      "Epoch 24, Batch 826, Loss: 0.250389963388443\n",
      "Epoch 24, Batch 827, Loss: 0.29995423555374146\n",
      "Epoch 24, Batch 828, Loss: 0.3462892174720764\n",
      "Epoch 24, Batch 829, Loss: 0.3058958947658539\n",
      "Epoch 24, Batch 830, Loss: 0.5754079818725586\n",
      "Epoch 24, Batch 831, Loss: 0.354859322309494\n",
      "Epoch 24, Batch 832, Loss: 0.22102998197078705\n",
      "Epoch 24, Batch 833, Loss: 0.47428277134895325\n",
      "Epoch 24, Batch 834, Loss: 0.2842942476272583\n",
      "Epoch 24, Batch 835, Loss: 0.2952260971069336\n",
      "Epoch 24, Batch 836, Loss: 0.3407028317451477\n",
      "Epoch 24, Batch 837, Loss: 0.3953535258769989\n",
      "Epoch 24, Batch 838, Loss: 0.3918103873729706\n",
      "Epoch 24, Batch 839, Loss: 0.4398440420627594\n",
      "Epoch 24, Batch 840, Loss: 0.5494719743728638\n",
      "Epoch 24, Batch 841, Loss: 0.42330506443977356\n",
      "Epoch 24, Batch 842, Loss: 0.31281206011772156\n",
      "Epoch 24, Batch 843, Loss: 0.42656975984573364\n",
      "Epoch 24, Batch 844, Loss: 0.37588152289390564\n",
      "Epoch 24, Batch 845, Loss: 0.4607185125350952\n",
      "Epoch 24, Batch 846, Loss: 0.29976895451545715\n",
      "Epoch 24, Batch 847, Loss: 0.4298449754714966\n",
      "Epoch 24, Batch 848, Loss: 0.2975091338157654\n",
      "Epoch 24, Batch 849, Loss: 0.22965361177921295\n",
      "Epoch 24, Batch 850, Loss: 0.257273405790329\n",
      "Epoch 24, Batch 851, Loss: 0.2569255530834198\n",
      "Epoch 24, Batch 852, Loss: 0.4824702739715576\n",
      "Epoch 24, Batch 853, Loss: 0.22605955600738525\n",
      "Epoch 24, Batch 854, Loss: 0.374635249376297\n",
      "Epoch 24, Batch 855, Loss: 0.25063079595565796\n",
      "Epoch 24, Batch 856, Loss: 0.3543384075164795\n",
      "Epoch 24, Batch 857, Loss: 0.48156219720840454\n",
      "Epoch 24, Batch 858, Loss: 0.42118075489997864\n",
      "Epoch 24, Batch 859, Loss: 0.4470790922641754\n",
      "Epoch 24, Batch 860, Loss: 0.44970211386680603\n",
      "Epoch 24, Batch 861, Loss: 0.32773950695991516\n",
      "Epoch 24, Batch 862, Loss: 0.37339460849761963\n",
      "Epoch 24, Batch 863, Loss: 0.33705392479896545\n",
      "Epoch 24, Batch 864, Loss: 0.36942797899246216\n",
      "Epoch 24, Batch 865, Loss: 0.3002687990665436\n",
      "Epoch 24, Batch 866, Loss: 0.5476740002632141\n",
      "Epoch 24, Batch 867, Loss: 0.1700643002986908\n",
      "Epoch 24, Batch 868, Loss: 0.36081168055534363\n",
      "Epoch 24, Batch 869, Loss: 0.5503479242324829\n",
      "Epoch 24, Batch 870, Loss: 0.3652092218399048\n",
      "Epoch 24, Batch 871, Loss: 0.31254538893699646\n",
      "Epoch 24, Batch 872, Loss: 0.3338596224784851\n",
      "Epoch 24, Batch 873, Loss: 0.36057937145233154\n",
      "Epoch 24, Batch 874, Loss: 0.3914822041988373\n",
      "Epoch 24, Batch 875, Loss: 0.4193572402000427\n",
      "Epoch 24, Batch 876, Loss: 0.3929668664932251\n",
      "Epoch 24, Batch 877, Loss: 0.556509792804718\n",
      "Epoch 24, Batch 878, Loss: 0.3400324881076813\n",
      "Epoch 24, Batch 879, Loss: 0.22965529561042786\n",
      "Epoch 24, Batch 880, Loss: 0.27445435523986816\n",
      "Epoch 24, Batch 881, Loss: 0.3404967784881592\n",
      "Epoch 24, Batch 882, Loss: 0.3722265958786011\n",
      "Epoch 24, Batch 883, Loss: 0.3877938985824585\n",
      "Epoch 24, Batch 884, Loss: 0.30662280321121216\n",
      "Epoch 24, Batch 885, Loss: 0.33662474155426025\n",
      "Epoch 24, Batch 886, Loss: 0.388810932636261\n",
      "Epoch 24, Batch 887, Loss: 0.38796889781951904\n",
      "Epoch 24, Batch 888, Loss: 0.23396804928779602\n",
      "Epoch 24, Batch 889, Loss: 0.3447518050670624\n",
      "Epoch 24, Batch 890, Loss: 0.3421745002269745\n",
      "Epoch 24, Batch 891, Loss: 0.36475539207458496\n",
      "Epoch 24, Batch 892, Loss: 0.3421878218650818\n",
      "Epoch 24, Batch 893, Loss: 0.3568034768104553\n",
      "Epoch 24, Batch 894, Loss: 0.31447267532348633\n",
      "Epoch 24, Batch 895, Loss: 0.399224191904068\n",
      "Epoch 24, Batch 896, Loss: 0.24383412301540375\n",
      "Epoch 24, Batch 897, Loss: 0.4278622269630432\n",
      "Epoch 24, Batch 898, Loss: 0.39982354640960693\n",
      "Epoch 24, Batch 899, Loss: 0.23347964882850647\n",
      "Epoch 24, Batch 900, Loss: 0.3131099343299866\n",
      "Epoch 24, Batch 901, Loss: 0.36474502086639404\n",
      "Epoch 24, Batch 902, Loss: 0.3032137453556061\n",
      "Epoch 24, Batch 903, Loss: 0.36534565687179565\n",
      "Epoch 24, Batch 904, Loss: 0.36521828174591064\n",
      "Epoch 24, Batch 905, Loss: 0.4043460190296173\n",
      "Epoch 24, Batch 906, Loss: 0.46921977400779724\n",
      "Epoch 24, Batch 907, Loss: 0.5011626482009888\n",
      "Epoch 24, Batch 908, Loss: 0.3096272945404053\n",
      "Epoch 24, Batch 909, Loss: 0.28885146975517273\n",
      "Epoch 24, Batch 910, Loss: 0.24802564084529877\n",
      "Epoch 24, Batch 911, Loss: 0.3675018548965454\n",
      "Epoch 24, Batch 912, Loss: 0.40331801772117615\n",
      "Epoch 24, Batch 913, Loss: 0.2865343689918518\n",
      "Epoch 24, Batch 914, Loss: 0.18745160102844238\n",
      "Epoch 24, Batch 915, Loss: 0.2938985824584961\n",
      "Epoch 24, Batch 916, Loss: 0.30452796816825867\n",
      "Epoch 24, Batch 917, Loss: 0.48140469193458557\n",
      "Epoch 24, Batch 918, Loss: 0.5435760617256165\n",
      "Epoch 24, Batch 919, Loss: 0.6931136250495911\n",
      "Epoch 24, Batch 920, Loss: 0.2671471834182739\n",
      "Epoch 24, Batch 921, Loss: 0.3517087697982788\n",
      "Epoch 24, Batch 922, Loss: 0.31607142090797424\n",
      "Epoch 24, Batch 923, Loss: 0.42772024869918823\n",
      "Epoch 24, Batch 924, Loss: 0.40331149101257324\n",
      "Epoch 24, Batch 925, Loss: 0.21126431226730347\n",
      "Epoch 24, Batch 926, Loss: 0.4757954180240631\n",
      "Epoch 24, Batch 927, Loss: 0.30447646975517273\n",
      "Epoch 24, Batch 928, Loss: 0.43707314133644104\n",
      "Epoch 24, Batch 929, Loss: 0.3315815031528473\n",
      "Epoch 24, Batch 930, Loss: 0.40026915073394775\n",
      "Epoch 24, Batch 931, Loss: 0.266510009765625\n",
      "Epoch 24, Batch 932, Loss: 0.27727070450782776\n",
      "Epoch 24, Batch 933, Loss: 0.3454745411872864\n",
      "Epoch 24, Batch 934, Loss: 0.4429726302623749\n",
      "Epoch 24, Batch 935, Loss: 0.32185664772987366\n",
      "Epoch 24, Batch 936, Loss: 0.31135934591293335\n",
      "Epoch 24, Batch 937, Loss: 0.366676390171051\n",
      "Epoch 24, Batch 938, Loss: 0.3290477693080902\n",
      "Accuracy of train set: 0.8649166666666667\n",
      "Epoch 24, Batch 1, Test Loss: 0.4429645538330078\n",
      "Epoch 24, Batch 2, Test Loss: 0.47111397981643677\n",
      "Epoch 24, Batch 3, Test Loss: 0.40592020750045776\n",
      "Epoch 24, Batch 4, Test Loss: 0.6303572058677673\n",
      "Epoch 24, Batch 5, Test Loss: 0.5220370888710022\n",
      "Epoch 24, Batch 6, Test Loss: 0.28877827525138855\n",
      "Epoch 24, Batch 7, Test Loss: 0.4414825141429901\n",
      "Epoch 24, Batch 8, Test Loss: 0.5573804378509521\n",
      "Epoch 24, Batch 9, Test Loss: 0.4416362941265106\n",
      "Epoch 24, Batch 10, Test Loss: 0.32621389627456665\n",
      "Epoch 24, Batch 11, Test Loss: 0.5303081274032593\n",
      "Epoch 24, Batch 12, Test Loss: 0.2999086380004883\n",
      "Epoch 24, Batch 13, Test Loss: 0.2807665765285492\n",
      "Epoch 24, Batch 14, Test Loss: 0.2629977762699127\n",
      "Epoch 24, Batch 15, Test Loss: 0.6402136087417603\n",
      "Epoch 24, Batch 16, Test Loss: 0.4123404026031494\n",
      "Epoch 24, Batch 17, Test Loss: 0.5015575289726257\n",
      "Epoch 24, Batch 18, Test Loss: 0.2676228880882263\n",
      "Epoch 24, Batch 19, Test Loss: 0.2873768210411072\n",
      "Epoch 24, Batch 20, Test Loss: 0.46006327867507935\n",
      "Epoch 24, Batch 21, Test Loss: 0.32220736145973206\n",
      "Epoch 24, Batch 22, Test Loss: 0.2975294589996338\n",
      "Epoch 24, Batch 23, Test Loss: 0.33528783917427063\n",
      "Epoch 24, Batch 24, Test Loss: 0.2655881941318512\n",
      "Epoch 24, Batch 25, Test Loss: 0.5320907831192017\n",
      "Epoch 24, Batch 26, Test Loss: 0.349979043006897\n",
      "Epoch 24, Batch 27, Test Loss: 0.5804743766784668\n",
      "Epoch 24, Batch 28, Test Loss: 0.44265633821487427\n",
      "Epoch 24, Batch 29, Test Loss: 0.42287299036979675\n",
      "Epoch 24, Batch 30, Test Loss: 0.3874147832393646\n",
      "Epoch 24, Batch 31, Test Loss: 0.5979911088943481\n",
      "Epoch 24, Batch 32, Test Loss: 0.424258291721344\n",
      "Epoch 24, Batch 33, Test Loss: 0.3454945683479309\n",
      "Epoch 24, Batch 34, Test Loss: 0.3831212818622589\n",
      "Epoch 24, Batch 35, Test Loss: 0.48116105794906616\n",
      "Epoch 24, Batch 36, Test Loss: 0.23193597793579102\n",
      "Epoch 24, Batch 37, Test Loss: 0.3510470688343048\n",
      "Epoch 24, Batch 38, Test Loss: 0.2835560142993927\n",
      "Epoch 24, Batch 39, Test Loss: 0.36724984645843506\n",
      "Epoch 24, Batch 40, Test Loss: 0.28894537687301636\n",
      "Epoch 24, Batch 41, Test Loss: 0.3185029923915863\n",
      "Epoch 24, Batch 42, Test Loss: 0.3587014675140381\n",
      "Epoch 24, Batch 43, Test Loss: 0.6442078351974487\n",
      "Epoch 24, Batch 44, Test Loss: 0.3417160212993622\n",
      "Epoch 24, Batch 45, Test Loss: 0.30218562483787537\n",
      "Epoch 24, Batch 46, Test Loss: 0.43391355872154236\n",
      "Epoch 24, Batch 47, Test Loss: 0.27321857213974\n",
      "Epoch 24, Batch 48, Test Loss: 0.35551726818084717\n",
      "Epoch 24, Batch 49, Test Loss: 0.2635011076927185\n",
      "Epoch 24, Batch 50, Test Loss: 0.41891711950302124\n",
      "Epoch 24, Batch 51, Test Loss: 0.38061317801475525\n",
      "Epoch 24, Batch 52, Test Loss: 0.3241391181945801\n",
      "Epoch 24, Batch 53, Test Loss: 0.4677584767341614\n",
      "Epoch 24, Batch 54, Test Loss: 0.3504709005355835\n",
      "Epoch 24, Batch 55, Test Loss: 0.29481494426727295\n",
      "Epoch 24, Batch 56, Test Loss: 0.3946971893310547\n",
      "Epoch 24, Batch 57, Test Loss: 0.3070114552974701\n",
      "Epoch 24, Batch 58, Test Loss: 0.39574480056762695\n",
      "Epoch 24, Batch 59, Test Loss: 0.3129405677318573\n",
      "Epoch 24, Batch 60, Test Loss: 0.2869665324687958\n",
      "Epoch 24, Batch 61, Test Loss: 0.5391151905059814\n",
      "Epoch 24, Batch 62, Test Loss: 0.42140257358551025\n",
      "Epoch 24, Batch 63, Test Loss: 0.3314574062824249\n",
      "Epoch 24, Batch 64, Test Loss: 0.31198012828826904\n",
      "Epoch 24, Batch 65, Test Loss: 0.6111786365509033\n",
      "Epoch 24, Batch 66, Test Loss: 0.43017658591270447\n",
      "Epoch 24, Batch 67, Test Loss: 0.3410223424434662\n",
      "Epoch 24, Batch 68, Test Loss: 0.34838148951530457\n",
      "Epoch 24, Batch 69, Test Loss: 0.295465886592865\n",
      "Epoch 24, Batch 70, Test Loss: 0.39875948429107666\n",
      "Epoch 24, Batch 71, Test Loss: 0.22301067411899567\n",
      "Epoch 24, Batch 72, Test Loss: 0.3879016041755676\n",
      "Epoch 24, Batch 73, Test Loss: 0.23450861871242523\n",
      "Epoch 24, Batch 74, Test Loss: 0.4037398397922516\n",
      "Epoch 24, Batch 75, Test Loss: 0.4116867184638977\n",
      "Epoch 24, Batch 76, Test Loss: 0.3996954560279846\n",
      "Epoch 24, Batch 77, Test Loss: 0.49963846802711487\n",
      "Epoch 24, Batch 78, Test Loss: 0.4228487014770508\n",
      "Epoch 24, Batch 79, Test Loss: 0.27700966596603394\n",
      "Epoch 24, Batch 80, Test Loss: 0.1911102831363678\n",
      "Epoch 24, Batch 81, Test Loss: 0.4339878559112549\n",
      "Epoch 24, Batch 82, Test Loss: 0.35880839824676514\n",
      "Epoch 24, Batch 83, Test Loss: 0.4572957456111908\n",
      "Epoch 24, Batch 84, Test Loss: 0.2475966513156891\n",
      "Epoch 24, Batch 85, Test Loss: 0.4581691026687622\n",
      "Epoch 24, Batch 86, Test Loss: 0.2690648138523102\n",
      "Epoch 24, Batch 87, Test Loss: 0.6202569603919983\n",
      "Epoch 24, Batch 88, Test Loss: 0.3301551342010498\n",
      "Epoch 24, Batch 89, Test Loss: 0.3936534523963928\n",
      "Epoch 24, Batch 90, Test Loss: 0.35559406876564026\n",
      "Epoch 24, Batch 91, Test Loss: 0.41944482922554016\n",
      "Epoch 24, Batch 92, Test Loss: 0.25799211859703064\n",
      "Epoch 24, Batch 93, Test Loss: 0.3413887321949005\n",
      "Epoch 24, Batch 94, Test Loss: 0.4353559613227844\n",
      "Epoch 24, Batch 95, Test Loss: 0.3522995710372925\n",
      "Epoch 24, Batch 96, Test Loss: 0.2873944342136383\n",
      "Epoch 24, Batch 97, Test Loss: 0.34069108963012695\n",
      "Epoch 24, Batch 98, Test Loss: 0.5469881892204285\n",
      "Epoch 24, Batch 99, Test Loss: 0.5115014910697937\n",
      "Epoch 24, Batch 100, Test Loss: 0.38938549160957336\n",
      "Epoch 24, Batch 101, Test Loss: 0.7014809846878052\n",
      "Epoch 24, Batch 102, Test Loss: 0.2708449959754944\n",
      "Epoch 24, Batch 103, Test Loss: 0.40011465549468994\n",
      "Epoch 24, Batch 104, Test Loss: 0.3921055197715759\n",
      "Epoch 24, Batch 105, Test Loss: 0.2837071418762207\n",
      "Epoch 24, Batch 106, Test Loss: 0.3433357775211334\n",
      "Epoch 24, Batch 107, Test Loss: 0.3691156804561615\n",
      "Epoch 24, Batch 108, Test Loss: 0.33508235216140747\n",
      "Epoch 24, Batch 109, Test Loss: 0.2319791316986084\n",
      "Epoch 24, Batch 110, Test Loss: 0.362647145986557\n",
      "Epoch 24, Batch 111, Test Loss: 0.4803685247898102\n",
      "Epoch 24, Batch 112, Test Loss: 0.31404244899749756\n",
      "Epoch 24, Batch 113, Test Loss: 0.4150250554084778\n",
      "Epoch 24, Batch 114, Test Loss: 0.4221794307231903\n",
      "Epoch 24, Batch 115, Test Loss: 0.40102964639663696\n",
      "Epoch 24, Batch 116, Test Loss: 0.29536890983581543\n",
      "Epoch 24, Batch 117, Test Loss: 0.4053051471710205\n",
      "Epoch 24, Batch 118, Test Loss: 0.4551423192024231\n",
      "Epoch 24, Batch 119, Test Loss: 0.3714328706264496\n",
      "Epoch 24, Batch 120, Test Loss: 0.3344564437866211\n",
      "Epoch 24, Batch 121, Test Loss: 0.35285520553588867\n",
      "Epoch 24, Batch 122, Test Loss: 0.2559269666671753\n",
      "Epoch 24, Batch 123, Test Loss: 0.2914496660232544\n",
      "Epoch 24, Batch 124, Test Loss: 0.4664658308029175\n",
      "Epoch 24, Batch 125, Test Loss: 0.4169544577598572\n",
      "Epoch 24, Batch 126, Test Loss: 0.3370668292045593\n",
      "Epoch 24, Batch 127, Test Loss: 0.41867372393608093\n",
      "Epoch 24, Batch 128, Test Loss: 0.3956564664840698\n",
      "Epoch 24, Batch 129, Test Loss: 0.6142016649246216\n",
      "Epoch 24, Batch 130, Test Loss: 0.5964464545249939\n",
      "Epoch 24, Batch 131, Test Loss: 0.35084521770477295\n",
      "Epoch 24, Batch 132, Test Loss: 0.27982059121131897\n",
      "Epoch 24, Batch 133, Test Loss: 0.23143556714057922\n",
      "Epoch 24, Batch 134, Test Loss: 0.5177097916603088\n",
      "Epoch 24, Batch 135, Test Loss: 0.25728118419647217\n",
      "Epoch 24, Batch 136, Test Loss: 0.2934826910495758\n",
      "Epoch 24, Batch 137, Test Loss: 0.31336236000061035\n",
      "Epoch 24, Batch 138, Test Loss: 0.46498966217041016\n",
      "Epoch 24, Batch 139, Test Loss: 0.4402722716331482\n",
      "Epoch 24, Batch 140, Test Loss: 0.3623340427875519\n",
      "Epoch 24, Batch 141, Test Loss: 0.4460127353668213\n",
      "Epoch 24, Batch 142, Test Loss: 0.422336608171463\n",
      "Epoch 24, Batch 143, Test Loss: 0.43134281039237976\n",
      "Epoch 24, Batch 144, Test Loss: 0.3144163489341736\n",
      "Epoch 24, Batch 145, Test Loss: 0.4016374945640564\n",
      "Epoch 24, Batch 146, Test Loss: 0.5027062296867371\n",
      "Epoch 24, Batch 147, Test Loss: 0.36110803484916687\n",
      "Epoch 24, Batch 148, Test Loss: 0.4499303698539734\n",
      "Epoch 24, Batch 149, Test Loss: 0.34151628613471985\n",
      "Epoch 24, Batch 150, Test Loss: 0.32667815685272217\n",
      "Epoch 24, Batch 151, Test Loss: 0.25140663981437683\n",
      "Epoch 24, Batch 152, Test Loss: 0.5209042429924011\n",
      "Epoch 24, Batch 153, Test Loss: 0.4065636396408081\n",
      "Epoch 24, Batch 154, Test Loss: 0.2799578905105591\n",
      "Epoch 24, Batch 155, Test Loss: 0.31818029284477234\n",
      "Epoch 24, Batch 156, Test Loss: 0.3159032464027405\n",
      "Epoch 24, Batch 157, Test Loss: 0.46734583377838135\n",
      "Epoch 24, Batch 158, Test Loss: 0.27956515550613403\n",
      "Epoch 24, Batch 159, Test Loss: 0.5462227463722229\n",
      "Epoch 24, Batch 160, Test Loss: 0.22069232165813446\n",
      "Epoch 24, Batch 161, Test Loss: 0.4702325463294983\n",
      "Epoch 24, Batch 162, Test Loss: 0.3871138095855713\n",
      "Epoch 24, Batch 163, Test Loss: 0.43906450271606445\n",
      "Epoch 24, Batch 164, Test Loss: 0.42494305968284607\n",
      "Epoch 24, Batch 165, Test Loss: 0.33490848541259766\n",
      "Epoch 24, Batch 166, Test Loss: 0.3702085614204407\n",
      "Epoch 24, Batch 167, Test Loss: 0.5355147123336792\n",
      "Epoch 24, Batch 168, Test Loss: 0.36175763607025146\n",
      "Epoch 24, Batch 169, Test Loss: 0.44013917446136475\n",
      "Epoch 24, Batch 170, Test Loss: 0.3510737419128418\n",
      "Epoch 24, Batch 171, Test Loss: 0.3988496959209442\n",
      "Epoch 24, Batch 172, Test Loss: 0.3145473897457123\n",
      "Epoch 24, Batch 173, Test Loss: 0.400557279586792\n",
      "Epoch 24, Batch 174, Test Loss: 0.3509479761123657\n",
      "Epoch 24, Batch 175, Test Loss: 0.48376286029815674\n",
      "Epoch 24, Batch 176, Test Loss: 0.2837110161781311\n",
      "Epoch 24, Batch 177, Test Loss: 0.3542155623435974\n",
      "Epoch 24, Batch 178, Test Loss: 0.4183334708213806\n",
      "Epoch 24, Batch 179, Test Loss: 0.32382792234420776\n",
      "Epoch 24, Batch 180, Test Loss: 0.5774118900299072\n",
      "Epoch 24, Batch 181, Test Loss: 0.4121536612510681\n",
      "Epoch 24, Batch 182, Test Loss: 0.4763539135456085\n",
      "Epoch 24, Batch 183, Test Loss: 0.42003732919692993\n",
      "Epoch 24, Batch 184, Test Loss: 0.4208804965019226\n",
      "Epoch 24, Batch 185, Test Loss: 0.2838604748249054\n",
      "Epoch 24, Batch 186, Test Loss: 0.46756061911582947\n",
      "Epoch 24, Batch 187, Test Loss: 0.3333805799484253\n",
      "Epoch 24, Batch 188, Test Loss: 0.41592782735824585\n",
      "Epoch 24, Batch 189, Test Loss: 0.5279772281646729\n",
      "Epoch 24, Batch 190, Test Loss: 0.35891368985176086\n",
      "Epoch 24, Batch 191, Test Loss: 0.5090395212173462\n",
      "Epoch 24, Batch 192, Test Loss: 0.3471076786518097\n",
      "Epoch 24, Batch 193, Test Loss: 0.258344829082489\n",
      "Epoch 24, Batch 194, Test Loss: 0.37260702252388\n",
      "Epoch 24, Batch 195, Test Loss: 0.37002813816070557\n",
      "Epoch 24, Batch 196, Test Loss: 0.6913486123085022\n",
      "Epoch 24, Batch 197, Test Loss: 0.4617239534854889\n",
      "Epoch 24, Batch 198, Test Loss: 0.267497181892395\n",
      "Epoch 24, Batch 199, Test Loss: 0.3434240221977234\n",
      "Epoch 24, Batch 200, Test Loss: 0.22121456265449524\n",
      "Epoch 24, Batch 201, Test Loss: 0.35757648944854736\n",
      "Epoch 24, Batch 202, Test Loss: 0.3038013279438019\n",
      "Epoch 24, Batch 203, Test Loss: 0.31903260946273804\n",
      "Epoch 24, Batch 204, Test Loss: 0.4472149610519409\n",
      "Epoch 24, Batch 205, Test Loss: 0.43379920721054077\n",
      "Epoch 24, Batch 206, Test Loss: 0.37760627269744873\n",
      "Epoch 24, Batch 207, Test Loss: 0.41615498065948486\n",
      "Epoch 24, Batch 208, Test Loss: 0.4134688973426819\n",
      "Epoch 24, Batch 209, Test Loss: 0.4731886684894562\n",
      "Epoch 24, Batch 210, Test Loss: 0.4456270933151245\n",
      "Epoch 24, Batch 211, Test Loss: 0.3263975977897644\n",
      "Epoch 24, Batch 212, Test Loss: 0.6138008236885071\n",
      "Epoch 24, Batch 213, Test Loss: 0.2726956903934479\n",
      "Epoch 24, Batch 214, Test Loss: 0.47273099422454834\n",
      "Epoch 24, Batch 215, Test Loss: 0.4474886655807495\n",
      "Epoch 24, Batch 216, Test Loss: 0.30548375844955444\n",
      "Epoch 24, Batch 217, Test Loss: 0.42005157470703125\n",
      "Epoch 24, Batch 218, Test Loss: 0.3411133587360382\n",
      "Epoch 24, Batch 219, Test Loss: 0.5095633864402771\n",
      "Epoch 24, Batch 220, Test Loss: 0.3040035665035248\n",
      "Epoch 24, Batch 221, Test Loss: 0.417522132396698\n",
      "Epoch 24, Batch 222, Test Loss: 0.48345667123794556\n",
      "Epoch 24, Batch 223, Test Loss: 0.29468703269958496\n",
      "Epoch 24, Batch 224, Test Loss: 0.22507329285144806\n",
      "Epoch 24, Batch 225, Test Loss: 0.3974515497684479\n",
      "Epoch 24, Batch 226, Test Loss: 0.34202882647514343\n",
      "Epoch 24, Batch 227, Test Loss: 0.2785349190235138\n",
      "Epoch 24, Batch 228, Test Loss: 0.34646543860435486\n",
      "Epoch 24, Batch 229, Test Loss: 0.3721214234828949\n",
      "Epoch 24, Batch 230, Test Loss: 0.1818116456270218\n",
      "Epoch 24, Batch 231, Test Loss: 0.23950636386871338\n",
      "Epoch 24, Batch 232, Test Loss: 0.3711305856704712\n",
      "Epoch 24, Batch 233, Test Loss: 0.4282223880290985\n",
      "Epoch 24, Batch 234, Test Loss: 0.15912477672100067\n",
      "Epoch 24, Batch 235, Test Loss: 0.39641860127449036\n",
      "Epoch 24, Batch 236, Test Loss: 0.30196434259414673\n",
      "Epoch 24, Batch 237, Test Loss: 0.4991537034511566\n",
      "Epoch 24, Batch 238, Test Loss: 0.3976929485797882\n",
      "Epoch 24, Batch 239, Test Loss: 0.22976955771446228\n",
      "Epoch 24, Batch 240, Test Loss: 0.6069212555885315\n",
      "Epoch 24, Batch 241, Test Loss: 0.40568771958351135\n",
      "Epoch 24, Batch 242, Test Loss: 0.329833447933197\n",
      "Epoch 24, Batch 243, Test Loss: 0.19203367829322815\n",
      "Epoch 24, Batch 244, Test Loss: 0.2673260569572449\n",
      "Epoch 24, Batch 245, Test Loss: 0.472573846578598\n",
      "Epoch 24, Batch 246, Test Loss: 0.26938554644584656\n",
      "Epoch 24, Batch 247, Test Loss: 0.21075432002544403\n",
      "Epoch 24, Batch 248, Test Loss: 0.44622790813446045\n",
      "Epoch 24, Batch 249, Test Loss: 0.2466346174478531\n",
      "Epoch 24, Batch 250, Test Loss: 0.26861006021499634\n",
      "Epoch 24, Batch 251, Test Loss: 0.368247926235199\n",
      "Epoch 24, Batch 252, Test Loss: 0.25249984860420227\n",
      "Epoch 24, Batch 253, Test Loss: 0.4318088889122009\n",
      "Epoch 24, Batch 254, Test Loss: 0.27799054980278015\n",
      "Epoch 24, Batch 255, Test Loss: 0.30219292640686035\n",
      "Epoch 24, Batch 256, Test Loss: 0.4347972869873047\n",
      "Epoch 24, Batch 257, Test Loss: 0.3011653423309326\n",
      "Epoch 24, Batch 258, Test Loss: 0.38790449500083923\n",
      "Epoch 24, Batch 259, Test Loss: 0.47977614402770996\n",
      "Epoch 24, Batch 260, Test Loss: 0.4747755229473114\n",
      "Epoch 24, Batch 261, Test Loss: 0.45162180066108704\n",
      "Epoch 24, Batch 262, Test Loss: 0.3925885260105133\n",
      "Epoch 24, Batch 263, Test Loss: 0.4418998062610626\n",
      "Epoch 24, Batch 264, Test Loss: 0.24108576774597168\n",
      "Epoch 24, Batch 265, Test Loss: 0.7146241068840027\n",
      "Epoch 24, Batch 266, Test Loss: 0.3020392656326294\n",
      "Epoch 24, Batch 267, Test Loss: 0.42361754179000854\n",
      "Epoch 24, Batch 268, Test Loss: 0.30371108651161194\n",
      "Epoch 24, Batch 269, Test Loss: 0.3910946846008301\n",
      "Epoch 24, Batch 270, Test Loss: 0.5014471411705017\n",
      "Epoch 24, Batch 271, Test Loss: 0.32934924960136414\n",
      "Epoch 24, Batch 272, Test Loss: 0.42007893323898315\n",
      "Epoch 24, Batch 273, Test Loss: 0.3289816975593567\n",
      "Epoch 24, Batch 274, Test Loss: 0.35304731130599976\n",
      "Epoch 24, Batch 275, Test Loss: 0.3793387711048126\n",
      "Epoch 24, Batch 276, Test Loss: 0.42624661326408386\n",
      "Epoch 24, Batch 277, Test Loss: 0.2933719754219055\n",
      "Epoch 24, Batch 278, Test Loss: 0.29864779114723206\n",
      "Epoch 24, Batch 279, Test Loss: 0.4422525465488434\n",
      "Epoch 24, Batch 280, Test Loss: 0.2931560277938843\n",
      "Epoch 24, Batch 281, Test Loss: 0.5598297119140625\n",
      "Epoch 24, Batch 282, Test Loss: 0.2534273862838745\n",
      "Epoch 24, Batch 283, Test Loss: 0.4059625267982483\n",
      "Epoch 24, Batch 284, Test Loss: 0.3875041604042053\n",
      "Epoch 24, Batch 285, Test Loss: 0.5959150791168213\n",
      "Epoch 24, Batch 286, Test Loss: 0.5191606283187866\n",
      "Epoch 24, Batch 287, Test Loss: 0.4275193214416504\n",
      "Epoch 24, Batch 288, Test Loss: 0.47944289445877075\n",
      "Epoch 24, Batch 289, Test Loss: 0.365932434797287\n",
      "Epoch 24, Batch 290, Test Loss: 0.41681742668151855\n",
      "Epoch 24, Batch 291, Test Loss: 0.3372154235839844\n",
      "Epoch 24, Batch 292, Test Loss: 0.403199702501297\n",
      "Epoch 24, Batch 293, Test Loss: 0.47054147720336914\n",
      "Epoch 24, Batch 294, Test Loss: 0.4161396622657776\n",
      "Epoch 24, Batch 295, Test Loss: 0.3749987483024597\n",
      "Epoch 24, Batch 296, Test Loss: 0.4116694927215576\n",
      "Epoch 24, Batch 297, Test Loss: 0.45602455735206604\n",
      "Epoch 24, Batch 298, Test Loss: 0.49953022599220276\n",
      "Epoch 24, Batch 299, Test Loss: 0.424930602312088\n",
      "Epoch 24, Batch 300, Test Loss: 0.48922476172447205\n",
      "Epoch 24, Batch 301, Test Loss: 0.5109608769416809\n",
      "Epoch 24, Batch 302, Test Loss: 0.28511273860931396\n",
      "Epoch 24, Batch 303, Test Loss: 0.3605477511882782\n",
      "Epoch 24, Batch 304, Test Loss: 0.36563825607299805\n",
      "Epoch 24, Batch 305, Test Loss: 0.3883184492588043\n",
      "Epoch 24, Batch 306, Test Loss: 0.27500054240226746\n",
      "Epoch 24, Batch 307, Test Loss: 0.4099642336368561\n",
      "Epoch 24, Batch 308, Test Loss: 0.41235703229904175\n",
      "Epoch 24, Batch 309, Test Loss: 0.3171469271183014\n",
      "Epoch 24, Batch 310, Test Loss: 0.25793716311454773\n",
      "Epoch 24, Batch 311, Test Loss: 0.3472993075847626\n",
      "Epoch 24, Batch 312, Test Loss: 0.3934178948402405\n",
      "Epoch 24, Batch 313, Test Loss: 0.34391820430755615\n",
      "Epoch 24, Batch 314, Test Loss: 0.3516402542591095\n",
      "Epoch 24, Batch 315, Test Loss: 0.46321985125541687\n",
      "Epoch 24, Batch 316, Test Loss: 0.4129166007041931\n",
      "Epoch 24, Batch 317, Test Loss: 0.6050064563751221\n",
      "Epoch 24, Batch 318, Test Loss: 0.4744107127189636\n",
      "Epoch 24, Batch 319, Test Loss: 0.3570154011249542\n",
      "Epoch 24, Batch 320, Test Loss: 0.40076324343681335\n",
      "Epoch 24, Batch 321, Test Loss: 0.32682403922080994\n",
      "Epoch 24, Batch 322, Test Loss: 0.32737302780151367\n",
      "Epoch 24, Batch 323, Test Loss: 0.22462043166160583\n",
      "Epoch 24, Batch 324, Test Loss: 0.42038315534591675\n",
      "Epoch 24, Batch 325, Test Loss: 0.37033742666244507\n",
      "Epoch 24, Batch 326, Test Loss: 0.48934927582740784\n",
      "Epoch 24, Batch 327, Test Loss: 0.345380574464798\n",
      "Epoch 24, Batch 328, Test Loss: 0.36991918087005615\n",
      "Epoch 24, Batch 329, Test Loss: 0.2753315269947052\n",
      "Epoch 24, Batch 330, Test Loss: 0.36913812160491943\n",
      "Epoch 24, Batch 331, Test Loss: 0.33000171184539795\n",
      "Epoch 24, Batch 332, Test Loss: 0.25974011421203613\n",
      "Epoch 24, Batch 333, Test Loss: 0.4418675899505615\n",
      "Epoch 24, Batch 334, Test Loss: 0.4386284053325653\n",
      "Epoch 24, Batch 335, Test Loss: 0.41902875900268555\n",
      "Epoch 24, Batch 336, Test Loss: 0.3978477120399475\n",
      "Epoch 24, Batch 337, Test Loss: 0.4475504755973816\n",
      "Epoch 24, Batch 338, Test Loss: 0.34787261486053467\n",
      "Epoch 24, Batch 339, Test Loss: 0.512321412563324\n",
      "Epoch 24, Batch 340, Test Loss: 0.2535798251628876\n",
      "Epoch 24, Batch 341, Test Loss: 0.4132333993911743\n",
      "Epoch 24, Batch 342, Test Loss: 0.3325486481189728\n",
      "Epoch 24, Batch 343, Test Loss: 0.5032441020011902\n",
      "Epoch 24, Batch 344, Test Loss: 0.4017539620399475\n",
      "Epoch 24, Batch 345, Test Loss: 0.4779926538467407\n",
      "Epoch 24, Batch 346, Test Loss: 0.41220152378082275\n",
      "Epoch 24, Batch 347, Test Loss: 0.3954547345638275\n",
      "Epoch 24, Batch 348, Test Loss: 0.3785315155982971\n",
      "Epoch 24, Batch 349, Test Loss: 0.4250720143318176\n",
      "Epoch 24, Batch 350, Test Loss: 0.5338555574417114\n",
      "Epoch 24, Batch 351, Test Loss: 0.48938262462615967\n",
      "Epoch 24, Batch 352, Test Loss: 0.326535701751709\n",
      "Epoch 24, Batch 353, Test Loss: 0.2542386054992676\n",
      "Epoch 24, Batch 354, Test Loss: 0.3300634026527405\n",
      "Epoch 24, Batch 355, Test Loss: 0.4231554865837097\n",
      "Epoch 24, Batch 356, Test Loss: 0.37838196754455566\n",
      "Epoch 24, Batch 357, Test Loss: 0.39955228567123413\n",
      "Epoch 24, Batch 358, Test Loss: 0.40373528003692627\n",
      "Epoch 24, Batch 359, Test Loss: 0.33472925424575806\n",
      "Epoch 24, Batch 360, Test Loss: 0.2387392371892929\n",
      "Epoch 24, Batch 361, Test Loss: 0.32295098900794983\n",
      "Epoch 24, Batch 362, Test Loss: 0.3033188581466675\n",
      "Epoch 24, Batch 363, Test Loss: 0.4669363796710968\n",
      "Epoch 24, Batch 364, Test Loss: 0.48395225405693054\n",
      "Epoch 24, Batch 365, Test Loss: 0.39684754610061646\n",
      "Epoch 24, Batch 366, Test Loss: 0.2327863872051239\n",
      "Epoch 24, Batch 367, Test Loss: 0.2656182646751404\n",
      "Epoch 24, Batch 368, Test Loss: 0.33016663789749146\n",
      "Epoch 24, Batch 369, Test Loss: 0.33756986260414124\n",
      "Epoch 24, Batch 370, Test Loss: 0.3231620490550995\n",
      "Epoch 24, Batch 371, Test Loss: 0.403952032327652\n",
      "Epoch 24, Batch 372, Test Loss: 0.57960045337677\n",
      "Epoch 24, Batch 373, Test Loss: 0.5651891231536865\n",
      "Epoch 24, Batch 374, Test Loss: 0.5998038649559021\n",
      "Epoch 24, Batch 375, Test Loss: 0.3212081491947174\n",
      "Epoch 24, Batch 376, Test Loss: 0.3724052906036377\n",
      "Epoch 24, Batch 377, Test Loss: 0.23383000493049622\n",
      "Epoch 24, Batch 378, Test Loss: 0.3823055624961853\n",
      "Epoch 24, Batch 379, Test Loss: 0.3017041087150574\n",
      "Epoch 24, Batch 380, Test Loss: 0.541150689125061\n",
      "Epoch 24, Batch 381, Test Loss: 0.2783762514591217\n",
      "Epoch 24, Batch 382, Test Loss: 0.4384075403213501\n",
      "Epoch 24, Batch 383, Test Loss: 0.280231237411499\n",
      "Epoch 24, Batch 384, Test Loss: 0.509736955165863\n",
      "Epoch 24, Batch 385, Test Loss: 0.3389376997947693\n",
      "Epoch 24, Batch 386, Test Loss: 0.4858577251434326\n",
      "Epoch 24, Batch 387, Test Loss: 0.379558801651001\n",
      "Epoch 24, Batch 388, Test Loss: 0.49147653579711914\n",
      "Epoch 24, Batch 389, Test Loss: 0.21776044368743896\n",
      "Epoch 24, Batch 390, Test Loss: 0.3773362636566162\n",
      "Epoch 24, Batch 391, Test Loss: 0.42075783014297485\n",
      "Epoch 24, Batch 392, Test Loss: 0.3638341724872589\n",
      "Epoch 24, Batch 393, Test Loss: 0.29832449555397034\n",
      "Epoch 24, Batch 394, Test Loss: 0.49253493547439575\n",
      "Epoch 24, Batch 395, Test Loss: 0.271029531955719\n",
      "Epoch 24, Batch 396, Test Loss: 0.5128360390663147\n",
      "Epoch 24, Batch 397, Test Loss: 0.3544779419898987\n",
      "Epoch 24, Batch 398, Test Loss: 0.42763417959213257\n",
      "Epoch 24, Batch 399, Test Loss: 0.40303656458854675\n",
      "Epoch 24, Batch 400, Test Loss: 0.38903361558914185\n",
      "Epoch 24, Batch 401, Test Loss: 0.4334802031517029\n",
      "Epoch 24, Batch 402, Test Loss: 0.1987863928079605\n",
      "Epoch 24, Batch 403, Test Loss: 0.34887152910232544\n",
      "Epoch 24, Batch 404, Test Loss: 0.29647213220596313\n",
      "Epoch 24, Batch 405, Test Loss: 0.25684797763824463\n",
      "Epoch 24, Batch 406, Test Loss: 0.24409903585910797\n",
      "Epoch 24, Batch 407, Test Loss: 0.30431684851646423\n",
      "Epoch 24, Batch 408, Test Loss: 0.2836199700832367\n",
      "Epoch 24, Batch 409, Test Loss: 0.39774349331855774\n",
      "Epoch 24, Batch 410, Test Loss: 0.23675400018692017\n",
      "Epoch 24, Batch 411, Test Loss: 0.19779326021671295\n",
      "Epoch 24, Batch 412, Test Loss: 0.32181572914123535\n",
      "Epoch 24, Batch 413, Test Loss: 0.3920592665672302\n",
      "Epoch 24, Batch 414, Test Loss: 0.39940088987350464\n",
      "Epoch 24, Batch 415, Test Loss: 0.4847857654094696\n",
      "Epoch 24, Batch 416, Test Loss: 0.3503304421901703\n",
      "Epoch 24, Batch 417, Test Loss: 0.28062736988067627\n",
      "Epoch 24, Batch 418, Test Loss: 0.4294162392616272\n",
      "Epoch 24, Batch 419, Test Loss: 0.3738395571708679\n",
      "Epoch 24, Batch 420, Test Loss: 0.3974422514438629\n",
      "Epoch 24, Batch 421, Test Loss: 0.2805342972278595\n",
      "Epoch 24, Batch 422, Test Loss: 0.3832095265388489\n",
      "Epoch 24, Batch 423, Test Loss: 0.4943290948867798\n",
      "Epoch 24, Batch 424, Test Loss: 0.4103187024593353\n",
      "Epoch 24, Batch 425, Test Loss: 0.6085134744644165\n",
      "Epoch 24, Batch 426, Test Loss: 0.24855078756809235\n",
      "Epoch 24, Batch 427, Test Loss: 0.42241108417510986\n",
      "Epoch 24, Batch 428, Test Loss: 0.3304484188556671\n",
      "Epoch 24, Batch 429, Test Loss: 0.3530768156051636\n",
      "Epoch 24, Batch 430, Test Loss: 0.6068398952484131\n",
      "Epoch 24, Batch 431, Test Loss: 0.5633701682090759\n",
      "Epoch 24, Batch 432, Test Loss: 0.42695051431655884\n",
      "Epoch 24, Batch 433, Test Loss: 0.27063870429992676\n",
      "Epoch 24, Batch 434, Test Loss: 0.41273942589759827\n",
      "Epoch 24, Batch 435, Test Loss: 0.4187506437301636\n",
      "Epoch 24, Batch 436, Test Loss: 0.20648802816867828\n",
      "Epoch 24, Batch 437, Test Loss: 0.3178351819515228\n",
      "Epoch 24, Batch 438, Test Loss: 0.3568674623966217\n",
      "Epoch 24, Batch 439, Test Loss: 0.3245337903499603\n",
      "Epoch 24, Batch 440, Test Loss: 0.284700483083725\n",
      "Epoch 24, Batch 441, Test Loss: 0.5057650804519653\n",
      "Epoch 24, Batch 442, Test Loss: 0.6446037292480469\n",
      "Epoch 24, Batch 443, Test Loss: 0.2502155900001526\n",
      "Epoch 24, Batch 444, Test Loss: 0.3782944679260254\n",
      "Epoch 24, Batch 445, Test Loss: 0.3688291907310486\n",
      "Epoch 24, Batch 446, Test Loss: 0.30085375905036926\n",
      "Epoch 24, Batch 447, Test Loss: 0.21902601420879364\n",
      "Epoch 24, Batch 448, Test Loss: 0.5343979001045227\n",
      "Epoch 24, Batch 449, Test Loss: 0.3491464853286743\n",
      "Epoch 24, Batch 450, Test Loss: 0.39099907875061035\n",
      "Epoch 24, Batch 451, Test Loss: 0.35883888602256775\n",
      "Epoch 24, Batch 452, Test Loss: 0.49903905391693115\n",
      "Epoch 24, Batch 453, Test Loss: 0.3879769444465637\n",
      "Epoch 24, Batch 454, Test Loss: 0.27694210410118103\n",
      "Epoch 24, Batch 455, Test Loss: 0.33688056468963623\n",
      "Epoch 24, Batch 456, Test Loss: 0.39944344758987427\n",
      "Epoch 24, Batch 457, Test Loss: 0.36652296781539917\n",
      "Epoch 24, Batch 458, Test Loss: 0.3937957286834717\n",
      "Epoch 24, Batch 459, Test Loss: 0.4641614854335785\n",
      "Epoch 24, Batch 460, Test Loss: 0.5200312733650208\n",
      "Epoch 24, Batch 461, Test Loss: 0.4862174987792969\n",
      "Epoch 24, Batch 462, Test Loss: 0.3121826648712158\n",
      "Epoch 24, Batch 463, Test Loss: 0.6804835796356201\n",
      "Epoch 24, Batch 464, Test Loss: 0.4591881334781647\n",
      "Epoch 24, Batch 465, Test Loss: 0.23237884044647217\n",
      "Epoch 24, Batch 466, Test Loss: 0.3215528726577759\n",
      "Epoch 24, Batch 467, Test Loss: 0.30118876695632935\n",
      "Epoch 24, Batch 468, Test Loss: 0.2813881039619446\n",
      "Epoch 24, Batch 469, Test Loss: 0.4522067904472351\n",
      "Epoch 24, Batch 470, Test Loss: 0.2808510661125183\n",
      "Epoch 24, Batch 471, Test Loss: 0.21272781491279602\n",
      "Epoch 24, Batch 472, Test Loss: 0.28001734614372253\n",
      "Epoch 24, Batch 473, Test Loss: 0.20531758666038513\n",
      "Epoch 24, Batch 474, Test Loss: 0.26854971051216125\n",
      "Epoch 24, Batch 475, Test Loss: 0.3064046800136566\n",
      "Epoch 24, Batch 476, Test Loss: 0.32127392292022705\n",
      "Epoch 24, Batch 477, Test Loss: 0.21920117735862732\n",
      "Epoch 24, Batch 478, Test Loss: 0.45662179589271545\n",
      "Epoch 24, Batch 479, Test Loss: 0.5197316408157349\n",
      "Epoch 24, Batch 480, Test Loss: 0.3718714118003845\n",
      "Epoch 24, Batch 481, Test Loss: 0.257612943649292\n",
      "Epoch 24, Batch 482, Test Loss: 0.41006919741630554\n",
      "Epoch 24, Batch 483, Test Loss: 0.4020153880119324\n",
      "Epoch 24, Batch 484, Test Loss: 0.4157552123069763\n",
      "Epoch 24, Batch 485, Test Loss: 0.36449122428894043\n",
      "Epoch 24, Batch 486, Test Loss: 0.39266490936279297\n",
      "Epoch 24, Batch 487, Test Loss: 0.34262552857398987\n",
      "Epoch 24, Batch 488, Test Loss: 0.38094282150268555\n",
      "Epoch 24, Batch 489, Test Loss: 0.5641515851020813\n",
      "Epoch 24, Batch 490, Test Loss: 0.5179964303970337\n",
      "Epoch 24, Batch 491, Test Loss: 0.35277384519577026\n",
      "Epoch 24, Batch 492, Test Loss: 0.33842483162879944\n",
      "Epoch 24, Batch 493, Test Loss: 0.34550610184669495\n",
      "Epoch 24, Batch 494, Test Loss: 0.4310635030269623\n",
      "Epoch 24, Batch 495, Test Loss: 0.3926585614681244\n",
      "Epoch 24, Batch 496, Test Loss: 0.4350748062133789\n",
      "Epoch 24, Batch 497, Test Loss: 0.46792033314704895\n",
      "Epoch 24, Batch 498, Test Loss: 0.5513691902160645\n",
      "Epoch 24, Batch 499, Test Loss: 0.2798117697238922\n",
      "Epoch 24, Batch 500, Test Loss: 0.2860780358314514\n",
      "Epoch 24, Batch 501, Test Loss: 0.5678389072418213\n",
      "Epoch 24, Batch 502, Test Loss: 0.21883471310138702\n",
      "Epoch 24, Batch 503, Test Loss: 0.4031260907649994\n",
      "Epoch 24, Batch 504, Test Loss: 0.3807462453842163\n",
      "Epoch 24, Batch 505, Test Loss: 0.4198399484157562\n",
      "Epoch 24, Batch 506, Test Loss: 0.3222343325614929\n",
      "Epoch 24, Batch 507, Test Loss: 0.2872961163520813\n",
      "Epoch 24, Batch 508, Test Loss: 0.36293715238571167\n",
      "Epoch 24, Batch 509, Test Loss: 0.38486984372138977\n",
      "Epoch 24, Batch 510, Test Loss: 0.4405136704444885\n",
      "Epoch 24, Batch 511, Test Loss: 0.3653831481933594\n",
      "Epoch 24, Batch 512, Test Loss: 0.356573224067688\n",
      "Epoch 24, Batch 513, Test Loss: 0.38542646169662476\n",
      "Epoch 24, Batch 514, Test Loss: 0.3173302710056305\n",
      "Epoch 24, Batch 515, Test Loss: 0.326092392206192\n",
      "Epoch 24, Batch 516, Test Loss: 0.41906794905662537\n",
      "Epoch 24, Batch 517, Test Loss: 0.27300146222114563\n",
      "Epoch 24, Batch 518, Test Loss: 0.19843789935112\n",
      "Epoch 24, Batch 519, Test Loss: 0.5384963750839233\n",
      "Epoch 24, Batch 520, Test Loss: 0.2851618826389313\n",
      "Epoch 24, Batch 521, Test Loss: 0.287447988986969\n",
      "Epoch 24, Batch 522, Test Loss: 0.314599871635437\n",
      "Epoch 24, Batch 523, Test Loss: 0.3589174747467041\n",
      "Epoch 24, Batch 524, Test Loss: 0.6231746077537537\n",
      "Epoch 24, Batch 525, Test Loss: 0.3388034403324127\n",
      "Epoch 24, Batch 526, Test Loss: 0.32686132192611694\n",
      "Epoch 24, Batch 527, Test Loss: 0.5072425603866577\n",
      "Epoch 24, Batch 528, Test Loss: 0.2168109267950058\n",
      "Epoch 24, Batch 529, Test Loss: 0.46266114711761475\n",
      "Epoch 24, Batch 530, Test Loss: 0.3885868191719055\n",
      "Epoch 24, Batch 531, Test Loss: 0.20814809203147888\n",
      "Epoch 24, Batch 532, Test Loss: 0.2989336848258972\n",
      "Epoch 24, Batch 533, Test Loss: 0.3374355733394623\n",
      "Epoch 24, Batch 534, Test Loss: 0.37050750851631165\n",
      "Epoch 24, Batch 535, Test Loss: 0.3254673480987549\n",
      "Epoch 24, Batch 536, Test Loss: 0.4948122203350067\n",
      "Epoch 24, Batch 537, Test Loss: 0.42672964930534363\n",
      "Epoch 24, Batch 538, Test Loss: 0.3940282464027405\n",
      "Epoch 24, Batch 539, Test Loss: 0.34235531091690063\n",
      "Epoch 24, Batch 540, Test Loss: 0.47286033630371094\n",
      "Epoch 24, Batch 541, Test Loss: 0.24924451112747192\n",
      "Epoch 24, Batch 542, Test Loss: 0.47533825039863586\n",
      "Epoch 24, Batch 543, Test Loss: 0.36525413393974304\n",
      "Epoch 24, Batch 544, Test Loss: 0.32589131593704224\n",
      "Epoch 24, Batch 545, Test Loss: 0.28252020478248596\n",
      "Epoch 24, Batch 546, Test Loss: 0.4606006145477295\n",
      "Epoch 24, Batch 547, Test Loss: 0.31697845458984375\n",
      "Epoch 24, Batch 548, Test Loss: 0.4601037800312042\n",
      "Epoch 24, Batch 549, Test Loss: 0.37029150128364563\n",
      "Epoch 24, Batch 550, Test Loss: 0.4147473871707916\n",
      "Epoch 24, Batch 551, Test Loss: 0.4322369694709778\n",
      "Epoch 24, Batch 552, Test Loss: 0.5094618201255798\n",
      "Epoch 24, Batch 553, Test Loss: 0.46389666199684143\n",
      "Epoch 24, Batch 554, Test Loss: 0.24358142912387848\n",
      "Epoch 24, Batch 555, Test Loss: 0.5381078720092773\n",
      "Epoch 24, Batch 556, Test Loss: 0.35763421654701233\n",
      "Epoch 24, Batch 557, Test Loss: 0.34564346075057983\n",
      "Epoch 24, Batch 558, Test Loss: 0.1859186291694641\n",
      "Epoch 24, Batch 559, Test Loss: 0.3794609010219574\n",
      "Epoch 24, Batch 560, Test Loss: 0.36927530169487\n",
      "Epoch 24, Batch 561, Test Loss: 0.4789661765098572\n",
      "Epoch 24, Batch 562, Test Loss: 0.3327023684978485\n",
      "Epoch 24, Batch 563, Test Loss: 0.5278662443161011\n",
      "Epoch 24, Batch 564, Test Loss: 0.4644795060157776\n",
      "Epoch 24, Batch 565, Test Loss: 0.2558637261390686\n",
      "Epoch 24, Batch 566, Test Loss: 0.4265325665473938\n",
      "Epoch 24, Batch 567, Test Loss: 0.35118621587753296\n",
      "Epoch 24, Batch 568, Test Loss: 0.6330403089523315\n",
      "Epoch 24, Batch 569, Test Loss: 0.319012314081192\n",
      "Epoch 24, Batch 570, Test Loss: 0.5233173370361328\n",
      "Epoch 24, Batch 571, Test Loss: 0.37773722410202026\n",
      "Epoch 24, Batch 572, Test Loss: 0.5608083009719849\n",
      "Epoch 24, Batch 573, Test Loss: 0.2920595705509186\n",
      "Epoch 24, Batch 574, Test Loss: 0.4993797242641449\n",
      "Epoch 24, Batch 575, Test Loss: 0.2525794506072998\n",
      "Epoch 24, Batch 576, Test Loss: 0.3604455590248108\n",
      "Epoch 24, Batch 577, Test Loss: 0.4149687588214874\n",
      "Epoch 24, Batch 578, Test Loss: 0.5512097477912903\n",
      "Epoch 24, Batch 579, Test Loss: 0.3282129466533661\n",
      "Epoch 24, Batch 580, Test Loss: 0.4599686861038208\n",
      "Epoch 24, Batch 581, Test Loss: 0.3055317997932434\n",
      "Epoch 24, Batch 582, Test Loss: 0.5670855045318604\n",
      "Epoch 24, Batch 583, Test Loss: 0.5713813304901123\n",
      "Epoch 24, Batch 584, Test Loss: 0.29125645756721497\n",
      "Epoch 24, Batch 585, Test Loss: 0.3645153343677521\n",
      "Epoch 24, Batch 586, Test Loss: 0.4264507591724396\n",
      "Epoch 24, Batch 587, Test Loss: 0.5017688274383545\n",
      "Epoch 24, Batch 588, Test Loss: 0.346823513507843\n",
      "Epoch 24, Batch 589, Test Loss: 0.6092888116836548\n",
      "Epoch 24, Batch 590, Test Loss: 0.41390326619148254\n",
      "Epoch 24, Batch 591, Test Loss: 0.44202449917793274\n",
      "Epoch 24, Batch 592, Test Loss: 0.45643651485443115\n",
      "Epoch 24, Batch 593, Test Loss: 0.37627673149108887\n",
      "Epoch 24, Batch 594, Test Loss: 0.4602369964122772\n",
      "Epoch 24, Batch 595, Test Loss: 0.3006003797054291\n",
      "Epoch 24, Batch 596, Test Loss: 0.2954656481742859\n",
      "Epoch 24, Batch 597, Test Loss: 0.4294959306716919\n",
      "Epoch 24, Batch 598, Test Loss: 0.45026499032974243\n",
      "Epoch 24, Batch 599, Test Loss: 0.40064695477485657\n",
      "Epoch 24, Batch 600, Test Loss: 0.313501238822937\n",
      "Epoch 24, Batch 601, Test Loss: 0.3868657052516937\n",
      "Epoch 24, Batch 602, Test Loss: 0.2650896906852722\n",
      "Epoch 24, Batch 603, Test Loss: 0.5745405554771423\n",
      "Epoch 24, Batch 604, Test Loss: 0.2924317717552185\n",
      "Epoch 24, Batch 605, Test Loss: 0.3543120324611664\n",
      "Epoch 24, Batch 606, Test Loss: 0.2968340516090393\n",
      "Epoch 24, Batch 607, Test Loss: 0.4589041471481323\n",
      "Epoch 24, Batch 608, Test Loss: 0.43505197763442993\n",
      "Epoch 24, Batch 609, Test Loss: 0.29807916283607483\n",
      "Epoch 24, Batch 610, Test Loss: 0.19975456595420837\n",
      "Epoch 24, Batch 611, Test Loss: 0.35235413908958435\n",
      "Epoch 24, Batch 612, Test Loss: 0.3675151467323303\n",
      "Epoch 24, Batch 613, Test Loss: 0.37155333161354065\n",
      "Epoch 24, Batch 614, Test Loss: 0.39532989263534546\n",
      "Epoch 24, Batch 615, Test Loss: 0.3544576168060303\n",
      "Epoch 24, Batch 616, Test Loss: 0.3971978724002838\n",
      "Epoch 24, Batch 617, Test Loss: 0.40745750069618225\n",
      "Epoch 24, Batch 618, Test Loss: 0.20253227651119232\n",
      "Epoch 24, Batch 619, Test Loss: 0.3272852301597595\n",
      "Epoch 24, Batch 620, Test Loss: 0.36727622151374817\n",
      "Epoch 24, Batch 621, Test Loss: 0.28660374879837036\n",
      "Epoch 24, Batch 622, Test Loss: 0.3732844591140747\n",
      "Epoch 24, Batch 623, Test Loss: 0.3061593472957611\n",
      "Epoch 24, Batch 624, Test Loss: 0.2866557538509369\n",
      "Epoch 24, Batch 625, Test Loss: 0.28358176350593567\n",
      "Epoch 24, Batch 626, Test Loss: 0.3509121537208557\n",
      "Epoch 24, Batch 627, Test Loss: 0.3921476900577545\n",
      "Epoch 24, Batch 628, Test Loss: 0.38623207807540894\n",
      "Epoch 24, Batch 629, Test Loss: 0.24585823714733124\n",
      "Epoch 24, Batch 630, Test Loss: 0.4226172864437103\n",
      "Epoch 24, Batch 631, Test Loss: 0.4122265577316284\n",
      "Epoch 24, Batch 632, Test Loss: 0.3244428336620331\n",
      "Epoch 24, Batch 633, Test Loss: 0.2659977674484253\n",
      "Epoch 24, Batch 634, Test Loss: 0.5878559350967407\n",
      "Epoch 24, Batch 635, Test Loss: 0.27275821566581726\n",
      "Epoch 24, Batch 636, Test Loss: 0.35926130414009094\n",
      "Epoch 24, Batch 637, Test Loss: 0.43738746643066406\n",
      "Epoch 24, Batch 638, Test Loss: 0.35402747988700867\n",
      "Epoch 24, Batch 639, Test Loss: 0.2820987105369568\n",
      "Epoch 24, Batch 640, Test Loss: 0.4722556471824646\n",
      "Epoch 24, Batch 641, Test Loss: 0.5143157839775085\n",
      "Epoch 24, Batch 642, Test Loss: 0.29153093695640564\n",
      "Epoch 24, Batch 643, Test Loss: 0.5062281489372253\n",
      "Epoch 24, Batch 644, Test Loss: 0.4704294800758362\n",
      "Epoch 24, Batch 645, Test Loss: 0.4275003671646118\n",
      "Epoch 24, Batch 646, Test Loss: 0.4563983976840973\n",
      "Epoch 24, Batch 647, Test Loss: 0.37946000695228577\n",
      "Epoch 24, Batch 648, Test Loss: 0.3519091010093689\n",
      "Epoch 24, Batch 649, Test Loss: 0.33315709233283997\n",
      "Epoch 24, Batch 650, Test Loss: 0.304892897605896\n",
      "Epoch 24, Batch 651, Test Loss: 0.5517894625663757\n",
      "Epoch 24, Batch 652, Test Loss: 0.4763922393321991\n",
      "Epoch 24, Batch 653, Test Loss: 0.4675201177597046\n",
      "Epoch 24, Batch 654, Test Loss: 0.30380749702453613\n",
      "Epoch 24, Batch 655, Test Loss: 0.25779423117637634\n",
      "Epoch 24, Batch 656, Test Loss: 0.5735051035881042\n",
      "Epoch 24, Batch 657, Test Loss: 0.38750606775283813\n",
      "Epoch 24, Batch 658, Test Loss: 0.42709097266197205\n",
      "Epoch 24, Batch 659, Test Loss: 0.40292614698410034\n",
      "Epoch 24, Batch 660, Test Loss: 0.34219130873680115\n",
      "Epoch 24, Batch 661, Test Loss: 0.2924538254737854\n",
      "Epoch 24, Batch 662, Test Loss: 0.441741943359375\n",
      "Epoch 24, Batch 663, Test Loss: 0.2170143723487854\n",
      "Epoch 24, Batch 664, Test Loss: 0.1872290074825287\n",
      "Epoch 24, Batch 665, Test Loss: 0.37048378586769104\n",
      "Epoch 24, Batch 666, Test Loss: 0.38735711574554443\n",
      "Epoch 24, Batch 667, Test Loss: 0.4535352885723114\n",
      "Epoch 24, Batch 668, Test Loss: 0.5348312854766846\n",
      "Epoch 24, Batch 669, Test Loss: 0.2785528302192688\n",
      "Epoch 24, Batch 670, Test Loss: 0.3174203336238861\n",
      "Epoch 24, Batch 671, Test Loss: 0.27756524085998535\n",
      "Epoch 24, Batch 672, Test Loss: 0.5955886840820312\n",
      "Epoch 24, Batch 673, Test Loss: 0.3470418155193329\n",
      "Epoch 24, Batch 674, Test Loss: 0.34444206953048706\n",
      "Epoch 24, Batch 675, Test Loss: 0.4660496115684509\n",
      "Epoch 24, Batch 676, Test Loss: 0.4615047574043274\n",
      "Epoch 24, Batch 677, Test Loss: 0.2454255223274231\n",
      "Epoch 24, Batch 678, Test Loss: 0.1486642062664032\n",
      "Epoch 24, Batch 679, Test Loss: 0.27540767192840576\n",
      "Epoch 24, Batch 680, Test Loss: 0.39442384243011475\n",
      "Epoch 24, Batch 681, Test Loss: 0.5711827278137207\n",
      "Epoch 24, Batch 682, Test Loss: 0.26314693689346313\n",
      "Epoch 24, Batch 683, Test Loss: 0.45587214827537537\n",
      "Epoch 24, Batch 684, Test Loss: 0.34443339705467224\n",
      "Epoch 24, Batch 685, Test Loss: 0.35745808482170105\n",
      "Epoch 24, Batch 686, Test Loss: 0.570618748664856\n",
      "Epoch 24, Batch 687, Test Loss: 0.4426754117012024\n",
      "Epoch 24, Batch 688, Test Loss: 0.49231216311454773\n",
      "Epoch 24, Batch 689, Test Loss: 0.312472403049469\n",
      "Epoch 24, Batch 690, Test Loss: 0.4565085172653198\n",
      "Epoch 24, Batch 691, Test Loss: 0.3299187421798706\n",
      "Epoch 24, Batch 692, Test Loss: 0.2836311459541321\n",
      "Epoch 24, Batch 693, Test Loss: 0.4099968671798706\n",
      "Epoch 24, Batch 694, Test Loss: 0.4987976551055908\n",
      "Epoch 24, Batch 695, Test Loss: 0.38470274209976196\n",
      "Epoch 24, Batch 696, Test Loss: 0.500242292881012\n",
      "Epoch 24, Batch 697, Test Loss: 0.18292324244976044\n",
      "Epoch 24, Batch 698, Test Loss: 0.41957786679267883\n",
      "Epoch 24, Batch 699, Test Loss: 0.4890344738960266\n",
      "Epoch 24, Batch 700, Test Loss: 0.2954536974430084\n",
      "Epoch 24, Batch 701, Test Loss: 0.34505462646484375\n",
      "Epoch 24, Batch 702, Test Loss: 0.30094408988952637\n",
      "Epoch 24, Batch 703, Test Loss: 0.33317646384239197\n",
      "Epoch 24, Batch 704, Test Loss: 0.4034718871116638\n",
      "Epoch 24, Batch 705, Test Loss: 0.36243486404418945\n",
      "Epoch 24, Batch 706, Test Loss: 0.2072184532880783\n",
      "Epoch 24, Batch 707, Test Loss: 0.283145934343338\n",
      "Epoch 24, Batch 708, Test Loss: 0.27955251932144165\n",
      "Epoch 24, Batch 709, Test Loss: 0.4184723496437073\n",
      "Epoch 24, Batch 710, Test Loss: 0.3978361189365387\n",
      "Epoch 24, Batch 711, Test Loss: 0.2529991865158081\n",
      "Epoch 24, Batch 712, Test Loss: 0.42614004015922546\n",
      "Epoch 24, Batch 713, Test Loss: 0.42611002922058105\n",
      "Epoch 24, Batch 714, Test Loss: 0.43032100796699524\n",
      "Epoch 24, Batch 715, Test Loss: 0.3117412030696869\n",
      "Epoch 24, Batch 716, Test Loss: 0.442727655172348\n",
      "Epoch 24, Batch 717, Test Loss: 0.2720980942249298\n",
      "Epoch 24, Batch 718, Test Loss: 0.34443747997283936\n",
      "Epoch 24, Batch 719, Test Loss: 0.454345166683197\n",
      "Epoch 24, Batch 720, Test Loss: 0.48253780603408813\n",
      "Epoch 24, Batch 721, Test Loss: 0.416005402803421\n",
      "Epoch 24, Batch 722, Test Loss: 0.4642293453216553\n",
      "Epoch 24, Batch 723, Test Loss: 0.43008509278297424\n",
      "Epoch 24, Batch 724, Test Loss: 0.2728378474712372\n",
      "Epoch 24, Batch 725, Test Loss: 0.3917195796966553\n",
      "Epoch 24, Batch 726, Test Loss: 0.3402857780456543\n",
      "Epoch 24, Batch 727, Test Loss: 0.39031994342803955\n",
      "Epoch 24, Batch 728, Test Loss: 0.3935278058052063\n",
      "Epoch 24, Batch 729, Test Loss: 0.4601372480392456\n",
      "Epoch 24, Batch 730, Test Loss: 0.2670110762119293\n",
      "Epoch 24, Batch 731, Test Loss: 0.4271695017814636\n",
      "Epoch 24, Batch 732, Test Loss: 0.32443904876708984\n",
      "Epoch 24, Batch 733, Test Loss: 0.4022156000137329\n",
      "Epoch 24, Batch 734, Test Loss: 0.5542733669281006\n",
      "Epoch 24, Batch 735, Test Loss: 0.3708702623844147\n",
      "Epoch 24, Batch 736, Test Loss: 0.483431339263916\n",
      "Epoch 24, Batch 737, Test Loss: 0.6628410220146179\n",
      "Epoch 24, Batch 738, Test Loss: 0.3138563930988312\n",
      "Epoch 24, Batch 739, Test Loss: 0.2955465614795685\n",
      "Epoch 24, Batch 740, Test Loss: 0.3640245199203491\n",
      "Epoch 24, Batch 741, Test Loss: 0.6800365447998047\n",
      "Epoch 24, Batch 742, Test Loss: 0.539303183555603\n",
      "Epoch 24, Batch 743, Test Loss: 0.27226847410202026\n",
      "Epoch 24, Batch 744, Test Loss: 0.4771665036678314\n",
      "Epoch 24, Batch 745, Test Loss: 0.5713433623313904\n",
      "Epoch 24, Batch 746, Test Loss: 0.40465235710144043\n",
      "Epoch 24, Batch 747, Test Loss: 0.4937594532966614\n",
      "Epoch 24, Batch 748, Test Loss: 0.5283519625663757\n",
      "Epoch 24, Batch 749, Test Loss: 0.4527912735939026\n",
      "Epoch 24, Batch 750, Test Loss: 0.4577257037162781\n",
      "Epoch 24, Batch 751, Test Loss: 0.3093813955783844\n",
      "Epoch 24, Batch 752, Test Loss: 0.3029825985431671\n",
      "Epoch 24, Batch 753, Test Loss: 0.391364723443985\n",
      "Epoch 24, Batch 754, Test Loss: 0.3627215623855591\n",
      "Epoch 24, Batch 755, Test Loss: 0.3845948874950409\n",
      "Epoch 24, Batch 756, Test Loss: 0.4385421872138977\n",
      "Epoch 24, Batch 757, Test Loss: 0.29564550518989563\n",
      "Epoch 24, Batch 758, Test Loss: 0.5573430061340332\n",
      "Epoch 24, Batch 759, Test Loss: 0.33061274886131287\n",
      "Epoch 24, Batch 760, Test Loss: 0.4292883574962616\n",
      "Epoch 24, Batch 761, Test Loss: 0.26096710562705994\n",
      "Epoch 24, Batch 762, Test Loss: 0.4548008143901825\n",
      "Epoch 24, Batch 763, Test Loss: 0.277761310338974\n",
      "Epoch 24, Batch 764, Test Loss: 0.41409820318222046\n",
      "Epoch 24, Batch 765, Test Loss: 0.3295878767967224\n",
      "Epoch 24, Batch 766, Test Loss: 0.3646267354488373\n",
      "Epoch 24, Batch 767, Test Loss: 0.33543673157691956\n",
      "Epoch 24, Batch 768, Test Loss: 0.41481250524520874\n",
      "Epoch 24, Batch 769, Test Loss: 0.4826834201812744\n",
      "Epoch 24, Batch 770, Test Loss: 0.4252457618713379\n",
      "Epoch 24, Batch 771, Test Loss: 0.4854775667190552\n",
      "Epoch 24, Batch 772, Test Loss: 0.21775208413600922\n",
      "Epoch 24, Batch 773, Test Loss: 0.1789999008178711\n",
      "Epoch 24, Batch 774, Test Loss: 0.4407997727394104\n",
      "Epoch 24, Batch 775, Test Loss: 0.424609899520874\n",
      "Epoch 24, Batch 776, Test Loss: 0.2875671982765198\n",
      "Epoch 24, Batch 777, Test Loss: 0.35762280225753784\n",
      "Epoch 24, Batch 778, Test Loss: 0.6396357417106628\n",
      "Epoch 24, Batch 779, Test Loss: 0.30393186211586\n",
      "Epoch 24, Batch 780, Test Loss: 0.23353125154972076\n",
      "Epoch 24, Batch 781, Test Loss: 0.39051079750061035\n",
      "Epoch 24, Batch 782, Test Loss: 0.38736942410469055\n",
      "Epoch 24, Batch 783, Test Loss: 0.3367394208908081\n",
      "Epoch 24, Batch 784, Test Loss: 0.519237220287323\n",
      "Epoch 24, Batch 785, Test Loss: 0.23509658873081207\n",
      "Epoch 24, Batch 786, Test Loss: 0.3043004274368286\n",
      "Epoch 24, Batch 787, Test Loss: 0.49816665053367615\n",
      "Epoch 24, Batch 788, Test Loss: 0.3373993933200836\n",
      "Epoch 24, Batch 789, Test Loss: 0.42276161909103394\n",
      "Epoch 24, Batch 790, Test Loss: 0.28172460198402405\n",
      "Epoch 24, Batch 791, Test Loss: 0.33385464549064636\n",
      "Epoch 24, Batch 792, Test Loss: 0.34762558341026306\n",
      "Epoch 24, Batch 793, Test Loss: 0.3513493537902832\n",
      "Epoch 24, Batch 794, Test Loss: 0.23563936352729797\n",
      "Epoch 24, Batch 795, Test Loss: 0.49747174978256226\n",
      "Epoch 24, Batch 796, Test Loss: 0.3957557678222656\n",
      "Epoch 24, Batch 797, Test Loss: 0.2579317092895508\n",
      "Epoch 24, Batch 798, Test Loss: 0.29408618807792664\n",
      "Epoch 24, Batch 799, Test Loss: 0.6548753976821899\n",
      "Epoch 24, Batch 800, Test Loss: 0.2520379424095154\n",
      "Epoch 24, Batch 801, Test Loss: 0.5684639811515808\n",
      "Epoch 24, Batch 802, Test Loss: 0.5226892232894897\n",
      "Epoch 24, Batch 803, Test Loss: 0.3101192116737366\n",
      "Epoch 24, Batch 804, Test Loss: 0.3703472912311554\n",
      "Epoch 24, Batch 805, Test Loss: 0.45886167883872986\n",
      "Epoch 24, Batch 806, Test Loss: 0.3622787296772003\n",
      "Epoch 24, Batch 807, Test Loss: 0.31423819065093994\n",
      "Epoch 24, Batch 808, Test Loss: 0.2753555178642273\n",
      "Epoch 24, Batch 809, Test Loss: 0.3646557331085205\n",
      "Epoch 24, Batch 810, Test Loss: 0.6029183864593506\n",
      "Epoch 24, Batch 811, Test Loss: 0.5289131999015808\n",
      "Epoch 24, Batch 812, Test Loss: 0.3135530650615692\n",
      "Epoch 24, Batch 813, Test Loss: 0.3858410716056824\n",
      "Epoch 24, Batch 814, Test Loss: 0.49676644802093506\n",
      "Epoch 24, Batch 815, Test Loss: 0.4846012592315674\n",
      "Epoch 24, Batch 816, Test Loss: 0.3962497115135193\n",
      "Epoch 24, Batch 817, Test Loss: 0.3555338978767395\n",
      "Epoch 24, Batch 818, Test Loss: 0.3111453950405121\n",
      "Epoch 24, Batch 819, Test Loss: 0.23058286309242249\n",
      "Epoch 24, Batch 820, Test Loss: 0.29542967677116394\n",
      "Epoch 24, Batch 821, Test Loss: 0.19777998328208923\n",
      "Epoch 24, Batch 822, Test Loss: 0.376006543636322\n",
      "Epoch 24, Batch 823, Test Loss: 0.560700535774231\n",
      "Epoch 24, Batch 824, Test Loss: 0.22547826170921326\n",
      "Epoch 24, Batch 825, Test Loss: 0.45562729239463806\n",
      "Epoch 24, Batch 826, Test Loss: 0.2753711938858032\n",
      "Epoch 24, Batch 827, Test Loss: 0.4622739255428314\n",
      "Epoch 24, Batch 828, Test Loss: 0.2978323996067047\n",
      "Epoch 24, Batch 829, Test Loss: 0.39303407073020935\n",
      "Epoch 24, Batch 830, Test Loss: 0.34149792790412903\n",
      "Epoch 24, Batch 831, Test Loss: 0.32595330476760864\n",
      "Epoch 24, Batch 832, Test Loss: 0.31355908513069153\n",
      "Epoch 24, Batch 833, Test Loss: 0.5306493639945984\n",
      "Epoch 24, Batch 834, Test Loss: 0.4562299847602844\n",
      "Epoch 24, Batch 835, Test Loss: 0.3220739960670471\n",
      "Epoch 24, Batch 836, Test Loss: 0.34653252363204956\n",
      "Epoch 24, Batch 837, Test Loss: 0.3847247362136841\n",
      "Epoch 24, Batch 838, Test Loss: 0.38300657272338867\n",
      "Epoch 24, Batch 839, Test Loss: 0.409701943397522\n",
      "Epoch 24, Batch 840, Test Loss: 0.2998669147491455\n",
      "Epoch 24, Batch 841, Test Loss: 0.36646008491516113\n",
      "Epoch 24, Batch 842, Test Loss: 0.39728906750679016\n",
      "Epoch 24, Batch 843, Test Loss: 0.3959878385066986\n",
      "Epoch 24, Batch 844, Test Loss: 0.3770482540130615\n",
      "Epoch 24, Batch 845, Test Loss: 0.42667722702026367\n",
      "Epoch 24, Batch 846, Test Loss: 0.3115403652191162\n",
      "Epoch 24, Batch 847, Test Loss: 0.29906708002090454\n",
      "Epoch 24, Batch 848, Test Loss: 0.4572976529598236\n",
      "Epoch 24, Batch 849, Test Loss: 0.49701035022735596\n",
      "Epoch 24, Batch 850, Test Loss: 0.336799293756485\n",
      "Epoch 24, Batch 851, Test Loss: 0.26076775789260864\n",
      "Epoch 24, Batch 852, Test Loss: 0.3345893621444702\n",
      "Epoch 24, Batch 853, Test Loss: 0.21133580803871155\n",
      "Epoch 24, Batch 854, Test Loss: 0.3653581738471985\n",
      "Epoch 24, Batch 855, Test Loss: 0.3722846508026123\n",
      "Epoch 24, Batch 856, Test Loss: 0.41409581899642944\n",
      "Epoch 24, Batch 857, Test Loss: 0.43897145986557007\n",
      "Epoch 24, Batch 858, Test Loss: 0.43557944893836975\n",
      "Epoch 24, Batch 859, Test Loss: 0.3839857280254364\n",
      "Epoch 24, Batch 860, Test Loss: 0.16630753874778748\n",
      "Epoch 24, Batch 861, Test Loss: 0.36655113101005554\n",
      "Epoch 24, Batch 862, Test Loss: 0.2993887960910797\n",
      "Epoch 24, Batch 863, Test Loss: 0.35057052969932556\n",
      "Epoch 24, Batch 864, Test Loss: 0.4634894132614136\n",
      "Epoch 24, Batch 865, Test Loss: 0.3504049479961395\n",
      "Epoch 24, Batch 866, Test Loss: 0.33456000685691833\n",
      "Epoch 24, Batch 867, Test Loss: 0.3014543652534485\n",
      "Epoch 24, Batch 868, Test Loss: 0.5927238464355469\n",
      "Epoch 24, Batch 869, Test Loss: 0.3348192870616913\n",
      "Epoch 24, Batch 870, Test Loss: 0.2823431193828583\n",
      "Epoch 24, Batch 871, Test Loss: 0.3296266198158264\n",
      "Epoch 24, Batch 872, Test Loss: 0.34289050102233887\n",
      "Epoch 24, Batch 873, Test Loss: 0.39605480432510376\n",
      "Epoch 24, Batch 874, Test Loss: 0.354941725730896\n",
      "Epoch 24, Batch 875, Test Loss: 0.3827558159828186\n",
      "Epoch 24, Batch 876, Test Loss: 0.3613339066505432\n",
      "Epoch 24, Batch 877, Test Loss: 0.3411090672016144\n",
      "Epoch 24, Batch 878, Test Loss: 0.38539648056030273\n",
      "Epoch 24, Batch 879, Test Loss: 0.40707433223724365\n",
      "Epoch 24, Batch 880, Test Loss: 0.4603109657764435\n",
      "Epoch 24, Batch 881, Test Loss: 0.34281235933303833\n",
      "Epoch 24, Batch 882, Test Loss: 0.3721427321434021\n",
      "Epoch 24, Batch 883, Test Loss: 0.2683151066303253\n",
      "Epoch 24, Batch 884, Test Loss: 0.27257952094078064\n",
      "Epoch 24, Batch 885, Test Loss: 0.37812820076942444\n",
      "Epoch 24, Batch 886, Test Loss: 0.20844894647598267\n",
      "Epoch 24, Batch 887, Test Loss: 0.390982985496521\n",
      "Epoch 24, Batch 888, Test Loss: 0.5879730582237244\n",
      "Epoch 24, Batch 889, Test Loss: 0.3619224429130554\n",
      "Epoch 24, Batch 890, Test Loss: 0.2489667534828186\n",
      "Epoch 24, Batch 891, Test Loss: 0.5348942279815674\n",
      "Epoch 24, Batch 892, Test Loss: 0.2785133421421051\n",
      "Epoch 24, Batch 893, Test Loss: 0.3111458420753479\n",
      "Epoch 24, Batch 894, Test Loss: 0.2637650966644287\n",
      "Epoch 24, Batch 895, Test Loss: 0.43867772817611694\n",
      "Epoch 24, Batch 896, Test Loss: 0.4532306492328644\n",
      "Epoch 24, Batch 897, Test Loss: 0.4828842878341675\n",
      "Epoch 24, Batch 898, Test Loss: 0.3704765737056732\n",
      "Epoch 24, Batch 899, Test Loss: 0.42688533663749695\n",
      "Epoch 24, Batch 900, Test Loss: 0.4494217336177826\n",
      "Epoch 24, Batch 901, Test Loss: 0.31240949034690857\n",
      "Epoch 24, Batch 902, Test Loss: 0.7314214706420898\n",
      "Epoch 24, Batch 903, Test Loss: 0.48283299803733826\n",
      "Epoch 24, Batch 904, Test Loss: 0.3154568374156952\n",
      "Epoch 24, Batch 905, Test Loss: 0.3433714210987091\n",
      "Epoch 24, Batch 906, Test Loss: 0.3231876790523529\n",
      "Epoch 24, Batch 907, Test Loss: 0.3194698691368103\n",
      "Epoch 24, Batch 908, Test Loss: 0.3027014434337616\n",
      "Epoch 24, Batch 909, Test Loss: 0.285992830991745\n",
      "Epoch 24, Batch 910, Test Loss: 0.3547947108745575\n",
      "Epoch 24, Batch 911, Test Loss: 0.40544775128364563\n",
      "Epoch 24, Batch 912, Test Loss: 0.24844230711460114\n",
      "Epoch 24, Batch 913, Test Loss: 0.3380662500858307\n",
      "Epoch 24, Batch 914, Test Loss: 0.43976569175720215\n",
      "Epoch 24, Batch 915, Test Loss: 0.47680243849754333\n",
      "Epoch 24, Batch 916, Test Loss: 0.26504993438720703\n",
      "Epoch 24, Batch 917, Test Loss: 0.3749277591705322\n",
      "Epoch 24, Batch 918, Test Loss: 0.34827321767807007\n",
      "Epoch 24, Batch 919, Test Loss: 0.3603660464286804\n",
      "Epoch 24, Batch 920, Test Loss: 0.39172685146331787\n",
      "Epoch 24, Batch 921, Test Loss: 0.28532999753952026\n",
      "Epoch 24, Batch 922, Test Loss: 0.3496842384338379\n",
      "Epoch 24, Batch 923, Test Loss: 0.28241053223609924\n",
      "Epoch 24, Batch 924, Test Loss: 0.40459421277046204\n",
      "Epoch 24, Batch 925, Test Loss: 0.4111498296260834\n",
      "Epoch 24, Batch 926, Test Loss: 0.5175464749336243\n",
      "Epoch 24, Batch 927, Test Loss: 0.4584168493747711\n",
      "Epoch 24, Batch 928, Test Loss: 0.2886923551559448\n",
      "Epoch 24, Batch 929, Test Loss: 0.4109039902687073\n",
      "Epoch 24, Batch 930, Test Loss: 0.34146440029144287\n",
      "Epoch 24, Batch 931, Test Loss: 0.2619655728340149\n",
      "Epoch 24, Batch 932, Test Loss: 0.3722694516181946\n",
      "Epoch 24, Batch 933, Test Loss: 0.43149468302726746\n",
      "Epoch 24, Batch 934, Test Loss: 0.38322922587394714\n",
      "Epoch 24, Batch 935, Test Loss: 0.310659259557724\n",
      "Epoch 24, Batch 936, Test Loss: 0.34682226181030273\n",
      "Epoch 24, Batch 937, Test Loss: 0.37248894572257996\n",
      "Epoch 24, Batch 938, Test Loss: 0.22037966549396515\n",
      "Accuracy of Test set: 0.8624833333333334\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model([train_loader, test_loader], num_epochs=25, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ceb5783f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy: 0.8624833333333334\n"
     ]
    }
   ],
   "source": [
    "print(f'Final test accuracy: {test_accuracies[-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e128ed",
   "metadata": {},
   "source": [
    "## Visualization of the labels and predictions\n",
    "\n",
    "In this section, you should visual one image from each class and show both the actual label and the predicted label for that image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2181d059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the simple neural network model parameters\n",
    "def initialize_parameters(input_size, hidden_size, output_size):\n",
    "    params = {\n",
    "        'W1': np.random.randn(hidden_size, input_size),\n",
    "        'b1': np.zeros((hidden_size, 1)),\n",
    "        'W2': np.random.randn(output_size, hidden_size),\n",
    "        'b2': np.zeros((output_size, 1))\n",
    "    }\n",
    "    return params\n",
    "\n",
    "# Forward pass through the model\n",
    "def model(x, params):\n",
    "    z1 = np.dot(params['W1'], x) + params['b1']\n",
    "    a1 = np.maximum(0, z1)  # ReLU activation\n",
    "    z2 = np.dot(params['W2'], a1) + params['b2']\n",
    "    return z2\n",
    "\n",
    "# Define predict function\n",
    "def predict(x, params):\n",
    "    output = model(x, params)\n",
    "    predicted_classes = np.argmax(output, axis=0)\n",
    "    return predicted_classes\n",
    "\n",
    "# Visualize one image from each class with actual and predicted labels\n",
    "def visualize_predictions(parameters, test_loader, num_classes):\n",
    "    class_indices = {i: [] for i in range(num_classes)}\n",
    "\n",
    "    # Collect indices for each class\n",
    "    for idx, (image, label) in enumerate(test_loader.dataset):\n",
    "        if len(class_indices[label]) < 1:\n",
    "            class_indices[label].append(idx)\n",
    "        if all(len(class_indices[c]) > 0 for c in range(num_classes)):\n",
    "            break\n",
    "\n",
    "    fig, axs = plt.subplots(1, num_classes, figsize=(20, 20))\n",
    "\n",
    "    for class_label, indices in class_indices.items():\n",
    "        idx = indices[0]\n",
    "        image, label = test_loader.dataset[idx]\n",
    "\n",
    "        # Assuming image is a numpy array\n",
    "        image = image.reshape(-1, 1)  # Reshape image\n",
    "        predicted_label = predict(image, parameters)[0]\n",
    "\n",
    "        axs[class_label].imshow(image.reshape(28, 28), cmap='gray')  # Assuming image size is 28x28\n",
    "        axs[class_label].set_title(f'Actual: {label},\\nPredicted: {predicted_label}', fontsize=12)\n",
    "        axs[class_label].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions\n",
    "# Assuming parameters, test_loader, and num_classes are defined elsewhere\n",
    "initialize_parameters(28*28, 128, num_classes)  # Initialize network parameters\n",
    "visualize_predictions(parameters, test_loader, num_classes)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
